{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.metrics import f1_score,accuracy_score,precision_score,recall_score\n",
    "import scipy.signal as signal\n",
    "from scipy.signal import butter, lfilter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.alexnet import alexnet\n",
    "from time import time\n",
    "seed=2020\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3807303354413779 0.6192696645586221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./seg\\\\a01', './seg\\\\a02', './seg\\\\a03', './seg\\\\a04', './seg\\\\a05']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reclist=glob(\"./seg/*\")\n",
    "dtlist=glob(\"./seg/*/*.npy*\")\n",
    "reclist.sort()\n",
    "dtlist.sort()\n",
    "lab_list=[seg.split('_')[0][-1] for seg in dtlist]#shuffle by idx\n",
    "print(lab_list.count('A')/len(lab_list),lab_list.count('N')/len(lab_list))\n",
    "\n",
    "#reclist=[rec for rec in reclist if rec[-3]!='b'] #Remove Borderline\n",
    "reclist[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cheb_bandpass_filter(data, lowcut, highcut, signal_freq, filter_order):\n",
    "        \"\"\"\n",
    "        Method responsible for creating and applying Butterworth filter.\n",
    "        :param deque data: raw data\n",
    "        :param float lowcut: filter lowcut frequency value\n",
    "        :param float highcut: filter highcut frequency value\n",
    "        :param int signal_freq: signal frequency in samples per second (Hz)\n",
    "        :param int filter_order: filter order\n",
    "        :return array: filtered data\n",
    "        \"\"\"\n",
    "        nyquist_freq = 0.5 * signal_freq\n",
    "        low = lowcut / nyquist_freq\n",
    "        high = highcut / nyquist_freq\n",
    "        #b, a = butter(filter_order, [low, high], btype=\"band\")\n",
    "        b, a = signal.cheby2(filter_order, 40, [low, high], 'band', analog=False)\n",
    "        y = lfilter(b, a, data)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def getTrainTestList(reclist,opt='rec_cv',fold=1):\n",
    "    train_dtlist=[]\n",
    "    test_dtlist=[]\n",
    "    if opt=='rec_cv':#reclist is rec list\n",
    "        kf = KFold(n_splits=10)\n",
    "        kf_idx=kf.split(reclist)\n",
    "        for fold_idx in range(fold):#1,2,3,4,5\n",
    "            train_idx,test_idx=kf_idx.__next__()\n",
    "        train_rec,test_rec=list(np.array(reclist)[train_idx]),list(np.array(reclist)[test_idx])\n",
    "        for rec in train_rec:\n",
    "            train_dtlist+=glob(rec+'/*.npy*')\n",
    "        for rec in test_rec:\n",
    "            test_dtlist+=glob(rec+'/*.npy*')\n",
    "    elif opt=='physionet':\n",
    "        train_rec=[rec for rec in reclist if rec[-3]!='x']\n",
    "        test_rec=[rec for rec in reclist if rec[-3]=='x']\n",
    "        print(len(train_rec),len(test_rec))\n",
    "        for rec in train_rec:\n",
    "            train_dtlist+=glob(rec+'/*.npy*')\n",
    "        for rec in test_rec:\n",
    "            test_dtlist+=glob(rec+'/*.npy*')\n",
    "    elif opt=='physionet_train':\n",
    "        train_rec=[rec for rec in reclist if rec[-3]!='x']\n",
    "        #print(len(train_rec),len(test_rec))\n",
    "        kf_idx=kf.split(dtlist)\n",
    "        for fold_idx in range(fold):#1,2,3,4,5\n",
    "            train_idx,test_idx=kf_idx.__next__()\n",
    "        train_dtlist,test_dtlist=list(np.array(dtlist)[train_idx]),list(np.array(dtlist)[test_idx])\n",
    "    elif opt=='blind':\n",
    "        subjects=[['a11'],\n",
    "                  ['a15','x27','x28'],\n",
    "                  ['a17','x12'],\n",
    "                  ['b01','x03'],\n",
    "                  ['c07','x34'],\n",
    "                  ['a11','a15','x27','x28','a17','x12','b01','x03','c07','x34'],\n",
    "                  ['a14','a19','x05','x08','x25','b05','x11','c01','x35','c07','x34'],\n",
    "                  ['a04','a19','x05','x08','x25','b05','x11','c01','x35','c09'],\n",
    "                  ['b02','b03','x16','x21']]\n",
    "        train_rec=[rec for rec in reclist if rec.split('/')[-1] not in subjects[fold-1]]\n",
    "        test_rec=[rec for rec in reclist if rec.split('/')[-1] in subjects[fold-1]]\n",
    "        print(len(train_rec),len(test_rec),test_rec)\n",
    "        for rec in train_rec:\n",
    "            train_dtlist+=glob(rec+'/*.npy*')\n",
    "        for rec in test_rec:\n",
    "            test_dtlist+=glob(rec+'/*.npy*')\n",
    "    elif opt=='leaveonepatient':\n",
    "        # This is an additional experiment, so we have to\n",
    "        #  use '\\\\' instead of '\\'\n",
    "        subjects=[\n",
    "            ['a01','a14']\n",
    "            ,['a02','x14']\n",
    "            ,['a03','x19']\n",
    "            ,['a04','a12']\n",
    "            ,['a05','a10','a20','x07']\n",
    "            ,['a06','x15']\n",
    "            ,['a07','a16','x01','x30']\n",
    "            ,['a08','a13','x20']\n",
    "            ,['a09','a18']\n",
    "            ,['a11']\n",
    "            ,['a15','x27','x28']\n",
    "            ,['a17','x12']\n",
    "            ,['a19','x05','x08','x25']\n",
    "            ,['b01','x03']\n",
    "            ,['b02','b03','x16','x21']\n",
    "            ,['b04','c08']\n",
    "            ,['b05','x11']\n",
    "            ,['c01','x35']\n",
    "            ,['c02','c09']\n",
    "            ,['c03','x04']\n",
    "            ,['c04','x29']\n",
    "            ,['c05','x33']\n",
    "            ,['c06']\n",
    "            ,['c07','x34']\n",
    "            ,['c10','x18']\n",
    "            ,['x02']\n",
    "            ,['x06','x24']\n",
    "            ,['x09','x23']\n",
    "            ,['x10']\n",
    "            ,['x13','x26']\n",
    "            ,['x17','x22']\n",
    "            ,['x31','x32']\n",
    "        ]\n",
    "        train_rec=[rec for rec in reclist if rec.split('\\\\')[-1] not in subjects[fold-1]]\n",
    "        test_rec=[rec for rec in reclist if rec.split('\\\\')[-1] in subjects[fold-1]]\n",
    "        print('test subjects: ',test_rec)\n",
    "        for rec in train_rec:\n",
    "            train_dtlist+=glob(rec+'/*.npy*')\n",
    "        for rec in test_rec:\n",
    "            test_dtlist+=glob(rec+'/*.npy*')\n",
    "\n",
    "    else:#reclist is npy data list\n",
    "        kf = KFold(n_splits=10)\n",
    "        kf_idx=kf.split(dtlist)\n",
    "        for fold_idx in range(fold):#1,2,3,4,5\n",
    "            train_idx,test_idx=kf_idx.__next__()\n",
    "        train_dtlist,test_dtlist=list(np.array(dtlist)[train_idx]),list(np.array(dtlist)[test_idx])\n",
    "         \n",
    "    return train_dtlist,test_dtlist\n",
    "\n",
    "def dtclean(dt_path):\n",
    "    dt=np.load(dt_path)\n",
    "    if dt.std()<0.1:\n",
    "        #print(dt_path)\n",
    "        return 1      \n",
    "    else:\n",
    "        return 0\n",
    "#train_rec,test_rec=reclist[train_idx],reclist[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "ApneaECGDict={'N':0,\n",
    "              'A':1}\n",
    "class ApneaECGDataset(Dataset):\n",
    "    def __init__(self, filelist, istrain=False):\n",
    "        # Get the filelist and img data\n",
    "        self.filelist = filelist\n",
    "        self.istrain = istrain\n",
    "        \n",
    "    def getFeature(self, dt):\n",
    "#         dt=dt.reshape(-1,100)\n",
    "#         if self.istrain:\n",
    "#             shift=np.random.randint(0,5)\n",
    "#             shift=0\n",
    "#         else:\n",
    "#             shift=0\n",
    "#         dt=dt[:,shift::]\n",
    "#         dt_flag=dt[:,20::]*dt[:,0:-20]\n",
    "#         pd_dt=pd.DataFrame(dt)\n",
    "#         pd_dt_flag=pd.DataFrame(dt_flag)\n",
    "#scipy.signal.stft(x,fs=1.0,window='hann',nperseg=256,noverlap=None,nfft=None,detrend=False,return_onesided=True,boundary='zeros',padded=True,axis=-1)\n",
    "        dt=cheb_bandpass_filter(dt, 0.01, 38, 100, 4)\n",
    "        f, t, Sxx=signal.spectrogram(dt, fs=100.0, window=('hamming'), nperseg=128, noverlap=64, nfft=128, detrend='constant',\n",
    "        return_onesided=True, scaling='density', axis=-1, mode='psd')\n",
    "        return Sxx[0:26]\n",
    "        \n",
    "#         return np.concatenate((dt_flag.max(axis=1,keepdims=True),\n",
    "#                                dt_flag.min(axis=1,keepdims=True),\n",
    "#                                dt_flag.std(axis=1,keepdims=True),\n",
    "#                                dt.std(axis=1,keepdims=True),\n",
    "#             abs(dt).sum(axis=1,keepdims=True)/100,\n",
    "#             abs(dt_flag).sum(axis=1,keepdims=True)/100,\n",
    "#             (dt.argmax(axis=1)-dt.argmax(axis=1)[0])[:,np.newaxis]/100,\n",
    "#             (dt.argmin(axis=1)-dt.argmin(axis=1)[0])[:,np.newaxis]/100,\n",
    "#             (dt_flag.argmin(axis=1)-dt_flag.argmin(axis=1)[0])[:,np.newaxis]/100,\n",
    "#             (dt_flag.argmax(axis=1)-dt_flag.argmax(axis=1)[0])[:,np.newaxis]/100,\n",
    "#                                dt.mean(axis=1,keepdims=True),#\n",
    "#                                dt.min(axis=1,keepdims=True),\n",
    "#                                dt.max(axis=1,keepdims=True),\n",
    "#                                #pd_dt.skew(axis=1).values[:,np.newaxis],\n",
    "#                                #pd_dt.kurt(axis=1).values[:,np.newaxis],\n",
    "#                                #pd_dt_flag.skew(axis=1).values[:,np.newaxis],\n",
    "#                                #pd_dt_flag.kurt(axis=1).values[:,np.newaxis]\n",
    "#                                ),axis=1)\n",
    "    def __getitem__(self, index):\n",
    "        # return to the data of a Picture\n",
    "        dt_path = self.filelist[index]\n",
    "        label = ApneaECGDict.__getitem__(dt_path.split('_')[0][-1])\n",
    "        data = np.load(dt_path)\n",
    "\n",
    "        if self.istrain:#results are not reproducable when self.istrain =True due to a variation of random seed\n",
    "            noise = np.random.normal(0, 0.1, data.shape[0])\n",
    "            data = noise+data\n",
    "        data=self.getFeature(data)\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filelist)\n",
    "\n",
    "class conv3x3(nn.Module):#ACNet\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, \n",
    "                 padding=1, dilation=1, groups=1, padding_mode='zeros', bias=False, deploy=False):\n",
    "        super(conv3x3, self).__init__()\n",
    "        self.deploy=deploy\n",
    "        if self.deploy:\n",
    "            self.fused_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(kernel_size,kernel_size), stride=stride,\n",
    "                                      padding=padding, dilation=dilation, groups=groups, bias=False, padding_mode=padding_mode)\n",
    "# c=a.square_conv.weight.data.numpy().copy()\n",
    "# c[:,:,1:2,:]=a.square_conv.weight.data[:,:,1:2,:,].numpy().copy()+a.hor_conv.weight.data[:,:,0:1,:,].numpy().copy()\n",
    "# c[:,:,:,1:2]+=a.ver_conv.weight.data[:,:,:,0:1].numpy().copy()\n",
    "# c-a.square_conv.weight.data.numpy()\n",
    "# b.fused_conv.weight.data=torch.FloatTensor(c)    \n",
    "        else:\n",
    "            self.square_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                         kernel_size=(kernel_size, kernel_size), stride=stride,\n",
    "                                         padding=padding, dilation=dilation, groups=groups, bias=False,\n",
    "                                         padding_mode=padding_mode)\n",
    "\n",
    "            center_offset_from_origin_border = padding - kernel_size // 2 #1-3//2=0\n",
    "            ver_pad_or_crop = (center_offset_from_origin_border + 1, center_offset_from_origin_border)\n",
    "            hor_pad_or_crop = (center_offset_from_origin_border, center_offset_from_origin_border + 1)\n",
    "            if center_offset_from_origin_border >= 0:\n",
    "                self.ver_conv_crop_layer = nn.Identity()\n",
    "                ver_conv_padding = ver_pad_or_crop\n",
    "                self.hor_conv_crop_layer = nn.Identity()\n",
    "                hor_conv_padding = hor_pad_or_crop\n",
    "\n",
    "            self.ver_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(3, 1),\n",
    "                                      stride=stride,\n",
    "                                      padding=ver_conv_padding, dilation=dilation, groups=groups, bias=False,\n",
    "                                      padding_mode=padding_mode)\n",
    "\n",
    "            self.hor_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=(1, 3),\n",
    "                                      stride=stride,\n",
    "                                      padding=hor_conv_padding, dilation=dilation, groups=groups, bias=False,\n",
    "                                      padding_mode=padding_mode)\n",
    "\n",
    "    # forward函数\n",
    "    def forward(self, input):\n",
    "        if self.deploy:\n",
    "            #assert self.square_conv\n",
    "            print(\"True\")\n",
    "            return self.fused_conv(input)\n",
    "        square_outputs = self.square_conv(input)\n",
    "        # print(square_outputs.size())\n",
    "        # return square_outputs\n",
    "        vertical_outputs = self.ver_conv(input)\n",
    "        # print(vertical_outputs.size())\n",
    "        #horizontal_outputs = self.hor_conv_crop_layer(input)\n",
    "        horizontal_outputs = self.hor_conv(input)\n",
    "        # print(horizontal_outputs.size())\n",
    "        return square_outputs + vertical_outputs + horizontal_outputs\n",
    "    \n",
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, class_num,fs):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        #self.bn=nn.BatchNorm1d(60)\n",
    "#         self.conv10=nn.Sequential(nn.Conv1d(1,64,kernel_size=10,stride=10,padding=0, bias=False),\n",
    "#                   nn.Conv1d(64,32,kernel_size=3,stride=2,padding=0, bias=False),\n",
    "#                   nn.MaxPool1d(kernel_size=3,stride=2),\n",
    "#                   nn.Conv1d(32,32,kernel_size=3,stride=2,padding=0, bias=False),\n",
    "#                   nn.MaxPool1d(kernel_size=3,stride=2),\n",
    "#                                nn.ReLU())1200\n",
    "#25 * 65\n",
    "#12* 32\n",
    "#b*64*6*16\n",
    "#b*6*64*16\n",
    "        self.branch1=nn.Sequential(nn.Conv2d(1,64,kernel_size=3,dilation=1,stride=1,padding=1, bias=False),\n",
    "                                nn.Conv2d(64,64,kernel_size=3,dilation=1,stride=1,padding=1, bias=False),\n",
    "                                #nn.Conv2d(32,64,kernel_size=3,dilation=1,stride=1,padding=1, bias=False),\n",
    "                                #nn.Conv2d(64,64,kernel_size=(3,1),dilation=1,stride=1,padding=0, bias=False),\n",
    "                                #nn.ReLU(),                               \n",
    "                                nn.LayerNorm([64,26,92],elementwise_affine=False),\n",
    "                                nn.ReLU(), \n",
    "                                nn.Dropout(p=0.25),\n",
    "                                #nn.Conv2d(64,64,kernel_size=3,dilation=1,stride=1,padding=1, bias=False),\n",
    "                                nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "                                #nn.Dropout(p=0.25),\n",
    "                                \n",
    "#                                 nn.Conv2d(64,64,kernel_size=3,dilation=1,stride=1,padding=1, bias=False),\n",
    "#                                 nn.ReLU(),\n",
    "#                                 nn.Dropout(p=0.25),\n",
    "                                nn.LayerNorm([64,13,46],elementwise_affine=False),\n",
    "                                nn.ReLU(), \n",
    "                                nn.Dropout(p=0.5),\n",
    "                                nn.Conv2d(64,64,kernel_size=3,dilation=1,stride=1,padding=1, bias=False),\n",
    "                                #nn.Conv2d(64,64,kernel_size=(5,1),dilation=1,stride=1,padding=0, bias=False),\n",
    "                                                          \n",
    "                                nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "                                #nn.Dropout(p=0.5),\n",
    "                                nn.LayerNorm([64,6,23],elementwise_affine=False),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(p=0.5),\n",
    "                                #nn.Conv2d(64,64,kernel_size=(1,3),dilation=1,stride=1,padding=0, bias=False),\n",
    "                                nn.Conv2d(64,64,kernel_size=3,dilation=1,stride=1,padding=1, bias=False))\n",
    "        \n",
    "#         self.branch2=nn.Sequential(nn.Conv2d(1,32,kernel_size=5,dilation=1,stride=1,padding=2, bias=False),\n",
    "#                                 nn.Conv2d(32,32,kernel_size=5,dilation=1,stride=1,padding=2, bias=False),\n",
    "#                                 #nn.Conv2d(32,64,kernel_size=3,dilation=1,stride=1,padding=1, bias=False),\n",
    "#                                 #nn.Conv2d(64,64,kernel_size=(3,1),dilation=1,stride=1,padding=0, bias=False),\n",
    "#                                 #nn.ReLU(),                               \n",
    "#                                 nn.LayerNorm([32,25,92]),\n",
    "#                                 nn.ReLU(), \n",
    "#                                 nn.Dropout(p=0.25),\n",
    "#                                 #nn.Conv2d(64,64,kernel_size=3,dilation=1,stride=1,padding=1, bias=False),\n",
    "#                                 nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "#                                 #nn.Dropout(p=0.25),\n",
    "                                \n",
    "# #                                 nn.Conv2d(64,64,kernel_size=3,dilation=1,stride=1,padding=1, bias=False),\n",
    "# #                                 nn.ReLU(),\n",
    "# #                                 nn.Dropout(p=0.25),\n",
    "#                                 nn.LayerNorm([32,12,46]),\n",
    "#                                 nn.ReLU(), \n",
    "#                                 nn.Dropout(p=0.25),\n",
    "#                                 nn.Conv2d(32,32,kernel_size=5,dilation=1,stride=1,padding=2, bias=False),\n",
    "#                                 #nn.Conv2d(64,64,kernel_size=(5,1),dilation=1,stride=1,padding=0, bias=False),\n",
    "                                                          \n",
    "#                                 nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "#                                 #nn.Dropout(p=0.5),\n",
    "#                                 nn.LayerNorm([32,6,23]),\n",
    "#                                 nn.ReLU(),\n",
    "#                                 nn.Dropout(p=0.5),\n",
    "#                                 #nn.Conv2d(64,64,kernel_size=(1,3),dilation=1,stride=1,padding=0, bias=False),\n",
    "#                                 nn.Conv2d(32,32,kernel_size=5,dilation=1,stride=1,padding=2, bias=False))\n",
    "        \n",
    "        self.avg = nn.Sequential(nn.AdaptiveMaxPool2d((8,8)),nn.LayerNorm([6,8,8],elementwise_affine=False))\n",
    "    \n",
    "        self.lstm = nn.LSTM(64, 8, 2,\n",
    "                            bias=False,\n",
    "                            batch_first=True,\n",
    "                            dropout=0.5,\n",
    "                            bidirectional=True)\n",
    "        #self.conv2=nn.Sequential(nn.Conv1d(64,64,kernel_size=1,stride=1,padding=0, bias=False))\n",
    "        \n",
    "        self.fc=nn.Sequential(nn.Linear(16*2,class_num))\n",
    "    def attention_net(self,lstm_output, final_state):\n",
    "        hidden = final_state.view(-1, 16, 2)   # hidden : [batch_size, n_hidden * num_directions(=2), 1(=n_layer)]\n",
    "        #print('hi',hidden.shape)\n",
    "        attn_weights = torch.bmm(lstm_output, hidden).squeeze(2) # attn_weights : [batch_size, n_step]\n",
    "        #print('att',attn_weights.shape)\n",
    "        soft_attn_weights = F.softmax(attn_weights, 1)\n",
    "        #print('sft',soft_attn_weights.shape)\n",
    "        #print('yimr',torch.bmm(lstm_output.transpose(1, 2),soft_attn_weights).shape)\n",
    "        # [batch_size, n_hidden * num_directions(=2), n_step] * [batch_size, n_step, 1] = [batch_size, n_hidden * num_directions(=2), 1]\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights)\n",
    "        return context#, soft_attn_weights.data.cpu().numpy() # context : [batch_size, n_hidden * num_directions(=2)]\n",
    "    def forward(self, x):\n",
    "        x=x.unsqueeze(dim=1)#x.reshape(x.shape[0],1,-1)\n",
    "        #x1=self.conv10(x)\n",
    "        out=self.branch1(x)\n",
    "        #out_1=self.branch1(x)\n",
    "        #out_2=self.branch2(x)\n",
    "        #out=torch.cat((out_1,out_2),1)\n",
    "        #print(out.shape)\n",
    "        out=out.permute(0,2,1,3)\n",
    "        out=self.avg(out)\n",
    "        #print(out.shape)\n",
    "        out=out.view(out.shape[0],out.shape[1],-1)\n",
    "        #print(x2.shape)\n",
    "        #print(x1.shape,x2.shape)\n",
    "        #x=torch.cat((x1,x2),dim=1)\n",
    "        #print(x.shape)\n",
    "        out,(h,c) = self.lstm(out)\n",
    "        out=self.attention_net(out, h)\n",
    "        \n",
    "        #out = self.conv2(out)\n",
    "        #print(out.shape)\n",
    "        out = self.fc(out.reshape(out.shape[0],-1))\n",
    "        return out#,attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Find mean and std for normalization\n",
    "# train_dataset = ApneaECGDataset(train_dtlist+test_dtlist,istrain=False)\n",
    "#     #test_dataset = ApneaECGDataset(test_dtlist,istrain=False)\n",
    "# train_loader=DataLoader(train_dataset, batch_size=256, shuffle=False, sampler=None, num_workers=0)\n",
    "# fe_list=torch.Tensor([])\n",
    "# label_list=torch.LongTensor([])\n",
    "# for fe, label in train_loader:\n",
    "#     fe=fe.float()\n",
    "#     label=label.long()\n",
    "#     fe_list=torch.cat((fe_list,fe.reshape(-1,fe.shape[-1])))\n",
    "#     label_list=torch.cat((label_list,label))\n",
    "# fe_mean=fe_list.mean(axis=0)\n",
    "# fe_std=fe_list.std(axis=0)#,fe.shape\n",
    "\n",
    "# label_list=label_list.numpy()\n",
    "# pos=sum(label_list)#/len(label_list.numpy())\n",
    "# neg=len(label_list)-pos\n",
    "# c_weight=torch.Tensor([pos,neg])/len(label_list)#反比\n",
    "\n",
    "# print(fe_mean,fe_std,c_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fe.shape\n",
    "# ?signal.spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m=nn.Conv2d(1,64,kernel_size=(1,3),dilation=1,stride=(1,2),padding=0, bias=False)\n",
    "# n=nn.Conv2d(64,64,kernel_size=(1,3),dilation=1,stride=(1,2),padding=0, bias=False)\n",
    "# fe.shape,fe.unsqueeze(dim=1).shape,n(m(fe.unsqueeze(dim=1))).shape,model(fe).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(np.corrcoef(fe_list.numpy(),rowvar=False),cmap='coolwarm')\n",
    "# plt.colorbar()\n",
    "# plt.show()\n",
    "# plt.close(\"all\")\n",
    "# model=alexnet(num_classes=2)\n",
    "# model.features[0]=nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model,test_dtlist,criterion):\n",
    "    test_dataset = ApneaECGDataset(test_dtlist,istrain=False)\n",
    "    test_loader=DataLoader(test_dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0)\n",
    "    #train_dataset = ApneaECGDataset(train_dtlist,istrain=False)\n",
    "        #test_dataset = ApneaECGDataset(test_dtlist,istrain=False)\n",
    "    #train_loader=DataLoader(train_dataset, batch_size=256, shuffle=True, sampler=None, num_workers=0)\n",
    "    sft=nn.Softmax().to(device)\n",
    "    all_pred_prob=torch.Tensor([]).to(device)\n",
    "    all_label=torch.Tensor([]).long().to(device)\n",
    "    flag=0\n",
    "    with torch.no_grad():\n",
    "        for fe, label in test_loader:\n",
    "            fe=fe.float().to(device)\n",
    "            #fe=(fe-fe_mean)/fe_std\n",
    "            label=label.long().to(device)\n",
    "            pred_prob_no_softmax =model(fe)\n",
    "            all_pred_prob=torch.cat((all_pred_prob,pred_prob_no_softmax),0)\n",
    "            all_label=torch.cat((all_label,label),0)\n",
    "            \n",
    "            loss=criterion(pred_prob_no_softmax,label)\n",
    "            flag+=1\n",
    "            if flag%8192==0:\n",
    "                print('Eval Loss: ',loss.item())\n",
    "            #pos=(sft(pred_prob_no_softmax)[:,1]>0.5)\n",
    "            #print(\"Acc: \", sum(label==pos).detach().numpy()/len(label))\n",
    "    all_pred=sft(all_pred_prob)[:,1].detach().cpu().numpy()\n",
    "    all_pred[all_pred>0.5]=1\n",
    "    all_pred[all_pred<=0.5]=0\n",
    "    all_label=all_label.detach().cpu().numpy()\n",
    "\n",
    "    print(confusion_matrix(all_label,all_pred))\n",
    "    print(classification_report(all_label,all_pred))\n",
    "    print(\"acc: \",accuracy_score(all_label,all_pred))\n",
    "    print(\"pre: \",precision_score(all_label,all_pred))\n",
    "    print(\"rec: \",recall_score(all_label,all_pred))\n",
    "    print(\"ma F1: \",f1_score(all_label,all_pred, average='macro'))\n",
    "    print(\"mi F1: \",f1_score(all_label,all_pred, average='micro'))\n",
    "    print(\"we F1: \",f1_score(all_label,all_pred, average='weighted'))\n",
    "    return accuracy_score(all_label,all_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test subjects:  ['./seg\\\\a01', './seg\\\\a14']\n",
      "*********\n",
      "33315 998\n",
      "31893 998\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.6684489250183105\n",
      "Eval Loss:  0.7222466468811035\n",
      "Eval Loss:  0.6797828078269958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\install\\envs\\pytorch-gpu\\lib\\site-packages\\ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2798 17221]\n",
      " [ 2933  8941]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.14      0.22     20019\n",
      "           1       0.34      0.75      0.47     11874\n",
      "\n",
      "    accuracy                           0.37     31893\n",
      "   macro avg       0.41      0.45      0.34     31893\n",
      "weighted avg       0.43      0.37      0.31     31893\n",
      "\n",
      "acc:  0.36807449910638695\n",
      "pre:  0.3417552174910175\n",
      "rec:  0.7529897254505643\n",
      "ma F1:  0.3437269730158552\n",
      "mi F1:  0.36807449910638695\n",
      "we F1:  0.31144460283721587\n",
      "[[ 48  97]\n",
      " [355 498]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.33      0.18       145\n",
      "           1       0.84      0.58      0.69       853\n",
      "\n",
      "    accuracy                           0.55       998\n",
      "   macro avg       0.48      0.46      0.43       998\n",
      "weighted avg       0.73      0.55      0.61       998\n",
      "\n",
      "acc:  0.5470941883767535\n",
      "pre:  0.8369747899159664\n",
      "rec:  0.5838218053927315\n",
      "ma F1:  0.43151389280961405\n",
      "mi F1:  0.5470941883767535\n",
      "we F1:  0.6133602245019134\n",
      "Subject 1 Current Train Acc:  0.36807449910638695 Current Test Acc:  0.5470941883767535\n",
      "Loss:  0.16712312400341034\n",
      "Loss:  0.16074931621551514\n",
      "Loss:  0.15100939571857452\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.13863015174865723\n",
      "Loss:  0.14145606756210327\n",
      "Loss:  0.14911700785160065\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.10672581940889359\n",
      "Loss:  0.09943507611751556\n",
      "Loss:  0.08832559734582901\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  3.3172199726104736\n",
      "Eval Loss:  0.1071234941482544\n",
      "Eval Loss:  0.2719845771789551\n",
      "[[17523  2496]\n",
      " [ 3010  8864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86     20019\n",
      "           1       0.78      0.75      0.76     11874\n",
      "\n",
      "    accuracy                           0.83     31893\n",
      "   macro avg       0.82      0.81      0.81     31893\n",
      "weighted avg       0.83      0.83      0.83     31893\n",
      "\n",
      "acc:  0.8273602357884176\n",
      "pre:  0.780281690140845\n",
      "rec:  0.7465049688394813\n",
      "ma F1:  0.8136217126270873\n",
      "mi F1:  0.8273602357884176\n",
      "we F1:  0.8265447142611136\n",
      "[[108  37]\n",
      " [185 668]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.74      0.49       145\n",
      "           1       0.95      0.78      0.86       853\n",
      "\n",
      "    accuracy                           0.78       998\n",
      "   macro avg       0.66      0.76      0.68       998\n",
      "weighted avg       0.86      0.78      0.80       998\n",
      "\n",
      "acc:  0.7775551102204409\n",
      "pre:  0.9475177304964539\n",
      "rec:  0.7831184056271981\n",
      "ma F1:  0.6753301563296815\n",
      "mi F1:  0.7775551102204409\n",
      "we F1:  0.8045717051772846\n",
      "Subject 1 Current Train Acc:  0.8273602357884176 Current Test Acc:  0.7775551102204409\n",
      "Loss:  0.07607658207416534\n",
      "Loss:  0.11371740698814392\n",
      "Loss:  0.07840454578399658\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.07880623638629913\n",
      "Loss:  0.13143426179885864\n",
      "Loss:  0.11599096655845642\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.10051503032445908\n",
      "Loss:  0.0933544784784317\n",
      "Loss:  0.07995592057704926\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  2.8521533012390137\n",
      "Eval Loss:  0.025658130645751953\n",
      "Eval Loss:  0.12963759899139404\n",
      "[[19193   826]\n",
      " [ 4399  7475]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.96      0.88     20019\n",
      "           1       0.90      0.63      0.74     11874\n",
      "\n",
      "    accuracy                           0.84     31893\n",
      "   macro avg       0.86      0.79      0.81     31893\n",
      "weighted avg       0.85      0.84      0.83     31893\n",
      "\n",
      "acc:  0.8361709466027027\n",
      "pre:  0.9004939163956149\n",
      "rec:  0.6295266969850093\n",
      "ma F1:  0.810603443300985\n",
      "mi F1:  0.8361709466027027\n",
      "we F1:  0.8283750181766029\n",
      "[[122  23]\n",
      " [229 624]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.84      0.49       145\n",
      "           1       0.96      0.73      0.83       853\n",
      "\n",
      "    accuracy                           0.75       998\n",
      "   macro avg       0.66      0.79      0.66       998\n",
      "weighted avg       0.87      0.75      0.78       998\n",
      "\n",
      "acc:  0.7474949899799599\n",
      "pre:  0.964451313755796\n",
      "rec:  0.731535756154748\n",
      "ma F1:  0.6619677419354839\n",
      "mi F1:  0.7474949899799598\n",
      "we F1:  0.7825918288189282\n",
      "Loss:  0.07680632919073105\n",
      "Loss:  0.1306883692741394\n",
      "Loss:  0.08440667390823364\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.07980233430862427\n",
      "Loss:  0.05575305595993996\n",
      "Loss:  0.08978873491287231\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.08638589084148407\n",
      "Loss:  0.0762879028916359\n",
      "Loss:  0.10723026096820831\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  1.4940959215164185\n",
      "Eval Loss:  0.03466165065765381\n",
      "Eval Loss:  0.13997513055801392\n",
      "[[18971  1048]\n",
      " [ 3192  8682]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90     20019\n",
      "           1       0.89      0.73      0.80     11874\n",
      "\n",
      "    accuracy                           0.87     31893\n",
      "   macro avg       0.87      0.84      0.85     31893\n",
      "weighted avg       0.87      0.87      0.86     31893\n",
      "\n",
      "acc:  0.8670554667168344\n",
      "pre:  0.8922918807810895\n",
      "rec:  0.7311773623041941\n",
      "ma F1:  0.8516116200110135\n",
      "mi F1:  0.8670554667168344\n",
      "we F1:  0.8638373106922178\n",
      "[[ 99  46]\n",
      " [173 680]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.68      0.47       145\n",
      "           1       0.94      0.80      0.86       853\n",
      "\n",
      "    accuracy                           0.78       998\n",
      "   macro avg       0.65      0.74      0.67       998\n",
      "weighted avg       0.85      0.78      0.81       998\n",
      "\n",
      "acc:  0.7805611222444889\n",
      "pre:  0.9366391184573003\n",
      "rec:  0.7971864009378663\n",
      "ma F1:  0.6680623835320597\n",
      "mi F1:  0.7805611222444889\n",
      "we F1:  0.805152068572335\n",
      "Subject 1 Current Train Acc:  0.8670554667168344 Current Test Acc:  0.7805611222444889\n",
      "Loss:  0.06667900085449219\n",
      "Loss:  0.07549857348203659\n",
      "Loss:  0.10216458886861801\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.08628781884908676\n",
      "Loss:  0.0672677531838417\n",
      "Loss:  0.07957495748996735\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.09023366868495941\n",
      "Loss:  0.07689376175403595\n",
      "Loss:  0.10740940272808075\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  1.005761981010437\n",
      "Eval Loss:  0.038887977600097656\n",
      "Eval Loss:  0.12920737266540527\n",
      "[[18986  1033]\n",
      " [ 2856  9018]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91     20019\n",
      "           1       0.90      0.76      0.82     11874\n",
      "\n",
      "    accuracy                           0.88     31893\n",
      "   macro avg       0.88      0.85      0.86     31893\n",
      "weighted avg       0.88      0.88      0.88     31893\n",
      "\n",
      "acc:  0.8780610165240021\n",
      "pre:  0.8972241568003184\n",
      "rec:  0.7594744820616472\n",
      "ma F1:  0.8648599375840612\n",
      "mi F1:  0.8780610165240021\n",
      "we F1:  0.8756467341295888\n",
      "[[ 91  54]\n",
      " [161 692]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.63      0.46       145\n",
      "           1       0.93      0.81      0.87       853\n",
      "\n",
      "    accuracy                           0.78       998\n",
      "   macro avg       0.64      0.72      0.66       998\n",
      "weighted avg       0.85      0.78      0.81       998\n",
      "\n",
      "acc:  0.7845691382765531\n",
      "pre:  0.9276139410187667\n",
      "rec:  0.8112543962485346\n",
      "ma F1:  0.6619896251277955\n",
      "mi F1:  0.7845691382765531\n",
      "we F1:  0.8063927787206746\n",
      "Subject 1 Current Train Acc:  0.8780610165240021 Current Test Acc:  0.7845691382765531\n",
      "Loss:  0.07404647022485733\n",
      "Loss:  0.07391395419836044\n",
      "Loss:  0.09346507489681244\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.07607666403055191\n",
      "Loss:  0.05708426237106323\n",
      "Loss:  0.08464307337999344\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.05810493230819702\n",
      "Loss:  0.06147584319114685\n",
      "Loss:  0.06163270026445389\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  1.5694844722747803\n",
      "Eval Loss:  0.1436440348625183\n",
      "Eval Loss:  0.11253613233566284\n",
      "[[18708  1311]\n",
      " [ 2268  9606]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91     20019\n",
      "           1       0.88      0.81      0.84     11874\n",
      "\n",
      "    accuracy                           0.89     31893\n",
      "   macro avg       0.89      0.87      0.88     31893\n",
      "weighted avg       0.89      0.89      0.89     31893\n",
      "\n",
      "acc:  0.8877810177781958\n",
      "pre:  0.8799120637537785\n",
      "rec:  0.8089944416371905\n",
      "ma F1:  0.8778304991749348\n",
      "mi F1:  0.8877810177781958\n",
      "we F1:  0.8867348030667785\n",
      "[[ 82  63]\n",
      " [147 706]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.57      0.44       145\n",
      "           1       0.92      0.83      0.87       853\n",
      "\n",
      "    accuracy                           0.79       998\n",
      "   macro avg       0.64      0.70      0.65       998\n",
      "weighted avg       0.84      0.79      0.81       998\n",
      "\n",
      "acc:  0.7895791583166333\n",
      "pre:  0.918075422626788\n",
      "rec:  0.8276670574443142\n",
      "ma F1:  0.6545164417072736\n",
      "mi F1:  0.7895791583166333\n",
      "we F1:  0.8077606778602008\n",
      "Subject 1 Current Train Acc:  0.8877810177781958 Current Test Acc:  0.7895791583166333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.05494740232825279\n",
      "Loss:  0.07942625135183334\n",
      "Loss:  0.07244600355625153\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.11197512596845627\n",
      "Loss:  0.04531416296958923\n",
      "Loss:  0.05306907743215561\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.05756701901555061\n",
      "Loss:  0.06613649427890778\n",
      "Loss:  0.05577581748366356\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  2.2524945735931396\n",
      "Eval Loss:  0.04508793354034424\n",
      "Eval Loss:  0.15590554475784302\n",
      "[[19350   669]\n",
      " [ 3317  8557]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91     20019\n",
      "           1       0.93      0.72      0.81     11874\n",
      "\n",
      "    accuracy                           0.88     31893\n",
      "   macro avg       0.89      0.84      0.86     31893\n",
      "weighted avg       0.88      0.88      0.87     31893\n",
      "\n",
      "acc:  0.8750195967767221\n",
      "pre:  0.9274875352265337\n",
      "rec:  0.7206501600134748\n",
      "ma F1:  0.8588552425037856\n",
      "mi F1:  0.8750195967767221\n",
      "we F1:  0.8710537661351271\n",
      "[[ 96  49]\n",
      " [202 651]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.66      0.43       145\n",
      "           1       0.93      0.76      0.84       853\n",
      "\n",
      "    accuracy                           0.75       998\n",
      "   macro avg       0.63      0.71      0.64       998\n",
      "weighted avg       0.84      0.75      0.78       998\n",
      "\n",
      "acc:  0.748496993987976\n",
      "pre:  0.93\n",
      "rec:  0.7631887456037515\n",
      "ma F1:  0.6358929560349953\n",
      "mi F1:  0.748496993987976\n",
      "we F1:  0.7795391882344733\n",
      "Loss:  0.06295450031757355\n",
      "Loss:  0.04984385892748833\n",
      "Loss:  0.0567229762673378\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.05508852005004883\n",
      "Loss:  0.05567767471075058\n",
      "Loss:  0.07582014054059982\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.08919888734817505\n",
      "Loss:  0.09702099859714508\n",
      "Loss:  0.08937546610832214\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  2.1686346530914307\n",
      "Eval Loss:  0.04797255992889404\n",
      "Eval Loss:  0.1330547332763672\n",
      "[[19339   680]\n",
      " [ 3145  8729]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91     20019\n",
      "           1       0.93      0.74      0.82     11874\n",
      "\n",
      "    accuracy                           0.88     31893\n",
      "   macro avg       0.89      0.85      0.87     31893\n",
      "weighted avg       0.89      0.88      0.88     31893\n",
      "\n",
      "acc:  0.8800677264603518\n",
      "pre:  0.9277287703262833\n",
      "rec:  0.7351355903655045\n",
      "ma F1:  0.8651427242424358\n",
      "mi F1:  0.8800677264603519\n",
      "we F1:  0.8766002306576033\n",
      "[[ 91  54]\n",
      " [179 674]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.63      0.44       145\n",
      "           1       0.93      0.79      0.85       853\n",
      "\n",
      "    accuracy                           0.77       998\n",
      "   macro avg       0.63      0.71      0.65       998\n",
      "weighted avg       0.84      0.77      0.79       998\n",
      "\n",
      "acc:  0.7665330661322646\n",
      "pre:  0.9258241758241759\n",
      "rec:  0.7901524032825322\n",
      "ma F1:  0.6455895689017931\n",
      "mi F1:  0.7665330661322645\n",
      "we F1:  0.7924643476996898\n",
      "Loss:  0.07992976158857346\n",
      "Loss:  0.07347128540277481\n",
      "Loss:  0.08175689727067947\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.10794069617986679\n",
      "Loss:  0.06894496083259583\n",
      "Loss:  0.07761250436306\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.06485983729362488\n",
      "Loss:  0.06329362839460373\n",
      "Loss:  0.06717459112405777\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  1.5426907539367676\n",
      "Eval Loss:  0.08063781261444092\n",
      "Eval Loss:  0.08175528049468994\n",
      "[[18835  1184]\n",
      " [ 1914  9960]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92     20019\n",
      "           1       0.89      0.84      0.87     11874\n",
      "\n",
      "    accuracy                           0.90     31893\n",
      "   macro avg       0.90      0.89      0.89     31893\n",
      "weighted avg       0.90      0.90      0.90     31893\n",
      "\n",
      "acc:  0.9028626971435738\n",
      "pre:  0.8937544867193108\n",
      "rec:  0.8388074785245073\n",
      "ma F1:  0.8947093530344759\n",
      "mi F1:  0.9028626971435738\n",
      "we F1:  0.9021920558816424\n",
      "[[ 63  82]\n",
      " [ 78 775]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.43      0.44       145\n",
      "           1       0.90      0.91      0.91       853\n",
      "\n",
      "    accuracy                           0.84       998\n",
      "   macro avg       0.68      0.67      0.67       998\n",
      "weighted avg       0.84      0.84      0.84       998\n",
      "\n",
      "acc:  0.8396793587174348\n",
      "pre:  0.9043173862310385\n",
      "rec:  0.9085580304806565\n",
      "ma F1:  0.673496094548726\n",
      "mi F1:  0.8396793587174348\n",
      "we F1:  0.8387457448737904\n",
      "Subject 1 Current Train Acc:  0.9028626971435738 Current Test Acc:  0.8396793587174348\n",
      "Loss:  0.09730210900306702\n",
      "Loss:  0.051250867545604706\n",
      "Loss:  0.08087649941444397\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.03632689639925957\n",
      "Loss:  0.06863576173782349\n",
      "Loss:  0.06249804049730301\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.07370439171791077\n",
      "Loss:  0.047540340572595596\n",
      "Loss:  0.06564638018608093\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.9555235505104065\n",
      "Eval Loss:  0.1479102373123169\n",
      "Eval Loss:  0.09192228317260742\n",
      "[[19065   954]\n",
      " [ 2217  9657]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     20019\n",
      "           1       0.91      0.81      0.86     11874\n",
      "\n",
      "    accuracy                           0.90     31893\n",
      "   macro avg       0.90      0.88      0.89     31893\n",
      "weighted avg       0.90      0.90      0.90     31893\n",
      "\n",
      "acc:  0.900573793622425\n",
      "pre:  0.9100932994062765\n",
      "rec:  0.813289540171804\n",
      "ma F1:  0.8910974232210108\n",
      "mi F1:  0.900573793622425\n",
      "we F1:  0.8993016150704495\n",
      "[[ 63  82]\n",
      " [ 68 785]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.43      0.46       145\n",
      "           1       0.91      0.92      0.91       853\n",
      "\n",
      "    accuracy                           0.85       998\n",
      "   macro avg       0.69      0.68      0.68       998\n",
      "weighted avg       0.84      0.85      0.85       998\n",
      "\n",
      "acc:  0.8496993987975952\n",
      "pre:  0.9054209919261822\n",
      "rec:  0.9202813599062134\n",
      "ma F1:  0.6846562184024267\n",
      "mi F1:  0.8496993987975952\n",
      "we F1:  0.8464991155212345\n",
      "Subject 1 Current Train Acc:  0.900573793622425 Current Test Acc:  0.8496993987975952\n",
      "Loss:  0.036153752356767654\n",
      "Loss:  0.040666364133358\n",
      "Loss:  0.06825078278779984\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.04953626170754433\n",
      "Loss:  0.05777539685368538\n",
      "Loss:  0.049850013107061386\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.06829974800348282\n",
      "Loss:  0.08797455579042435\n",
      "Loss:  0.069638192653656\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  1.6843397617340088\n",
      "Eval Loss:  0.10970711708068848\n",
      "Eval Loss:  0.10188555717468262\n",
      "[[19191   828]\n",
      " [ 2408  9466]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92     20019\n",
      "           1       0.92      0.80      0.85     11874\n",
      "\n",
      "    accuracy                           0.90     31893\n",
      "   macro avg       0.90      0.88      0.89     31893\n",
      "weighted avg       0.90      0.90      0.90     31893\n",
      "\n",
      "acc:  0.8985357288433199\n",
      "pre:  0.9195647950262289\n",
      "rec:  0.797203975071585\n",
      "ma F1:  0.8881345002446077\n",
      "mi F1:  0.8985357288433199\n",
      "we F1:  0.8968458634257399\n",
      "[[ 86  59]\n",
      " [139 714]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.59      0.46       145\n",
      "           1       0.92      0.84      0.88       853\n",
      "\n",
      "    accuracy                           0.80       998\n",
      "   macro avg       0.65      0.72      0.67       998\n",
      "weighted avg       0.85      0.80      0.82       998\n",
      "\n",
      "acc:  0.8016032064128257\n",
      "pre:  0.9236739974126779\n",
      "rec:  0.8370457209847597\n",
      "ma F1:  0.6715468235763439\n",
      "mi F1:  0.8016032064128257\n",
      "we F1:  0.8181708984939062\n",
      "Loss:  0.10780499875545502\n",
      "Loss:  0.07615010440349579\n",
      "Loss:  0.08643805235624313\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.04630032926797867\n",
      "Loss:  0.07600696384906769\n",
      "Loss:  0.10004086792469025\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.06924012303352356\n",
      "Loss:  0.05297242850065231\n",
      "Loss:  0.03839796409010887\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  1.264444351196289\n",
      "Eval Loss:  0.15065205097198486\n",
      "Eval Loss:  0.10436040163040161\n",
      "[[19022   997]\n",
      " [ 2096  9778]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     20019\n",
      "           1       0.91      0.82      0.86     11874\n",
      "\n",
      "    accuracy                           0.90     31893\n",
      "   macro avg       0.90      0.89      0.89     31893\n",
      "weighted avg       0.90      0.90      0.90     31893\n",
      "\n",
      "acc:  0.9030194713573512\n",
      "pre:  0.9074709976798144\n",
      "rec:  0.8234798719892201\n",
      "ma F1:  0.8941249461087443\n",
      "mi F1:  0.9030194713573512\n",
      "we F1:  0.9019620196861893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 69  76]\n",
      " [ 88 765]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.48      0.46       145\n",
      "           1       0.91      0.90      0.90       853\n",
      "\n",
      "    accuracy                           0.84       998\n",
      "   macro avg       0.67      0.69      0.68       998\n",
      "weighted avg       0.84      0.84      0.84       998\n",
      "\n",
      "acc:  0.8356713426853707\n",
      "pre:  0.9096313912009513\n",
      "rec:  0.8968347010550997\n",
      "ma F1:  0.6800706818768227\n",
      "mi F1:  0.8356713426853707\n",
      "we F1:  0.8383541126993113\n",
      "Loss:  0.07124912738800049\n",
      "Loss:  0.05556640401482582\n",
      "Loss:  0.041635893285274506\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.08024516701698303\n",
      "Loss:  0.0910240039229393\n",
      "Loss:  0.07312063872814178\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.0531756728887558\n",
      "Loss:  0.04534219205379486\n",
      "Loss:  0.04298719763755798\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  1.6014165878295898\n",
      "Eval Loss:  0.06640326976776123\n",
      "Eval Loss:  0.09109342098236084\n",
      "[[19116   903]\n",
      " [ 2159  9715]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93     20019\n",
      "           1       0.91      0.82      0.86     11874\n",
      "\n",
      "    accuracy                           0.90     31893\n",
      "   macro avg       0.91      0.89      0.89     31893\n",
      "weighted avg       0.90      0.90      0.90     31893\n",
      "\n",
      "acc:  0.9039914714827705\n",
      "pre:  0.9149557355434168\n",
      "rec:  0.8181741620346976\n",
      "ma F1:  0.8948557491660866\n",
      "mi F1:  0.9039914714827705\n",
      "we F1:  0.9027709133262175\n",
      "[[ 77  68]\n",
      " [133 720]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.53      0.43       145\n",
      "           1       0.91      0.84      0.88       853\n",
      "\n",
      "    accuracy                           0.80       998\n",
      "   macro avg       0.64      0.69      0.66       998\n",
      "weighted avg       0.83      0.80      0.81       998\n",
      "\n",
      "acc:  0.7985971943887775\n",
      "pre:  0.9137055837563451\n",
      "rec:  0.8440797186400938\n",
      "ma F1:  0.6556582640265726\n",
      "mi F1:  0.7985971943887775\n",
      "we F1:  0.8130466974580518\n",
      "Loss:  0.10229338705539703\n",
      "Loss:  0.06558515876531601\n",
      "Loss:  0.0654057115316391\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.04678129032254219\n",
      "Loss:  0.044349607080221176\n",
      "Loss:  0.06278584897518158\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.06731495261192322\n",
      "Loss:  0.0841107964515686\n",
      "Loss:  0.08000075072050095\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.9434728622436523\n",
      "Eval Loss:  0.07094907760620117\n",
      "Eval Loss:  0.10961318016052246\n",
      "[[19282   737]\n",
      " [ 2362  9512]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.93     20019\n",
      "           1       0.93      0.80      0.86     11874\n",
      "\n",
      "    accuracy                           0.90     31893\n",
      "   macro avg       0.91      0.88      0.89     31893\n",
      "weighted avg       0.90      0.90      0.90     31893\n",
      "\n",
      "acc:  0.9028313423008184\n",
      "pre:  0.9280905454190653\n",
      "rec:  0.8010779855145697\n",
      "ma F1:  0.8927684975427139\n",
      "mi F1:  0.9028313423008184\n",
      "we F1:  0.9011576347540508\n",
      "[[ 88  57]\n",
      " [113 740]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.61      0.51       145\n",
      "           1       0.93      0.87      0.90       853\n",
      "\n",
      "    accuracy                           0.83       998\n",
      "   macro avg       0.68      0.74      0.70       998\n",
      "weighted avg       0.86      0.83      0.84       998\n",
      "\n",
      "acc:  0.8296593186372746\n",
      "pre:  0.9284818067754078\n",
      "rec:  0.8675263774912075\n",
      "ma F1:  0.7028201086004553\n",
      "mi F1:  0.8296593186372746\n",
      "we F1:  0.8405534839165105\n",
      "Loss:  0.053545109927654266\n",
      "Loss:  0.04129837453365326\n",
      "Loss:  0.04653448611497879\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.08629090338945389\n",
      "Loss:  0.0628359317779541\n",
      "Loss:  0.06311796605587006\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.026286832988262177\n",
      "Loss:  0.063954196870327\n",
      "Loss:  0.06455504149198532\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  1.691209316253662\n",
      "Eval Loss:  0.08853065967559814\n",
      "Eval Loss:  0.07584822177886963\n",
      "[[19176   843]\n",
      " [ 2184  9690]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     20019\n",
      "           1       0.92      0.82      0.86     11874\n",
      "\n",
      "    accuracy                           0.91     31893\n",
      "   macro avg       0.91      0.89      0.90     31893\n",
      "weighted avg       0.91      0.91      0.90     31893\n",
      "\n",
      "acc:  0.9050888909792117\n",
      "pre:  0.9199658217032185\n",
      "rec:  0.8160687215765539\n",
      "ma F1:  0.8958776194704176\n",
      "mi F1:  0.9050888909792117\n",
      "we F1:  0.9037867283086137\n",
      "[[ 75  70]\n",
      " [139 714]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.52      0.42       145\n",
      "           1       0.91      0.84      0.87       853\n",
      "\n",
      "    accuracy                           0.79       998\n",
      "   macro avg       0.63      0.68      0.65       998\n",
      "weighted avg       0.83      0.79      0.81       998\n",
      "\n",
      "acc:  0.7905811623246493\n",
      "pre:  0.9107142857142857\n",
      "rec:  0.8370457209847597\n",
      "ma F1:  0.6450773631362487\n",
      "mi F1:  0.7905811623246493\n",
      "we F1:  0.8062928401712842\n",
      "Loss:  0.04294150322675705\n",
      "Loss:  0.05405391752719879\n",
      "Loss:  0.057195767760276794\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.0678001418709755\n",
      "Loss:  0.05366339161992073\n",
      "Loss:  0.07255571335554123\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.08618257194757462\n",
      "Loss:  0.07081472128629684\n",
      "Loss:  0.043813444674015045\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  1.532104730606079\n",
      "Eval Loss:  0.1042892336845398\n",
      "Eval Loss:  0.08194518089294434\n",
      "[[19110   909]\n",
      " [ 1988  9886]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     20019\n",
      "           1       0.92      0.83      0.87     11874\n",
      "\n",
      "    accuracy                           0.91     31893\n",
      "   macro avg       0.91      0.89      0.90     31893\n",
      "weighted avg       0.91      0.91      0.91     31893\n",
      "\n",
      "acc:  0.909165020537422\n",
      "pre:  0.9157943492357573\n",
      "rec:  0.8325753747684016\n",
      "ma F1:  0.9008734284483366\n",
      "mi F1:  0.909165020537422\n",
      "we F1:  0.908195091237322\n",
      "[[ 74  71]\n",
      " [126 727]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.51      0.43       145\n",
      "           1       0.91      0.85      0.88       853\n",
      "\n",
      "    accuracy                           0.80       998\n",
      "   macro avg       0.64      0.68      0.65       998\n",
      "weighted avg       0.83      0.80      0.82       998\n",
      "\n",
      "acc:  0.8026052104208417\n",
      "pre:  0.9110275689223057\n",
      "rec:  0.8522860492379836\n",
      "ma F1:  0.6548319419938728\n",
      "mi F1:  0.8026052104208418\n",
      "we F1:  0.8150516572255634\n",
      "Loss:  0.07100623846054077\n",
      "Loss:  0.06663921475410461\n",
      "Loss:  0.042243875563144684\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.0585191547870636\n",
      "Loss:  0.04074464365839958\n",
      "Loss:  0.08220232278108597\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.060225918889045715\n",
      "Loss:  0.052403900772333145\n",
      "Loss:  0.025344280526041985\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  2.424429178237915\n",
      "Eval Loss:  0.11550521850585938\n",
      "Eval Loss:  0.07111215591430664\n",
      "[[19237   782]\n",
      " [ 2233  9641]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     20019\n",
      "           1       0.92      0.81      0.86     11874\n",
      "\n",
      "    accuracy                           0.91     31893\n",
      "   macro avg       0.91      0.89      0.90     31893\n",
      "weighted avg       0.91      0.91      0.90     31893\n",
      "\n",
      "acc:  0.9054651490922773\n",
      "pre:  0.9249736160414468\n",
      "rec:  0.8119420582785919\n",
      "ma F1:  0.8960550754736536\n",
      "mi F1:  0.9054651490922772\n",
      "we F1:  0.9040422628041757\n",
      "[[ 85  60]\n",
      " [194 659]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.59      0.40       145\n",
      "           1       0.92      0.77      0.84       853\n",
      "\n",
      "    accuracy                           0.75       998\n",
      "   macro avg       0.61      0.68      0.62       998\n",
      "weighted avg       0.83      0.75      0.77       998\n",
      "\n",
      "acc:  0.7454909819639278\n",
      "pre:  0.9165507649513213\n",
      "rec:  0.772567409144197\n",
      "ma F1:  0.6196828940419608\n",
      "mi F1:  0.7454909819639278\n",
      "we F1:  0.774860814335955\n",
      "Loss:  0.07892534136772156\n",
      "Loss:  0.08267240226268768\n",
      "Loss:  0.06465692073106766\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.05198192596435547\n",
      "Loss:  0.06559968739748001\n",
      "Loss:  0.032979801297187805\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.07414107769727707\n",
      "Loss:  0.05079106613993645\n",
      "Loss:  0.03866877406835556\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  1.6054458618164062\n",
      "Eval Loss:  0.12203270196914673\n",
      "Eval Loss:  0.05944979190826416\n",
      "[[19182   837]\n",
      " [ 2155  9719]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     20019\n",
      "           1       0.92      0.82      0.87     11874\n",
      "\n",
      "    accuracy                           0.91     31893\n",
      "   macro avg       0.91      0.89      0.90     31893\n",
      "weighted avg       0.91      0.91      0.90     31893\n",
      "\n",
      "acc:  0.906186310475653\n",
      "pre:  0.9207086017430846\n",
      "rec:  0.8185110325080007\n",
      "ma F1:  0.8971299000443158\n",
      "mi F1:  0.906186310475653\n",
      "we F1:  0.9049249399854804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 78  67]\n",
      " [128 725]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.54      0.44       145\n",
      "           1       0.92      0.85      0.88       853\n",
      "\n",
      "    accuracy                           0.80       998\n",
      "   macro avg       0.65      0.69      0.66       998\n",
      "weighted avg       0.84      0.80      0.82       998\n",
      "\n",
      "acc:  0.8046092184368737\n",
      "pre:  0.9154040404040404\n",
      "rec:  0.8499413833528722\n",
      "ma F1:  0.662951705504897\n",
      "mi F1:  0.8046092184368737\n",
      "we F1:  0.8179648726700276\n",
      "Loss:  0.05966540426015854\n",
      "Loss:  0.072503961622715\n",
      "Loss:  0.06168914958834648\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.053672682493925095\n",
      "Loss:  0.053890109062194824\n",
      "Loss:  0.053078725934028625\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.0546586737036705\n",
      "Loss:  0.05181866139173508\n",
      "Loss:  0.08489358425140381\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  1.287349820137024\n",
      "Eval Loss:  0.07895827293395996\n",
      "Eval Loss:  0.07218170166015625\n",
      "[[19257   762]\n",
      " [ 2083  9791]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     20019\n",
      "           1       0.93      0.82      0.87     11874\n",
      "\n",
      "    accuracy                           0.91     31893\n",
      "   macro avg       0.92      0.89      0.90     31893\n",
      "weighted avg       0.91      0.91      0.91     31893\n",
      "\n",
      "acc:  0.9107954723607061\n",
      "pre:  0.9277930446318582\n",
      "rec:  0.8245747010274549\n",
      "ma F1:  0.9021780240824173\n",
      "mi F1:  0.9107954723607061\n",
      "we F1:  0.9095928895194194\n",
      "[[ 75  70]\n",
      " [106 747]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.52      0.46       145\n",
      "           1       0.91      0.88      0.89       853\n",
      "\n",
      "    accuracy                           0.82       998\n",
      "   macro avg       0.66      0.70      0.68       998\n",
      "weighted avg       0.84      0.82      0.83       998\n",
      "\n",
      "acc:  0.8236472945891784\n",
      "pre:  0.9143206854345165\n",
      "rec:  0.8757327080890973\n",
      "ma F1:  0.6773667389148084\n",
      "mi F1:  0.8236472945891784\n",
      "we F1:  0.8314837529288768\n",
      "Loss:  0.06416771560907364\n",
      "Loss:  0.06701550632715225\n",
      "Loss:  0.05059811472892761\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.06643160432577133\n",
      "Loss:  0.042527936398983\n",
      "Loss:  0.07375826686620712\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.04910816624760628\n",
      "Loss:  0.04095904529094696\n",
      "Loss:  0.04640396684408188\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  1.0310879945755005\n",
      "Eval Loss:  0.1095348596572876\n",
      "Eval Loss:  0.05931699275970459\n",
      "[[19098   921]\n",
      " [ 1850 10024]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     20019\n",
      "           1       0.92      0.84      0.88     11874\n",
      "\n",
      "    accuracy                           0.91     31893\n",
      "   macro avg       0.91      0.90      0.91     31893\n",
      "weighted avg       0.91      0.91      0.91     31893\n",
      "\n",
      "acc:  0.9131157307246104\n",
      "pre:  0.9158519872087711\n",
      "rec:  0.8441974060973556\n",
      "ma F1:  0.9054631497706579\n",
      "mi F1:  0.9131157307246104\n",
      "we F1:  0.9123322562143368\n",
      "[[ 79  66]\n",
      " [137 716]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.54      0.44       145\n",
      "           1       0.92      0.84      0.88       853\n",
      "\n",
      "    accuracy                           0.80       998\n",
      "   macro avg       0.64      0.69      0.66       998\n",
      "weighted avg       0.84      0.80      0.81       998\n",
      "\n",
      "acc:  0.7965931863727455\n",
      "pre:  0.9156010230179028\n",
      "rec:  0.839390386869871\n",
      "ma F1:  0.656757054393589\n",
      "mi F1:  0.7965931863727453\n",
      "we F1:  0.812179317252683\n",
      "Loss:  0.04517856240272522\n",
      "Loss:  0.04307871311903\n",
      "Loss:  0.04256916418671608\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.06982198357582092\n",
      "Loss:  0.06436996161937714\n",
      "Loss:  0.060651615262031555\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.04332633689045906\n",
      "Loss:  0.06719105690717697\n",
      "Loss:  0.048885151743888855\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  1 Training Time 5756.250954389572 Best Test Acc:  0.8496993987975952\n",
      "test subjects:  ['./seg\\\\a02', './seg\\\\x14']\n",
      "*********\n",
      "33295 1018\n",
      "31873 1018\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.6054941415786743\n",
      "Eval Loss:  0.6315019130706787\n",
      "Eval Loss:  0.6349784731864929\n",
      "[[19891   114]\n",
      " [11711   157]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.99      0.77     20005\n",
      "           1       0.58      0.01      0.03     11868\n",
      "\n",
      "    accuracy                           0.63     31873\n",
      "   macro avg       0.60      0.50      0.40     31873\n",
      "weighted avg       0.61      0.63      0.49     31873\n",
      "\n",
      "acc:  0.6289963291814389\n",
      "pre:  0.5793357933579336\n",
      "rec:  0.013228850690933602\n",
      "ma F1:  0.3983657288681955\n",
      "mi F1:  0.6289963291814389\n",
      "we F1:  0.4934625766815563\n",
      "[[158   1]\n",
      " [850   9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.99      0.27       159\n",
      "           1       0.90      0.01      0.02       859\n",
      "\n",
      "    accuracy                           0.16      1018\n",
      "   macro avg       0.53      0.50      0.15      1018\n",
      "weighted avg       0.78      0.16      0.06      1018\n",
      "\n",
      "acc:  0.16404715127701375\n",
      "pre:  0.9\n",
      "rec:  0.010477299185098952\n",
      "ma F1:  0.14574662047897544\n",
      "mi F1:  0.16404715127701375\n",
      "we F1:  0.05977097243450009\n",
      "Subject 2 Current Train Acc:  0.6289963291814389 Current Test Acc:  0.16404715127701375\n",
      "Loss:  0.15882083773612976\n",
      "Loss:  0.16145193576812744\n",
      "Loss:  0.16006767749786377\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.1520323008298874\n",
      "Loss:  0.1484648436307907\n",
      "Loss:  0.1225985661149025\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.1131606251001358\n",
      "Loss:  0.105470672249794\n",
      "Loss:  0.11543414741754532\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.10580718517303467\n",
      "Eval Loss:  0.06313169002532959\n",
      "Eval Loss:  0.04550731182098389\n",
      "[[17178  2827]\n",
      " [ 2572  9296]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86     20005\n",
      "           1       0.77      0.78      0.77     11868\n",
      "\n",
      "    accuracy                           0.83     31873\n",
      "   macro avg       0.82      0.82      0.82     31873\n",
      "weighted avg       0.83      0.83      0.83     31873\n",
      "\n",
      "acc:  0.8306089793869419\n",
      "pre:  0.7668068959828426\n",
      "rec:  0.7832827772160431\n",
      "ma F1:  0.819575229446212\n",
      "mi F1:  0.8306089793869419\n",
      "we F1:  0.83096594541522\n",
      "[[145  14]\n",
      " [672 187]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.91      0.30       159\n",
      "           1       0.93      0.22      0.35       859\n",
      "\n",
      "    accuracy                           0.33      1018\n",
      "   macro avg       0.55      0.56      0.32      1018\n",
      "weighted avg       0.81      0.33      0.34      1018\n",
      "\n",
      "acc:  0.32612966601178783\n",
      "pre:  0.9303482587064676\n",
      "rec:  0.21769499417927823\n",
      "ma F1:  0.3249806681101145\n",
      "mi F1:  0.32612966601178783\n",
      "we F1:  0.34413063313800407\n",
      "Subject 2 Current Train Acc:  0.8306089793869419 Current Test Acc:  0.32612966601178783\n",
      "Loss:  0.09484350681304932\n",
      "Loss:  0.09355710446834564\n",
      "Loss:  0.09974093735218048\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.0623503178358078\n",
      "Loss:  0.06944174319505692\n",
      "Loss:  0.09801171720027924\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.06985166668891907\n",
      "Loss:  0.0823247879743576\n",
      "Loss:  0.10364651679992676\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.03034234046936035\n",
      "Eval Loss:  0.028363943099975586\n",
      "Eval Loss:  0.02538895606994629\n",
      "[[18590  1415]\n",
      " [ 2766  9102]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90     20005\n",
      "           1       0.87      0.77      0.81     11868\n",
      "\n",
      "    accuracy                           0.87     31873\n",
      "   macro avg       0.87      0.85      0.86     31873\n",
      "weighted avg       0.87      0.87      0.87     31873\n",
      "\n",
      "acc:  0.8688231418441942\n",
      "pre:  0.8654559284967196\n",
      "rec:  0.7669362992922144\n",
      "ma F1:  0.8560687884002995\n",
      "mi F1:  0.8688231418441942\n",
      "we F1:  0.8670070445104355\n",
      "[[148  11]\n",
      " [662 197]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.93      0.31       159\n",
      "           1       0.95      0.23      0.37       859\n",
      "\n",
      "    accuracy                           0.34      1018\n",
      "   macro avg       0.56      0.58      0.34      1018\n",
      "weighted avg       0.83      0.34      0.36      1018\n",
      "\n",
      "acc:  0.3388998035363458\n",
      "pre:  0.9471153846153846\n",
      "rec:  0.22933643771827705\n",
      "ma F1:  0.3373645813082792\n",
      "mi F1:  0.3388998035363458\n",
      "we F1:  0.35929632742351536\n",
      "Subject 2 Current Train Acc:  0.8688231418441942 Current Test Acc:  0.3388998035363458\n",
      "Loss:  0.09776400029659271\n",
      "Loss:  0.0762687474489212\n",
      "Loss:  0.08294691890478134\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.06935964524745941\n",
      "Loss:  0.08768704533576965\n",
      "Loss:  0.07953168451786041\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.07611466944217682\n",
      "Loss:  0.09249408543109894\n",
      "Loss:  0.08520997315645218\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.023571252822875977\n",
      "Eval Loss:  0.03007352352142334\n",
      "Eval Loss:  0.030427932739257812\n",
      "[[18318  1687]\n",
      " [ 2011  9857]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91     20005\n",
      "           1       0.85      0.83      0.84     11868\n",
      "\n",
      "    accuracy                           0.88     31873\n",
      "   macro avg       0.88      0.87      0.88     31873\n",
      "weighted avg       0.88      0.88      0.88     31873\n",
      "\n",
      "acc:  0.8839770338531046\n",
      "pre:  0.8538634788634789\n",
      "rec:  0.8305527468823728\n",
      "ma F1:  0.8751811893159231\n",
      "mi F1:  0.8839770338531046\n",
      "we F1:  0.883640211535406\n",
      "[[138  21]\n",
      " [587 272]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.87      0.31       159\n",
      "           1       0.93      0.32      0.47       859\n",
      "\n",
      "    accuracy                           0.40      1018\n",
      "   macro avg       0.56      0.59      0.39      1018\n",
      "weighted avg       0.81      0.40      0.45      1018\n",
      "\n",
      "acc:  0.4027504911591356\n",
      "pre:  0.9283276450511946\n",
      "rec:  0.31664726426076834\n",
      "ma F1:  0.3922197083961789\n",
      "mi F1:  0.4027504911591356\n",
      "we F1:  0.447231260142967\n",
      "Subject 2 Current Train Acc:  0.8839770338531046 Current Test Acc:  0.4027504911591356\n",
      "Loss:  0.07738114148378372\n",
      "Loss:  0.08904402703046799\n",
      "Loss:  0.08686434477567673\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.06277800351381302\n",
      "Loss:  0.07582883536815643\n",
      "Loss:  0.08319912105798721\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.07103392481803894\n",
      "Loss:  0.09455851465463638\n",
      "Loss:  0.0921698659658432\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.023278236389160156\n",
      "Eval Loss:  0.0319441556930542\n",
      "Eval Loss:  0.01313924789428711\n",
      "[[18925  1080]\n",
      " [ 2580  9288]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91     20005\n",
      "           1       0.90      0.78      0.84     11868\n",
      "\n",
      "    accuracy                           0.89     31873\n",
      "   macro avg       0.89      0.86      0.87     31873\n",
      "weighted avg       0.89      0.89      0.88     31873\n",
      "\n",
      "acc:  0.8851692655225426\n",
      "pre:  0.8958333333333334\n",
      "rec:  0.782608695652174\n",
      "ma F1:  0.8736152628973987\n",
      "mi F1:  0.8851692655225425\n",
      "we F1:  0.883370883874964\n",
      "[[151   8]\n",
      " [692 167]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.95      0.30       159\n",
      "           1       0.95      0.19      0.32       859\n",
      "\n",
      "    accuracy                           0.31      1018\n",
      "   macro avg       0.57      0.57      0.31      1018\n",
      "weighted avg       0.83      0.31      0.32      1018\n",
      "\n",
      "acc:  0.31237721021611004\n",
      "pre:  0.9542857142857143\n",
      "rec:  0.19441210710128057\n",
      "ma F1:  0.3122073068563067\n",
      "mi F1:  0.31237721021611004\n",
      "we F1:  0.31964057884770064\n",
      "Loss:  0.08748581260442734\n",
      "Loss:  0.07481790333986282\n",
      "Loss:  0.0879392996430397\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.07836300879716873\n",
      "Loss:  0.08772563934326172\n",
      "Loss:  0.08398536592721939\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.052325375378131866\n",
      "Loss:  0.062378425151109695\n",
      "Loss:  0.08638034015893936\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.034278035163879395\n",
      "Eval Loss:  0.028468728065490723\n",
      "Eval Loss:  0.007227182388305664\n",
      "[[18869  1136]\n",
      " [ 2266  9602]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92     20005\n",
      "           1       0.89      0.81      0.85     11868\n",
      "\n",
      "    accuracy                           0.89     31873\n",
      "   macro avg       0.89      0.88      0.88     31873\n",
      "weighted avg       0.89      0.89      0.89     31873\n",
      "\n",
      "acc:  0.8932638910676748\n",
      "pre:  0.8942074874278264\n",
      "rec:  0.8090663970340412\n",
      "ma F1:  0.8834078686652729\n",
      "mi F1:  0.8932638910676748\n",
      "we F1:  0.8920620668187579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[154   5]\n",
      " [721 138]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.97      0.30       159\n",
      "           1       0.97      0.16      0.28       859\n",
      "\n",
      "    accuracy                           0.29      1018\n",
      "   macro avg       0.57      0.56      0.29      1018\n",
      "weighted avg       0.84      0.29      0.28      1018\n",
      "\n",
      "acc:  0.2868369351669941\n",
      "pre:  0.965034965034965\n",
      "rec:  0.16065192083818394\n",
      "ma F1:  0.2866607211109695\n",
      "mi F1:  0.2868369351669941\n",
      "we F1:  0.27895135615989525\n",
      "Loss:  0.06788226962089539\n",
      "Loss:  0.1084505096077919\n",
      "Loss:  0.05324641615152359\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.07953336834907532\n",
      "Loss:  0.05617992952466011\n",
      "Loss:  0.09775302559137344\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.07792703807353973\n",
      "Loss:  0.09677634388208389\n",
      "Loss:  0.09355318546295166\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.021998882293701172\n",
      "Eval Loss:  0.03615772724151611\n",
      "Eval Loss:  0.0037755966186523438\n",
      "[[19026   979]\n",
      " [ 2254  9614]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     20005\n",
      "           1       0.91      0.81      0.86     11868\n",
      "\n",
      "    accuracy                           0.90     31873\n",
      "   macro avg       0.90      0.88      0.89     31873\n",
      "weighted avg       0.90      0.90      0.90     31873\n",
      "\n",
      "acc:  0.8985661845449127\n",
      "pre:  0.9075804776739356\n",
      "rec:  0.810077519379845\n",
      "ma F1:  0.8888761523027895\n",
      "mi F1:  0.8985661845449127\n",
      "we F1:  0.8972535208062059\n",
      "[[153   6]\n",
      " [732 127]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.96      0.29       159\n",
      "           1       0.95      0.15      0.26       859\n",
      "\n",
      "    accuracy                           0.28      1018\n",
      "   macro avg       0.56      0.56      0.27      1018\n",
      "weighted avg       0.83      0.28      0.26      1018\n",
      "\n",
      "acc:  0.275049115913556\n",
      "pre:  0.9548872180451128\n",
      "rec:  0.1478463329452852\n",
      "ma F1:  0.2745759176863181\n",
      "mi F1:  0.275049115913556\n",
      "we F1:  0.2618359654145296\n",
      "Loss:  0.04203549027442932\n",
      "Loss:  0.0925629660487175\n",
      "Loss:  0.0504223108291626\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.060635823756456375\n",
      "Loss:  0.044047143310308456\n",
      "Loss:  0.08270176500082016\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.09554596990346909\n",
      "Loss:  0.061083439737558365\n",
      "Loss:  0.0747702494263649\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.024509191513061523\n",
      "Eval Loss:  0.03432595729827881\n",
      "Eval Loss:  0.002510547637939453\n",
      "[[19048   957]\n",
      " [ 2143  9725]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     20005\n",
      "           1       0.91      0.82      0.86     11868\n",
      "\n",
      "    accuracy                           0.90     31873\n",
      "   macro avg       0.90      0.89      0.89     31873\n",
      "weighted avg       0.90      0.90      0.90     31873\n",
      "\n",
      "acc:  0.9027389953879459\n",
      "pre:  0.9104100355738626\n",
      "rec:  0.8194304010785305\n",
      "ma F1:  0.8936388459560256\n",
      "mi F1:  0.9027389953879459\n",
      "we F1:  0.9015813447147445\n",
      "[[154   5]\n",
      " [729 130]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.97      0.30       159\n",
      "           1       0.96      0.15      0.26       859\n",
      "\n",
      "    accuracy                           0.28      1018\n",
      "   macro avg       0.57      0.56      0.28      1018\n",
      "weighted avg       0.84      0.28      0.27      1018\n",
      "\n",
      "acc:  0.27897838899803534\n",
      "pre:  0.9629629629629629\n",
      "rec:  0.15133876600698487\n",
      "ma F1:  0.2785774145834702\n",
      "mi F1:  0.27897838899803534\n",
      "we F1:  0.2668823274919836\n",
      "Loss:  0.06742249429225922\n",
      "Loss:  0.054007284343242645\n",
      "Loss:  0.06537053734064102\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.052263662219047546\n",
      "Loss:  0.03326740488409996\n",
      "Loss:  0.07093405723571777\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.06650196760892868\n",
      "Loss:  0.09744051098823547\n",
      "Loss:  0.05379088968038559\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.01981973648071289\n",
      "Eval Loss:  0.05507814884185791\n",
      "Eval Loss:  0.0045833587646484375\n",
      "[[18492  1513]\n",
      " [ 1627 10241]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92     20005\n",
      "           1       0.87      0.86      0.87     11868\n",
      "\n",
      "    accuracy                           0.90     31873\n",
      "   macro avg       0.90      0.89      0.89     31873\n",
      "weighted avg       0.90      0.90      0.90     31873\n",
      "\n",
      "acc:  0.9014840146832742\n",
      "pre:  0.8712778628551983\n",
      "rec:  0.8629086619480957\n",
      "ma F1:  0.8944078327129172\n",
      "mi F1:  0.9014840146832742\n",
      "we F1:  0.9013862465649103\n",
      "[[148  11]\n",
      " [647 212]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.93      0.31       159\n",
      "           1       0.95      0.25      0.39       859\n",
      "\n",
      "    accuracy                           0.35      1018\n",
      "   macro avg       0.57      0.59      0.35      1018\n",
      "weighted avg       0.83      0.35      0.38      1018\n",
      "\n",
      "acc:  0.35363457760314343\n",
      "pre:  0.9506726457399103\n",
      "rec:  0.2467986030267753\n",
      "ma F1:  0.35106972490573785\n",
      "mi F1:  0.35363457760314343\n",
      "we F1:  0.3791228012836109\n",
      "Loss:  0.08869908004999161\n",
      "Loss:  0.06942619383335114\n",
      "Loss:  0.05314207077026367\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.050351582467556\n",
      "Loss:  0.050276465713977814\n",
      "Loss:  0.06689628213644028\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.04689744487404823\n",
      "Loss:  0.0654369443655014\n",
      "Loss:  0.058737415820360184\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.05947697162628174\n",
      "Eval Loss:  0.058611154556274414\n",
      "Eval Loss:  0.002115488052368164\n",
      "[[18725  1280]\n",
      " [ 1758 10110]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92     20005\n",
      "           1       0.89      0.85      0.87     11868\n",
      "\n",
      "    accuracy                           0.90     31873\n",
      "   macro avg       0.90      0.89      0.90     31873\n",
      "weighted avg       0.90      0.90      0.90     31873\n",
      "\n",
      "acc:  0.904684215480187\n",
      "pre:  0.887620719929763\n",
      "rec:  0.8518705763397371\n",
      "ma F1:  0.8971718501476076\n",
      "mi F1:  0.904684215480187\n",
      "we F1:  0.9042673947455413\n",
      "[[128  31]\n",
      " [548 311]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.81      0.31       159\n",
      "           1       0.91      0.36      0.52       859\n",
      "\n",
      "    accuracy                           0.43      1018\n",
      "   macro avg       0.55      0.58      0.41      1018\n",
      "weighted avg       0.80      0.43      0.48      1018\n",
      "\n",
      "acc:  0.431237721021611\n",
      "pre:  0.9093567251461988\n",
      "rec:  0.3620488940628638\n",
      "ma F1:  0.41224428744509317\n",
      "mi F1:  0.431237721021611\n",
      "we F1:  0.4848967656066368\n",
      "Subject 2 Current Train Acc:  0.904684215480187 Current Test Acc:  0.431237721021611\n",
      "Loss:  0.034345559775829315\n",
      "Loss:  0.0954091027379036\n",
      "Loss:  0.04024171829223633\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.06111966818571091\n",
      "Loss:  0.07336065173149109\n",
      "Loss:  0.08171681314706802\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.054952774196863174\n",
      "Loss:  0.04065050557255745\n",
      "Loss:  0.05029817670583725\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.03930389881134033\n",
      "Eval Loss:  0.07830572128295898\n",
      "Eval Loss:  0.0017361640930175781\n",
      "[[18821  1184]\n",
      " [ 1691 10177]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     20005\n",
      "           1       0.90      0.86      0.88     11868\n",
      "\n",
      "    accuracy                           0.91     31873\n",
      "   macro avg       0.91      0.90      0.90     31873\n",
      "weighted avg       0.91      0.91      0.91     31873\n",
      "\n",
      "acc:  0.9097982618517241\n",
      "pre:  0.8957838218466684\n",
      "rec:  0.8575160094371419\n",
      "ma F1:  0.902637213150428\n",
      "mi F1:  0.9097982618517241\n",
      "we F1:  0.9093782419892116\n",
      "[[141  18]\n",
      " [612 247]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.89      0.31       159\n",
      "           1       0.93      0.29      0.44       859\n",
      "\n",
      "    accuracy                           0.38      1018\n",
      "   macro avg       0.56      0.59      0.37      1018\n",
      "weighted avg       0.82      0.38      0.42      1018\n",
      "\n",
      "acc:  0.381139489194499\n",
      "pre:  0.9320754716981132\n",
      "rec:  0.28754365541327126\n",
      "ma F1:  0.37435615283761003\n",
      "mi F1:  0.381139489194499\n",
      "we F1:  0.4191517702887636\n",
      "Loss:  0.0534922257065773\n",
      "Loss:  0.05836859345436096\n",
      "Loss:  0.07984616607427597\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.06556835025548935\n",
      "Loss:  0.08232729136943817\n",
      "Loss:  0.06315406411886215\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.0693877637386322\n",
      "Loss:  0.03428598493337631\n",
      "Loss:  0.07286354899406433\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.024411678314208984\n",
      "Eval Loss:  0.03574824333190918\n",
      "Eval Loss:  0.001753091812133789\n",
      "[[18919  1086]\n",
      " [ 1626 10242]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     20005\n",
      "           1       0.90      0.86      0.88     11868\n",
      "\n",
      "    accuracy                           0.91     31873\n",
      "   macro avg       0.91      0.90      0.91     31873\n",
      "weighted avg       0.91      0.91      0.91     31873\n",
      "\n",
      "acc:  0.9149123082232611\n",
      "pre:  0.9041313559322034\n",
      "rec:  0.8629929221435794\n",
      "ma F1:  0.9081014478239264\n",
      "mi F1:  0.9149123082232611\n",
      "we F1:  0.9144884446050012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[144  15]\n",
      " [638 221]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.91      0.31       159\n",
      "           1       0.94      0.26      0.40       859\n",
      "\n",
      "    accuracy                           0.36      1018\n",
      "   macro avg       0.56      0.58      0.35      1018\n",
      "weighted avg       0.82      0.36      0.39      1018\n",
      "\n",
      "acc:  0.35854616895874264\n",
      "pre:  0.9364406779661016\n",
      "rec:  0.2572759022118743\n",
      "ma F1:  0.35485517689817986\n",
      "mi F1:  0.3585461689587427\n",
      "we F1:  0.38840965017602364\n",
      "Loss:  0.05965542048215866\n",
      "Loss:  0.08562697470188141\n",
      "Loss:  0.06587574630975723\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.06029229611158371\n",
      "Loss:  0.043907955288887024\n",
      "Loss:  0.0842248946428299\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.07114464789628983\n",
      "Loss:  0.028487619012594223\n",
      "Loss:  0.06536387652158737\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.033109426498413086\n",
      "Eval Loss:  0.05152738094329834\n",
      "Eval Loss:  0.0016636848449707031\n",
      "[[18238  1767]\n",
      " [ 1172 10696]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.93     20005\n",
      "           1       0.86      0.90      0.88     11868\n",
      "\n",
      "    accuracy                           0.91     31873\n",
      "   macro avg       0.90      0.91      0.90     31873\n",
      "weighted avg       0.91      0.91      0.91     31873\n",
      "\n",
      "acc:  0.9077902927242494\n",
      "pre:  0.8582203321832624\n",
      "rec:  0.901247050893158\n",
      "ma F1:  0.9023210372540122\n",
      "mi F1:  0.9077902927242494\n",
      "we F1:  0.9082217707148075\n",
      "[[138  21]\n",
      " [596 263]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.87      0.31       159\n",
      "           1       0.93      0.31      0.46       859\n",
      "\n",
      "    accuracy                           0.39      1018\n",
      "   macro avg       0.56      0.59      0.38      1018\n",
      "weighted avg       0.81      0.39      0.44      1018\n",
      "\n",
      "acc:  0.393909626719057\n",
      "pre:  0.926056338028169\n",
      "rec:  0.3061699650756694\n",
      "ma F1:  0.3846315123263568\n",
      "mi F1:  0.393909626719057\n",
      "we F1:  0.4365889529254781\n",
      "Loss:  0.056100424379110336\n",
      "Loss:  0.06470392644405365\n",
      "Loss:  0.06184452027082443\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.057625263929367065\n",
      "Loss:  0.051592595875263214\n",
      "Loss:  0.06509336084127426\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.08554226160049438\n",
      "Loss:  0.04873548075556755\n",
      "Loss:  0.0439702533185482\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.026903390884399414\n",
      "Eval Loss:  0.04327845573425293\n",
      "Eval Loss:  0.0012898445129394531\n",
      "[[18600  1405]\n",
      " [ 1329 10539]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     20005\n",
      "           1       0.88      0.89      0.89     11868\n",
      "\n",
      "    accuracy                           0.91     31873\n",
      "   macro avg       0.91      0.91      0.91     31873\n",
      "weighted avg       0.91      0.91      0.91     31873\n",
      "\n",
      "acc:  0.9142220688356917\n",
      "pre:  0.8823677160080375\n",
      "rec:  0.8880182002022244\n",
      "ma F1:  0.9083604884898652\n",
      "mi F1:  0.9142220688356918\n",
      "we F1:  0.9142773324638126\n",
      "[[139  20]\n",
      " [593 266]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.87      0.31       159\n",
      "           1       0.93      0.31      0.46       859\n",
      "\n",
      "    accuracy                           0.40      1018\n",
      "   macro avg       0.56      0.59      0.39      1018\n",
      "weighted avg       0.81      0.40      0.44      1018\n",
      "\n",
      "acc:  0.39783889980353637\n",
      "pre:  0.9300699300699301\n",
      "rec:  0.309662398137369\n",
      "ma F1:  0.388318899818172\n",
      "mi F1:  0.39783889980353637\n",
      "we F1:  0.4407913406823848\n",
      "Loss:  0.05815954506397247\n",
      "Loss:  0.09806661307811737\n",
      "Loss:  0.058970533311367035\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.05527426302433014\n",
      "Loss:  0.07721784710884094\n",
      "Loss:  0.05719277635216713\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.07163341343402863\n",
      "Loss:  0.057652488350868225\n",
      "Loss:  0.04380827024579048\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.0625307559967041\n",
      "Eval Loss:  0.051410675048828125\n",
      "Eval Loss:  0.0018286705017089844\n",
      "[[18458  1547]\n",
      " [ 1192 10676]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     20005\n",
      "           1       0.87      0.90      0.89     11868\n",
      "\n",
      "    accuracy                           0.91     31873\n",
      "   macro avg       0.91      0.91      0.91     31873\n",
      "weighted avg       0.91      0.91      0.91     31873\n",
      "\n",
      "acc:  0.9140651962476077\n",
      "pre:  0.8734353268428373\n",
      "rec:  0.899561846983485\n",
      "ma F1:  0.9086176771604153\n",
      "mi F1:  0.9140651962476077\n",
      "we F1:  0.9143137016801383\n",
      "[[132  27]\n",
      " [584 275]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.83      0.30       159\n",
      "           1       0.91      0.32      0.47       859\n",
      "\n",
      "    accuracy                           0.40      1018\n",
      "   macro avg       0.55      0.58      0.39      1018\n",
      "weighted avg       0.80      0.40      0.45      1018\n",
      "\n",
      "acc:  0.39980353634577603\n",
      "pre:  0.9105960264900662\n",
      "rec:  0.320139697322468\n",
      "ma F1:  0.38772191460563554\n",
      "mi F1:  0.399803536345776\n",
      "we F1:  0.446862720326603\n",
      "Loss:  0.04606710374355316\n",
      "Loss:  0.04083392769098282\n",
      "Loss:  0.0767495408654213\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.09634340554475784\n",
      "Loss:  0.044067904353141785\n",
      "Loss:  0.07241830229759216\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.060408029705286026\n",
      "Loss:  0.06549403071403503\n",
      "Loss:  0.0742417722940445\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.049469828605651855\n",
      "Eval Loss:  0.06367170810699463\n",
      "Eval Loss:  0.0012254714965820312\n",
      "[[18602  1403]\n",
      " [ 1244 10624]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93     20005\n",
      "           1       0.88      0.90      0.89     11868\n",
      "\n",
      "    accuracy                           0.92     31873\n",
      "   macro avg       0.91      0.91      0.91     31873\n",
      "weighted avg       0.92      0.92      0.92     31873\n",
      "\n",
      "acc:  0.9169516518683525\n",
      "pre:  0.8833458052714726\n",
      "rec:  0.895180316818335\n",
      "ma F1:  0.9114006317180797\n",
      "mi F1:  0.9169516518683525\n",
      "we F1:  0.9170622826284294\n",
      "[[139  20]\n",
      " [587 272]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.87      0.31       159\n",
      "           1       0.93      0.32      0.47       859\n",
      "\n",
      "    accuracy                           0.40      1018\n",
      "   macro avg       0.56      0.60      0.39      1018\n",
      "weighted avg       0.82      0.40      0.45      1018\n",
      "\n",
      "acc:  0.4037328094302554\n",
      "pre:  0.9315068493150684\n",
      "rec:  0.31664726426076834\n",
      "ma F1:  0.3933783936346189\n",
      "mi F1:  0.40373280943025536\n",
      "we F1:  0.44787531887481113\n",
      "Loss:  0.09171097725629807\n",
      "Loss:  0.056512944400310516\n",
      "Loss:  0.057836417108774185\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.045204490423202515\n",
      "Loss:  0.058893389999866486\n",
      "Loss:  0.07432533800601959\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.070248082280159\n",
      "Loss:  0.03710566461086273\n",
      "Loss:  0.05263923108577728\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.020961880683898926\n",
      "Eval Loss:  0.09397029876708984\n",
      "Eval Loss:  0.0010631084442138672\n",
      "[[18282  1723]\n",
      " [  997 10871]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93     20005\n",
      "           1       0.86      0.92      0.89     11868\n",
      "\n",
      "    accuracy                           0.91     31873\n",
      "   macro avg       0.91      0.91      0.91     31873\n",
      "weighted avg       0.92      0.91      0.92     31873\n",
      "\n",
      "acc:  0.9146613120823267\n",
      "pre:  0.8631888200730506\n",
      "rec:  0.9159925851027975\n",
      "ma F1:  0.909783872216943\n",
      "mi F1:  0.9146613120823267\n",
      "we F1:  0.9151391182275526\n",
      "[[123  36]\n",
      " [503 356]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.77      0.31       159\n",
      "           1       0.91      0.41      0.57       859\n",
      "\n",
      "    accuracy                           0.47      1018\n",
      "   macro avg       0.55      0.59      0.44      1018\n",
      "weighted avg       0.80      0.47      0.53      1018\n",
      "\n",
      "acc:  0.47053045186640474\n",
      "pre:  0.9081632653061225\n",
      "rec:  0.41443538998835855\n",
      "ma F1:  0.44126024021547094\n",
      "mi F1:  0.47053045186640474\n",
      "we F1:  0.529196498394242\n",
      "Subject 2 Current Train Acc:  0.9146613120823267 Current Test Acc:  0.47053045186640474\n",
      "Loss:  0.04589303955435753\n",
      "Loss:  0.060920193791389465\n",
      "Loss:  0.04306023195385933\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.07290041446685791\n",
      "Loss:  0.07001364231109619\n",
      "Loss:  0.054990965873003006\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.06353986263275146\n",
      "Loss:  0.0484452024102211\n",
      "Loss:  0.07471291720867157\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.04386186599731445\n",
      "Eval Loss:  0.054401397705078125\n",
      "Eval Loss:  0.0015773773193359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18006  1999]\n",
      " [  851 11017]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93     20005\n",
      "           1       0.85      0.93      0.89     11868\n",
      "\n",
      "    accuracy                           0.91     31873\n",
      "   macro avg       0.90      0.91      0.91     31873\n",
      "weighted avg       0.91      0.91      0.91     31873\n",
      "\n",
      "acc:  0.9105826247921438\n",
      "pre:  0.8464197910264291\n",
      "rec:  0.9282945736434108\n",
      "ma F1:  0.9060660765007842\n",
      "mi F1:  0.9105826247921438\n",
      "we F1:  0.9113245045229322\n",
      "[[120  39]\n",
      " [482 377]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.75      0.32       159\n",
      "           1       0.91      0.44      0.59       859\n",
      "\n",
      "    accuracy                           0.49      1018\n",
      "   macro avg       0.55      0.60      0.45      1018\n",
      "weighted avg       0.80      0.49      0.55      1018\n",
      "\n",
      "acc:  0.48821218074656186\n",
      "pre:  0.90625\n",
      "rec:  0.43888242142025613\n",
      "ma F1:  0.4533735281234702\n",
      "mi F1:  0.48821218074656186\n",
      "we F1:  0.5482647998595177\n",
      "Subject 2 Current Train Acc:  0.9105826247921438 Current Test Acc:  0.48821218074656186\n",
      "Loss:  0.07923904806375504\n",
      "Loss:  0.051958534866571426\n",
      "Loss:  0.039115678519010544\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.05198120325803757\n",
      "Loss:  0.034440528601408005\n",
      "Loss:  0.06593481451272964\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.058930642902851105\n",
      "Loss:  0.05367635190486908\n",
      "Loss:  0.05678698047995567\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.04137575626373291\n",
      "Eval Loss:  0.060724496841430664\n",
      "Eval Loss:  0.0012764930725097656\n",
      "[[18387  1618]\n",
      " [  935 10933]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94     20005\n",
      "           1       0.87      0.92      0.90     11868\n",
      "\n",
      "    accuracy                           0.92     31873\n",
      "   macro avg       0.91      0.92      0.92     31873\n",
      "weighted avg       0.92      0.92      0.92     31873\n",
      "\n",
      "acc:  0.919900856524331\n",
      "pre:  0.8710859692454784\n",
      "rec:  0.921216717222784\n",
      "ma F1:  0.9152665158514247\n",
      "mi F1:  0.919900856524331\n",
      "we F1:  0.9203254949305014\n",
      "[[120  39]\n",
      " [469 390]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.75      0.32       159\n",
      "           1       0.91      0.45      0.61       859\n",
      "\n",
      "    accuracy                           0.50      1018\n",
      "   macro avg       0.56      0.60      0.46      1018\n",
      "weighted avg       0.80      0.50      0.56      1018\n",
      "\n",
      "acc:  0.5009823182711198\n",
      "pre:  0.9090909090909091\n",
      "rec:  0.4540162980209546\n",
      "ma F1:  0.46322283854253166\n",
      "mi F1:  0.5009823182711198\n",
      "we F1:  0.5611177859870196\n",
      "Subject 2 Current Train Acc:  0.919900856524331 Current Test Acc:  0.5009823182711198\n",
      "Loss:  0.03370542451739311\n",
      "Loss:  0.07064903527498245\n",
      "Loss:  0.0589672327041626\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.0405355803668499\n",
      "Loss:  0.03763590008020401\n",
      "Loss:  0.04818526655435562\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.0603875070810318\n",
      "Loss:  0.06361313164234161\n",
      "Loss:  0.053887855261564255\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.03054189682006836\n",
      "Eval Loss:  0.02029252052307129\n",
      "Eval Loss:  0.002110719680786133\n",
      "[[18062  1943]\n",
      " [  880 10988]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93     20005\n",
      "           1       0.85      0.93      0.89     11868\n",
      "\n",
      "    accuracy                           0.91     31873\n",
      "   macro avg       0.90      0.91      0.91     31873\n",
      "weighted avg       0.91      0.91      0.91     31873\n",
      "\n",
      "acc:  0.9114297367677971\n",
      "pre:  0.8497409326424871\n",
      "rec:  0.9258510279743849\n",
      "ma F1:  0.9068408233123525\n",
      "mi F1:  0.9114297367677971\n",
      "we F1:  0.9121193063187073\n",
      "[[134  25]\n",
      " [528 331]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.84      0.33       159\n",
      "           1       0.93      0.39      0.54       859\n",
      "\n",
      "    accuracy                           0.46      1018\n",
      "   macro avg       0.57      0.61      0.44      1018\n",
      "weighted avg       0.82      0.46      0.51      1018\n",
      "\n",
      "acc:  0.4567779960707269\n",
      "pre:  0.9297752808988764\n",
      "rec:  0.3853317811408615\n",
      "ma F1:  0.4356435742820911\n",
      "mi F1:  0.4567779960707269\n",
      "we F1:  0.5107405044955182\n",
      "Loss:  0.04983178526163101\n",
      "Loss:  0.06561034172773361\n",
      "Loss:  0.05482226237654686\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.08818919956684113\n",
      "Loss:  0.06752035021781921\n",
      "Loss:  0.04567527770996094\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.05881832167506218\n",
      "Loss:  0.07348910719156265\n",
      "Loss:  0.07444694638252258\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  2 Training Time 5770.963615894318 Best Test Acc:  0.5009823182711198\n",
      "test subjects:  ['./seg\\\\a03', './seg\\\\x19']\n",
      "*********\n",
      "33307 1006\n",
      "31885 1006\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.8116292953491211\n",
      "Eval Loss:  0.7895532250404358\n",
      "Eval Loss:  0.8268351554870605\n",
      "[[    0 19811]\n",
      " [    0 12074]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     19811\n",
      "           1       0.38      1.00      0.55     12074\n",
      "\n",
      "    accuracy                           0.38     31885\n",
      "   macro avg       0.19      0.50      0.27     31885\n",
      "weighted avg       0.14      0.38      0.21     31885\n",
      "\n",
      "acc:  0.37867335737807745\n",
      "pre:  0.37867335737807745\n",
      "rec:  1.0\n",
      "ma F1:  0.27466502877681476\n",
      "mi F1:  0.37867335737807745\n",
      "we F1:  0.2080166572025254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\install\\envs\\pytorch-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 353]\n",
      " [  0 653]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       353\n",
      "           1       0.65      1.00      0.79       653\n",
      "\n",
      "    accuracy                           0.65      1006\n",
      "   macro avg       0.32      0.50      0.39      1006\n",
      "weighted avg       0.42      0.65      0.51      1006\n",
      "\n",
      "acc:  0.6491053677932406\n",
      "pre:  0.6491053677932406\n",
      "rec:  1.0\n",
      "ma F1:  0.3936106088004822\n",
      "mi F1:  0.6491053677932406\n",
      "we F1:  0.5109895179855166\n",
      "Subject 3 Current Train Acc:  0.37867335737807745 Current Test Acc:  0.6491053677932406\n",
      "Loss:  0.17041602730751038\n",
      "Loss:  0.16309350728988647\n",
      "Loss:  0.16367478668689728\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.1280374675989151\n",
      "Loss:  0.13905373215675354\n",
      "Loss:  0.13234946131706238\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.14415547251701355\n",
      "Loss:  0.09496816992759705\n",
      "Loss:  0.10203898698091507\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.06100630760192871\n",
      "Eval Loss:  0.034014225006103516\n",
      "Eval Loss:  0.023370981216430664\n",
      "[[16634  3177]\n",
      " [ 2548  9526]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85     19811\n",
      "           1       0.75      0.79      0.77     12074\n",
      "\n",
      "    accuracy                           0.82     31885\n",
      "   macro avg       0.81      0.81      0.81     31885\n",
      "weighted avg       0.82      0.82      0.82     31885\n",
      "\n",
      "acc:  0.8204484867492552\n",
      "pre:  0.7499015980477053\n",
      "rec:  0.7889680304787146\n",
      "ma F1:  0.8110588554902938\n",
      "mi F1:  0.8204484867492552\n",
      "we F1:  0.821279392497973\n",
      "[[283  70]\n",
      " [100 553]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.80      0.77       353\n",
      "           1       0.89      0.85      0.87       653\n",
      "\n",
      "    accuracy                           0.83      1006\n",
      "   macro avg       0.81      0.82      0.82      1006\n",
      "weighted avg       0.84      0.83      0.83      1006\n",
      "\n",
      "acc:  0.831013916500994\n",
      "pre:  0.8876404494382022\n",
      "rec:  0.8468606431852986\n",
      "ma F1:  0.8178964495025216\n",
      "mi F1:  0.831013916500994\n",
      "we F1:  0.8324714128341577\n",
      "Subject 3 Current Train Acc:  0.8204484867492552 Current Test Acc:  0.831013916500994\n",
      "Loss:  0.11793477833271027\n",
      "Loss:  0.07810039818286896\n",
      "Loss:  0.0961647629737854\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.0708143413066864\n",
      "Loss:  0.06891380250453949\n",
      "Loss:  0.05847172439098358\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.0906609520316124\n",
      "Loss:  0.1056654304265976\n",
      "Loss:  0.0852930024266243\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.014798641204833984\n",
      "Eval Loss:  0.0175936222076416\n",
      "Eval Loss:  0.008809328079223633\n",
      "[[19123   688]\n",
      " [ 4956  7118]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.97      0.87     19811\n",
      "           1       0.91      0.59      0.72     12074\n",
      "\n",
      "    accuracy                           0.82     31885\n",
      "   macro avg       0.85      0.78      0.79     31885\n",
      "weighted avg       0.84      0.82      0.81     31885\n",
      "\n",
      "acc:  0.822988866238043\n",
      "pre:  0.9118626697412247\n",
      "rec:  0.5895312241179393\n",
      "ma F1:  0.793751183336061\n",
      "mi F1:  0.822988866238043\n",
      "we F1:  0.8125943280768052\n",
      "[[342  11]\n",
      " [312 341]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.97      0.68       353\n",
      "           1       0.97      0.52      0.68       653\n",
      "\n",
      "    accuracy                           0.68      1006\n",
      "   macro avg       0.75      0.75      0.68      1006\n",
      "weighted avg       0.81      0.68      0.68      1006\n",
      "\n",
      "acc:  0.6789264413518886\n",
      "pre:  0.96875\n",
      "rec:  0.5222052067381318\n",
      "ma F1:  0.6789261240964988\n",
      "mi F1:  0.6789264413518886\n",
      "we F1:  0.6788309474794899\n",
      "Loss:  0.07568065822124481\n",
      "Loss:  0.07326051592826843\n",
      "Loss:  0.05422325059771538\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.10711060464382172\n",
      "Loss:  0.08805681765079498\n",
      "Loss:  0.11858906596899033\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.08796608448028564\n",
      "Loss:  0.07502595335245132\n",
      "Loss:  0.09627697616815567\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.01184988021850586\n",
      "Eval Loss:  0.013236284255981445\n",
      "Eval Loss:  0.0067789554595947266\n",
      "[[18567  1244]\n",
      " [ 2910  9164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90     19811\n",
      "           1       0.88      0.76      0.82     12074\n",
      "\n",
      "    accuracy                           0.87     31885\n",
      "   macro avg       0.87      0.85      0.86     31885\n",
      "weighted avg       0.87      0.87      0.87     31885\n",
      "\n",
      "acc:  0.8697193037478438\n",
      "pre:  0.8804765564950039\n",
      "rec:  0.7589862514493954\n",
      "ma F1:  0.8573098074575831\n",
      "mi F1:  0.8697193037478438\n",
      "we F1:  0.8675206202617676\n",
      "[[325  28]\n",
      " [139 514]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.92      0.80       353\n",
      "           1       0.95      0.79      0.86       653\n",
      "\n",
      "    accuracy                           0.83      1006\n",
      "   macro avg       0.82      0.85      0.83      1006\n",
      "weighted avg       0.86      0.83      0.84      1006\n",
      "\n",
      "acc:  0.8339960238568589\n",
      "pre:  0.948339483394834\n",
      "rec:  0.7871362940275651\n",
      "ma F1:  0.8279223406380114\n",
      "mi F1:  0.8339960238568589\n",
      "we F1:  0.8375631076520551\n",
      "Subject 3 Current Train Acc:  0.8697193037478438 Current Test Acc:  0.8339960238568589\n",
      "Loss:  0.07499518990516663\n",
      "Loss:  0.07353493571281433\n",
      "Loss:  0.07767418026924133\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.09699740260839462\n",
      "Loss:  0.06243117153644562\n",
      "Loss:  0.09528031200170517\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.06234440207481384\n",
      "Loss:  0.06663470715284348\n",
      "Loss:  0.08129855990409851\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.008046388626098633\n",
      "Eval Loss:  0.007301807403564453\n",
      "Eval Loss:  0.0042133331298828125\n",
      "[[18775  1036]\n",
      " [ 2898  9176]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91     19811\n",
      "           1       0.90      0.76      0.82     12074\n",
      "\n",
      "    accuracy                           0.88     31885\n",
      "   macro avg       0.88      0.85      0.86     31885\n",
      "weighted avg       0.88      0.88      0.87     31885\n",
      "\n",
      "acc:  0.8766190998902306\n",
      "pre:  0.8985507246376812\n",
      "rec:  0.7599801225774391\n",
      "ma F1:  0.8643224398680445\n",
      "mi F1:  0.8766190998902306\n",
      "we F1:  0.8742338117392449\n",
      "[[323  30]\n",
      " [152 501]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.92      0.78       353\n",
      "           1       0.94      0.77      0.85       653\n",
      "\n",
      "    accuracy                           0.82      1006\n",
      "   macro avg       0.81      0.84      0.81      1006\n",
      "weighted avg       0.85      0.82      0.82      1006\n",
      "\n",
      "acc:  0.8190854870775348\n",
      "pre:  0.943502824858757\n",
      "rec:  0.7672281776416539\n",
      "ma F1:  0.8132385102493798\n",
      "mi F1:  0.8190854870775348\n",
      "we F1:  0.8230929655777308\n",
      "Loss:  0.06345544755458832\n",
      "Loss:  0.07166314125061035\n",
      "Loss:  0.11947295069694519\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.07154478132724762\n",
      "Loss:  0.07085444033145905\n",
      "Loss:  0.07394153624773026\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.06707888841629028\n",
      "Loss:  0.07828927040100098\n",
      "Loss:  0.09421291947364807\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.0066983699798583984\n",
      "Eval Loss:  0.00516819953918457\n",
      "Eval Loss:  0.0037012100219726562\n",
      "[[19024   787]\n",
      " [ 3062  9012]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91     19811\n",
      "           1       0.92      0.75      0.82     12074\n",
      "\n",
      "    accuracy                           0.88     31885\n",
      "   macro avg       0.89      0.85      0.87     31885\n",
      "weighted avg       0.88      0.88      0.88     31885\n",
      "\n",
      "acc:  0.8792849302179708\n",
      "pre:  0.9196856822124707\n",
      "rec:  0.7463972171608415\n",
      "ma F1:  0.8660807363573634\n",
      "mi F1:  0.8792849302179708\n",
      "we F1:  0.8762845765390973\n",
      "[[326  27]\n",
      " [183 470]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.92      0.76       353\n",
      "           1       0.95      0.72      0.82       653\n",
      "\n",
      "    accuracy                           0.79      1006\n",
      "   macro avg       0.79      0.82      0.79      1006\n",
      "weighted avg       0.84      0.79      0.80      1006\n",
      "\n",
      "acc:  0.7912524850894632\n",
      "pre:  0.9456740442655935\n",
      "rec:  0.7197549770290965\n",
      "ma F1:  0.7868859073943306\n",
      "mi F1:  0.7912524850894632\n",
      "we F1:  0.7959829442591901\n",
      "Loss:  0.053925804793834686\n",
      "Loss:  0.11693689227104187\n",
      "Loss:  0.07831670343875885\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.05612783506512642\n",
      "Loss:  0.09426159411668777\n",
      "Loss:  0.08528730273246765\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.05619509518146515\n",
      "Loss:  0.04055049642920494\n",
      "Loss:  0.052570946514606476\n",
      "17 **********\n",
      "epoch:  18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss:  0.0072176456451416016\n",
      "Eval Loss:  0.004019975662231445\n",
      "Eval Loss:  0.0031130313873291016\n",
      "[[18747  1064]\n",
      " [ 2254  9820]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     19811\n",
      "           1       0.90      0.81      0.86     12074\n",
      "\n",
      "    accuracy                           0.90     31885\n",
      "   macro avg       0.90      0.88      0.89     31885\n",
      "weighted avg       0.90      0.90      0.89     31885\n",
      "\n",
      "acc:  0.8959385290889132\n",
      "pre:  0.9022418228592429\n",
      "rec:  0.813317873115786\n",
      "ma F1:  0.8870877989258166\n",
      "mi F1:  0.8959385290889132\n",
      "we F1:  0.8947586961221736\n",
      "[[324  29]\n",
      " [179 474]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.92      0.76       353\n",
      "           1       0.94      0.73      0.82       653\n",
      "\n",
      "    accuracy                           0.79      1006\n",
      "   macro avg       0.79      0.82      0.79      1006\n",
      "weighted avg       0.84      0.79      0.80      1006\n",
      "\n",
      "acc:  0.7932405566600398\n",
      "pre:  0.9423459244532804\n",
      "rec:  0.7258805513016845\n",
      "ma F1:  0.7885392749733208\n",
      "mi F1:  0.7932405566600398\n",
      "we F1:  0.7979418383467586\n",
      "Loss:  0.08616378158330917\n",
      "Loss:  0.0764002874493599\n",
      "Loss:  0.05781872197985649\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.07774294167757034\n",
      "Loss:  0.0717676430940628\n",
      "Loss:  0.03625250607728958\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.07873262465000153\n",
      "Loss:  0.09224987030029297\n",
      "Loss:  0.1155323013663292\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.005433082580566406\n",
      "Eval Loss:  0.003099679946899414\n",
      "Eval Loss:  0.0025191307067871094\n",
      "[[19069   742]\n",
      " [ 2869  9205]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91     19811\n",
      "           1       0.93      0.76      0.84     12074\n",
      "\n",
      "    accuracy                           0.89     31885\n",
      "   macro avg       0.90      0.86      0.87     31885\n",
      "weighted avg       0.89      0.89      0.88     31885\n",
      "\n",
      "acc:  0.8867492551356437\n",
      "pre:  0.9254046446164673\n",
      "rec:  0.7623819778035448\n",
      "ma F1:  0.8747635364586241\n",
      "mi F1:  0.8867492551356437\n",
      "we F1:  0.8841647434136221\n",
      "[[336  17]\n",
      " [210 443]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.95      0.75       353\n",
      "           1       0.96      0.68      0.80       653\n",
      "\n",
      "    accuracy                           0.77      1006\n",
      "   macro avg       0.79      0.82      0.77      1006\n",
      "weighted avg       0.84      0.77      0.78      1006\n",
      "\n",
      "acc:  0.7743538767395626\n",
      "pre:  0.9630434782608696\n",
      "rec:  0.678407350689127\n",
      "ma F1:  0.771771969853696\n",
      "mi F1:  0.7743538767395626\n",
      "we F1:  0.7790109611224811\n",
      "Loss:  0.05701703950762749\n",
      "Loss:  0.059786103665828705\n",
      "Loss:  0.09302394837141037\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.06432244181632996\n",
      "Loss:  0.07839412987232208\n",
      "Loss:  0.06879573315382004\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.05695865675806999\n",
      "Loss:  0.06635956466197968\n",
      "Loss:  0.07083038985729218\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.005287647247314453\n",
      "Eval Loss:  0.0021669864654541016\n",
      "Eval Loss:  0.001886129379272461\n",
      "[[18579  1232]\n",
      " [ 1819 10255]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92     19811\n",
      "           1       0.89      0.85      0.87     12074\n",
      "\n",
      "    accuracy                           0.90     31885\n",
      "   macro avg       0.90      0.89      0.90     31885\n",
      "weighted avg       0.90      0.90      0.90     31885\n",
      "\n",
      "acc:  0.9043123725889917\n",
      "pre:  0.8927483241925656\n",
      "rec:  0.8493457015073712\n",
      "ma F1:  0.8973139052875534\n",
      "mi F1:  0.9043123725889917\n",
      "we F1:  0.9038188478045199\n",
      "[[305  48]\n",
      " [134 519]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.86      0.77       353\n",
      "           1       0.92      0.79      0.85       653\n",
      "\n",
      "    accuracy                           0.82      1006\n",
      "   macro avg       0.81      0.83      0.81      1006\n",
      "weighted avg       0.84      0.82      0.82      1006\n",
      "\n",
      "acc:  0.8190854870775348\n",
      "pre:  0.9153439153439153\n",
      "rec:  0.7947932618683001\n",
      "ma F1:  0.810510846166584\n",
      "mi F1:  0.8190854870775348\n",
      "we F1:  0.8225313708081038\n",
      "Loss:  0.11785519123077393\n",
      "Loss:  0.060857661068439484\n",
      "Loss:  0.07291633635759354\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.04357404261827469\n",
      "Loss:  0.049561139196157455\n",
      "Loss:  0.08065325021743774\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.04549466073513031\n",
      "Loss:  0.07724589854478836\n",
      "Loss:  0.040494367480278015\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.004651546478271484\n",
      "Eval Loss:  0.002163410186767578\n",
      "Eval Loss:  0.0017282962799072266\n",
      "[[18766  1045]\n",
      " [ 1996 10078]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93     19811\n",
      "           1       0.91      0.83      0.87     12074\n",
      "\n",
      "    accuracy                           0.90     31885\n",
      "   macro avg       0.90      0.89      0.90     31885\n",
      "weighted avg       0.90      0.90      0.90     31885\n",
      "\n",
      "acc:  0.9046259996863729\n",
      "pre:  0.9060505259372471\n",
      "rec:  0.8346861023687262\n",
      "ma F1:  0.8969770698026198\n",
      "mi F1:  0.9046259996863729\n",
      "we F1:  0.9037887376790699\n",
      "[[307  46]\n",
      " [133 520]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.87      0.77       353\n",
      "           1       0.92      0.80      0.85       653\n",
      "\n",
      "    accuracy                           0.82      1006\n",
      "   macro avg       0.81      0.83      0.81      1006\n",
      "weighted avg       0.84      0.82      0.83      1006\n",
      "\n",
      "acc:  0.8220675944333996\n",
      "pre:  0.9187279151943463\n",
      "rec:  0.7963246554364471\n",
      "ma F1:  0.8137166159597876\n",
      "mi F1:  0.8220675944333995\n",
      "we F1:  0.8254785574719173\n",
      "Loss:  0.10421079397201538\n",
      "Loss:  0.0746152400970459\n",
      "Loss:  0.0661701112985611\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.08381745964288712\n",
      "Loss:  0.08710907399654388\n",
      "Loss:  0.06316875666379929\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.06291191279888153\n",
      "Loss:  0.08389681577682495\n",
      "Loss:  0.06411653757095337\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.004362583160400391\n",
      "Eval Loss:  0.0017015933990478516\n",
      "Eval Loss:  0.0018572807312011719\n",
      "[[18595  1216]\n",
      " [ 1729 10345]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93     19811\n",
      "           1       0.89      0.86      0.88     12074\n",
      "\n",
      "    accuracy                           0.91     31885\n",
      "   macro avg       0.90      0.90      0.90     31885\n",
      "weighted avg       0.91      0.91      0.91     31885\n",
      "\n",
      "acc:  0.9076368198212326\n",
      "pre:  0.8948187873021365\n",
      "rec:  0.8567997349676992\n",
      "ma F1:  0.9010096530302887\n",
      "mi F1:  0.9076368198212326\n",
      "we F1:  0.9072247305407776\n",
      "[[293  60]\n",
      " [ 95 558]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.79       353\n",
      "           1       0.90      0.85      0.88       653\n",
      "\n",
      "    accuracy                           0.85      1006\n",
      "   macro avg       0.83      0.84      0.83      1006\n",
      "weighted avg       0.85      0.85      0.85      1006\n",
      "\n",
      "acc:  0.8459244532803181\n",
      "pre:  0.9029126213592233\n",
      "rec:  0.8545176110260337\n",
      "ma F1:  0.8344359961818242\n",
      "mi F1:  0.8459244532803181\n",
      "we F1:  0.8474417966706853\n",
      "Subject 3 Current Train Acc:  0.9076368198212326 Current Test Acc:  0.8459244532803181\n",
      "Loss:  0.04445274919271469\n",
      "Loss:  0.05579639598727226\n",
      "Loss:  0.0659942477941513\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.03762403130531311\n",
      "Loss:  0.07018960267305374\n",
      "Loss:  0.06905406713485718\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.05231988802552223\n",
      "Loss:  0.08288008719682693\n",
      "Loss:  0.06327114999294281\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.004180192947387695\n",
      "Eval Loss:  0.0012695789337158203\n",
      "Eval Loss:  0.0016226768493652344\n",
      "[[18930   881]\n",
      " [ 2248  9826]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92     19811\n",
      "           1       0.92      0.81      0.86     12074\n",
      "\n",
      "    accuracy                           0.90     31885\n",
      "   macro avg       0.91      0.88      0.89     31885\n",
      "weighted avg       0.90      0.90      0.90     31885\n",
      "\n",
      "acc:  0.9018660812294182\n",
      "pre:  0.917717381152517\n",
      "rec:  0.8138148086798078\n",
      "ma F1:  0.8931555722494521\n",
      "mi F1:  0.9018660812294182\n",
      "we F1:  0.9005581653929051\n",
      "[[316  37]\n",
      " [150 503]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.90      0.77       353\n",
      "           1       0.93      0.77      0.84       653\n",
      "\n",
      "    accuracy                           0.81      1006\n",
      "   macro avg       0.80      0.83      0.81      1006\n",
      "weighted avg       0.84      0.81      0.82      1006\n",
      "\n",
      "acc:  0.8141153081510935\n",
      "pre:  0.9314814814814815\n",
      "rec:  0.7702909647779479\n",
      "ma F1:  0.8074625383929659\n",
      "mi F1:  0.8141153081510935\n",
      "we F1:  0.8181354310530635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0469401590526104\n",
      "Loss:  0.06073326990008354\n",
      "Loss:  0.04116009920835495\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.07276497036218643\n",
      "Loss:  0.0491732582449913\n",
      "Loss:  0.07254333049058914\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.04603331536054611\n",
      "Loss:  0.04539577662944794\n",
      "Loss:  0.07203417271375656\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.0031783580780029297\n",
      "Eval Loss:  0.0011696815490722656\n",
      "Eval Loss:  0.0013804435729980469\n",
      "[[18808  1003]\n",
      " [ 1858 10216]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19811\n",
      "           1       0.91      0.85      0.88     12074\n",
      "\n",
      "    accuracy                           0.91     31885\n",
      "   macro avg       0.91      0.90      0.90     31885\n",
      "weighted avg       0.91      0.91      0.91     31885\n",
      "\n",
      "acc:  0.9102712874392348\n",
      "pre:  0.9105980925216152\n",
      "rec:  0.8461156203412291\n",
      "ma F1:  0.9032456419821882\n",
      "mi F1:  0.9102712874392349\n",
      "we F1:  0.9095721572174268\n",
      "[[309  44]\n",
      " [112 541]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.88      0.80       353\n",
      "           1       0.92      0.83      0.87       653\n",
      "\n",
      "    accuracy                           0.84      1006\n",
      "   macro avg       0.83      0.85      0.84      1006\n",
      "weighted avg       0.86      0.84      0.85      1006\n",
      "\n",
      "acc:  0.8449304174950298\n",
      "pre:  0.9247863247863248\n",
      "rec:  0.8284839203675345\n",
      "ma F1:  0.8362199596748945\n",
      "mi F1:  0.8449304174950298\n",
      "we F1:  0.8474834827181729\n",
      "Loss:  0.06823225319385529\n",
      "Loss:  0.050191834568977356\n",
      "Loss:  0.09324653446674347\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.07781179994344711\n",
      "Loss:  0.049984075129032135\n",
      "Loss:  0.06706781685352325\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.05414887145161629\n",
      "Loss:  0.06462960690259933\n",
      "Loss:  0.05270650237798691\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.002817869186401367\n",
      "Eval Loss:  0.0010578632354736328\n",
      "Eval Loss:  0.001293182373046875\n",
      "[[18779  1032]\n",
      " [ 1789 10285]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19811\n",
      "           1       0.91      0.85      0.88     12074\n",
      "\n",
      "    accuracy                           0.91     31885\n",
      "   macro avg       0.91      0.90      0.90     31885\n",
      "weighted avg       0.91      0.91      0.91     31885\n",
      "\n",
      "acc:  0.9115257958287596\n",
      "pre:  0.9088097552354865\n",
      "rec:  0.8518303793274805\n",
      "ma F1:  0.9047675057293942\n",
      "mi F1:  0.9115257958287596\n",
      "we F1:  0.9109234853030687\n",
      "[[319  34]\n",
      " [151 502]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.90      0.78       353\n",
      "           1       0.94      0.77      0.84       653\n",
      "\n",
      "    accuracy                           0.82      1006\n",
      "   macro avg       0.81      0.84      0.81      1006\n",
      "weighted avg       0.85      0.82      0.82      1006\n",
      "\n",
      "acc:  0.81610337972167\n",
      "pre:  0.9365671641791045\n",
      "rec:  0.7687595712098009\n",
      "ma F1:  0.8098098507276605\n",
      "mi F1:  0.8161033797216701\n",
      "we F1:  0.8201271113735777\n",
      "Loss:  0.07977056503295898\n",
      "Loss:  0.050976116210222244\n",
      "Loss:  0.0537637434899807\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.047205936163663864\n",
      "Loss:  0.06624637544155121\n",
      "Loss:  0.060423269867897034\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.05802253633737564\n",
      "Loss:  0.06666901707649231\n",
      "Loss:  0.0654388964176178\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.003154277801513672\n",
      "Eval Loss:  0.0010619163513183594\n",
      "Eval Loss:  0.0014731884002685547\n",
      "[[18811  1000]\n",
      " [ 1663 10411]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     19811\n",
      "           1       0.91      0.86      0.89     12074\n",
      "\n",
      "    accuracy                           0.92     31885\n",
      "   macro avg       0.92      0.91      0.91     31885\n",
      "weighted avg       0.92      0.92      0.92     31885\n",
      "\n",
      "acc:  0.9164811039673828\n",
      "pre:  0.9123652615896941\n",
      "rec:  0.8622660261719397\n",
      "ma F1:  0.9102522322786953\n",
      "mi F1:  0.9164811039673828\n",
      "we F1:  0.9159894680233828\n",
      "[[302  51]\n",
      " [ 95 558]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.86      0.81       353\n",
      "           1       0.92      0.85      0.88       653\n",
      "\n",
      "    accuracy                           0.85      1006\n",
      "   macro avg       0.84      0.86      0.84      1006\n",
      "weighted avg       0.86      0.85      0.86      1006\n",
      "\n",
      "acc:  0.8548707753479126\n",
      "pre:  0.916256157635468\n",
      "rec:  0.8545176110260337\n",
      "ma F1:  0.8448219756999471\n",
      "mi F1:  0.8548707753479126\n",
      "we F1:  0.8565979127874066\n",
      "Subject 3 Current Train Acc:  0.9164811039673828 Current Test Acc:  0.8548707753479126\n",
      "Loss:  0.0662057176232338\n",
      "Loss:  0.07869092375040054\n",
      "Loss:  0.056734491139650345\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.06341993063688278\n",
      "Loss:  0.0822858065366745\n",
      "Loss:  0.07615704089403152\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.040773868560791016\n",
      "Loss:  0.04756351187825203\n",
      "Loss:  0.05784721300005913\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.0023012161254882812\n",
      "Eval Loss:  0.0008282661437988281\n",
      "Eval Loss:  0.0014619827270507812\n",
      "[[18518  1293]\n",
      " [ 1391 10683]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19811\n",
      "           1       0.89      0.88      0.89     12074\n",
      "\n",
      "    accuracy                           0.92     31885\n",
      "   macro avg       0.91      0.91      0.91     31885\n",
      "weighted avg       0.92      0.92      0.92     31885\n",
      "\n",
      "acc:  0.9158224870628823\n",
      "pre:  0.8920340681362725\n",
      "rec:  0.8847937717409309\n",
      "ma F1:  0.9104130786608128\n",
      "mi F1:  0.9158224870628823\n",
      "we F1:  0.9157548263068641\n",
      "[[295  58]\n",
      " [ 90 563]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.84      0.80       353\n",
      "           1       0.91      0.86      0.88       653\n",
      "\n",
      "    accuracy                           0.85      1006\n",
      "   macro avg       0.84      0.85      0.84      1006\n",
      "weighted avg       0.86      0.85      0.85      1006\n",
      "\n",
      "acc:  0.852882703777336\n",
      "pre:  0.9066022544283414\n",
      "rec:  0.8621745788667687\n",
      "ma F1:  0.8416442249194862\n",
      "mi F1:  0.852882703777336\n",
      "we F1:  0.8542246117006612\n",
      "Loss:  0.06053099408745766\n",
      "Loss:  0.08953079581260681\n",
      "Loss:  0.0627136081457138\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.028994228690862656\n",
      "Loss:  0.09196843951940536\n",
      "Loss:  0.05071572959423065\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.04993542283773422\n",
      "Loss:  0.060769252479076385\n",
      "Loss:  0.033297158777713776\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.0023310184478759766\n",
      "Eval Loss:  0.0008745193481445312\n",
      "Eval Loss:  0.0015456676483154297\n",
      "[[18933   878]\n",
      " [ 1828 10246]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93     19811\n",
      "           1       0.92      0.85      0.88     12074\n",
      "\n",
      "    accuracy                           0.92     31885\n",
      "   macro avg       0.92      0.90      0.91     31885\n",
      "weighted avg       0.92      0.92      0.91     31885\n",
      "\n",
      "acc:  0.9151325074486436\n",
      "pre:  0.9210715569938871\n",
      "rec:  0.8486002981613384\n",
      "ma F1:  0.9083278846948504\n",
      "mi F1:  0.9151325074486436\n",
      "we F1:  0.9143883619880584\n",
      "[[312  41]\n",
      " [128 525]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.88      0.79       353\n",
      "           1       0.93      0.80      0.86       653\n",
      "\n",
      "    accuracy                           0.83      1006\n",
      "   macro avg       0.82      0.84      0.82      1006\n",
      "weighted avg       0.85      0.83      0.84      1006\n",
      "\n",
      "acc:  0.8320079522862823\n",
      "pre:  0.9275618374558304\n",
      "rec:  0.8039816232771823\n",
      "ma F1:  0.824123508922928\n",
      "mi F1:  0.8320079522862823\n",
      "we F1:  0.8352283587304694\n",
      "Loss:  0.045618705451488495\n",
      "Loss:  0.05584074184298515\n",
      "Loss:  0.04318412020802498\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.06120312213897705\n",
      "Loss:  0.040773097425699234\n",
      "Loss:  0.04266245290637016\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.04462215304374695\n",
      "Loss:  0.042814966291189194\n",
      "Loss:  0.062375299632549286\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.002627849578857422\n",
      "Eval Loss:  0.0008561611175537109\n",
      "Eval Loss:  0.001409292221069336\n",
      "[[18717  1094]\n",
      " [ 1446 10628]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     19811\n",
      "           1       0.91      0.88      0.89     12074\n",
      "\n",
      "    accuracy                           0.92     31885\n",
      "   macro avg       0.92      0.91      0.91     31885\n",
      "weighted avg       0.92      0.92      0.92     31885\n",
      "\n",
      "acc:  0.9203387172651717\n",
      "pre:  0.906671216515953\n",
      "rec:  0.8802385290707305\n",
      "ma F1:  0.9148590347383556\n",
      "mi F1:  0.9203387172651717\n",
      "we F1:  0.9201002640262745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[288  65]\n",
      " [ 77 576]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.82      0.80       353\n",
      "           1       0.90      0.88      0.89       653\n",
      "\n",
      "    accuracy                           0.86      1006\n",
      "   macro avg       0.84      0.85      0.85      1006\n",
      "weighted avg       0.86      0.86      0.86      1006\n",
      "\n",
      "acc:  0.8588469184890656\n",
      "pre:  0.8985959438377535\n",
      "rec:  0.8820826952526799\n",
      "ma F1:  0.8462455817077319\n",
      "mi F1:  0.8588469184890656\n",
      "we F1:  0.8593719741882879\n",
      "Subject 3 Current Train Acc:  0.9203387172651717 Current Test Acc:  0.8588469184890656\n",
      "Loss:  0.062479205429553986\n",
      "Loss:  0.078945092856884\n",
      "Loss:  0.050740472972393036\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.03553767874836922\n",
      "Loss:  0.058991171419620514\n",
      "Loss:  0.05758526176214218\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.08994396775960922\n",
      "Loss:  0.06029357761144638\n",
      "Loss:  0.06265679001808167\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.002053976058959961\n",
      "Eval Loss:  0.0008327960968017578\n",
      "Eval Loss:  0.0010676383972167969\n",
      "[[18829   982]\n",
      " [ 1570 10504]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94     19811\n",
      "           1       0.91      0.87      0.89     12074\n",
      "\n",
      "    accuracy                           0.92     31885\n",
      "   macro avg       0.92      0.91      0.91     31885\n",
      "weighted avg       0.92      0.92      0.92     31885\n",
      "\n",
      "acc:  0.9199623647483143\n",
      "pre:  0.9145046143130768\n",
      "rec:  0.8699685274142787\n",
      "ma F1:  0.9141070078184608\n",
      "mi F1:  0.9199623647483143\n",
      "we F1:  0.9195487971957913\n",
      "[[300  53]\n",
      " [ 94 559]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.85      0.80       353\n",
      "           1       0.91      0.86      0.88       653\n",
      "\n",
      "    accuracy                           0.85      1006\n",
      "   macro avg       0.84      0.85      0.84      1006\n",
      "weighted avg       0.86      0.85      0.86      1006\n",
      "\n",
      "acc:  0.8538767395626242\n",
      "pre:  0.9133986928104575\n",
      "rec:  0.8560490045941807\n",
      "ma F1:  0.8435036589043923\n",
      "mi F1:  0.8538767395626242\n",
      "we F1:  0.8555188102459736\n",
      "Loss:  0.04717222601175308\n",
      "Loss:  0.034695450216531754\n",
      "Loss:  0.04672440141439438\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.07307576388120651\n",
      "Loss:  0.035547755658626556\n",
      "Loss:  0.06415507197380066\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.07031131535768509\n",
      "Loss:  0.04080608859658241\n",
      "Loss:  0.08305303752422333\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.002065420150756836\n",
      "Eval Loss:  0.0007691383361816406\n",
      "Eval Loss:  0.0014882087707519531\n",
      "[[19045   766]\n",
      " [ 1855 10219]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.94     19811\n",
      "           1       0.93      0.85      0.89     12074\n",
      "\n",
      "    accuracy                           0.92     31885\n",
      "   macro avg       0.92      0.90      0.91     31885\n",
      "weighted avg       0.92      0.92      0.92     31885\n",
      "\n",
      "acc:  0.9177983377763839\n",
      "pre:  0.9302685480200273\n",
      "rec:  0.84636408812324\n",
      "ma F1:  0.910977209665821\n",
      "mi F1:  0.917798337776384\n",
      "we F1:  0.9169567098008113\n",
      "[[306  47]\n",
      " [108 545]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.87      0.80       353\n",
      "           1       0.92      0.83      0.88       653\n",
      "\n",
      "    accuracy                           0.85      1006\n",
      "   macro avg       0.83      0.85      0.84      1006\n",
      "weighted avg       0.86      0.85      0.85      1006\n",
      "\n",
      "acc:  0.8459244532803181\n",
      "pre:  0.9206081081081081\n",
      "rec:  0.8346094946401225\n",
      "ma F1:  0.8367079792442259\n",
      "mi F1:  0.8459244532803181\n",
      "we F1:  0.8482767751054293\n",
      "Loss:  0.04466360807418823\n",
      "Loss:  0.08149799704551697\n",
      "Loss:  0.05027910694479942\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.05095336586236954\n",
      "Loss:  0.03903418406844139\n",
      "Loss:  0.04085277020931244\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.03821748122572899\n",
      "Loss:  0.03975312411785126\n",
      "Loss:  0.055357325822114944\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  3 Training Time 5810.173776388168 Best Test Acc:  0.8588469184890656\n",
      "test subjects:  ['./seg\\\\a04', './seg\\\\a12']\n",
      "*********\n",
      "33244 1069\n",
      "32160 731\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.5566058158874512\n",
      "Eval Loss:  0.8447078466415405\n",
      "Eval Loss:  0.5683379173278809\n",
      "[[20104     0]\n",
      " [12056     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77     20104\n",
      "           1       0.00      0.00      0.00     12056\n",
      "\n",
      "    accuracy                           0.63     32160\n",
      "   macro avg       0.31      0.50      0.38     32160\n",
      "weighted avg       0.39      0.63      0.48     32160\n",
      "\n",
      "acc:  0.6251243781094528\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.3846624827797337\n",
      "mi F1:  0.6251243781094528\n",
      "we F1:  0.48092379065943824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\install\\envs\\pytorch-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 60   0]\n",
      " [671   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.08      1.00      0.15        60\n",
      "           1       0.00      0.00      0.00       671\n",
      "\n",
      "    accuracy                           0.08       731\n",
      "   macro avg       0.04      0.50      0.08       731\n",
      "weighted avg       0.01      0.08      0.01       731\n",
      "\n",
      "acc:  0.08207934336525308\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.07585335018963336\n",
      "mi F1:  0.08207934336525308\n",
      "we F1:  0.012451986351239402\n",
      "Subject 4 Current Train Acc:  0.6251243781094528 Current Test Acc:  0.08207934336525308\n",
      "Loss:  0.15527421236038208\n",
      "Loss:  0.16259485483169556\n",
      "Loss:  0.14117708802223206\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.13147927820682526\n",
      "Loss:  0.12404484301805496\n",
      "Loss:  0.11476148664951324\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.1010369062423706\n",
      "Loss:  0.12150849401950836\n",
      "Loss:  0.10488597303628922\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.032297611236572266\n",
      "Eval Loss:  0.3539867103099823\n",
      "Eval Loss:  0.7020819187164307\n",
      "[[16621  3483]\n",
      " [ 2523  9533]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.83      0.85     20104\n",
      "           1       0.73      0.79      0.76     12056\n",
      "\n",
      "    accuracy                           0.81     32160\n",
      "   macro avg       0.80      0.81      0.80     32160\n",
      "weighted avg       0.82      0.81      0.81     32160\n",
      "\n",
      "acc:  0.8132462686567165\n",
      "pre:  0.7324062692071297\n",
      "rec:  0.7907266091572661\n",
      "ma F1:  0.8037114992230449\n",
      "mi F1:  0.8132462686567165\n",
      "we F1:  0.8145376595506675\n",
      "[[ 35  25]\n",
      " [192 479]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.58      0.24        60\n",
      "           1       0.95      0.71      0.82       671\n",
      "\n",
      "    accuracy                           0.70       731\n",
      "   macro avg       0.55      0.65      0.53       731\n",
      "weighted avg       0.89      0.70      0.77       731\n",
      "\n",
      "acc:  0.7031463748290013\n",
      "pre:  0.9503968253968254\n",
      "rec:  0.713859910581222\n",
      "ma F1:  0.5296107939802802\n",
      "mi F1:  0.7031463748290013\n",
      "we F1:  0.7684176405986779\n",
      "Subject 4 Current Train Acc:  0.8132462686567165 Current Test Acc:  0.7031463748290013\n",
      "Loss:  0.09341392666101456\n",
      "Loss:  0.10684234648942947\n",
      "Loss:  0.08272266387939453\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.09310778975486755\n",
      "Loss:  0.09936926513910294\n",
      "Loss:  0.09861622750759125\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.09455877542495728\n",
      "Loss:  0.08316130936145782\n",
      "Loss:  0.10550694167613983\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.015886306762695312\n",
      "Eval Loss:  0.3304919898509979\n",
      "Eval Loss:  0.2310367226600647\n",
      "[[18894  1210]\n",
      " [ 3570  8486]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.89     20104\n",
      "           1       0.88      0.70      0.78     12056\n",
      "\n",
      "    accuracy                           0.85     32160\n",
      "   macro avg       0.86      0.82      0.83     32160\n",
      "weighted avg       0.85      0.85      0.85     32160\n",
      "\n",
      "acc:  0.8513681592039801\n",
      "pre:  0.8752062706270627\n",
      "rec:  0.7038818845388188\n",
      "ma F1:  0.8339795845933441\n",
      "mi F1:  0.8513681592039801\n",
      "we F1:  0.8474253233007228\n",
      "[[ 59   1]\n",
      " [341 330]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.98      0.26        60\n",
      "           1       1.00      0.49      0.66       671\n",
      "\n",
      "    accuracy                           0.53       731\n",
      "   macro avg       0.57      0.74      0.46       731\n",
      "weighted avg       0.93      0.53      0.63       731\n",
      "\n",
      "acc:  0.5321477428180574\n",
      "pre:  0.9969788519637462\n",
      "rec:  0.4918032786885246\n",
      "ma F1:  0.4576021869304868\n",
      "mi F1:  0.5321477428180574\n",
      "we F1:  0.6256735324925002\n",
      "Loss:  0.10087031871080399\n",
      "Loss:  0.051929257810115814\n",
      "Loss:  0.08538079261779785\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.07700461894273758\n",
      "Loss:  0.07338064908981323\n",
      "Loss:  0.10144233703613281\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.0649331659078598\n",
      "Loss:  0.0944126546382904\n",
      "Loss:  0.11080863326787949\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.014581084251403809\n",
      "Eval Loss:  0.2137056589126587\n",
      "Eval Loss:  0.17099714279174805\n",
      "[[19004  1100]\n",
      " [ 3121  8935]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90     20104\n",
      "           1       0.89      0.74      0.81     12056\n",
      "\n",
      "    accuracy                           0.87     32160\n",
      "   macro avg       0.87      0.84      0.85     32160\n",
      "weighted avg       0.87      0.87      0.87     32160\n",
      "\n",
      "acc:  0.86875\n",
      "pre:  0.8903836571998007\n",
      "rec:  0.7411247511612475\n",
      "ma F1:  0.8544858525066137\n",
      "mi F1:  0.86875\n",
      "we F1:  0.8658869706937994\n",
      "[[ 59   1]\n",
      " [286 385]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.98      0.29        60\n",
      "           1       1.00      0.57      0.73       671\n",
      "\n",
      "    accuracy                           0.61       731\n",
      "   macro avg       0.58      0.78      0.51       731\n",
      "weighted avg       0.93      0.61      0.69       731\n",
      "\n",
      "acc:  0.6073871409028728\n",
      "pre:  0.9974093264248705\n",
      "rec:  0.5737704918032787\n",
      "ma F1:  0.5099174229417055\n",
      "mi F1:  0.6073871409028728\n",
      "we F1:  0.6925983974026662\n",
      "Loss:  0.05857032909989357\n",
      "Loss:  0.12078195065259933\n",
      "Loss:  0.09236282855272293\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.06686446070671082\n",
      "Loss:  0.09079064428806305\n",
      "Loss:  0.03705241158604622\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.06546950340270996\n",
      "Loss:  0.08231758326292038\n",
      "Loss:  0.052235931158065796\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.011642932891845703\n",
      "Eval Loss:  0.2602246403694153\n",
      "Eval Loss:  0.0615772008895874\n",
      "[[19302   802]\n",
      " [ 3354  8702]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90     20104\n",
      "           1       0.92      0.72      0.81     12056\n",
      "\n",
      "    accuracy                           0.87     32160\n",
      "   macro avg       0.88      0.84      0.86     32160\n",
      "weighted avg       0.88      0.87      0.87     32160\n",
      "\n",
      "acc:  0.8707711442786069\n",
      "pre:  0.9156144781144782\n",
      "rec:  0.7217982747179827\n",
      "ma F1:  0.8550209913032311\n",
      "mi F1:  0.8707711442786068\n",
      "we F1:  0.8669792206566108\n",
      "[[ 60   0]\n",
      " [336 335]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      1.00      0.26        60\n",
      "           1       1.00      0.50      0.67       671\n",
      "\n",
      "    accuracy                           0.54       731\n",
      "   macro avg       0.58      0.75      0.46       731\n",
      "weighted avg       0.93      0.54      0.63       731\n",
      "\n",
      "acc:  0.5403556771545828\n",
      "pre:  1.0\n",
      "rec:  0.4992548435171386\n",
      "ma F1:  0.46458093543999157\n",
      "mi F1:  0.5403556771545828\n",
      "we F1:  0.6329386343040467\n",
      "Loss:  0.06618911027908325\n",
      "Loss:  0.1022944301366806\n",
      "Loss:  0.09982296079397202\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.06534774601459503\n",
      "Loss:  0.10331737995147705\n",
      "Loss:  0.07740917056798935\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.1014782041311264\n",
      "Loss:  0.06616785377264023\n",
      "Loss:  0.049616117030382156\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.009720087051391602\n",
      "Eval Loss:  0.21310138702392578\n",
      "Eval Loss:  0.09020400047302246\n",
      "[[18990  1114]\n",
      " [ 2509  9547]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91     20104\n",
      "           1       0.90      0.79      0.84     12056\n",
      "\n",
      "    accuracy                           0.89     32160\n",
      "   macro avg       0.89      0.87      0.88     32160\n",
      "weighted avg       0.89      0.89      0.89     32160\n",
      "\n",
      "acc:  0.887344527363184\n",
      "pre:  0.8955069880874215\n",
      "rec:  0.7918878566688786\n",
      "ma F1:  0.8767154236059722\n",
      "mi F1:  0.887344527363184\n",
      "we F1:  0.8857743060626112\n",
      "[[ 56   4]\n",
      " [245 426]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.93      0.31        60\n",
      "           1       0.99      0.63      0.77       671\n",
      "\n",
      "    accuracy                           0.66       731\n",
      "   macro avg       0.59      0.78      0.54       731\n",
      "weighted avg       0.92      0.66      0.74       731\n",
      "\n",
      "acc:  0.6593707250341997\n",
      "pre:  0.9906976744186047\n",
      "rec:  0.6348733233979136\n",
      "ma F1:  0.5420456346660427\n",
      "mi F1:  0.6593707250341997\n",
      "we F1:  0.7357905811929182\n",
      "Loss:  0.05698343366384506\n",
      "Loss:  0.06241921707987785\n",
      "Loss:  0.08060634881258011\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.06507308036088943\n",
      "Loss:  0.056720759719610214\n",
      "Loss:  0.08506648242473602\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.07281692326068878\n",
      "Loss:  0.05016297474503517\n",
      "Loss:  0.05278245359659195\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.008060455322265625\n",
      "Eval Loss:  0.4314996600151062\n",
      "Eval Loss:  0.06409215927124023\n",
      "[[19353   751]\n",
      " [ 2899  9157]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91     20104\n",
      "           1       0.92      0.76      0.83     12056\n",
      "\n",
      "    accuracy                           0.89     32160\n",
      "   macro avg       0.90      0.86      0.87     32160\n",
      "weighted avg       0.89      0.89      0.88     32160\n",
      "\n",
      "acc:  0.8865049751243781\n",
      "pre:  0.9242026645135244\n",
      "rec:  0.7595388188453882\n",
      "ma F1:  0.8738223223266005\n",
      "mi F1:  0.8865049751243781\n",
      "we F1:  0.8838331098625475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 60   0]\n",
      " [345 326]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      1.00      0.26        60\n",
      "           1       1.00      0.49      0.65       671\n",
      "\n",
      "    accuracy                           0.53       731\n",
      "   macro avg       0.57      0.74      0.46       731\n",
      "weighted avg       0.93      0.53      0.62       731\n",
      "\n",
      "acc:  0.5280437756497948\n",
      "pre:  1.0\n",
      "rec:  0.4858420268256334\n",
      "ma F1:  0.45601320089300157\n",
      "mi F1:  0.5280437756497948\n",
      "we F1:  0.6214668895260868\n",
      "Loss:  0.054590918123722076\n",
      "Loss:  0.052174102514982224\n",
      "Loss:  0.06518105417490005\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.045638859272003174\n",
      "Loss:  0.023533092811703682\n",
      "Loss:  0.05878020450472832\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.0640261173248291\n",
      "Loss:  0.04909495264291763\n",
      "Loss:  0.0646742731332779\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.03912174701690674\n",
      "Eval Loss:  0.4678824245929718\n",
      "Eval Loss:  0.046666741371154785\n",
      "[[19288   816]\n",
      " [ 2645  9411]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     20104\n",
      "           1       0.92      0.78      0.84     12056\n",
      "\n",
      "    accuracy                           0.89     32160\n",
      "   macro avg       0.90      0.87      0.88     32160\n",
      "weighted avg       0.89      0.89      0.89     32160\n",
      "\n",
      "acc:  0.8923818407960199\n",
      "pre:  0.9202112056321502\n",
      "rec:  0.7806071665560717\n",
      "ma F1:  0.8811737848076218\n",
      "mi F1:  0.8923818407960199\n",
      "we F1:  0.8903063589287749\n",
      "[[ 57   3]\n",
      " [306 365]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.95      0.27        60\n",
      "           1       0.99      0.54      0.70       671\n",
      "\n",
      "    accuracy                           0.58       731\n",
      "   macro avg       0.57      0.75      0.49       731\n",
      "weighted avg       0.92      0.58      0.67       731\n",
      "\n",
      "acc:  0.5772913816689467\n",
      "pre:  0.9918478260869565\n",
      "rec:  0.5439642324888226\n",
      "ma F1:  0.48605109932491\n",
      "mi F1:  0.5772913816689467\n",
      "we F1:  0.6670504905983072\n",
      "Loss:  0.060313545167446136\n",
      "Loss:  0.06182146444916725\n",
      "Loss:  0.05832449719309807\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.07907970249652863\n",
      "Loss:  0.0817507952451706\n",
      "Loss:  0.08638322353363037\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.05385948717594147\n",
      "Loss:  0.07431111484766006\n",
      "Loss:  0.041238751262426376\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.21273738145828247\n",
      "Eval Loss:  0.35757845640182495\n",
      "Eval Loss:  0.11028534173965454\n",
      "[[18816  1288]\n",
      " [ 1805 10251]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92     20104\n",
      "           1       0.89      0.85      0.87     12056\n",
      "\n",
      "    accuracy                           0.90     32160\n",
      "   macro avg       0.90      0.89      0.90     32160\n",
      "weighted avg       0.90      0.90      0.90     32160\n",
      "\n",
      "acc:  0.9038246268656717\n",
      "pre:  0.888378542334691\n",
      "rec:  0.8502820172528202\n",
      "ma F1:  0.8964822353270369\n",
      "mi F1:  0.9038246268656717\n",
      "we F1:  0.9033814258819618\n",
      "[[ 54   6]\n",
      " [148 523]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.90      0.41        60\n",
      "           1       0.99      0.78      0.87       671\n",
      "\n",
      "    accuracy                           0.79       731\n",
      "   macro avg       0.63      0.84      0.64       731\n",
      "weighted avg       0.93      0.79      0.83       731\n",
      "\n",
      "acc:  0.7893296853625171\n",
      "pre:  0.9886578449905482\n",
      "rec:  0.7794336810730254\n",
      "ma F1:  0.6419402035623409\n",
      "mi F1:  0.7893296853625171\n",
      "we F1:  0.8339550721762164\n",
      "Subject 4 Current Train Acc:  0.9038246268656717 Current Test Acc:  0.7893296853625171\n",
      "Loss:  0.06360787898302078\n",
      "Loss:  0.04953048378229141\n",
      "Loss:  0.07537513971328735\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.054773565381765366\n",
      "Loss:  0.08529239892959595\n",
      "Loss:  0.07374636828899384\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.059712156653404236\n",
      "Loss:  0.08450590819120407\n",
      "Loss:  0.06978929042816162\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.17406582832336426\n",
      "Eval Loss:  0.14290940761566162\n",
      "Eval Loss:  0.19993263483047485\n",
      "[[18786  1318]\n",
      " [ 1663 10393]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93     20104\n",
      "           1       0.89      0.86      0.87     12056\n",
      "\n",
      "    accuracy                           0.91     32160\n",
      "   macro avg       0.90      0.90      0.90     32160\n",
      "weighted avg       0.91      0.91      0.91     32160\n",
      "\n",
      "acc:  0.9073072139303483\n",
      "pre:  0.8874562377252156\n",
      "rec:  0.8620603848706039\n",
      "ma F1:  0.9005326237489392\n",
      "mi F1:  0.9073072139303483\n",
      "we F1:  0.9070287397718131\n",
      "[[ 55   5]\n",
      " [179 492]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.92      0.37        60\n",
      "           1       0.99      0.73      0.84       671\n",
      "\n",
      "    accuracy                           0.75       731\n",
      "   macro avg       0.61      0.82      0.61       731\n",
      "weighted avg       0.93      0.75      0.80       731\n",
      "\n",
      "acc:  0.7482900136798906\n",
      "pre:  0.9899396378269618\n",
      "rec:  0.7332339791356185\n",
      "ma F1:  0.6083077066443017\n",
      "mi F1:  0.7482900136798906\n",
      "we F1:  0.8040266759778141\n",
      "Loss:  0.05532702058553696\n",
      "Loss:  0.04695819690823555\n",
      "Loss:  0.049224622547626495\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.05740780755877495\n",
      "Loss:  0.10081277042627335\n",
      "Loss:  0.06434839963912964\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.05269281566143036\n",
      "Loss:  0.03664824366569519\n",
      "Loss:  0.049980923533439636\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.02140188217163086\n",
      "Eval Loss:  0.6711398363113403\n",
      "Eval Loss:  0.03800547122955322\n",
      "[[19474   630]\n",
      " [ 2650  9406]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92     20104\n",
      "           1       0.94      0.78      0.85     12056\n",
      "\n",
      "    accuracy                           0.90     32160\n",
      "   macro avg       0.91      0.87      0.89     32160\n",
      "weighted avg       0.90      0.90      0.90     32160\n",
      "\n",
      "acc:  0.8980099502487562\n",
      "pre:  0.9372259864487844\n",
      "rec:  0.7801924353019244\n",
      "ma F1:  0.886928192044254\n",
      "mi F1:  0.8980099502487562\n",
      "we F1:  0.8957865541846824\n",
      "[[ 60   0]\n",
      " [427 244]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      1.00      0.22        60\n",
      "           1       1.00      0.36      0.53       671\n",
      "\n",
      "    accuracy                           0.42       731\n",
      "   macro avg       0.56      0.68      0.38       731\n",
      "weighted avg       0.93      0.42      0.51       731\n",
      "\n",
      "acc:  0.4158686730506156\n",
      "pre:  1.0\n",
      "rec:  0.36363636363636365\n",
      "ma F1:  0.37635588056063374\n",
      "mi F1:  0.41586867305061564\n",
      "we F1:  0.5075641208398669\n",
      "Loss:  0.05722113326191902\n",
      "Loss:  0.11750350892543793\n",
      "Loss:  0.06104134023189545\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.06971289217472076\n",
      "Loss:  0.1008497104048729\n",
      "Loss:  0.04755910485982895\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.07415834814310074\n",
      "Loss:  0.04031778872013092\n",
      "Loss:  0.06135633960366249\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.024091601371765137\n",
      "Eval Loss:  0.8758112192153931\n",
      "Eval Loss:  0.1231120228767395\n",
      "[[19221   883]\n",
      " [ 2127  9929]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     20104\n",
      "           1       0.92      0.82      0.87     12056\n",
      "\n",
      "    accuracy                           0.91     32160\n",
      "   macro avg       0.91      0.89      0.90     32160\n",
      "weighted avg       0.91      0.91      0.91     32160\n",
      "\n",
      "acc:  0.9064054726368159\n",
      "pre:  0.9183314835368109\n",
      "rec:  0.8235733244857333\n",
      "ma F1:  0.8978804569903984\n",
      "mi F1:  0.9064054726368159\n",
      "we F1:  0.9052641554323235\n",
      "[[ 59   1]\n",
      " [271 400]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.98      0.30        60\n",
      "           1       1.00      0.60      0.75       671\n",
      "\n",
      "    accuracy                           0.63       731\n",
      "   macro avg       0.59      0.79      0.52       731\n",
      "weighted avg       0.93      0.63      0.71       731\n",
      "\n",
      "acc:  0.627906976744186\n",
      "pre:  0.9975062344139651\n",
      "rec:  0.5961251862891207\n",
      "ma F1:  0.5244163796402602\n",
      "mi F1:  0.627906976744186\n",
      "we F1:  0.7098496782634234\n",
      "Loss:  0.04653635993599892\n",
      "Loss:  0.07428088039159775\n",
      "Loss:  0.0708295926451683\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.06781519204378128\n",
      "Loss:  0.05174043029546738\n",
      "Loss:  0.04599621146917343\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.040344320237636566\n",
      "Loss:  0.051589999347925186\n",
      "Loss:  0.045531243085861206\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.03392159938812256\n",
      "Eval Loss:  0.3393280506134033\n",
      "Eval Loss:  0.4690282642841339\n",
      "[[18840  1264]\n",
      " [ 1577 10479]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     20104\n",
      "           1       0.89      0.87      0.88     12056\n",
      "\n",
      "    accuracy                           0.91     32160\n",
      "   macro avg       0.91      0.90      0.91     32160\n",
      "weighted avg       0.91      0.91      0.91     32160\n",
      "\n",
      "acc:  0.911660447761194\n",
      "pre:  0.8923614067955378\n",
      "rec:  0.8691937624419376\n",
      "ma F1:  0.9052567212349081\n",
      "mi F1:  0.9116604477611941\n",
      "we F1:  0.911420719690063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 56   4]\n",
      " [214 457]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.93      0.34        60\n",
      "           1       0.99      0.68      0.81       671\n",
      "\n",
      "    accuracy                           0.70       731\n",
      "   macro avg       0.60      0.81      0.57       731\n",
      "weighted avg       0.93      0.70      0.77       731\n",
      "\n",
      "acc:  0.7017783857729138\n",
      "pre:  0.9913232104121475\n",
      "rec:  0.6810730253353204\n",
      "ma F1:  0.573407217046793\n",
      "mi F1:  0.7017783857729138\n",
      "we F1:  0.7690051823626529\n",
      "Loss:  0.05506144464015961\n",
      "Loss:  0.05899607390165329\n",
      "Loss:  0.06883640587329865\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.053105052560567856\n",
      "Loss:  0.052979692816734314\n",
      "Loss:  0.05155421048402786\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.03922662138938904\n",
      "Loss:  0.057934705168008804\n",
      "Loss:  0.06192786619067192\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.07701599597930908\n",
      "Eval Loss:  0.8036577105522156\n",
      "Eval Loss:  0.2173340916633606\n",
      "[[19166   938]\n",
      " [ 1943 10113]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     20104\n",
      "           1       0.92      0.84      0.88     12056\n",
      "\n",
      "    accuracy                           0.91     32160\n",
      "   macro avg       0.91      0.90      0.90     32160\n",
      "weighted avg       0.91      0.91      0.91     32160\n",
      "\n",
      "acc:  0.9104166666666667\n",
      "pre:  0.9151208035471903\n",
      "rec:  0.8388354346383543\n",
      "ma F1:  0.9027070201644289\n",
      "mi F1:  0.9104166666666667\n",
      "we F1:  0.9095607962662747\n",
      "[[ 58   2]\n",
      " [271 400]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.97      0.30        60\n",
      "           1       1.00      0.60      0.75       671\n",
      "\n",
      "    accuracy                           0.63       731\n",
      "   macro avg       0.59      0.78      0.52       731\n",
      "weighted avg       0.93      0.63      0.71       731\n",
      "\n",
      "acc:  0.6265389876880985\n",
      "pre:  0.9950248756218906\n",
      "rec:  0.5961251862891207\n",
      "ma F1:  0.5218868367525401\n",
      "mi F1:  0.6265389876880985\n",
      "we F1:  0.7088531064064177\n",
      "Loss:  0.0749329924583435\n",
      "Loss:  0.03907152637839317\n",
      "Loss:  0.028600817546248436\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.0445590615272522\n",
      "Loss:  0.05562617629766464\n",
      "Loss:  0.0744432806968689\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.04001309350132942\n",
      "Loss:  0.050998181104660034\n",
      "Loss:  0.041115064173936844\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.057444214820861816\n",
      "Eval Loss:  0.3874956965446472\n",
      "Eval Loss:  0.07156145572662354\n",
      "[[19148   956]\n",
      " [ 1646 10410]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94     20104\n",
      "           1       0.92      0.86      0.89     12056\n",
      "\n",
      "    accuracy                           0.92     32160\n",
      "   macro avg       0.92      0.91      0.91     32160\n",
      "weighted avg       0.92      0.92      0.92     32160\n",
      "\n",
      "acc:  0.9190920398009951\n",
      "pre:  0.9158894949850431\n",
      "rec:  0.8634704711347048\n",
      "ma F1:  0.9126430857043341\n",
      "mi F1:  0.9190920398009951\n",
      "we F1:  0.9185827953140763\n",
      "[[ 58   2]\n",
      " [469 202]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.97      0.20        60\n",
      "           1       0.99      0.30      0.46       671\n",
      "\n",
      "    accuracy                           0.36       731\n",
      "   macro avg       0.55      0.63      0.33       731\n",
      "weighted avg       0.92      0.36      0.44       731\n",
      "\n",
      "acc:  0.35567715458276333\n",
      "pre:  0.9901960784313726\n",
      "rec:  0.30104321907600595\n",
      "ma F1:  0.32966463859819906\n",
      "mi F1:  0.35567715458276333\n",
      "we F1:  0.44003718906048206\n",
      "Loss:  0.07382866740226746\n",
      "Loss:  0.04788718745112419\n",
      "Loss:  0.05171401426196098\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.06130528822541237\n",
      "Loss:  0.07313213497400284\n",
      "Loss:  0.08711614459753036\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.054755862802267075\n",
      "Loss:  0.059003498405218124\n",
      "Loss:  0.05010794848203659\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.052197933197021484\n",
      "Eval Loss:  0.2913917899131775\n",
      "Eval Loss:  0.5761877298355103\n",
      "[[18916  1188]\n",
      " [ 1390 10666]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     20104\n",
      "           1       0.90      0.88      0.89     12056\n",
      "\n",
      "    accuracy                           0.92     32160\n",
      "   macro avg       0.92      0.91      0.91     32160\n",
      "weighted avg       0.92      0.92      0.92     32160\n",
      "\n",
      "acc:  0.9198383084577114\n",
      "pre:  0.8997806647545132\n",
      "rec:  0.8847047113470471\n",
      "ma F1:  0.914191457261936\n",
      "mi F1:  0.9198383084577114\n",
      "we F1:  0.9197000461617664\n",
      "[[ 55   5]\n",
      " [295 376]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.92      0.27        60\n",
      "           1       0.99      0.56      0.71       671\n",
      "\n",
      "    accuracy                           0.59       731\n",
      "   macro avg       0.57      0.74      0.49       731\n",
      "weighted avg       0.92      0.59      0.68       731\n",
      "\n",
      "acc:  0.5896032831737346\n",
      "pre:  0.9868766404199475\n",
      "rec:  0.5603576751117735\n",
      "ma F1:  0.4915607901326162\n",
      "mi F1:  0.5896032831737346\n",
      "we F1:  0.6781774980706953\n",
      "Loss:  0.09478847682476044\n",
      "Loss:  0.0319565124809742\n",
      "Loss:  0.057767920196056366\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.04038852080702782\n",
      "Loss:  0.046136774122714996\n",
      "Loss:  0.06922923028469086\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.03229385241866112\n",
      "Loss:  0.04294063150882721\n",
      "Loss:  0.044881921261548996\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.06762468814849854\n",
      "Eval Loss:  0.11272227764129639\n",
      "Eval Loss:  0.24566233158111572\n",
      "[[18608  1496]\n",
      " [ 1153 10903]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93     20104\n",
      "           1       0.88      0.90      0.89     12056\n",
      "\n",
      "    accuracy                           0.92     32160\n",
      "   macro avg       0.91      0.91      0.91     32160\n",
      "weighted avg       0.92      0.92      0.92     32160\n",
      "\n",
      "acc:  0.9176305970149253\n",
      "pre:  0.8793451084764901\n",
      "rec:  0.9043629727936298\n",
      "ma F1:  0.9126146635305115\n",
      "mi F1:  0.9176305970149252\n",
      "we F1:  0.9178538890571258\n",
      "[[ 56   4]\n",
      " [182 489]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.93      0.38        60\n",
      "           1       0.99      0.73      0.84       671\n",
      "\n",
      "    accuracy                           0.75       731\n",
      "   macro avg       0.61      0.83      0.61       731\n",
      "weighted avg       0.93      0.75      0.80       731\n",
      "\n",
      "acc:  0.7455540355677155\n",
      "pre:  0.9918864097363083\n",
      "rec:  0.7287630402384501\n",
      "ma F1:  0.6080225558707535\n",
      "mi F1:  0.7455540355677155\n",
      "we F1:  0.8020912258357508\n",
      "Loss:  0.026946675032377243\n",
      "Loss:  0.0672912746667862\n",
      "Loss:  0.03545229136943817\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.06226165592670441\n",
      "Loss:  0.05131061002612114\n",
      "Loss:  0.056848712265491486\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.03246637433767319\n",
      "Loss:  0.04960259422659874\n",
      "Loss:  0.05897802114486694\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.06789195537567139\n",
      "Eval Loss:  0.15108174085617065\n",
      "Eval Loss:  0.38318756222724915\n",
      "[[19055  1049]\n",
      " [ 1463 10593]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     20104\n",
      "           1       0.91      0.88      0.89     12056\n",
      "\n",
      "    accuracy                           0.92     32160\n",
      "   macro avg       0.92      0.91      0.92     32160\n",
      "weighted avg       0.92      0.92      0.92     32160\n",
      "\n",
      "acc:  0.9218905472636816\n",
      "pre:  0.909895207009105\n",
      "rec:  0.8786496350364964\n",
      "ma F1:  0.9160805404726857\n",
      "mi F1:  0.9218905472636816\n",
      "we F1:  0.9216062949815411\n",
      "[[ 56   4]\n",
      " [351 320]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.93      0.24        60\n",
      "           1       0.99      0.48      0.64       671\n",
      "\n",
      "    accuracy                           0.51       731\n",
      "   macro avg       0.56      0.71      0.44       731\n",
      "weighted avg       0.92      0.51      0.61       731\n",
      "\n",
      "acc:  0.5143638850889193\n",
      "pre:  0.9876543209876543\n",
      "rec:  0.47690014903129657\n",
      "ma F1:  0.44152238709608\n",
      "mi F1:  0.5143638850889193\n",
      "we F1:  0.6101063085870831\n",
      "Loss:  0.04887361824512482\n",
      "Loss:  0.0565357431769371\n",
      "Loss:  0.06140000745654106\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.06156395748257637\n",
      "Loss:  0.04515207186341286\n",
      "Loss:  0.04151187092065811\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.06790676712989807\n",
      "Loss:  0.056568413972854614\n",
      "Loss:  0.06539907306432724\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.02228069305419922\n",
      "Eval Loss:  0.2524533271789551\n",
      "Eval Loss:  0.274661123752594\n",
      "[[18926  1178]\n",
      " [ 1320 10736]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     20104\n",
      "           1       0.90      0.89      0.90     12056\n",
      "\n",
      "    accuracy                           0.92     32160\n",
      "   macro avg       0.92      0.92      0.92     32160\n",
      "weighted avg       0.92      0.92      0.92     32160\n",
      "\n",
      "acc:  0.9223258706467662\n",
      "pre:  0.9011247272116837\n",
      "rec:  0.8905109489051095\n",
      "ma F1:  0.9169390486559252\n",
      "mi F1:  0.9223258706467662\n",
      "we F1:  0.9222324727563267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 56   4]\n",
      " [407 264]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.93      0.21        60\n",
      "           1       0.99      0.39      0.56       671\n",
      "\n",
      "    accuracy                           0.44       731\n",
      "   macro avg       0.55      0.66      0.39       731\n",
      "weighted avg       0.91      0.44      0.53       731\n",
      "\n",
      "acc:  0.4377564979480164\n",
      "pre:  0.9850746268656716\n",
      "rec:  0.39344262295081966\n",
      "ma F1:  0.3882247295340839\n",
      "mi F1:  0.43775649794801635\n",
      "we F1:  0.5337242992500106\n",
      "Loss:  0.08340386301279068\n",
      "Loss:  0.055607449263334274\n",
      "Loss:  0.07277020812034607\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.052634112536907196\n",
      "Loss:  0.05200750753283501\n",
      "Loss:  0.02919716015458107\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.04002414643764496\n",
      "Loss:  0.0674293041229248\n",
      "Loss:  0.10678774863481522\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.029600858688354492\n",
      "Eval Loss:  0.26270872354507446\n",
      "Eval Loss:  0.28981056809425354\n",
      "[[19239   865]\n",
      " [ 1611 10445]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94     20104\n",
      "           1       0.92      0.87      0.89     12056\n",
      "\n",
      "    accuracy                           0.92     32160\n",
      "   macro avg       0.92      0.91      0.92     32160\n",
      "weighted avg       0.92      0.92      0.92     32160\n",
      "\n",
      "acc:  0.9230099502487562\n",
      "pre:  0.9235190097259063\n",
      "rec:  0.8663735899137359\n",
      "ma F1:  0.9167879958395837\n",
      "mi F1:  0.9230099502487562\n",
      "we F1:  0.9224821383327633\n",
      "[[ 58   2]\n",
      " [320 351]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.97      0.26        60\n",
      "           1       0.99      0.52      0.69       671\n",
      "\n",
      "    accuracy                           0.56       731\n",
      "   macro avg       0.57      0.74      0.48       731\n",
      "weighted avg       0.93      0.56      0.65       731\n",
      "\n",
      "acc:  0.5595075239398085\n",
      "pre:  0.9943342776203966\n",
      "rec:  0.5230998509687034\n",
      "ma F1:  0.47519352882420085\n",
      "mi F1:  0.5595075239398085\n",
      "we F1:  0.6510155459424132\n",
      "Loss:  0.08460084348917007\n",
      "Loss:  0.07112475484609604\n",
      "Loss:  0.033284660428762436\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.05790525674819946\n",
      "Loss:  0.07042837888002396\n",
      "Loss:  0.03659747913479805\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.06818699091672897\n",
      "Loss:  0.053655415773391724\n",
      "Loss:  0.05296611040830612\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  4 Training Time 5802.136266469955 Best Test Acc:  0.7893296853625171\n",
      "test subjects:  ['./seg\\\\a05', './seg\\\\a10', './seg\\\\a20', './seg\\\\x07']\n",
      "*********\n",
      "32323 1990\n",
      "30901 1990\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.6534038186073303\n",
      "Eval Loss:  0.6341980695724487\n",
      "Eval Loss:  0.646928071975708\n",
      "[[16949  2156]\n",
      " [ 9958  1838]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.89      0.74     19105\n",
      "           1       0.46      0.16      0.23     11796\n",
      "\n",
      "    accuracy                           0.61     30901\n",
      "   macro avg       0.55      0.52      0.48     30901\n",
      "weighted avg       0.57      0.61      0.54     30901\n",
      "\n",
      "acc:  0.6079738519789003\n",
      "pre:  0.4601902854281422\n",
      "rec:  0.15581553068836893\n",
      "ma F1:  0.48476321428829006\n",
      "mi F1:  0.6079738519789003\n",
      "we F1:  0.5443586448276766\n",
      "[[931 128]\n",
      " [755 176]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.88      0.68      1059\n",
      "           1       0.58      0.19      0.29       931\n",
      "\n",
      "    accuracy                           0.56      1990\n",
      "   macro avg       0.57      0.53      0.48      1990\n",
      "weighted avg       0.56      0.56      0.49      1990\n",
      "\n",
      "acc:  0.5562814070351759\n",
      "pre:  0.5789473684210527\n",
      "rec:  0.18904403866809882\n",
      "ma F1:  0.48167223439009466\n",
      "mi F1:  0.5562814070351759\n",
      "we F1:  0.4943212067060819\n",
      "Subject 5 Current Train Acc:  0.6079738519789003 Current Test Acc:  0.5562814070351759\n",
      "Loss:  0.16398537158966064\n",
      "Loss:  0.1552978754043579\n",
      "Loss:  0.15881440043449402\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.13851286470890045\n",
      "Loss:  0.1199575886130333\n",
      "Loss:  0.11266043037176132\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.11955347657203674\n",
      "Loss:  0.09658434987068176\n",
      "Loss:  0.09843754768371582\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.0554351806640625\n",
      "Eval Loss:  0.048754096031188965\n",
      "Eval Loss:  0.05670297145843506\n",
      "[[15992  3113]\n",
      " [ 2204  9592]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86     19105\n",
      "           1       0.75      0.81      0.78     11796\n",
      "\n",
      "    accuracy                           0.83     30901\n",
      "   macro avg       0.82      0.83      0.82     30901\n",
      "weighted avg       0.83      0.83      0.83     30901\n",
      "\n",
      "acc:  0.8279343710559529\n",
      "pre:  0.754978354978355\n",
      "rec:  0.813157002373686\n",
      "ma F1:  0.8202226770458345\n",
      "mi F1:  0.8279343710559529\n",
      "we F1:  0.8290296725958275\n",
      "[[517 542]\n",
      " [184 747]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.49      0.59      1059\n",
      "           1       0.58      0.80      0.67       931\n",
      "\n",
      "    accuracy                           0.64      1990\n",
      "   macro avg       0.66      0.65      0.63      1990\n",
      "weighted avg       0.66      0.64      0.63      1990\n",
      "\n",
      "acc:  0.6351758793969849\n",
      "pre:  0.5795190069821567\n",
      "rec:  0.8023630504833512\n",
      "ma F1:  0.6302364864864864\n",
      "mi F1:  0.6351758793969849\n",
      "we F1:  0.6274876069536873\n",
      "Subject 5 Current Train Acc:  0.8279343710559529 Current Test Acc:  0.6351758793969849\n",
      "Loss:  0.09142626821994781\n",
      "Loss:  0.1065659373998642\n",
      "Loss:  0.08500904589891434\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.09897921979427338\n",
      "Loss:  0.10766547173261642\n",
      "Loss:  0.10036048293113708\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.11583498865365982\n",
      "Loss:  0.1047859787940979\n",
      "Loss:  0.08789491653442383\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.01467752456665039\n",
      "Eval Loss:  0.04359793663024902\n",
      "Eval Loss:  0.036603569984436035\n",
      "[[18000  1105]\n",
      " [ 3177  8619]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89     19105\n",
      "           1       0.89      0.73      0.80     11796\n",
      "\n",
      "    accuracy                           0.86     30901\n",
      "   macro avg       0.87      0.84      0.85     30901\n",
      "weighted avg       0.86      0.86      0.86     30901\n",
      "\n",
      "acc:  0.8614284327368046\n",
      "pre:  0.8863636363636364\n",
      "rec:  0.7306714140386572\n",
      "ma F1:  0.8473608619640458\n",
      "mi F1:  0.8614284327368046\n",
      "we F1:  0.8583213005929867\n",
      "[[777 282]\n",
      " [336 595]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.73      0.72      1059\n",
      "           1       0.68      0.64      0.66       931\n",
      "\n",
      "    accuracy                           0.69      1990\n",
      "   macro avg       0.69      0.69      0.69      1990\n",
      "weighted avg       0.69      0.69      0.69      1990\n",
      "\n",
      "acc:  0.6894472361809045\n",
      "pre:  0.6784492588369442\n",
      "rec:  0.6390977443609023\n",
      "ma F1:  0.6868277269838166\n",
      "mi F1:  0.6894472361809045\n",
      "we F1:  0.6886700191663838\n",
      "Subject 5 Current Train Acc:  0.8614284327368046 Current Test Acc:  0.6894472361809045\n",
      "Loss:  0.05664685368537903\n",
      "Loss:  0.06109113246202469\n",
      "Loss:  0.0782884880900383\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.09596304595470428\n",
      "Loss:  0.05703229829668999\n",
      "Loss:  0.12430359423160553\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.05473180487751961\n",
      "Loss:  0.10744404792785645\n",
      "Loss:  0.09899736940860748\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.010327577590942383\n",
      "Eval Loss:  0.021198034286499023\n",
      "Eval Loss:  0.03316950798034668\n",
      "[[18117   988]\n",
      " [ 2928  8868]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90     19105\n",
      "           1       0.90      0.75      0.82     11796\n",
      "\n",
      "    accuracy                           0.87     30901\n",
      "   macro avg       0.88      0.85      0.86     30901\n",
      "weighted avg       0.88      0.87      0.87     30901\n",
      "\n",
      "acc:  0.8732727096210479\n",
      "pre:  0.8997564935064936\n",
      "rec:  0.7517802644964394\n",
      "ma F1:  0.8608024314878691\n",
      "mi F1:  0.8732727096210479\n",
      "we F1:  0.870657038783296\n",
      "[[789 270]\n",
      " [337 594]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.75      0.72      1059\n",
      "           1       0.69      0.64      0.66       931\n",
      "\n",
      "    accuracy                           0.69      1990\n",
      "   macro avg       0.69      0.69      0.69      1990\n",
      "weighted avg       0.69      0.69      0.69      1990\n",
      "\n",
      "acc:  0.6949748743718593\n",
      "pre:  0.6875\n",
      "rec:  0.6380236305048335\n",
      "ma F1:  0.6920176182250466\n",
      "mi F1:  0.6949748743718593\n",
      "we F1:  0.6939587914906467\n",
      "Subject 5 Current Train Acc:  0.8732727096210479 Current Test Acc:  0.6949748743718593\n",
      "Loss:  0.07193095237016678\n",
      "Loss:  0.07064365595579147\n",
      "Loss:  0.0953163355588913\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.07864860445261002\n",
      "Loss:  0.06873933225870132\n",
      "Loss:  0.07435206323862076\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.07166044414043427\n",
      "Loss:  0.09172913432121277\n",
      "Loss:  0.07901445031166077\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.006261587142944336\n",
      "Eval Loss:  0.022595882415771484\n",
      "Eval Loss:  0.02781057357788086\n",
      "[[18338   767]\n",
      " [ 2964  8832]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91     19105\n",
      "           1       0.92      0.75      0.83     11796\n",
      "\n",
      "    accuracy                           0.88     30901\n",
      "   macro avg       0.89      0.85      0.87     30901\n",
      "weighted avg       0.88      0.88      0.88     30901\n",
      "\n",
      "acc:  0.8792595708876736\n",
      "pre:  0.9200958433170122\n",
      "rec:  0.7487283825025433\n",
      "ma F1:  0.8666389873317101\n",
      "mi F1:  0.8792595708876735\n",
      "we F1:  0.8763427370908661\n",
      "[[849 210]\n",
      " [356 575]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.80      0.75      1059\n",
      "           1       0.73      0.62      0.67       931\n",
      "\n",
      "    accuracy                           0.72      1990\n",
      "   macro avg       0.72      0.71      0.71      1990\n",
      "weighted avg       0.72      0.72      0.71      1990\n",
      "\n",
      "acc:  0.7155778894472362\n",
      "pre:  0.732484076433121\n",
      "rec:  0.6176154672395274\n",
      "ma F1:  0.7100815850815851\n",
      "mi F1:  0.7155778894472362\n",
      "we F1:  0.7126492017195536\n",
      "Subject 5 Current Train Acc:  0.8792595708876736 Current Test Acc:  0.7155778894472362\n",
      "Loss:  0.05737825110554695\n",
      "Loss:  0.049401022493839264\n",
      "Loss:  0.058170512318611145\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.09398648887872696\n",
      "Loss:  0.08677636086940765\n",
      "Loss:  0.08638706803321838\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.06524688005447388\n",
      "Loss:  0.06858062744140625\n",
      "Loss:  0.03464929759502411\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.004231691360473633\n",
      "Eval Loss:  0.013081073760986328\n",
      "Eval Loss:  0.01654362678527832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18492   613]\n",
      " [ 3218  8578]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91     19105\n",
      "           1       0.93      0.73      0.82     11796\n",
      "\n",
      "    accuracy                           0.88     30901\n",
      "   macro avg       0.89      0.85      0.86     30901\n",
      "weighted avg       0.88      0.88      0.87     30901\n",
      "\n",
      "acc:  0.8760234296624705\n",
      "pre:  0.9333043194429334\n",
      "rec:  0.7271956595456087\n",
      "ma F1:  0.861797938056087\n",
      "mi F1:  0.8760234296624705\n",
      "we F1:  0.8722855432760847\n",
      "[[878 181]\n",
      " [416 515]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.83      0.75      1059\n",
      "           1       0.74      0.55      0.63       931\n",
      "\n",
      "    accuracy                           0.70      1990\n",
      "   macro avg       0.71      0.69      0.69      1990\n",
      "weighted avg       0.71      0.70      0.69      1990\n",
      "\n",
      "acc:  0.7\n",
      "pre:  0.7399425287356322\n",
      "rec:  0.5531686358754028\n",
      "ma F1:  0.6896741687173862\n",
      "mi F1:  0.7\n",
      "we F1:  0.6933152331916963\n",
      "Loss:  0.058677878230810165\n",
      "Loss:  0.08615554124116898\n",
      "Loss:  0.03986753150820732\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.04731173813343048\n",
      "Loss:  0.06275159865617752\n",
      "Loss:  0.07110786437988281\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.055399339646101\n",
      "Loss:  0.07013196498155594\n",
      "Loss:  0.06493688374757767\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.00357818603515625\n",
      "Eval Loss:  0.028307318687438965\n",
      "Eval Loss:  0.025548458099365234\n",
      "[[18051  1054]\n",
      " [ 2151  9645]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92     19105\n",
      "           1       0.90      0.82      0.86     11796\n",
      "\n",
      "    accuracy                           0.90     30901\n",
      "   macro avg       0.90      0.88      0.89     30901\n",
      "weighted avg       0.90      0.90      0.90     30901\n",
      "\n",
      "acc:  0.8962816737322417\n",
      "pre:  0.9014861201981493\n",
      "rec:  0.8176500508646999\n",
      "ma F1:  0.8879931272962669\n",
      "mi F1:  0.8962816737322417\n",
      "we F1:  0.8952000016598809\n",
      "[[789 270]\n",
      " [353 578]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.75      0.72      1059\n",
      "           1       0.68      0.62      0.65       931\n",
      "\n",
      "    accuracy                           0.69      1990\n",
      "   macro avg       0.69      0.68      0.68      1990\n",
      "weighted avg       0.69      0.69      0.69      1990\n",
      "\n",
      "acc:  0.6869346733668342\n",
      "pre:  0.6816037735849056\n",
      "rec:  0.6208378088077336\n",
      "ma F1:  0.6833750513014805\n",
      "mi F1:  0.6869346733668342\n",
      "we F1:  0.68553444288615\n",
      "Loss:  0.05432397872209549\n",
      "Loss:  0.0645500048995018\n",
      "Loss:  0.0805257111787796\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.06243346631526947\n",
      "Loss:  0.04832266643643379\n",
      "Loss:  0.07211533188819885\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.09113926440477371\n",
      "Loss:  0.10281124711036682\n",
      "Loss:  0.08673205971717834\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.0029768943786621094\n",
      "Eval Loss:  0.018128156661987305\n",
      "Eval Loss:  0.018895864486694336\n",
      "[[18324   781]\n",
      " [ 2312  9484]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92     19105\n",
      "           1       0.92      0.80      0.86     11796\n",
      "\n",
      "    accuracy                           0.90     30901\n",
      "   macro avg       0.91      0.88      0.89     30901\n",
      "weighted avg       0.90      0.90      0.90     30901\n",
      "\n",
      "acc:  0.8999061519044691\n",
      "pre:  0.9239162201656113\n",
      "rec:  0.8040013563919973\n",
      "ma F1:  0.8909844454391982\n",
      "mi F1:  0.8999061519044691\n",
      "we F1:  0.8983610011580516\n",
      "[[829 230]\n",
      " [346 585]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.78      0.74      1059\n",
      "           1       0.72      0.63      0.67       931\n",
      "\n",
      "    accuracy                           0.71      1990\n",
      "   macro avg       0.71      0.71      0.71      1990\n",
      "weighted avg       0.71      0.71      0.71      1990\n",
      "\n",
      "acc:  0.7105527638190955\n",
      "pre:  0.7177914110429447\n",
      "rec:  0.6283566058002148\n",
      "ma F1:  0.7061348051204903\n",
      "mi F1:  0.7105527638190955\n",
      "we F1:  0.7084524227984471\n",
      "Loss:  0.05037357658147812\n",
      "Loss:  0.051005903631448746\n",
      "Loss:  0.06280583888292313\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.07556576281785965\n",
      "Loss:  0.05109162628650665\n",
      "Loss:  0.07139026373624802\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.039082251489162445\n",
      "Loss:  0.07901235669851303\n",
      "Loss:  0.06623345613479614\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.002445220947265625\n",
      "Eval Loss:  0.005911350250244141\n",
      "Eval Loss:  0.046114444732666016\n",
      "[[17981  1124]\n",
      " [ 1791 10005]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93     19105\n",
      "           1       0.90      0.85      0.87     11796\n",
      "\n",
      "    accuracy                           0.91     30901\n",
      "   macro avg       0.90      0.89      0.90     30901\n",
      "weighted avg       0.91      0.91      0.91     30901\n",
      "\n",
      "acc:  0.9056664832853306\n",
      "pre:  0.8990026058046545\n",
      "rec:  0.8481688708036622\n",
      "ma F1:  0.8989330861987374\n",
      "mi F1:  0.9056664832853306\n",
      "we F1:  0.9051033970445136\n",
      "[[812 247]\n",
      " [380 551]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72      1059\n",
      "           1       0.69      0.59      0.64       931\n",
      "\n",
      "    accuracy                           0.68      1990\n",
      "   macro avg       0.69      0.68      0.68      1990\n",
      "weighted avg       0.69      0.68      0.68      1990\n",
      "\n",
      "acc:  0.6849246231155779\n",
      "pre:  0.6904761904761905\n",
      "rec:  0.5918367346938775\n",
      "ma F1:  0.6794098837635043\n",
      "mi F1:  0.6849246231155779\n",
      "we F1:  0.6821144302656706\n",
      "Loss:  0.07618149369955063\n",
      "Loss:  0.06991849839687347\n",
      "Loss:  0.04851081967353821\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.037484899163246155\n",
      "Loss:  0.041944462805986404\n",
      "Loss:  0.10936020314693451\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.061744578182697296\n",
      "Loss:  0.05764069780707359\n",
      "Loss:  0.07726164162158966\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.0023217201232910156\n",
      "Eval Loss:  0.003045797348022461\n",
      "Eval Loss:  0.03343689441680908\n",
      "[[18140   965]\n",
      " [ 1898  9898]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19105\n",
      "           1       0.91      0.84      0.87     11796\n",
      "\n",
      "    accuracy                           0.91     30901\n",
      "   macro avg       0.91      0.89      0.90     30901\n",
      "weighted avg       0.91      0.91      0.91     30901\n",
      "\n",
      "acc:  0.9073492767224361\n",
      "pre:  0.9111663444720611\n",
      "rec:  0.839097999321804\n",
      "ma F1:  0.9002531855444669\n",
      "mi F1:  0.9073492767224361\n",
      "we F1:  0.9065459943796741\n",
      "[[799 260]\n",
      " [362 569]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.75      0.72      1059\n",
      "           1       0.69      0.61      0.65       931\n",
      "\n",
      "    accuracy                           0.69      1990\n",
      "   macro avg       0.69      0.68      0.68      1990\n",
      "weighted avg       0.69      0.69      0.69      1990\n",
      "\n",
      "acc:  0.6874371859296482\n",
      "pre:  0.6863691194209891\n",
      "rec:  0.611170784103115\n",
      "ma F1:  0.6832053644553644\n",
      "mi F1:  0.6874371859296482\n",
      "we F1:  0.6855604651019224\n",
      "Loss:  0.03746138885617256\n",
      "Loss:  0.07371233403682709\n",
      "Loss:  0.05473388731479645\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.06602828949689865\n",
      "Loss:  0.05694323405623436\n",
      "Loss:  0.05698832869529724\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.05481734126806259\n",
      "Loss:  0.0709238350391388\n",
      "Loss:  0.06797896325588226\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.002108335494995117\n",
      "Eval Loss:  0.007664680480957031\n",
      "Eval Loss:  0.020947933197021484\n",
      "[[18289   816]\n",
      " [ 2086  9710]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     19105\n",
      "           1       0.92      0.82      0.87     11796\n",
      "\n",
      "    accuracy                           0.91     30901\n",
      "   macro avg       0.91      0.89      0.90     30901\n",
      "weighted avg       0.91      0.91      0.90     30901\n",
      "\n",
      "acc:  0.906087181644607\n",
      "pre:  0.9224776743302299\n",
      "rec:  0.8231603933536792\n",
      "ma F1:  0.8982440778594081\n",
      "mi F1:  0.9060871816446069\n",
      "we F1:  0.9049261207042638\n",
      "[[708 351]\n",
      " [365 566]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.67      0.66      1059\n",
      "           1       0.62      0.61      0.61       931\n",
      "\n",
      "    accuracy                           0.64      1990\n",
      "   macro avg       0.64      0.64      0.64      1990\n",
      "weighted avg       0.64      0.64      0.64      1990\n",
      "\n",
      "acc:  0.6402010050251257\n",
      "pre:  0.6172300981461287\n",
      "rec:  0.6079484425349087\n",
      "ma F1:  0.638359607871803\n",
      "mi F1:  0.6402010050251257\n",
      "we F1:  0.6400194588269107\n",
      "Loss:  0.06020859628915787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.0563637837767601\n",
      "Loss:  0.05974331125617027\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.0804901048541069\n",
      "Loss:  0.08616647869348526\n",
      "Loss:  0.04320260137319565\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.06047957018017769\n",
      "Loss:  0.1314544379711151\n",
      "Loss:  0.05584467574954033\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.0020759105682373047\n",
      "Eval Loss:  0.006057262420654297\n",
      "Eval Loss:  0.02718937397003174\n",
      "[[18090  1015]\n",
      " [ 1604 10192]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     19105\n",
      "           1       0.91      0.86      0.89     11796\n",
      "\n",
      "    accuracy                           0.92     30901\n",
      "   macro avg       0.91      0.91      0.91     30901\n",
      "weighted avg       0.92      0.92      0.91     30901\n",
      "\n",
      "acc:  0.9152454613119316\n",
      "pre:  0.9094316052467208\n",
      "rec:  0.8640217022719566\n",
      "ma F1:  0.9093217728309984\n",
      "mi F1:  0.9152454613119316\n",
      "we F1:  0.9148036972558075\n",
      "[[726 333]\n",
      " [363 568]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68      1059\n",
      "           1       0.63      0.61      0.62       931\n",
      "\n",
      "    accuracy                           0.65      1990\n",
      "   macro avg       0.65      0.65      0.65      1990\n",
      "weighted avg       0.65      0.65      0.65      1990\n",
      "\n",
      "acc:  0.650251256281407\n",
      "pre:  0.6304106548279689\n",
      "rec:  0.6100966702470462\n",
      "ma F1:  0.6480324949379133\n",
      "mi F1:  0.650251256281407\n",
      "we F1:  0.6498299724820095\n",
      "Loss:  0.06603020429611206\n",
      "Loss:  0.05468159541487694\n",
      "Loss:  0.0634390264749527\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.06383704394102097\n",
      "Loss:  0.06583774834871292\n",
      "Loss:  0.03827740252017975\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.040976423770189285\n",
      "Loss:  0.04349217563867569\n",
      "Loss:  0.025152714923024178\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.0017459392547607422\n",
      "Eval Loss:  0.011890172958374023\n",
      "Eval Loss:  0.053295135498046875\n",
      "[[18135   970]\n",
      " [ 1602 10194]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     19105\n",
      "           1       0.91      0.86      0.89     11796\n",
      "\n",
      "    accuracy                           0.92     30901\n",
      "   macro avg       0.92      0.91      0.91     30901\n",
      "weighted avg       0.92      0.92      0.92     30901\n",
      "\n",
      "acc:  0.916766447687777\n",
      "pre:  0.9131135793622358\n",
      "rec:  0.8641912512716174\n",
      "ma F1:  0.9108810562294093\n",
      "mi F1:  0.916766447687777\n",
      "we F1:  0.9162980473097783\n",
      "[[790 269]\n",
      " [377 554]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.75      0.71      1059\n",
      "           1       0.67      0.60      0.63       931\n",
      "\n",
      "    accuracy                           0.68      1990\n",
      "   macro avg       0.68      0.67      0.67      1990\n",
      "weighted avg       0.68      0.68      0.67      1990\n",
      "\n",
      "acc:  0.6753768844221105\n",
      "pre:  0.6731470230862697\n",
      "rec:  0.5950590762620838\n",
      "ma F1:  0.6707461625385078\n",
      "mi F1:  0.6753768844221105\n",
      "we F1:  0.6732577405092754\n",
      "Loss:  0.051871269941329956\n",
      "Loss:  0.05704885348677635\n",
      "Loss:  0.056104883551597595\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.06831398606300354\n",
      "Loss:  0.06515505909919739\n",
      "Loss:  0.07493294775485992\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.0579221136868\n",
      "Loss:  0.042705290019512177\n",
      "Loss:  0.042234599590301514\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.0014386177062988281\n",
      "Eval Loss:  0.014171361923217773\n",
      "Eval Loss:  0.03404521942138672\n",
      "[[17627  1478]\n",
      " [ 1098 10698]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     19105\n",
      "           1       0.88      0.91      0.89     11796\n",
      "\n",
      "    accuracy                           0.92     30901\n",
      "   macro avg       0.91      0.91      0.91     30901\n",
      "weighted avg       0.92      0.92      0.92     30901\n",
      "\n",
      "acc:  0.916637002038769\n",
      "pre:  0.8786136662286466\n",
      "rec:  0.9069175991861648\n",
      "ma F1:  0.9122235964868521\n",
      "mi F1:  0.916637002038769\n",
      "we F1:  0.9168790418871929\n",
      "[[683 376]\n",
      " [362 569]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.64      0.65      1059\n",
      "           1       0.60      0.61      0.61       931\n",
      "\n",
      "    accuracy                           0.63      1990\n",
      "   macro avg       0.63      0.63      0.63      1990\n",
      "weighted avg       0.63      0.63      0.63      1990\n",
      "\n",
      "acc:  0.629145728643216\n",
      "pre:  0.6021164021164022\n",
      "rec:  0.611170784103115\n",
      "ma F1:  0.6279246759142905\n",
      "mi F1:  0.629145728643216\n",
      "we F1:  0.6292956824871192\n",
      "Loss:  0.043726950883865356\n",
      "Loss:  0.03627558797597885\n",
      "Loss:  0.059479594230651855\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.07329604029655457\n",
      "Loss:  0.058359336107969284\n",
      "Loss:  0.1185779944062233\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.06981973350048065\n",
      "Loss:  0.05111735686659813\n",
      "Loss:  0.043635886162519455\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.0014064311981201172\n",
      "Eval Loss:  0.01805281639099121\n",
      "Eval Loss:  0.046993136405944824\n",
      "[[17513  1592]\n",
      " [ 1021 10775]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     19105\n",
      "           1       0.87      0.91      0.89     11796\n",
      "\n",
      "    accuracy                           0.92     30901\n",
      "   macro avg       0.91      0.92      0.91     30901\n",
      "weighted avg       0.92      0.92      0.92     30901\n",
      "\n",
      "acc:  0.9154396297854438\n",
      "pre:  0.8712703161639848\n",
      "rec:  0.9134452356731095\n",
      "ma F1:  0.9112183906234642\n",
      "mi F1:  0.9154396297854438\n",
      "we F1:  0.9157973512994674\n",
      "[[551 508]\n",
      " [333 598]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.52      0.57      1059\n",
      "           1       0.54      0.64      0.59       931\n",
      "\n",
      "    accuracy                           0.58      1990\n",
      "   macro avg       0.58      0.58      0.58      1990\n",
      "weighted avg       0.58      0.58      0.58      1990\n",
      "\n",
      "acc:  0.5773869346733669\n",
      "pre:  0.5406871609403255\n",
      "rec:  0.6423200859291085\n",
      "ma F1:  0.5771510635335839\n",
      "mi F1:  0.5773869346733669\n",
      "we F1:  0.5765086910677922\n",
      "Loss:  0.05099518224596977\n",
      "Loss:  0.07068976014852524\n",
      "Loss:  0.0449519008398056\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.05735038220882416\n",
      "Loss:  0.05944455415010452\n",
      "Loss:  0.0560024231672287\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.04421614855527878\n",
      "Loss:  0.057687003165483475\n",
      "Loss:  0.07431738078594208\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.001691579818725586\n",
      "Eval Loss:  0.01733231544494629\n",
      "Eval Loss:  0.04038369655609131\n",
      "[[18042  1063]\n",
      " [ 1296 10500]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     19105\n",
      "           1       0.91      0.89      0.90     11796\n",
      "\n",
      "    accuracy                           0.92     30901\n",
      "   macro avg       0.92      0.92      0.92     30901\n",
      "weighted avg       0.92      0.92      0.92     30901\n",
      "\n",
      "acc:  0.9236594284974596\n",
      "pre:  0.9080688402663668\n",
      "rec:  0.8901322482197355\n",
      "ma F1:  0.9188237552795923\n",
      "mi F1:  0.9236594284974596\n",
      "we F1:  0.9235100368427576\n",
      "[[727 332]\n",
      " [373 558]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.69      0.67      1059\n",
      "           1       0.63      0.60      0.61       931\n",
      "\n",
      "    accuracy                           0.65      1990\n",
      "   macro avg       0.64      0.64      0.64      1990\n",
      "weighted avg       0.65      0.65      0.65      1990\n",
      "\n",
      "acc:  0.6457286432160804\n",
      "pre:  0.6269662921348315\n",
      "rec:  0.5993555316863588\n",
      "ma F1:  0.6431550087637437\n",
      "mi F1:  0.6457286432160804\n",
      "we F1:  0.6451042703607798\n",
      "Loss:  0.041732799261808395\n",
      "Loss:  0.04127376526594162\n",
      "Loss:  0.04864168539643288\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.03656673803925514\n",
      "Loss:  0.02865242213010788\n",
      "Loss:  0.049895867705345154\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.061962202191352844\n",
      "Loss:  0.039972830563783646\n",
      "Loss:  0.051265258342027664\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.0014128684997558594\n",
      "Eval Loss:  0.016626834869384766\n",
      "Eval Loss:  0.03959453105926514\n",
      "[[18086  1019]\n",
      " [ 1335 10461]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     19105\n",
      "           1       0.91      0.89      0.90     11796\n",
      "\n",
      "    accuracy                           0.92     30901\n",
      "   macro avg       0.92      0.92      0.92     30901\n",
      "weighted avg       0.92      0.92      0.92     30901\n",
      "\n",
      "acc:  0.9238212355587198\n",
      "pre:  0.9112369337979094\n",
      "rec:  0.8868260427263479\n",
      "ma F1:  0.9188820953850723\n",
      "mi F1:  0.9238212355587198\n",
      "we F1:  0.9236165446348019\n",
      "[[780 279]\n",
      " [398 533]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.74      0.70      1059\n",
      "           1       0.66      0.57      0.61       931\n",
      "\n",
      "    accuracy                           0.66      1990\n",
      "   macro avg       0.66      0.65      0.65      1990\n",
      "weighted avg       0.66      0.66      0.66      1990\n",
      "\n",
      "acc:  0.6597989949748744\n",
      "pre:  0.6564039408866995\n",
      "rec:  0.5725026852846402\n",
      "ma F1:  0.6544758765568693\n",
      "mi F1:  0.6597989949748744\n",
      "we F1:  0.6572344156560785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.06368640065193176\n",
      "Loss:  0.0409354642033577\n",
      "Loss:  0.03855869174003601\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.053611479699611664\n",
      "Loss:  0.09974689781665802\n",
      "Loss:  0.04250288009643555\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.024438275024294853\n",
      "Loss:  0.04342386871576309\n",
      "Loss:  0.05851375684142113\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.002016782760620117\n",
      "Eval Loss:  0.040557026863098145\n",
      "Eval Loss:  0.06936705112457275\n",
      "[[17623  1482]\n",
      " [  847 10949]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94     19105\n",
      "           1       0.88      0.93      0.90     11796\n",
      "\n",
      "    accuracy                           0.92     30901\n",
      "   macro avg       0.92      0.93      0.92     30901\n",
      "weighted avg       0.93      0.92      0.92     30901\n",
      "\n",
      "acc:  0.9246302708650206\n",
      "pre:  0.8807819161772987\n",
      "rec:  0.928195998643608\n",
      "ma F1:  0.9209424422436481\n",
      "mi F1:  0.9246302708650206\n",
      "we F1:  0.924981150573527\n",
      "[[399 660]\n",
      " [274 657]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.38      0.46      1059\n",
      "           1       0.50      0.71      0.58       931\n",
      "\n",
      "    accuracy                           0.53      1990\n",
      "   macro avg       0.55      0.54      0.52      1990\n",
      "weighted avg       0.55      0.53      0.52      1990\n",
      "\n",
      "acc:  0.5306532663316583\n",
      "pre:  0.4988610478359909\n",
      "rec:  0.7056928034371643\n",
      "ma F1:  0.5226293014884156\n",
      "mi F1:  0.5306532663316583\n",
      "we F1:  0.5186484197057215\n",
      "Loss:  0.043253783136606216\n",
      "Loss:  0.05499415099620819\n",
      "Loss:  0.05123472958803177\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.03787378594279289\n",
      "Loss:  0.030113695189356804\n",
      "Loss:  0.03533069044351578\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.056595686823129654\n",
      "Loss:  0.042079199105501175\n",
      "Loss:  0.03165123984217644\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.0015194416046142578\n",
      "Eval Loss:  0.06698215007781982\n",
      "Eval Loss:  0.0285416841506958\n",
      "[[17513  1592]\n",
      " [  733 11063]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94     19105\n",
      "           1       0.87      0.94      0.90     11796\n",
      "\n",
      "    accuracy                           0.92     30901\n",
      "   macro avg       0.92      0.93      0.92     30901\n",
      "weighted avg       0.93      0.92      0.93     30901\n",
      "\n",
      "acc:  0.9247597165140287\n",
      "pre:  0.8741999209798499\n",
      "rec:  0.9378602916242794\n",
      "ma F1:  0.9213322675783229\n",
      "mi F1:  0.9247597165140287\n",
      "we F1:  0.9252161783180242\n",
      "[[560 499]\n",
      " [328 603]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.53      0.58      1059\n",
      "           1       0.55      0.65      0.59       931\n",
      "\n",
      "    accuracy                           0.58      1990\n",
      "   macro avg       0.59      0.59      0.58      1990\n",
      "weighted avg       0.59      0.58      0.58      1990\n",
      "\n",
      "acc:  0.5844221105527638\n",
      "pre:  0.5471869328493648\n",
      "rec:  0.6476906552094522\n",
      "ma F1:  0.5842279835210047\n",
      "mi F1:  0.5844221105527638\n",
      "we F1:  0.583650117007861\n",
      "Loss:  0.05242178589105606\n",
      "Loss:  0.03343214839696884\n",
      "Loss:  0.04437602311372757\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.06892438232898712\n",
      "Loss:  0.029295766726136208\n",
      "Loss:  0.06946690380573273\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.043765150010585785\n",
      "Loss:  0.07984700053930283\n",
      "Loss:  0.06295009702444077\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.001399993896484375\n",
      "Eval Loss:  0.03619575500488281\n",
      "Eval Loss:  0.02298414707183838\n",
      "[[17691  1414]\n",
      " [  803 10993]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94     19105\n",
      "           1       0.89      0.93      0.91     11796\n",
      "\n",
      "    accuracy                           0.93     30901\n",
      "   macro avg       0.92      0.93      0.92     30901\n",
      "weighted avg       0.93      0.93      0.93     30901\n",
      "\n",
      "acc:  0.928254749037248\n",
      "pre:  0.8860320786652696\n",
      "rec:  0.9319260766361478\n",
      "ma F1:  0.9247177254963889\n",
      "mi F1:  0.928254749037248\n",
      "we F1:  0.9285774007815694\n",
      "[[550 509]\n",
      " [339 592]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.52      0.56      1059\n",
      "           1       0.54      0.64      0.58       931\n",
      "\n",
      "    accuracy                           0.57      1990\n",
      "   macro avg       0.58      0.58      0.57      1990\n",
      "weighted avg       0.58      0.57      0.57      1990\n",
      "\n",
      "acc:  0.5738693467336683\n",
      "pre:  0.5376930063578564\n",
      "rec:  0.635875402792696\n",
      "ma F1:  0.5736794451001632\n",
      "mi F1:  0.5738693467336683\n",
      "we F1:  0.5731006972647193\n",
      "Loss:  0.05039812996983528\n",
      "Loss:  0.04056146368384361\n",
      "Loss:  0.08111290633678436\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.024806782603263855\n",
      "Loss:  0.04546261951327324\n",
      "Loss:  0.06522016227245331\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.051794372498989105\n",
      "Loss:  0.059814441949129105\n",
      "Loss:  0.03915512189269066\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  5 Training Time 5680.420709133148 Best Test Acc:  0.7155778894472362\n",
      "test subjects:  ['./seg\\\\a06', './seg\\\\x15']\n",
      "*********\n",
      "33305 1008\n",
      "31883 1008\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.8026854395866394\n",
      "Eval Loss:  0.5899487733840942\n",
      "Eval Loss:  0.7721058130264282\n",
      "[[    0 19562]\n",
      " [    0 12321]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     19562\n",
      "           1       0.39      1.00      0.56     12321\n",
      "\n",
      "    accuracy                           0.39     31883\n",
      "   macro avg       0.19      0.50      0.28     31883\n",
      "weighted avg       0.15      0.39      0.22     31883\n",
      "\n",
      "acc:  0.3864441865570994\n",
      "pre:  0.3864441865570994\n",
      "rec:  1.0\n",
      "ma F1:  0.27873043163514616\n",
      "mi F1:  0.38644418655709933\n",
      "we F1:  0.21542750984390652\n",
      "[[  0 602]\n",
      " [  0 406]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       602\n",
      "           1       0.40      1.00      0.57       406\n",
      "\n",
      "    accuracy                           0.40      1008\n",
      "   macro avg       0.20      0.50      0.29      1008\n",
      "weighted avg       0.16      0.40      0.23      1008\n",
      "\n",
      "acc:  0.4027777777777778\n",
      "pre:  0.4027777777777778\n",
      "rec:  1.0\n",
      "ma F1:  0.28712871287128716\n",
      "mi F1:  0.4027777777777778\n",
      "we F1:  0.2312981298129813\n",
      "Subject 6 Current Train Acc:  0.3864441865570994 Current Test Acc:  0.4027777777777778\n",
      "Loss:  0.17460542917251587\n",
      "Loss:  0.16813631355762482\n",
      "Loss:  0.17403961718082428\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.13947735726833344\n",
      "Loss:  0.13028621673583984\n",
      "Loss:  0.11207114905118942\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.10430192202329636\n",
      "Loss:  0.10898631066083908\n",
      "Loss:  0.12189403921365738\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.04201030731201172\n",
      "Eval Loss:  0.5073143243789673\n",
      "Eval Loss:  0.02977442741394043\n",
      "[[17620  1942]\n",
      " [ 3558  8763]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.86     19562\n",
      "           1       0.82      0.71      0.76     12321\n",
      "\n",
      "    accuracy                           0.83     31883\n",
      "   macro avg       0.83      0.81      0.81     31883\n",
      "weighted avg       0.83      0.83      0.82     31883\n",
      "\n",
      "acc:  0.8274942759464291\n",
      "pre:  0.8185894441849603\n",
      "rec:  0.7112247382517652\n",
      "ma F1:  0.8130685633763731\n",
      "mi F1:  0.8274942759464291\n",
      "we F1:  0.8248622389685346\n",
      "[[564  38]\n",
      " [211 195]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.94      0.82       602\n",
      "           1       0.84      0.48      0.61       406\n",
      "\n",
      "    accuracy                           0.75      1008\n",
      "   macro avg       0.78      0.71      0.71      1008\n",
      "weighted avg       0.77      0.75      0.74      1008\n",
      "\n",
      "acc:  0.7529761904761905\n",
      "pre:  0.8369098712446352\n",
      "rec:  0.4802955665024631\n",
      "ma F1:  0.7147503758937066\n",
      "mi F1:  0.7529761904761905\n",
      "we F1:  0.735054602609606\n",
      "Subject 6 Current Train Acc:  0.8274942759464291 Current Test Acc:  0.7529761904761905\n",
      "Loss:  0.09696780890226364\n",
      "Loss:  0.08280175179243088\n",
      "Loss:  0.09804845601320267\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.08673886209726334\n",
      "Loss:  0.09719732403755188\n",
      "Loss:  0.09993752092123032\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.05268425494432449\n",
      "Loss:  0.10874366015195847\n",
      "Loss:  0.08347280323505402\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.058553218841552734\n",
      "Eval Loss:  0.9870318174362183\n",
      "Eval Loss:  0.017385482788085938\n",
      "[[18734   828]\n",
      " [ 4164  8157]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.96      0.88     19562\n",
      "           1       0.91      0.66      0.77     12321\n",
      "\n",
      "    accuracy                           0.84     31883\n",
      "   macro avg       0.86      0.81      0.82     31883\n",
      "weighted avg       0.85      0.84      0.84     31883\n",
      "\n",
      "acc:  0.8434275319135589\n",
      "pre:  0.9078464106844741\n",
      "rec:  0.6620404187971756\n",
      "ma F1:  0.8240651628587305\n",
      "mi F1:  0.8434275319135589\n",
      "we F1:  0.8373206147189945\n",
      "[[589  13]\n",
      " [265 141]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.98      0.81       602\n",
      "           1       0.92      0.35      0.50       406\n",
      "\n",
      "    accuracy                           0.72      1008\n",
      "   macro avg       0.80      0.66      0.66      1008\n",
      "weighted avg       0.78      0.72      0.69      1008\n",
      "\n",
      "acc:  0.7242063492063492\n",
      "pre:  0.9155844155844156\n",
      "rec:  0.3472906403940887\n",
      "ma F1:  0.6563186813186813\n",
      "mi F1:  0.7242063492063492\n",
      "we F1:  0.686019536019536\n",
      "Loss:  0.12971335649490356\n",
      "Loss:  0.1153598502278328\n",
      "Loss:  0.07399314641952515\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.08451679348945618\n",
      "Loss:  0.08966752886772156\n",
      "Loss:  0.06428402662277222\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.10935100167989731\n",
      "Loss:  0.0869937539100647\n",
      "Loss:  0.0717431902885437\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.04679560661315918\n",
      "Eval Loss:  1.9200438261032104\n",
      "Eval Loss:  0.014685392379760742\n",
      "[[18751   811]\n",
      " [ 3884  8437]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89     19562\n",
      "           1       0.91      0.68      0.78     12321\n",
      "\n",
      "    accuracy                           0.85     31883\n",
      "   macro avg       0.87      0.82      0.84     31883\n",
      "weighted avg       0.86      0.85      0.85     31883\n",
      "\n",
      "acc:  0.8527428410124518\n",
      "pre:  0.9123053633217993\n",
      "rec:  0.6847658469280091\n",
      "ma F1:  0.8355313265830537\n",
      "mi F1:  0.8527428410124518\n",
      "we F1:  0.8476147642389847\n",
      "[[596   6]\n",
      " [293 113]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.99      0.80       602\n",
      "           1       0.95      0.28      0.43       406\n",
      "\n",
      "    accuracy                           0.70      1008\n",
      "   macro avg       0.81      0.63      0.61      1008\n",
      "weighted avg       0.78      0.70      0.65      1008\n",
      "\n",
      "acc:  0.7033730158730159\n",
      "pre:  0.9495798319327731\n",
      "rec:  0.27832512315270935\n",
      "ma F1:  0.6149698189134809\n",
      "mi F1:  0.7033730158730159\n",
      "we F1:  0.6508435799985096\n",
      "Loss:  0.1331675499677658\n",
      "Loss:  0.08238011598587036\n",
      "Loss:  0.06599660962820053\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.09760940819978714\n",
      "Loss:  0.07463004440069199\n",
      "Loss:  0.0672864317893982\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.08044154942035675\n",
      "Loss:  0.06494832783937454\n",
      "Loss:  0.06676327437162399\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.04573249816894531\n",
      "Eval Loss:  0.8842555284500122\n",
      "Eval Loss:  0.011774778366088867\n",
      "[[18795   767]\n",
      " [ 3642  8679]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90     19562\n",
      "           1       0.92      0.70      0.80     12321\n",
      "\n",
      "    accuracy                           0.86     31883\n",
      "   macro avg       0.88      0.83      0.85     31883\n",
      "weighted avg       0.87      0.86      0.86     31883\n",
      "\n",
      "acc:  0.8617131386632375\n",
      "pre:  0.9188016091467288\n",
      "rec:  0.7044071098125152\n",
      "ma F1:  0.846233492338148\n",
      "mi F1:  0.8617131386632375\n",
      "we F1:  0.8573137729866229\n",
      "[[591  11]\n",
      " [260 146]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.98      0.81       602\n",
      "           1       0.93      0.36      0.52       406\n",
      "\n",
      "    accuracy                           0.73      1008\n",
      "   macro avg       0.81      0.67      0.67      1008\n",
      "weighted avg       0.79      0.73      0.69      1008\n",
      "\n",
      "acc:  0.7311507936507936\n",
      "pre:  0.9299363057324841\n",
      "rec:  0.35960591133004927\n",
      "ma F1:  0.6660697106128193\n",
      "mi F1:  0.7311507936507936\n",
      "we F1:  0.6947346370744889\n",
      "Loss:  0.08345578610897064\n",
      "Loss:  0.051955122500658035\n",
      "Loss:  0.06442694365978241\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.07782739400863647\n",
      "Loss:  0.07165143638849258\n",
      "Loss:  0.0710492730140686\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.048603810369968414\n",
      "Loss:  0.07525413483381271\n",
      "Loss:  0.10040804743766785\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.025732994079589844\n",
      "Eval Loss:  1.1614491939544678\n",
      "Eval Loss:  0.010593175888061523\n",
      "[[19021   541]\n",
      " [ 4330  7991]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.97      0.89     19562\n",
      "           1       0.94      0.65      0.77     12321\n",
      "\n",
      "    accuracy                           0.85     31883\n",
      "   macro avg       0.88      0.81      0.83     31883\n",
      "weighted avg       0.86      0.85      0.84     31883\n",
      "\n",
      "acc:  0.8472226578427375\n",
      "pre:  0.9365916549460853\n",
      "rec:  0.6485674864053242\n",
      "ma F1:  0.8264518898172895\n",
      "mi F1:  0.8472226578427375\n",
      "we F1:  0.8400875318184019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[602   0]\n",
      " [334  72]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      1.00      0.78       602\n",
      "           1       1.00      0.18      0.30       406\n",
      "\n",
      "    accuracy                           0.67      1008\n",
      "   macro avg       0.82      0.59      0.54      1008\n",
      "weighted avg       0.79      0.67      0.59      1008\n",
      "\n",
      "acc:  0.6686507936507936\n",
      "pre:  1.0\n",
      "rec:  0.17733990147783252\n",
      "ma F1:  0.5420450402903298\n",
      "mi F1:  0.6686507936507936\n",
      "we F1:  0.588865281155709\n",
      "Loss:  0.05351709946990013\n",
      "Loss:  0.04542950168251991\n",
      "Loss:  0.07087201625108719\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.06763684004545212\n",
      "Loss:  0.04807901754975319\n",
      "Loss:  0.049881674349308014\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.054911334067583084\n",
      "Loss:  0.07946427166461945\n",
      "Loss:  0.07437507063150406\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.09389352798461914\n",
      "Eval Loss:  0.6571735739707947\n",
      "Eval Loss:  0.012449264526367188\n",
      "[[18720   842]\n",
      " [ 3031  9290]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91     19562\n",
      "           1       0.92      0.75      0.83     12321\n",
      "\n",
      "    accuracy                           0.88     31883\n",
      "   macro avg       0.89      0.86      0.87     31883\n",
      "weighted avg       0.88      0.88      0.88     31883\n",
      "\n",
      "acc:  0.8785246055891854\n",
      "pre:  0.9168969601263324\n",
      "rec:  0.753997240483727\n",
      "ma F1:  0.8668793079260737\n",
      "mi F1:  0.8785246055891854\n",
      "we F1:  0.8758213652302722\n",
      "[[595   7]\n",
      " [275 131]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.99      0.81       602\n",
      "           1       0.95      0.32      0.48       406\n",
      "\n",
      "    accuracy                           0.72      1008\n",
      "   macro avg       0.82      0.66      0.65      1008\n",
      "weighted avg       0.79      0.72      0.68      1008\n",
      "\n",
      "acc:  0.7202380952380952\n",
      "pre:  0.9492753623188406\n",
      "rec:  0.3226600985221675\n",
      "ma F1:  0.6450207800511509\n",
      "mi F1:  0.7202380952380952\n",
      "we F1:  0.6767936114663257\n",
      "Loss:  0.06984438747167587\n",
      "Loss:  0.0691501721739769\n",
      "Loss:  0.06907958537340164\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.04730509594082832\n",
      "Loss:  0.08233781158924103\n",
      "Loss:  0.048085056245326996\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.058178097009658813\n",
      "Loss:  0.10137569904327393\n",
      "Loss:  0.10472520440816879\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.12027579545974731\n",
      "Eval Loss:  0.40587180852890015\n",
      "Eval Loss:  0.01318669319152832\n",
      "[[18911   651]\n",
      " [ 3380  8941]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.90     19562\n",
      "           1       0.93      0.73      0.82     12321\n",
      "\n",
      "    accuracy                           0.87     31883\n",
      "   macro avg       0.89      0.85      0.86     31883\n",
      "weighted avg       0.88      0.87      0.87     31883\n",
      "\n",
      "acc:  0.8735689866072829\n",
      "pre:  0.9321309424520434\n",
      "rec:  0.7256716175635095\n",
      "ma F1:  0.8598659914752342\n",
      "mi F1:  0.8735689866072829\n",
      "we F1:  0.8698181868364343\n",
      "[[594   8]\n",
      " [260 146]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.99      0.82       602\n",
      "           1       0.95      0.36      0.52       406\n",
      "\n",
      "    accuracy                           0.73      1008\n",
      "   macro avg       0.82      0.67      0.67      1008\n",
      "weighted avg       0.80      0.73      0.70      1008\n",
      "\n",
      "acc:  0.7341269841269841\n",
      "pre:  0.948051948051948\n",
      "rec:  0.35960591133004927\n",
      "ma F1:  0.6686813186813187\n",
      "mi F1:  0.7341269841269842\n",
      "we F1:  0.6973137973137973\n",
      "Loss:  0.05786769092082977\n",
      "Loss:  0.05573014169931412\n",
      "Loss:  0.06459780782461166\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.08557119220495224\n",
      "Loss:  0.04544559866189957\n",
      "Loss:  0.06826288998126984\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.0670849084854126\n",
      "Loss:  0.05000666528940201\n",
      "Loss:  0.06524820625782013\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.4729289412498474\n",
      "Eval Loss:  0.13898223638534546\n",
      "Eval Loss:  0.04147374629974365\n",
      "[[18131  1431]\n",
      " [ 1742 10579]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92     19562\n",
      "           1       0.88      0.86      0.87     12321\n",
      "\n",
      "    accuracy                           0.90     31883\n",
      "   macro avg       0.90      0.89      0.89     31883\n",
      "weighted avg       0.90      0.90      0.90     31883\n",
      "\n",
      "acc:  0.9004798795596399\n",
      "pre:  0.8808492922564529\n",
      "rec:  0.8586153721288856\n",
      "ma F1:  0.8945643578623974\n",
      "mi F1:  0.9004798795596399\n",
      "we F1:  0.9002362716083896\n",
      "[[541  61]\n",
      " [190 216]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.90      0.81       602\n",
      "           1       0.78      0.53      0.63       406\n",
      "\n",
      "    accuracy                           0.75      1008\n",
      "   macro avg       0.76      0.72      0.72      1008\n",
      "weighted avg       0.76      0.75      0.74      1008\n",
      "\n",
      "acc:  0.7509920634920635\n",
      "pre:  0.779783393501805\n",
      "rec:  0.5320197044334976\n",
      "ma F1:  0.7221032930267706\n",
      "mi F1:  0.7509920634920635\n",
      "we F1:  0.7395254438304548\n",
      "Loss:  0.10442283004522324\n",
      "Loss:  0.0831819474697113\n",
      "Loss:  0.08457125723361969\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.04237717017531395\n",
      "Loss:  0.07121476531028748\n",
      "Loss:  0.0837407186627388\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.034720342606306076\n",
      "Loss:  0.07531724870204926\n",
      "Loss:  0.05642108991742134\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.3611048460006714\n",
      "Eval Loss:  0.23512113094329834\n",
      "Eval Loss:  0.02917015552520752\n",
      "[[18640   922]\n",
      " [ 2184 10137]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     19562\n",
      "           1       0.92      0.82      0.87     12321\n",
      "\n",
      "    accuracy                           0.90     31883\n",
      "   macro avg       0.91      0.89      0.90     31883\n",
      "weighted avg       0.90      0.90      0.90     31883\n",
      "\n",
      "acc:  0.9025813129253835\n",
      "pre:  0.9166289899629261\n",
      "rec:  0.8227416605794984\n",
      "ma F1:  0.8951217860562594\n",
      "mi F1:  0.9025813129253835\n",
      "we F1:  0.9014741833347878\n",
      "[[560  42]\n",
      " [206 200]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.93      0.82       602\n",
      "           1       0.83      0.49      0.62       406\n",
      "\n",
      "    accuracy                           0.75      1008\n",
      "   macro avg       0.78      0.71      0.72      1008\n",
      "weighted avg       0.77      0.75      0.74      1008\n",
      "\n",
      "acc:  0.753968253968254\n",
      "pre:  0.8264462809917356\n",
      "rec:  0.49261083743842365\n",
      "ma F1:  0.7179987004548408\n",
      "mi F1:  0.753968253968254\n",
      "we F1:  0.7375821240343657\n",
      "Subject 6 Current Train Acc:  0.9025813129253835 Current Test Acc:  0.753968253968254\n",
      "Loss:  0.07468297332525253\n",
      "Loss:  0.06928472220897675\n",
      "Loss:  0.03569963201880455\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.05965205654501915\n",
      "Loss:  0.08169528096914291\n",
      "Loss:  0.07435565441846848\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.052947621792554855\n",
      "Loss:  0.06600917130708694\n",
      "Loss:  0.07715348899364471\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.10913991928100586\n",
      "Eval Loss:  0.0970923900604248\n",
      "Eval Loss:  0.020801186561584473\n",
      "[[18591   971]\n",
      " [ 1950 10371]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19562\n",
      "           1       0.91      0.84      0.88     12321\n",
      "\n",
      "    accuracy                           0.91     31883\n",
      "   macro avg       0.91      0.90      0.90     31883\n",
      "weighted avg       0.91      0.91      0.91     31883\n",
      "\n",
      "acc:  0.9083837781890035\n",
      "pre:  0.9143889966496209\n",
      "rec:  0.8417336255174093\n",
      "ma F1:  0.9018604482208683\n",
      "mi F1:  0.9083837781890035\n",
      "we F1:  0.9076068511769835\n",
      "[[525  77]\n",
      " [169 237]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.87      0.81       602\n",
      "           1       0.75      0.58      0.66       406\n",
      "\n",
      "    accuracy                           0.76      1008\n",
      "   macro avg       0.76      0.73      0.73      1008\n",
      "weighted avg       0.76      0.76      0.75      1008\n",
      "\n",
      "acc:  0.7559523809523809\n",
      "pre:  0.7547770700636943\n",
      "rec:  0.583743842364532\n",
      "ma F1:  0.7342592592592593\n",
      "mi F1:  0.7559523809523809\n",
      "we F1:  0.749022633744856\n",
      "Subject 6 Current Train Acc:  0.9083837781890035 Current Test Acc:  0.7559523809523809\n",
      "Loss:  0.06138424947857857\n",
      "Loss:  0.05579441040754318\n",
      "Loss:  0.0629134401679039\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.05848006531596184\n",
      "Loss:  0.05120345205068588\n",
      "Loss:  0.055593833327293396\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.07192599773406982\n",
      "Loss:  0.06370403617620468\n",
      "Loss:  0.06228692829608917\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.16048043966293335\n",
      "Eval Loss:  0.35798490047454834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss:  0.02996659278869629\n",
      "[[18761   801]\n",
      " [ 2233 10088]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.93     19562\n",
      "           1       0.93      0.82      0.87     12321\n",
      "\n",
      "    accuracy                           0.90     31883\n",
      "   macro avg       0.91      0.89      0.90     31883\n",
      "weighted avg       0.91      0.90      0.90     31883\n",
      "\n",
      "acc:  0.9048395696766302\n",
      "pre:  0.9264395261272844\n",
      "rec:  0.8187647106566025\n",
      "ma F1:  0.8972351717418279\n",
      "mi F1:  0.9048395696766302\n",
      "we F1:  0.903584006683129\n",
      "[[473 129]\n",
      " [160 246]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.79      0.77       602\n",
      "           1       0.66      0.61      0.63       406\n",
      "\n",
      "    accuracy                           0.71      1008\n",
      "   macro avg       0.70      0.70      0.70      1008\n",
      "weighted avg       0.71      0.71      0.71      1008\n",
      "\n",
      "acc:  0.7132936507936508\n",
      "pre:  0.656\n",
      "rec:  0.6059113300492611\n",
      "ma F1:  0.6979767452710374\n",
      "mi F1:  0.7132936507936508\n",
      "we F1:  0.7112019147971707\n",
      "Loss:  0.0447932630777359\n",
      "Loss:  0.07407119125127792\n",
      "Loss:  0.057309310883283615\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.06405030190944672\n",
      "Loss:  0.039826326072216034\n",
      "Loss:  0.05703084170818329\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.05785229057073593\n",
      "Loss:  0.10060259699821472\n",
      "Loss:  0.0754513368010521\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.3546207845211029\n",
      "Eval Loss:  0.1725102663040161\n",
      "Eval Loss:  0.05680227279663086\n",
      "[[18442  1120]\n",
      " [ 1756 10565]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93     19562\n",
      "           1       0.90      0.86      0.88     12321\n",
      "\n",
      "    accuracy                           0.91     31883\n",
      "   macro avg       0.91      0.90      0.90     31883\n",
      "weighted avg       0.91      0.91      0.91     31883\n",
      "\n",
      "acc:  0.9097951886585327\n",
      "pre:  0.904150620453573\n",
      "rec:  0.857479100722344\n",
      "ma F1:  0.903931306744072\n",
      "mi F1:  0.9097951886585327\n",
      "we F1:  0.9093217306291311\n",
      "[[558  44]\n",
      " [168 238]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.93      0.84       602\n",
      "           1       0.84      0.59      0.69       406\n",
      "\n",
      "    accuracy                           0.79      1008\n",
      "   macro avg       0.81      0.76      0.77      1008\n",
      "weighted avg       0.80      0.79      0.78      1008\n",
      "\n",
      "acc:  0.7896825396825397\n",
      "pre:  0.8439716312056738\n",
      "rec:  0.5862068965517241\n",
      "ma F1:  0.7661109554497056\n",
      "mi F1:  0.7896825396825397\n",
      "we F1:  0.7805485507923164\n",
      "Subject 6 Current Train Acc:  0.9097951886585327 Current Test Acc:  0.7896825396825397\n",
      "Loss:  0.05115465447306633\n",
      "Loss:  0.05589926615357399\n",
      "Loss:  0.07679110765457153\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.0495123416185379\n",
      "Loss:  0.04892739653587341\n",
      "Loss:  0.05866144225001335\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.07694219797849655\n",
      "Loss:  0.04631584510207176\n",
      "Loss:  0.060149554163217545\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.39465612173080444\n",
      "Eval Loss:  0.639851450920105\n",
      "Eval Loss:  0.0277559757232666\n",
      "[[18732   830]\n",
      " [ 2082 10239]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     19562\n",
      "           1       0.93      0.83      0.88     12321\n",
      "\n",
      "    accuracy                           0.91     31883\n",
      "   macro avg       0.91      0.89      0.90     31883\n",
      "weighted avg       0.91      0.91      0.91     31883\n",
      "\n",
      "acc:  0.9086660602829094\n",
      "pre:  0.9250158099195953\n",
      "rec:  0.8310202093985878\n",
      "ma F1:  0.9016901493638296\n",
      "mi F1:  0.9086660602829094\n",
      "we F1:  0.9076377027566304\n",
      "[[554  48]\n",
      " [169 237]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.92      0.84       602\n",
      "           1       0.83      0.58      0.69       406\n",
      "\n",
      "    accuracy                           0.78      1008\n",
      "   macro avg       0.80      0.75      0.76      1008\n",
      "weighted avg       0.79      0.78      0.78      1008\n",
      "\n",
      "acc:  0.7847222222222222\n",
      "pre:  0.8315789473684211\n",
      "rec:  0.583743842364532\n",
      "ma F1:  0.7610943942331323\n",
      "mi F1:  0.7847222222222222\n",
      "we F1:  0.7757033982894781\n",
      "Loss:  0.06482241302728653\n",
      "Loss:  0.0719296857714653\n",
      "Loss:  0.055927030742168427\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.06748455762863159\n",
      "Loss:  0.0732022225856781\n",
      "Loss:  0.053372081369161606\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.06398139894008636\n",
      "Loss:  0.05752808228135109\n",
      "Loss:  0.0465889610350132\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.3801599144935608\n",
      "Eval Loss:  0.11673367023468018\n",
      "Eval Loss:  0.052326321601867676\n",
      "[[18373  1189]\n",
      " [ 1424 10897]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     19562\n",
      "           1       0.90      0.88      0.89     12321\n",
      "\n",
      "    accuracy                           0.92     31883\n",
      "   macro avg       0.91      0.91      0.91     31883\n",
      "weighted avg       0.92      0.92      0.92     31883\n",
      "\n",
      "acc:  0.9180440987360036\n",
      "pre:  0.9016217110706602\n",
      "rec:  0.8844249655060465\n",
      "ma F1:  0.9132758340054554\n",
      "mi F1:  0.9180440987360036\n",
      "we F1:  0.9178942134749443\n",
      "[[544  58]\n",
      " [123 283]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.90      0.86       602\n",
      "           1       0.83      0.70      0.76       406\n",
      "\n",
      "    accuracy                           0.82      1008\n",
      "   macro avg       0.82      0.80      0.81      1008\n",
      "weighted avg       0.82      0.82      0.82      1008\n",
      "\n",
      "acc:  0.8204365079365079\n",
      "pre:  0.8299120234604106\n",
      "rec:  0.6970443349753694\n",
      "ma F1:  0.8075327313984069\n",
      "mi F1:  0.8204365079365079\n",
      "we F1:  0.8172229237411954\n",
      "Subject 6 Current Train Acc:  0.9180440987360036 Current Test Acc:  0.8204365079365079\n",
      "Loss:  0.0771820917725563\n",
      "Loss:  0.06482357531785965\n",
      "Loss:  0.07187910377979279\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.06655096262693405\n",
      "Loss:  0.04625306278467178\n",
      "Loss:  0.0558885857462883\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.045305803418159485\n",
      "Loss:  0.04295947402715683\n",
      "Loss:  0.08211620151996613\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.7377937436103821\n",
      "Eval Loss:  0.6126494407653809\n",
      "Eval Loss:  0.10499989986419678\n",
      "[[18089  1473]\n",
      " [ 1257 11064]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     19562\n",
      "           1       0.88      0.90      0.89     12321\n",
      "\n",
      "    accuracy                           0.91     31883\n",
      "   macro avg       0.91      0.91      0.91     31883\n",
      "weighted avg       0.91      0.91      0.91     31883\n",
      "\n",
      "acc:  0.9143744315152276\n",
      "pre:  0.8825077769801388\n",
      "rec:  0.8979790601412223\n",
      "ma F1:  0.9100053410806299\n",
      "mi F1:  0.9143744315152276\n",
      "we F1:  0.914508769384818\n",
      "[[550  52]\n",
      " [143 263]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.85       602\n",
      "           1       0.83      0.65      0.73       406\n",
      "\n",
      "    accuracy                           0.81      1008\n",
      "   macro avg       0.81      0.78      0.79      1008\n",
      "weighted avg       0.81      0.81      0.80      1008\n",
      "\n",
      "acc:  0.8065476190476191\n",
      "pre:  0.834920634920635\n",
      "rec:  0.6477832512315271\n",
      "ma F1:  0.789481575889343\n",
      "mi F1:  0.8065476190476192\n",
      "we F1:  0.8011364346315802\n",
      "Loss:  0.040350694209337234\n",
      "Loss:  0.05417904257774353\n",
      "Loss:  0.0439833402633667\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.05410263314843178\n",
      "Loss:  0.06720028072595596\n",
      "Loss:  0.03839203715324402\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.04707801714539528\n",
      "Loss:  0.05371343716979027\n",
      "Loss:  0.03699706494808197\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.817703902721405\n",
      "Eval Loss:  0.06982207298278809\n",
      "Eval Loss:  0.12730979919433594\n",
      "[[17664  1898]\n",
      " [  891 11430]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.93     19562\n",
      "           1       0.86      0.93      0.89     12321\n",
      "\n",
      "    accuracy                           0.91     31883\n",
      "   macro avg       0.90      0.92      0.91     31883\n",
      "weighted avg       0.92      0.91      0.91     31883\n",
      "\n",
      "acc:  0.9125239155662892\n",
      "pre:  0.857593037214886\n",
      "rec:  0.9276844411979547\n",
      "ma F1:  0.9090466826566288\n",
      "mi F1:  0.9125239155662892\n",
      "we F1:  0.9130856052583053\n",
      "[[526  76]\n",
      " [113 293]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.85       602\n",
      "           1       0.79      0.72      0.76       406\n",
      "\n",
      "    accuracy                           0.81      1008\n",
      "   macro avg       0.81      0.80      0.80      1008\n",
      "weighted avg       0.81      0.81      0.81      1008\n",
      "\n",
      "acc:  0.8125\n",
      "pre:  0.7940379403794038\n",
      "rec:  0.7216748768472906\n",
      "ma F1:  0.8019162486028437\n",
      "mi F1:  0.8125\n",
      "we F1:  0.8108193184476619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.07034910470247269\n",
      "Loss:  0.055760763585567474\n",
      "Loss:  0.05778679996728897\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.07714997977018356\n",
      "Loss:  0.04999712109565735\n",
      "Loss:  0.011895699426531792\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.05789926275610924\n",
      "Loss:  0.06364940106868744\n",
      "Loss:  0.05325448885560036\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  1.2064781188964844\n",
      "Eval Loss:  0.08079159259796143\n",
      "Eval Loss:  0.09115147590637207\n",
      "[[17951  1611]\n",
      " [ 1023 11298]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.93     19562\n",
      "           1       0.88      0.92      0.90     12321\n",
      "\n",
      "    accuracy                           0.92     31883\n",
      "   macro avg       0.91      0.92      0.91     31883\n",
      "weighted avg       0.92      0.92      0.92     31883\n",
      "\n",
      "acc:  0.9173854405168899\n",
      "pre:  0.8752033465024401\n",
      "rec:  0.9169710250791332\n",
      "ma F1:  0.9136244022298158\n",
      "mi F1:  0.9173854405168899\n",
      "we F1:  0.9177178455240745\n",
      "[[509  93]\n",
      " [ 92 314]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       602\n",
      "           1       0.77      0.77      0.77       406\n",
      "\n",
      "    accuracy                           0.82      1008\n",
      "   macro avg       0.81      0.81      0.81      1008\n",
      "weighted avg       0.82      0.82      0.82      1008\n",
      "\n",
      "acc:  0.816468253968254\n",
      "pre:  0.7714987714987716\n",
      "rec:  0.7733990147783252\n",
      "ma F1:  0.8093327566692126\n",
      "mi F1:  0.816468253968254\n",
      "we F1:  0.8165048462620953\n",
      "Loss:  0.039332564920186996\n",
      "Loss:  0.057316623628139496\n",
      "Loss:  0.062161970883607864\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.05539605766534805\n",
      "Loss:  0.06976065784692764\n",
      "Loss:  0.059939250349998474\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.06270958483219147\n",
      "Loss:  0.04986007511615753\n",
      "Loss:  0.05386803671717644\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.8845800161361694\n",
      "Eval Loss:  0.32639673352241516\n",
      "Eval Loss:  0.05216670036315918\n",
      "[[18724   838]\n",
      " [ 1739 10582]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94     19562\n",
      "           1       0.93      0.86      0.89     12321\n",
      "\n",
      "    accuracy                           0.92     31883\n",
      "   macro avg       0.92      0.91      0.91     31883\n",
      "weighted avg       0.92      0.92      0.92     31883\n",
      "\n",
      "acc:  0.9191732271116269\n",
      "pre:  0.9266199649737303\n",
      "rec:  0.8588588588588588\n",
      "ma F1:  0.9135344219727461\n",
      "mi F1:  0.9191732271116269\n",
      "we F1:  0.9185492325856958\n",
      "[[531  71]\n",
      " [107 299]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.86       602\n",
      "           1       0.81      0.74      0.77       406\n",
      "\n",
      "    accuracy                           0.82      1008\n",
      "   macro avg       0.82      0.81      0.81      1008\n",
      "weighted avg       0.82      0.82      0.82      1008\n",
      "\n",
      "acc:  0.8234126984126984\n",
      "pre:  0.8081081081081081\n",
      "rec:  0.7364532019704434\n",
      "ma F1:  0.8135350848021283\n",
      "mi F1:  0.8234126984126985\n",
      "we F1:  0.8218799652662304\n",
      "Subject 6 Current Train Acc:  0.9191732271116269 Current Test Acc:  0.8234126984126984\n",
      "Loss:  0.041382141411304474\n",
      "Loss:  0.04412541165947914\n",
      "Loss:  0.07612928748130798\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.04900399222970009\n",
      "Loss:  0.07896864414215088\n",
      "Loss:  0.057030096650123596\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.03880512714385986\n",
      "Loss:  0.058062486350536346\n",
      "Loss:  0.036383699625730515\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.3460487425327301\n",
      "Eval Loss:  0.11027669906616211\n",
      "Eval Loss:  0.049245476722717285\n",
      "[[18476  1086]\n",
      " [ 1453 10868]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     19562\n",
      "           1       0.91      0.88      0.90     12321\n",
      "\n",
      "    accuracy                           0.92     31883\n",
      "   macro avg       0.92      0.91      0.92     31883\n",
      "weighted avg       0.92      0.92      0.92     31883\n",
      "\n",
      "acc:  0.9203650848414515\n",
      "pre:  0.9091517483687469\n",
      "rec:  0.8820712604496388\n",
      "ma F1:  0.9155568335180517\n",
      "mi F1:  0.9203650848414515\n",
      "we F1:  0.9201331410670446\n",
      "[[535  67]\n",
      " [118 288]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.89      0.85       602\n",
      "           1       0.81      0.71      0.76       406\n",
      "\n",
      "    accuracy                           0.82      1008\n",
      "   macro avg       0.82      0.80      0.80      1008\n",
      "weighted avg       0.82      0.82      0.81      1008\n",
      "\n",
      "acc:  0.816468253968254\n",
      "pre:  0.8112676056338028\n",
      "rec:  0.7093596059113301\n",
      "ma F1:  0.8047442293899304\n",
      "mi F1:  0.816468253968254\n",
      "we F1:  0.8140475039541062\n",
      "Loss:  0.0642569437623024\n",
      "Loss:  0.09427917003631592\n",
      "Loss:  0.036753326654434204\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.034808263182640076\n",
      "Loss:  0.0760650783777237\n",
      "Loss:  0.031076345592737198\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.07637374103069305\n",
      "Loss:  0.06272897869348526\n",
      "Loss:  0.035911742597818375\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  6 Training Time 5777.847210645676 Best Test Acc:  0.8234126984126984\n",
      "test subjects:  ['./seg\\\\a07', './seg\\\\a16', './seg\\\\x01', './seg\\\\x30']\n",
      "*********\n",
      "32286 2027\n",
      "30874 2017\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.7214403748512268\n",
      "Eval Loss:  0.6637871861457825\n",
      "Eval Loss:  0.7323856353759766\n",
      "[[ 1197 18293]\n",
      " [ 1189 10195]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.06      0.11     19490\n",
      "           1       0.36      0.90      0.51     11384\n",
      "\n",
      "    accuracy                           0.37     30874\n",
      "   macro avg       0.43      0.48      0.31     30874\n",
      "weighted avg       0.45      0.37      0.26     30874\n",
      "\n",
      "acc:  0.36898361080520825\n",
      "pre:  0.35786998034260037\n",
      "rec:  0.8955551651440619\n",
      "ma F1:  0.31041071692718947\n",
      "mi F1:  0.36898361080520825\n",
      "we F1:  0.25764433797906544\n",
      "[[  39  635]\n",
      " [ 110 1233]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.06      0.09       674\n",
      "           1       0.66      0.92      0.77      1343\n",
      "\n",
      "    accuracy                           0.63      2017\n",
      "   macro avg       0.46      0.49      0.43      2017\n",
      "weighted avg       0.53      0.63      0.54      2017\n",
      "\n",
      "acc:  0.630639563708478\n",
      "pre:  0.6600642398286938\n",
      "rec:  0.9180938198064036\n",
      "ma F1:  0.431380132011278\n",
      "mi F1:  0.630639563708478\n",
      "we F1:  0.5430254919823222\n",
      "Subject 7 Current Train Acc:  0.36898361080520825 Current Test Acc:  0.630639563708478\n",
      "Loss:  0.16890472173690796\n",
      "Loss:  0.16504177451133728\n",
      "Loss:  0.17031480371952057\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.15044666826725006\n",
      "Loss:  0.14949703216552734\n",
      "Loss:  0.13056743144989014\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.12659430503845215\n",
      "Loss:  0.09475581347942352\n",
      "Loss:  0.10731707513332367\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.03207087516784668\n",
      "Eval Loss:  0.7196483016014099\n",
      "Eval Loss:  0.0629878044128418\n",
      "[[17376  2114]\n",
      " [ 3319  8065]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86     19490\n",
      "           1       0.79      0.71      0.75     11384\n",
      "\n",
      "    accuracy                           0.82     30874\n",
      "   macro avg       0.82      0.80      0.81     30874\n",
      "weighted avg       0.82      0.82      0.82     30874\n",
      "\n",
      "acc:  0.8240266891235344\n",
      "pre:  0.7923175164554475\n",
      "rec:  0.7084504567814477\n",
      "ma F1:  0.806420461881906\n",
      "mi F1:  0.8240266891235344\n",
      "we F1:  0.8217481472025633\n",
      "[[ 514  160]\n",
      " [ 251 1092]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.76      0.71       674\n",
      "           1       0.87      0.81      0.84      1343\n",
      "\n",
      "    accuracy                           0.80      2017\n",
      "   macro avg       0.77      0.79      0.78      2017\n",
      "weighted avg       0.81      0.80      0.80      2017\n",
      "\n",
      "acc:  0.7962320277640059\n",
      "pre:  0.8722044728434505\n",
      "rec:  0.8131049888309755\n",
      "ma F1:  0.7780017433429607\n",
      "mi F1:  0.7962320277640059\n",
      "we F1:  0.7991021936503644\n",
      "Subject 7 Current Train Acc:  0.8240266891235344 Current Test Acc:  0.7962320277640059\n",
      "Loss:  0.10622624307870865\n",
      "Loss:  0.10162930190563202\n",
      "Loss:  0.10020165145397186\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.08468989282846451\n",
      "Loss:  0.10871928930282593\n",
      "Loss:  0.10935793071985245\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.0881328284740448\n",
      "Loss:  0.09535643458366394\n",
      "Loss:  0.06841647624969482\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.013902425765991211\n",
      "Eval Loss:  0.3167459964752197\n",
      "Eval Loss:  0.03509938716888428\n",
      "[[18622   868]\n",
      " [ 3892  7492]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89     19490\n",
      "           1       0.90      0.66      0.76     11384\n",
      "\n",
      "    accuracy                           0.85     30874\n",
      "   macro avg       0.86      0.81      0.82     30874\n",
      "weighted avg       0.85      0.85      0.84     30874\n",
      "\n",
      "acc:  0.8458249659908014\n",
      "pre:  0.8961722488038277\n",
      "rec:  0.6581166549543218\n",
      "ma F1:  0.8227957798879075\n",
      "mi F1:  0.8458249659908014\n",
      "we F1:  0.8395679795779396\n",
      "[[ 520  154]\n",
      " [ 245 1098]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.77      0.72       674\n",
      "           1       0.88      0.82      0.85      1343\n",
      "\n",
      "    accuracy                           0.80      2017\n",
      "   macro avg       0.78      0.79      0.78      2017\n",
      "weighted avg       0.81      0.80      0.80      2017\n",
      "\n",
      "acc:  0.8021814576103123\n",
      "pre:  0.8769968051118211\n",
      "rec:  0.8175725986597171\n",
      "ma F1:  0.7844834442672537\n",
      "mi F1:  0.8021814576103123\n",
      "we F1:  0.8049678230328355\n",
      "Subject 7 Current Train Acc:  0.8458249659908014 Current Test Acc:  0.8021814576103123\n",
      "Loss:  0.09067773818969727\n",
      "Loss:  0.06186347082257271\n",
      "Loss:  0.0906459391117096\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.05179687961935997\n",
      "Loss:  0.10724060982465744\n",
      "Loss:  0.09797704219818115\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.0849616751074791\n",
      "Loss:  0.08236056566238403\n",
      "Loss:  0.07994399964809418\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.01259160041809082\n",
      "Eval Loss:  0.40233200788497925\n",
      "Eval Loss:  0.06429636478424072\n",
      "[[18875   615]\n",
      " [ 3898  7486]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.89     19490\n",
      "           1       0.92      0.66      0.77     11384\n",
      "\n",
      "    accuracy                           0.85     30874\n",
      "   macro avg       0.88      0.81      0.83     30874\n",
      "weighted avg       0.86      0.85      0.85     30874\n",
      "\n",
      "acc:  0.8538252251085056\n",
      "pre:  0.9240834464880879\n",
      "rec:  0.6575895994378075\n",
      "ma F1:  0.830801113190117\n",
      "mi F1:  0.8538252251085056\n",
      "we F1:  0.8471882807386688\n",
      "[[ 542  132]\n",
      " [ 282 1061]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.80      0.72       674\n",
      "           1       0.89      0.79      0.84      1343\n",
      "\n",
      "    accuracy                           0.79      2017\n",
      "   macro avg       0.77      0.80      0.78      2017\n",
      "weighted avg       0.81      0.79      0.80      2017\n",
      "\n",
      "acc:  0.7947446703024293\n",
      "pre:  0.8893545683151718\n",
      "rec:  0.7900223380491437\n",
      "ma F1:  0.7801911486608855\n",
      "mi F1:  0.7947446703024295\n",
      "we F1:  0.7989508904300432\n",
      "Loss:  0.07802420854568481\n",
      "Loss:  0.05864717811346054\n",
      "Loss:  0.09060704708099365\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.061730287969112396\n",
      "Loss:  0.09647761285305023\n",
      "Loss:  0.05233411863446236\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.07060343772172928\n",
      "Loss:  0.07744228839874268\n",
      "Loss:  0.06795653700828552\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.010365009307861328\n",
      "Eval Loss:  0.2362903356552124\n",
      "Eval Loss:  0.048078298568725586\n",
      "[[19039   451]\n",
      " [ 4173  7211]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.98      0.89     19490\n",
      "           1       0.94      0.63      0.76     11384\n",
      "\n",
      "    accuracy                           0.85     30874\n",
      "   macro avg       0.88      0.81      0.82     30874\n",
      "weighted avg       0.86      0.85      0.84     30874\n",
      "\n",
      "acc:  0.8502299669624928\n",
      "pre:  0.9411380840511616\n",
      "rec:  0.6334328882642305\n",
      "ma F1:  0.8244670187158405\n",
      "mi F1:  0.8502299669624928\n",
      "we F1:  0.8421229587299901\n",
      "[[ 538  136]\n",
      " [ 288 1055]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.80      0.72       674\n",
      "           1       0.89      0.79      0.83      1343\n",
      "\n",
      "    accuracy                           0.79      2017\n",
      "   macro avg       0.77      0.79      0.78      2017\n",
      "weighted avg       0.81      0.79      0.79      2017\n",
      "\n",
      "acc:  0.789786812097174\n",
      "pre:  0.8858102434928632\n",
      "rec:  0.7855547282204021\n",
      "ma F1:  0.7750044725072349\n",
      "mi F1:  0.789786812097174\n",
      "we F1:  0.7941328771216821\n",
      "Loss:  0.04539456218481064\n",
      "Loss:  0.07015708088874817\n",
      "Loss:  0.06667712330818176\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.11374881863594055\n",
      "Loss:  0.062282733619213104\n",
      "Loss:  0.07112395763397217\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.06556127965450287\n",
      "Loss:  0.07289281487464905\n",
      "Loss:  0.06974148005247116\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.017127037048339844\n",
      "Eval Loss:  0.21048009395599365\n",
      "Eval Loss:  0.26836222410202026\n",
      "[[18601   889]\n",
      " [ 2652  8732]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91     19490\n",
      "           1       0.91      0.77      0.83     11384\n",
      "\n",
      "    accuracy                           0.89     30874\n",
      "   macro avg       0.89      0.86      0.87     30874\n",
      "weighted avg       0.89      0.89      0.88     30874\n",
      "\n",
      "acc:  0.8853080261708881\n",
      "pre:  0.9075979627897308\n",
      "rec:  0.7670414617006325\n",
      "ma F1:  0.8722552276304585\n",
      "mi F1:  0.8853080261708881\n",
      "we F1:  0.8829762718060307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 456  218]\n",
      " [ 172 1171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.68      0.70       674\n",
      "           1       0.84      0.87      0.86      1343\n",
      "\n",
      "    accuracy                           0.81      2017\n",
      "   macro avg       0.78      0.77      0.78      2017\n",
      "weighted avg       0.80      0.81      0.80      2017\n",
      "\n",
      "acc:  0.8066435299950422\n",
      "pre:  0.8430525557955364\n",
      "rec:  0.8719285182427401\n",
      "ma F1:  0.7788541336338058\n",
      "mi F1:  0.8066435299950422\n",
      "we F1:  0.8048556807186549\n",
      "Subject 7 Current Train Acc:  0.8853080261708881 Current Test Acc:  0.8066435299950422\n",
      "Loss:  0.106001116335392\n",
      "Loss:  0.07625915855169296\n",
      "Loss:  0.05805701017379761\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.0598566047847271\n",
      "Loss:  0.05334740877151489\n",
      "Loss:  0.07365667074918747\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.11144693195819855\n",
      "Loss:  0.0890921875834465\n",
      "Loss:  0.05660754814743996\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.019910812377929688\n",
      "Eval Loss:  0.15572011470794678\n",
      "Eval Loss:  0.2404773235321045\n",
      "[[18595   895]\n",
      " [ 2376  9008]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     19490\n",
      "           1       0.91      0.79      0.85     11384\n",
      "\n",
      "    accuracy                           0.89     30874\n",
      "   macro avg       0.90      0.87      0.88     30874\n",
      "weighted avg       0.90      0.89      0.89     30874\n",
      "\n",
      "acc:  0.8940532486882167\n",
      "pre:  0.9096233464606684\n",
      "rec:  0.7912860154602952\n",
      "ma F1:  0.8827474297324363\n",
      "mi F1:  0.8940532486882167\n",
      "we F1:  0.892306725492899\n",
      "[[ 436  238]\n",
      " [ 148 1195]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.65      0.69       674\n",
      "           1       0.83      0.89      0.86      1343\n",
      "\n",
      "    accuracy                           0.81      2017\n",
      "   macro avg       0.79      0.77      0.78      2017\n",
      "weighted avg       0.80      0.81      0.80      2017\n",
      "\n",
      "acc:  0.8086266732771443\n",
      "pre:  0.8339148639218423\n",
      "rec:  0.8897989575577067\n",
      "ma F1:  0.7770573803164074\n",
      "mi F1:  0.8086266732771443\n",
      "we F1:  0.8048832788549226\n",
      "Subject 7 Current Train Acc:  0.8940532486882167 Current Test Acc:  0.8086266732771443\n",
      "Loss:  0.050216276198625565\n",
      "Loss:  0.07288586348295212\n",
      "Loss:  0.053076133131980896\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.0954342782497406\n",
      "Loss:  0.07880798727273941\n",
      "Loss:  0.057430706918239594\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.03566008433699608\n",
      "Loss:  0.04770452529191971\n",
      "Loss:  0.04320914298295975\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.015089988708496094\n",
      "Eval Loss:  0.23142963647842407\n",
      "Eval Loss:  0.12005770206451416\n",
      "[[18777   713]\n",
      " [ 2744  8640]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.92     19490\n",
      "           1       0.92      0.76      0.83     11384\n",
      "\n",
      "    accuracy                           0.89     30874\n",
      "   macro avg       0.90      0.86      0.87     30874\n",
      "weighted avg       0.89      0.89      0.89     30874\n",
      "\n",
      "acc:  0.8880287620651681\n",
      "pre:  0.9237677750454399\n",
      "rec:  0.7589599437807449\n",
      "ma F1:  0.8744993449647773\n",
      "mi F1:  0.8880287620651681\n",
      "we F1:  0.8853180738802127\n",
      "[[ 519  155]\n",
      " [ 257 1086]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.77      0.72       674\n",
      "           1       0.88      0.81      0.84      1343\n",
      "\n",
      "    accuracy                           0.80      2017\n",
      "   macro avg       0.77      0.79      0.78      2017\n",
      "weighted avg       0.81      0.80      0.80      2017\n",
      "\n",
      "acc:  0.7957362419434805\n",
      "pre:  0.8751007252215954\n",
      "rec:  0.8086373790022338\n",
      "ma F1:  0.7782096722536564\n",
      "mi F1:  0.7957362419434805\n",
      "we F1:  0.7988891698241893\n",
      "Loss:  0.06454771757125854\n",
      "Loss:  0.03547096997499466\n",
      "Loss:  0.04640224948525429\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.05737384036183357\n",
      "Loss:  0.027169428765773773\n",
      "Loss:  0.08998812735080719\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.0675559788942337\n",
      "Loss:  0.07604897767305374\n",
      "Loss:  0.07700488716363907\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.08951902389526367\n",
      "Eval Loss:  0.12924540042877197\n",
      "Eval Loss:  0.47342365980148315\n",
      "[[17500  1990]\n",
      " [ 1240 10144]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92     19490\n",
      "           1       0.84      0.89      0.86     11384\n",
      "\n",
      "    accuracy                           0.90     30874\n",
      "   macro avg       0.88      0.89      0.89     30874\n",
      "weighted avg       0.90      0.90      0.90     30874\n",
      "\n",
      "acc:  0.8953812269223295\n",
      "pre:  0.8359980220866985\n",
      "rec:  0.8910751932536893\n",
      "ma F1:  0.8890848839086904\n",
      "mi F1:  0.8953812269223295\n",
      "we F1:  0.8960231868543889\n",
      "[[ 386  288]\n",
      " [ 109 1234]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.57      0.66       674\n",
      "           1       0.81      0.92      0.86      1343\n",
      "\n",
      "    accuracy                           0.80      2017\n",
      "   macro avg       0.80      0.75      0.76      2017\n",
      "weighted avg       0.80      0.80      0.79      2017\n",
      "\n",
      "acc:  0.8031730292513634\n",
      "pre:  0.8107752956636005\n",
      "rec:  0.9188384214445272\n",
      "ma F1:  0.760912281644639\n",
      "mi F1:  0.8031730292513634\n",
      "we F1:  0.7942524233296611\n",
      "Loss:  0.05716228485107422\n",
      "Loss:  0.053276270627975464\n",
      "Loss:  0.04471411556005478\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.056497205048799515\n",
      "Loss:  0.06386856734752655\n",
      "Loss:  0.0739467591047287\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.04270190745592117\n",
      "Loss:  0.050726376473903656\n",
      "Loss:  0.07611522078514099\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.02547156810760498\n",
      "Eval Loss:  0.19348549842834473\n",
      "Eval Loss:  0.06447350978851318\n",
      "[[18968   522]\n",
      " [ 2753  8631]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92     19490\n",
      "           1       0.94      0.76      0.84     11384\n",
      "\n",
      "    accuracy                           0.89     30874\n",
      "   macro avg       0.91      0.87      0.88     30874\n",
      "weighted avg       0.90      0.89      0.89     30874\n",
      "\n",
      "acc:  0.893923689836108\n",
      "pre:  0.9429695181907571\n",
      "rec:  0.7581693605059733\n",
      "ma F1:  0.8805313247201315\n",
      "mi F1:  0.893923689836108\n",
      "we F1:  0.8910332606425564\n",
      "[[ 544  130]\n",
      " [ 310 1033]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.81      0.71       674\n",
      "           1       0.89      0.77      0.82      1343\n",
      "\n",
      "    accuracy                           0.78      2017\n",
      "   macro avg       0.76      0.79      0.77      2017\n",
      "weighted avg       0.80      0.78      0.79      2017\n",
      "\n",
      "acc:  0.7818542389687655\n",
      "pre:  0.8882201203783319\n",
      "rec:  0.7691734921816828\n",
      "ma F1:  0.7682316367419764\n",
      "mi F1:  0.7818542389687655\n",
      "we F1:  0.7868686937761726\n",
      "Loss:  0.06903474032878876\n",
      "Loss:  0.033977046608924866\n",
      "Loss:  0.07107903063297272\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.043605152517557144\n",
      "Loss:  0.04320058971643448\n",
      "Loss:  0.07356859743595123\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.06858006119728088\n",
      "Loss:  0.047639116644859314\n",
      "Loss:  0.06563392281532288\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.04347538948059082\n",
      "Eval Loss:  0.12037193775177002\n",
      "Eval Loss:  0.15992093086242676\n",
      "[[18112  1378]\n",
      " [ 1337 10047]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19490\n",
      "           1       0.88      0.88      0.88     11384\n",
      "\n",
      "    accuracy                           0.91     30874\n",
      "   macro avg       0.91      0.91      0.91     30874\n",
      "weighted avg       0.91      0.91      0.91     30874\n",
      "\n",
      "acc:  0.9120619291313079\n",
      "pre:  0.8793873085339169\n",
      "rec:  0.8825544624033732\n",
      "ma F1:  0.9056217990699963\n",
      "mi F1:  0.9120619291313079\n",
      "we F1:  0.9120946687881601\n",
      "[[ 452  222]\n",
      " [ 166 1177]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.67      0.70       674\n",
      "           1       0.84      0.88      0.86      1343\n",
      "\n",
      "    accuracy                           0.81      2017\n",
      "   macro avg       0.79      0.77      0.78      2017\n",
      "weighted avg       0.80      0.81      0.81      2017\n",
      "\n",
      "acc:  0.8076351016360932\n",
      "pre:  0.8413152251608291\n",
      "rec:  0.8763961280714817\n",
      "ma F1:  0.7790939247978357\n",
      "mi F1:  0.8076351016360931\n",
      "we F1:  0.8054305417699656\n",
      "Loss:  0.08171835541725159\n",
      "Loss:  0.053423553705215454\n",
      "Loss:  0.05696922913193703\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.07400619983673096\n",
      "Loss:  0.08607018738985062\n",
      "Loss:  0.0614505372941494\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.05566069856286049\n",
      "Loss:  0.0650230199098587\n",
      "Loss:  0.06578598916530609\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.04946482181549072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss:  0.12086188793182373\n",
      "Eval Loss:  0.20157063007354736\n",
      "[[18033  1457]\n",
      " [ 1261 10123]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19490\n",
      "           1       0.87      0.89      0.88     11384\n",
      "\n",
      "    accuracy                           0.91     30874\n",
      "   macro avg       0.90      0.91      0.91     30874\n",
      "weighted avg       0.91      0.91      0.91     30874\n",
      "\n",
      "acc:  0.9119647599922265\n",
      "pre:  0.8741796200345423\n",
      "rec:  0.889230498945889\n",
      "ma F1:  0.9057801917896459\n",
      "mi F1:  0.9119647599922265\n",
      "we F1:  0.9121180059299896\n",
      "[[ 374  300]\n",
      " [ 115 1228]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.55      0.64       674\n",
      "           1       0.80      0.91      0.86      1343\n",
      "\n",
      "    accuracy                           0.79      2017\n",
      "   macro avg       0.78      0.73      0.75      2017\n",
      "weighted avg       0.79      0.79      0.78      2017\n",
      "\n",
      "acc:  0.7942488844819038\n",
      "pre:  0.8036649214659686\n",
      "rec:  0.9143708116157856\n",
      "ma F1:  0.7493076463930675\n",
      "mi F1:  0.7942488844819037\n",
      "we F1:  0.7845133703760083\n",
      "Loss:  0.07085110247135162\n",
      "Loss:  0.06540204584598541\n",
      "Loss:  0.056400734931230545\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.059784937649965286\n",
      "Loss:  0.058032963424921036\n",
      "Loss:  0.05766989290714264\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.0697215124964714\n",
      "Loss:  0.08325467258691788\n",
      "Loss:  0.049439892172813416\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.022026658058166504\n",
      "Eval Loss:  0.14855986833572388\n",
      "Eval Loss:  0.08043622970581055\n",
      "[[18645   845]\n",
      " [ 1782  9602]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93     19490\n",
      "           1       0.92      0.84      0.88     11384\n",
      "\n",
      "    accuracy                           0.91     30874\n",
      "   macro avg       0.92      0.90      0.91     30874\n",
      "weighted avg       0.92      0.91      0.91     30874\n",
      "\n",
      "acc:  0.9149122238776964\n",
      "pre:  0.919115535560448\n",
      "rec:  0.8434645115952214\n",
      "ma F1:  0.9069274851312534\n",
      "mi F1:  0.9149122238776964\n",
      "we F1:  0.914084876735662\n",
      "[[ 497  177]\n",
      " [ 223 1120]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.74      0.71       674\n",
      "           1       0.86      0.83      0.85      1343\n",
      "\n",
      "    accuracy                           0.80      2017\n",
      "   macro avg       0.78      0.79      0.78      2017\n",
      "weighted avg       0.81      0.80      0.80      2017\n",
      "\n",
      "acc:  0.8016856717897868\n",
      "pre:  0.8635312259059368\n",
      "rec:  0.8339538346984363\n",
      "ma F1:  0.7807704012869006\n",
      "mi F1:  0.8016856717897868\n",
      "we F1:  0.8032299774769984\n",
      "Loss:  0.06311986595392227\n",
      "Loss:  0.06451553106307983\n",
      "Loss:  0.07343677431344986\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.06605947017669678\n",
      "Loss:  0.052616387605667114\n",
      "Loss:  0.04995196312665939\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.060983408242464066\n",
      "Loss:  0.07612735033035278\n",
      "Loss:  0.026765624061226845\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.05168461799621582\n",
      "Eval Loss:  0.11264050006866455\n",
      "Eval Loss:  0.19124561548233032\n",
      "[[18018  1472]\n",
      " [ 1119 10265]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     19490\n",
      "           1       0.87      0.90      0.89     11384\n",
      "\n",
      "    accuracy                           0.92     30874\n",
      "   macro avg       0.91      0.91      0.91     30874\n",
      "weighted avg       0.92      0.92      0.92     30874\n",
      "\n",
      "acc:  0.9160782535466736\n",
      "pre:  0.874584646843316\n",
      "rec:  0.9017041461700632\n",
      "ma F1:  0.9104299700336314\n",
      "mi F1:  0.9160782535466736\n",
      "we F1:  0.9163354242006274\n",
      "[[ 456  218]\n",
      " [ 155 1188]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.68      0.71       674\n",
      "           1       0.84      0.88      0.86      1343\n",
      "\n",
      "    accuracy                           0.82      2017\n",
      "   macro avg       0.80      0.78      0.79      2017\n",
      "weighted avg       0.81      0.82      0.81      2017\n",
      "\n",
      "acc:  0.8150718889439762\n",
      "pre:  0.844950213371266\n",
      "rec:  0.8845867460908414\n",
      "ma F1:  0.7870209612834098\n",
      "mi F1:  0.8150718889439762\n",
      "we F1:  0.8126576697600749\n",
      "Subject 7 Current Train Acc:  0.9160782535466736 Current Test Acc:  0.8150718889439762\n",
      "Loss:  0.07454006373882294\n",
      "Loss:  0.050856731832027435\n",
      "Loss:  0.0357145220041275\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.056772634387016296\n",
      "Loss:  0.06117802485823631\n",
      "Loss:  0.07263636589050293\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.05380864813923836\n",
      "Loss:  0.05255736783146858\n",
      "Loss:  0.04973839595913887\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.05218541622161865\n",
      "Eval Loss:  0.12310457229614258\n",
      "Eval Loss:  0.11599063873291016\n",
      "[[18504   986]\n",
      " [ 1497  9887]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     19490\n",
      "           1       0.91      0.87      0.89     11384\n",
      "\n",
      "    accuracy                           0.92     30874\n",
      "   macro avg       0.92      0.91      0.91     30874\n",
      "weighted avg       0.92      0.92      0.92     30874\n",
      "\n",
      "acc:  0.919576342553605\n",
      "pre:  0.909316655936724\n",
      "rec:  0.8684996486296557\n",
      "ma F1:  0.9127822532879746\n",
      "mi F1:  0.919576342553605\n",
      "we F1:  0.9191734436775765\n",
      "[[ 466  208]\n",
      " [ 197 1146]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.69      0.70       674\n",
      "           1       0.85      0.85      0.85      1343\n",
      "\n",
      "    accuracy                           0.80      2017\n",
      "   macro avg       0.77      0.77      0.77      2017\n",
      "weighted avg       0.80      0.80      0.80      2017\n",
      "\n",
      "acc:  0.7992067426871592\n",
      "pre:  0.8463810930576071\n",
      "rec:  0.85331347728965\n",
      "ma F1:  0.7734580848162547\n",
      "mi F1:  0.7992067426871592\n",
      "we F1:  0.7987902202804239\n",
      "Loss:  0.043297357857227325\n",
      "Loss:  0.05350930243730545\n",
      "Loss:  0.0985303521156311\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.05519986152648926\n",
      "Loss:  0.04747689515352249\n",
      "Loss:  0.07818694412708282\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.05796404927968979\n",
      "Loss:  0.08171931654214859\n",
      "Loss:  0.05016551539301872\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.028160929679870605\n",
      "Eval Loss:  0.13810640573501587\n",
      "Eval Loss:  0.0852118730545044\n",
      "[[18632   858]\n",
      " [ 1487  9897]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94     19490\n",
      "           1       0.92      0.87      0.89     11384\n",
      "\n",
      "    accuracy                           0.92     30874\n",
      "   macro avg       0.92      0.91      0.92     30874\n",
      "weighted avg       0.92      0.92      0.92     30874\n",
      "\n",
      "acc:  0.9240461229513507\n",
      "pre:  0.9202231520223152\n",
      "rec:  0.869378074490513\n",
      "ma F1:  0.9174373034968695\n",
      "mi F1:  0.9240461229513507\n",
      "we F1:  0.9235702274233748\n",
      "[[ 495  179]\n",
      " [ 221 1122]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.73      0.71       674\n",
      "           1       0.86      0.84      0.85      1343\n",
      "\n",
      "    accuracy                           0.80      2017\n",
      "   macro avg       0.78      0.78      0.78      2017\n",
      "weighted avg       0.81      0.80      0.80      2017\n",
      "\n",
      "acc:  0.8016856717897868\n",
      "pre:  0.862413528055342\n",
      "rec:  0.8354430379746836\n",
      "ma F1:  0.7804721427094331\n",
      "mi F1:  0.8016856717897868\n",
      "we F1:  0.8031066737377532\n",
      "Loss:  0.07137487083673477\n",
      "Loss:  0.05050159990787506\n",
      "Loss:  0.06494491547346115\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.050685372203588486\n",
      "Loss:  0.06394807994365692\n",
      "Loss:  0.04068991541862488\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.046964552253484726\n",
      "Loss:  0.04293961077928543\n",
      "Loss:  0.04083370417356491\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.02881479263305664\n",
      "Eval Loss:  0.11576592922210693\n",
      "Eval Loss:  0.10313975811004639\n",
      "[[18608   882]\n",
      " [ 1469  9915]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     19490\n",
      "           1       0.92      0.87      0.89     11384\n",
      "\n",
      "    accuracy                           0.92     30874\n",
      "   macro avg       0.92      0.91      0.92     30874\n",
      "weighted avg       0.92      0.92      0.92     30874\n",
      "\n",
      "acc:  0.9238517846731878\n",
      "pre:  0.9183106418449569\n",
      "rec:  0.8709592410400562\n",
      "ma F1:  0.9172950917590769\n",
      "mi F1:  0.923851784673188\n",
      "we F1:  0.9234090400809201\n",
      "[[ 502  172]\n",
      " [ 232 1111]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.74      0.71       674\n",
      "           1       0.87      0.83      0.85      1343\n",
      "\n",
      "    accuracy                           0.80      2017\n",
      "   macro avg       0.77      0.79      0.78      2017\n",
      "weighted avg       0.81      0.80      0.80      2017\n",
      "\n",
      "acc:  0.7997025285076846\n",
      "pre:  0.8659392049883087\n",
      "rec:  0.827252419955324\n",
      "ma F1:  0.779611013986014\n",
      "mi F1:  0.7997025285076846\n",
      "we F1:  0.80168198806647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.05220770463347435\n",
      "Loss:  0.0584288127720356\n",
      "Loss:  0.07323186844587326\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.0470334030687809\n",
      "Loss:  0.06434682756662369\n",
      "Loss:  0.057004787027835846\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.04170054942369461\n",
      "Loss:  0.05614311248064041\n",
      "Loss:  0.03633546084165573\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.053082823753356934\n",
      "Eval Loss:  0.09293568134307861\n",
      "Eval Loss:  0.06871592998504639\n",
      "[[18075  1415]\n",
      " [ 1042 10342]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94     19490\n",
      "           1       0.88      0.91      0.89     11384\n",
      "\n",
      "    accuracy                           0.92     30874\n",
      "   macro avg       0.91      0.92      0.92     30874\n",
      "weighted avg       0.92      0.92      0.92     30874\n",
      "\n",
      "acc:  0.9204184750923107\n",
      "pre:  0.8796461682401974\n",
      "rec:  0.9084680252986648\n",
      "ma F1:  0.9150917522656508\n",
      "mi F1:  0.9204184750923107\n",
      "we F1:  0.9206754087033729\n",
      "[[ 400  274]\n",
      " [ 152 1191]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.59      0.65       674\n",
      "           1       0.81      0.89      0.85      1343\n",
      "\n",
      "    accuracy                           0.79      2017\n",
      "   macro avg       0.77      0.74      0.75      2017\n",
      "weighted avg       0.78      0.79      0.78      2017\n",
      "\n",
      "acc:  0.7887952404561229\n",
      "pre:  0.8129692832764505\n",
      "rec:  0.8868205510052122\n",
      "ma F1:  0.7504095732072893\n",
      "mi F1:  0.7887952404561229\n",
      "we F1:  0.7828748214872763\n",
      "Loss:  0.048997148871421814\n",
      "Loss:  0.0594513826072216\n",
      "Loss:  0.05644608289003372\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.05854932218790054\n",
      "Loss:  0.07313793897628784\n",
      "Loss:  0.04948897659778595\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.07533861696720123\n",
      "Loss:  0.057190146297216415\n",
      "Loss:  0.040773794054985046\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.03700137138366699\n",
      "Eval Loss:  0.1277308464050293\n",
      "Eval Loss:  0.1037452220916748\n",
      "[[18417  1073]\n",
      " [ 1191 10193]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94     19490\n",
      "           1       0.90      0.90      0.90     11384\n",
      "\n",
      "    accuracy                           0.93     30874\n",
      "   macro avg       0.92      0.92      0.92     30874\n",
      "weighted avg       0.93      0.93      0.93     30874\n",
      "\n",
      "acc:  0.9266696897065492\n",
      "pre:  0.9047576779691106\n",
      "rec:  0.8953794799718904\n",
      "ma F1:  0.9210691874394529\n",
      "mi F1:  0.9266696897065492\n",
      "we F1:  0.9265893323053435\n",
      "[[ 499  175]\n",
      " [ 249 1094]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.74      0.70       674\n",
      "           1       0.86      0.81      0.84      1343\n",
      "\n",
      "    accuracy                           0.79      2017\n",
      "   macro avg       0.76      0.78      0.77      2017\n",
      "weighted avg       0.80      0.79      0.79      2017\n",
      "\n",
      "acc:  0.789786812097174\n",
      "pre:  0.8620961386918834\n",
      "rec:  0.8145941921072226\n",
      "ma F1:  0.7697503462327934\n",
      "mi F1:  0.789786812097174\n",
      "we F1:  0.7922787423055172\n",
      "Loss:  0.0454147532582283\n",
      "Loss:  0.028702665120363235\n",
      "Loss:  0.04745018109679222\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.06982937455177307\n",
      "Loss:  0.06011104956269264\n",
      "Loss:  0.0511457696557045\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.05683049187064171\n",
      "Loss:  0.03863341361284256\n",
      "Loss:  0.04242489114403725\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.025182127952575684\n",
      "Eval Loss:  0.09510636329650879\n",
      "Eval Loss:  0.11756682395935059\n",
      "[[18328  1162]\n",
      " [ 1009 10375]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94     19490\n",
      "           1       0.90      0.91      0.91     11384\n",
      "\n",
      "    accuracy                           0.93     30874\n",
      "   macro avg       0.92      0.93      0.92     30874\n",
      "weighted avg       0.93      0.93      0.93     30874\n",
      "\n",
      "acc:  0.9296819330180734\n",
      "pre:  0.8992805755395683\n",
      "rec:  0.911366830639494\n",
      "ma F1:  0.9246843330328116\n",
      "mi F1:  0.9296819330180734\n",
      "we F1:  0.9297780769634707\n",
      "[[ 450  224]\n",
      " [ 181 1162]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.67      0.69       674\n",
      "           1       0.84      0.87      0.85      1343\n",
      "\n",
      "    accuracy                           0.80      2017\n",
      "   macro avg       0.78      0.77      0.77      2017\n",
      "weighted avg       0.80      0.80      0.80      2017\n",
      "\n",
      "acc:  0.7992067426871592\n",
      "pre:  0.8383838383838383\n",
      "rec:  0.8652271034996277\n",
      "ma F1:  0.7706245814432469\n",
      "mi F1:  0.7992067426871592\n",
      "we F1:  0.7974805728367542\n",
      "Loss:  0.02835073322057724\n",
      "Loss:  0.03925783187150955\n",
      "Loss:  0.05407802760601044\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.031762827187776566\n",
      "Loss:  0.05429045110940933\n",
      "Loss:  0.038421496748924255\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.03808782994747162\n",
      "Loss:  0.044843029230833054\n",
      "Loss:  0.022379757836461067\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD4CAYAAADo30HgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAA1iUlEQVR4nO3deXhU5dn48e+dkLDvhDUgoAEaEQUj4IIbiKBWatUWXGtrKa97rVUs/tS6VNtXrbVaEJdal1e0qBUVxRWFipiw7xAwQNgSdgiELDy/P2YmzHJm5sxkJmeW+3NdXMyc85wzz8kk5z7PLsYYlFJKKY8MpzOglFIqsWhgUEop5UMDg1JKKR8aGJRSSvnQwKCUUspHI6czYKVDhw6mZ8+eTmdDKaWSxoIFC3YaY3Jica6EDAw9e/akqKjI6WwopVTSEJGNsTqXViUppZTyYSswiMgoEVkjIsUiMtFifz8RmSciR0TkLr99vxWRFSKyXETeFJEmscq8Ukqp2AsbGEQkE3gOGA3kA+NEJN8v2W7gNuAJv2O7ubcXGGP6A5nA2BjkWymlVJzYKTEMBoqNMRuMMVXANGCMdwJjTJkxphCotji+EdBURBoBzYCt9cyzUkqpOLITGLoBm73el7q3hWWM2YKrFLEJ2AbsM8Z8GmkmlVJKNRw7gUEsttmaeU9E2uIqXfQCugLNReSaIGnHi0iRiBSVl5fbOb1SSqk4sBMYSoHuXu9zsV8dNAL4wRhTboypBt4FzrBKaIyZaowpMMYU5OTEpCuuUkqpKNgJDIVAnoj0EpFsXI3HM2yefxMwVESaiYgAw4FV0WU1vDnrytm4qyJep1dKqbQQdoCbMaZGRG4BZuHqVfSyMWaFiExw758iIp2BIqAVcFRE7gDyjTHzRWQ6sBCoARYBU+NzKXDtS98DUPL4xfH6CKWUSnm2Rj4bY2YCM/22TfF6vR1XFZPVsQ8AD9Qjj1Hbvq+SWmPo1qapEx+vlFJJKSGnxIiVoY99AWgJQimlIpGSU2LcM30ptUd1yVKllIpGSpYY3irazOLNe53OhlJKJaWULDEArNlxwOksKKVUUkrZwOBtzjodMKeUUnalRWDwdGNVSikVXkoFhn/ecJrTWVBKqaSXUoHhvL4dnc6CUkolvZQKDEoppepPA4NSSikfGhiUUkr5SJvAsHzLPqezoJRSSSFtAkOJTsetlFK2pE1gUEopZU/KBYYrT7Wc/RuxXKFUKaWUv5QLDK2bZjmdBaWUSmq2AoOIjBKRNSJSLCITLfb3E5F5InJERO7y29dGRKaLyGoRWSUip8cq81a6BlmURwT2Hqpi8+5D8fx4pZRKemGn3RaRTOA54AKgFCgUkRnGmJVeyXYDtwE/sTjF34BPjDFXuNeMblbvXEfhpjcW1r3WhXuUUio4OyWGwUCxMWaDMaYKmAaM8U5gjCkzxhQC1d7bRaQVcDbwkjtdlTFmbywyXh/lB45Ybl+z/QA9J37Ewk17GjhHSimVOOwEhm7AZq/3pe5tdvQGyoF/isgiEXlRRJpbJRSR8SJSJCJF5eXRT5P900Hhs7Z5j3V10uw1ZQB8snx71J+vlFLJzk5gsOrOY3fdzEbAIGCyMWYgUAEEtFEAGGOmGmMKjDEFOTk5Nk8fqE2z7LBpissOsmKrDnhTSikrdgJDKdDd630usNXm+UuBUmPMfPf76bgChaPunr6Ui5+Z63Q2lFIqIdkJDIVAnoj0cjcejwVm2Dm5MWY7sFlE+ro3DQdWhjhEKaWUw8L2SjLG1IjILcAsIBN42RizQkQmuPdPEZHOQBHQCjgqIncA+caY/cCtwBvuoLIBuCE+lxJ7+w5XU7rnECd2be10VpRSqsGEDQwAxpiZwEy/bVO8Xm/HVcVkdexioCD6LMbXlr2H+cXL3/P6jUMC9l31wnes2Lpfu7cqpdJKyo18jtRr8zayruwg7ywsDdi3Yut+B3KklFLOSsnAMOOWM53OglJKJa2UDAwDcts4nYWUU1VzlCc/XcOhqhqns6KUirOUDAwq9t4u2szfvyzmb1+sczorSqk408DgVltrd8xeeqqqOQrAkeqjDudEKRVvGhjcnvxsLQcqtZpEKaXSPjAYr9k9dlW4JtdbtU17IwXzyrclTmdBKRVnKRsYbh+eF/ExW/dWAjBn3U6emLUm1llSSqmkkLKB4Tfn9K7X8c9+VRyjnKQG0ZVRlUobKRsYmmXbGtTtI9rm5/kbdnHwiLZPKKVSQ8oGhliqqXX1xKk9arjvP8so2VlRt293RRU/n/odt/7fwmCHK6VUUtHAYMOizXsBWLl1P69/t4mbvYJAZXUtAKu3H4jb53+5egefr9wRl3MfqqqpuwallII0Dwx/+WQ1z3+9IWw6/+r1FVv3Y0zDjXv45StF3PhqUVzOnX//LIb86Yu4nFsplZxSOjBkZYZuMf3H7PU+74tKdlum8zS8lh+srNu2MoW6tO47XB0+kVIqbaR0YBiZ3zmi9IeqQlep/PKVY0/tNTpSWimVomwFBhEZJSJrRKRYRALWbBaRfiIyT0SOiMhdFvszRWSRiHwYi0zbFrMulunVV/NwVS1Pfba2bhqMWFi4aQ/Dn5ytk/AplQTCBgYRyQSeA0YD+cA4Ecn3S7YbuA14IshpbgdW1SOfUfn1sPqNZUhXz361jme+WMdbhZtids4/fbSK9eUVusaFUknATolhMFBsjNlgjKkCpgFjvBMYY8qMMYVAQGW1iOQCFwMvxiC/EenftVVMzmM1uOujZdsY/uTsurEP3m3Rxhg+WrqNIzW+VVP7K6t5+MOVAdsTzeEq94R5MSwxKKWSh53A0A3Y7PW+1L3NrqeBu4GQdxkRGS8iRSJSVF5eHsHpQ54zNuex2Db1mw2sL6+wnDrjm3U7ufn/FvLkp2t9tv/1s7W8NPcH/l0UuFpcokuvyrTkcPBIDR8s2ep0NlQKshMYrO4JtlpeReQSoMwYsyBcWmPMVGNMgTGmICcnx87pw39+TM4S2nuLtgRs23uoCoBt+yp9tnsarI9G2dX10mfnah29qnPPO0u59c1FOumjijk7gaEU6O71Phew+5hyJnCpiJTgqoI6X0RejyiH9RCr+X0SpUplaek+Fmzc43Q2VILYuvcwEL43nVKRshMYCoE8EeklItnAWGCGnZMbY+41xuQaY3q6j/vSGHNN1LmNUKyqkm78V3wGlyWT+nbO1c69KtEZY+qmv0l3YQODMaYGuAWYhatn0dvGmBUiMkFEJgCISGcRKQXuBO4TkVIRiU3LbwKIdoI8u6Ojd1dU8ccPVlBt45eyAQdc22aM4cEZK1i9PXyVhrZVqET1wIwVnDDpY6ezkRBsTUFqjJkJzPTbNsXr9XZcVUyhzjEbmB1xDpOQf0mlpvYojTIzfBYF8vbIhyt5d9EWBvVoy49P7toQWbSl5uix/Ia6oe/Yf4RXvi3h4+XbmP+HEfHPmJ9Plm9jcK/2tGue3eCf7aREfEhIZq/O2+h0FhJGSo98doLn6dnjo6XbOGHSxxSXHazb5n+TrT4aWaP0R0u3MX1BKSU7Kzh6NPZ3h0p3d9rHP14d83PH2p6KKia8vpAb/1XodFYco2tlqFiLfNECFVJFVS27K6rq3n+yYjsAK7bui8n5DfjM7jpxdD8mnHN8TM7tkUyzrXqq3zbvOexwTpyjJQcVa1piSHLBJv5TqU9LCipetMQQI0dqatlfWU2G11+rIfUjb3HZAWYs2RY2XUNOU66Uqp9Uv281mD2Hqhnw4Kc+2z5auo15G3YBruJ+sHuj/4Pfwk176DnxI8uBS07dYB/8YKXl9lFPz+GZL9bZPo8+5caOxloVLxoYbLpn+tKojis/cCRwY5i74yfLXe0S36wNPzWI/81h3+FqTnv0cxZuapiBcDVxaPxWkdFgq2JNA4NNbxVtDp+I4N0673hr8bE39XjUC3fkgo27KT9whL9H8BTvLx49nZRSySPlA8OC+xq+X30wwZ7sPLOtTnpvOcu3HOu9ZHCmh9B/Fkc3MZvo8LWY27irgs27DzmdjbhYvHkvfe/72LpU7SBtD0uDwNC+RWOnsxCgsvooV075tq4L66wVOwDXCOurX5zvc3st3eN7U/C/9Qb7Ff5qTbnlL3h17VEqq2t5q3ATo57+JsorUA3lnP+dzbC/fBUyTbLex16cs4EjNUfr2uFU4tBeSQ3offeT+KLNeygs2cPDH65k2vjTA9J56u2t/uD9N4V6uvl8VRkX5HfiQGU1Jz34KQ/8OJ//m7+JdV6D7RpKkt67lEpLGhhi7KNlwbtuHqh0zbkUrsrlpbk/xCQvuytcRfQyd1H9tXkb2bCzIibnjp5WN8WaNj6rWEv5qqSGdneUvZc8Av/I7f3Vv120mXFTv6vXZ1s5/8nZ7NhfGT5hnMxcto0Sx4OZUulFA4ODyvYfCVjmc++hgNVRfRnrt3dPXxpQVxuLuucN5RVMX+DcinM3vbGQEU997djnK5WONDA4aMPOCgoe/jyun1FxpIaNuyrqPi+Yad9v4tqX5sctH/UJUtGMlXh3YSkbyhu+LSWe3i7czBerdtS913abxPP81+t9JsxMVmkRGP73igFOZyGoAyHWejCYetcfX/vSfH75SviFhia+u4w563ZSG4MxDA99sJI7vcdteIlXfbh/4Lnz7SWM/Gtq9bq6+52l/EoXjUpYR2pqeezj1fz0H/91Oiv1ZiswiMgoEVkjIsUiMtFifz8RmSciR0TkLq/t3UXkKxFZJSIrROT2WGberisLuodP1IDsrtFbWVUb0OuoZJfvU3+oJ3EDLNy0N6K8zVkXfrR1MJ6b/sv//YF3/dbCtrMIUXQfGnyXVUnjte82cvnkb+OTlzhasnlvwDZtc46P+lbBVlYn/ypwYXsliUgm8BxwAa71nwtFZIYxxnvynN3AbcBP/A6vAX5njFkoIi2BBSLymd+xacduz6BnvixmzY4DPtv+6Ddn0ddry7nC60ZX31/qePWJX7HVFQydvpn9v/8sdzgH0RnzXMM9hc7fsIsTOrZIyDFADe2rNWV0atmE/K4psyClLXZKDIOBYmPMBmNMFTANGOOdwBhTZowpBKr9tm8zxix0vz6Aa2nQbjHJeZrwDH4LpWij9bxIyTrwKdk98P5y/vDeMqezAcAb8zfy2crwv0Pefj71O658fl6ccpRcbvhnIRc9M8dW2lT6e7MTGLoB3hMFlRLFzV1EegIDAcsWThEZLyJFIlJUXh59dYaqn2/X72ywzzpQWc2n7oWMks23xTuD5v1f8zbyf/M3xT0Pdu5Dk95bzq9fjbxdYkN5w3URbogpKN78fhPjo/g5RMXpYnEM2AkMVpcZ0TcpIi2Ad4A7jDGWFezGmKnGmAJjTEFOTk4kp1dBbNwV+R/3C3MCB9fNXlMW1ecbY3j+6/Vs22e9utpv31rC+NcWRJTPo0cNf/9inaNjKwoe+ZyrXpzP+NcWOJYHb07fh8r2V/J5hKUSCFwbPZ7ufXcZn0aRx1Aqq12rNRaXHWTlVq/bWgqUHOwEhlLAu/U2F7A9y5qIZOEKCm8YY96NLHuqPp7/ZkNMzlNYEt0U3iW7DvHYx6u5/B/Wjb2eyeEOe00UWHGkhrunL2HfYevxHG8VbebJz9Yy5E9fRJWnWNh5MLEmfXPaz56fx42vFvnMyjtz2TYOVQXvcZcKrn/5ewY9/Bkjnvqai56Zk1Ij0O0EhkIgT0R6iUg2MBaYYefk4nokeAlYZYx5Kvps1l+nVtqQFitrdxyg0MaSop6ur1v3HXu6D/WU+G3xTsa98B1vF5Xyj9nFPvveX+zq5bRLb8oBnH5ALdnlCvCer3ZZ6T5uemMh9znc0L9t32GKyw6ETxil+T/4/g2kVRuDMaYGuAWYhavx+G1jzAoRmSAiEwBEpLOIlAJ3AveJSKmItALOBK4FzheRxe5/F8XtakJolJEWQzYaxMi/fsOVUwIbJ+v7wHTVi/NZWrrPct9//Lq/xsqy0n18sjz80qSJKFEfUA8ccZX2tu61rkJsKKc/9iUjnnJgLEuYL6aq5iiPf7yagyHGMDnN1iR6xpiZwEy/bVO8Xm/HVcXkby6J+/urYmzrvkqfapZ6Fa39nr7iVR/942fnAlDy+MVRHb+h/CCvztvI/Zfkk5HRsL/qKfSAmlDi/XN9u2gzU75eT03tUe67JD/OnxYdfYxOMa/O2+jo59/0+sKwaTaUH+RAZZg5oWIk3qvRnf/k17zybQlLt1iXdLzzsaeiKqJzH/+HmeETcezJ61/flvD7fy+J6DPUMaGePTbtOsTbhfZWcQynxj3YM26DPmNAA0OK+drGOtHxZKdh9vwnv+Znz4eeCfatos0+I8Q9f7ORlhxK99irzvBfRcwYw6vzSmw3oB4NsiiSx18/X8vAhz/z+fnsr6xm9XbXNU5fUBrQnTLS6UkemLGCfzs44WEqu+wf/+Xud5amzbK3aRMYfnFGT6ezkHZC3cK9b/r7DwfefPceqmb0344NLKo1hmtfmk9RkEbvw1U1nPHYFwEr3tl177u+06V/vqqM+99fwWMzV9s6/k2LcQvex36y3DXmYbdXqeGaF+cz6mnXNd717yUx707ptO37Kuk58aO6jgOJpCbCp/Vd7u8tlXoehZI2geHXZ/d2OgsK32BR7J799Gc2Rtlu3XuYOet28tUa6xJRRVUtW/dVctafQy+DGUxVre+ToKeksDdIt1l/7y8J7MG9fGvo6qVgDe3x8MGSrcxcto3dFVVsiVGjcGV1LWVW40ncP0pPb6V3F27hua+K49pDKBKFJbs5YdLHzNclRYPSFdxU1Cqra8MnCsG7quSyMDNSrt1hfyrjeeud+4O3043XCbe+uQiARhlCzVETdWO7t+tf/j6gy6aVw1W1/O+sNbwwZwOL7x8Z9eeVHzjCpt0VnHpcu6jPAfDfYtfo/v+u38WQ3u3rda5UlTYlBhV7/f7fJ4EbvYoER2oCi+ubdltX9SyKcBbYUMa9EPuV7Ox6IUaDCiNx1NgfnR7N2hbBBAsK/iUlT/tLld/vQ7hamZVb9/usbXDps3O5fLJ16XJ3RRUVDdD987CNh6FUqG3SwKBiynuOHe82Ao/bpi2K6eeFWz+7qraWP3+yOmyPoHoPnLN5v43HIKiX5/7AL/5ZGPsT+1lfftBWe8F/FtmbGMFTXx/sZ3LRM3N8Vu/bts96GpRNuw4x6OHPOP/J2bY+Nxg7382ecCsskhrdiLUqSaW0hz5cxTdry5k8e71P9ckjH67khI4t6t6v2GpvjYxgTJjbQahGy/pOIhesFBZrFzz1NUcNjDklsjk0432jPPt/Xe1KO/bbC+7hftylew5xXPvm9c1WUtPAoFLaN0G67744N3CyQIC3CjfRLDv2fxaem9HLc3/gz34rCnqPPRn9tznsrjjC/D+MsH/SKFVW17Jx1yFEoE+nlmHTR1sL5fTcUv4jsP3n4TpcVcszX66re1+653DaBwatSlINKtb1r+Ge1CN1zzvL6hpqwfdpfvaasqA9eqprDT0nfuTTHfX7H3bzp5mrGDt1HuvcdeVvFQUOkvKe6nzVtv22n3w9/Esjnyzfzt5D4QfT/X76Ui58+htG/vUbvopyBl1vnh+V/zeycZf9Es2k95ZxxmOxnSDxmhePzfTfc+JHvPJtic/+ybOLmTx7ve3zzVy2LeRU5qnQxqAlBpU2IhltbTWQzk49vv/qfFOjbIwOtma2Ff+Cw4TXF3B67/a8OX5oyOM+8Opiu77sIOf17RhJFuPijRisY/HG/I0M6tG27v2uMO1LVp0kQrnpjfCj++1I5LYIDQwqbZz04Kch9y/3mtYi2np/u0+L4QZY+a+Zbf1hwT9tc5iBfh9YjLuIN//cerqNWvneRjdYgK9WB5Z0Jr3nO6trsCncw/nBYgne7UEawEPZtOsQhSW7ufxU13RyDbkORbS0Kkkpt395VTHcPm0xAAeP1LDPRk8UD7vhxDO4D2B2kEF7wRSV7OZw1bFuk9HcZz5c6hsYqhyYt2fnweBP8nYGPW7Ze5gbXolfbyyrksHNIUoLwbrLXvrcXH7nNYdVQ6xYV18aGFSDiuVUM5t2HeKMx76M3QktzF5TzskPhS5p1FekVRlXTJnH76cviWnf15Vb95M3aSbXv/x9zM5ppWRnRcC8VNHyDo6xdPWL8+sWkfIXau6s3wWZwHCv+8GiuOwgI576uq7LayKXGzQwqKT1xvcbYzpgK5kmoPtw6bE1JGJxg9m46xDVtSbukzCe+8RsTnv087h+Riy8FKTXWije839Z+cdXxRSXHYxJQ3+82QoMIjJKRNaISLGITLTY309E5onIERG5K5JjlVLOs+rdtXzLvqjr5+v72cpZYQODiGQCzwGjgXxgnIj4ry6xG7gNeCKKYxtMqyba1p5K1pcFNg7GSyzrhSuO1ARMDxGNJWEm4Qs12nvWivAzuV7y97lc+9L8sOn8NVQd+gqLSQqjmRYjWG6tSmL7K4OfP9rLfm9RqU/Hh0Rgp8QwGCg2xmwwxlQB04Ax3gmMMWXGmELA//Ei7LEN6eM7znbqo1UcfL6q4aapnhajRVoATnxgFpc+G3rSwEjUWtyRSvccZvCf6l9l05AzwC7atCfoPs+05d48HQS8JWPZ47dvLeGSv891Ohs+7ASGboD3X0Wpe5sd9Tk25rq1aerUR6skt8NqeukEsXyLdd12dW1sSzn1Fa6b5mX/+DbovgmvL6j35/vbtvdwyOtaGabNIFLJFLTsBAarb9PuNdo+VkTGi0iRiBSVlzu7CplS0bK74lsyOVBZzYkPzKp7H2pUdXXt0Yh7WUUqFtVw4OpsEKxbbDRdgK1ubN6zw3raazzT1W/Zm7gPG3YCQynQ3et9LmB3dIztY40xU40xBcaYgpycHJunVyqxVFYn7jq+oYSapda/AfrcJ2YH7SqaN+ljy4Fh4BoT4lGf+ZNOenBW+EQ21XfyRG+eGj3voHLDK8e6/37pHoznWVukIatCI2UnMBQCeSLSS0SygbHADJvnr8+xcfH0z09x8uNVkkqCMUn1sixE46d/Fc/eQ9U8OnNlvT5vyJ9850OKpKou1iWS/RY9r8JN5x5KZfVR3ip0Te2RrL83YQODMaYGuAWYBawC3jbGrBCRCSIyAUBEOotIKXAncJ+IlIpIq2DHxutilIqX+T+k7zKQVgPStu8L3BbJPbDWb/yJf6CI1hdRPIVbdSyo76wV97yzzFa6tTsSY7lTf7b6bxpjZgIz/bZN8Xq9HVc1ka1jlUo2321IzCU7ndKQ0zoEq5qyYtVTKRrRVHVFs9TtyL9+E/ExDUFHPiulIrZ6e+CT7rZ91lOS19d5T8wO2PbgjPhWPLy/OPJJBg82wNKiDSXtAoOOslSq/qzWpTjrz1812Of7r6mQiBZs3MOBEAPi/N365iIKHkmM6UJ0KLBSylHJ2kAbzuWTg4/LsOLEVOjBpF2JQSmVWGI1LkHFjgYGpZSjXvtuY/hEqkGlXWBI1WKrUsnqcBS9eVR8pV1gUEollg3lDTdLrrJHA4NSSikfaRcYWjXJcjoLSimV0NIuMAz/UUf+cvkAp7OhlFIJK+0Cg4hw2SDHloRQSqmEl3aBQSmlVGhpGRi8u6w2z850LiNKKZWA0jIwKKWUCi7tA0OjzLT/ESillI+0vitmZQqXDdSGaKWU8mYrMIjIKBFZIyLFIjLRYr+IyDPu/UtFZJDXvt+KyAoRWS4ib4pIk1heQDS8p97OzKjnUk1KKZViwgYGEckEngNGA/nAOBHJ90s2Gshz/xsPTHYf2w24DSgwxvQHMnGt+5wQ6rOuq1JKpSo7JYbBQLExZoMxpgqYBozxSzMGeNW4fAe0EZEu7n2NgKYi0ghoBiTOpONKKaUC2AkM3QDv1bJL3dvCpjHGbAGeADYB24B9xphPrT5ERMaLSJGIFJWXl9vNf1SyMjJo0yyLh39yos62qpRSfuwEBqv6Fv/bqWUaEWmLqzTRC+gKNBeRa6w+xBgz1RhTYIwpyMnJsZGt6GVkCIvvH8nPT+sR189RSqlkZCcwlALdvd7nElgdFCzNCOAHY0y5MaYaeBc4I/rsKqWUijc7gaEQyBORXiKSjavxeIZfmhnAde7eSUNxVRltw1WFNFREmomIAMOBVTHMf72ZgMKPUkqlt0bhEhhjakTkFmAWrl5FLxtjVojIBPf+KcBM4CKgGDgE3ODeN19EpgMLgRpgETA1HhcSLW1jUEopX2EDA4AxZiaum7/3tilerw1wc5BjHwAeqEcelVJKNaC0HvmslFIqkAYGpZRSPtI+MBhtZFBKKR9pHxiUUkr5SvvAoOUFpZTylfaBQSmllC8NDEoppXykfWDwtD2f0ye+8zMppVSySPvA4DH8Rx1Z/scLOT6nudNZUUopR6V9YPCeK6lF40Z88btzKXn8YgdzpJRSzkr7wODhP294++bZjuRDKaWcpoEhiBuH9XY6C0op5Yi0DwxnHt8BgP7dWvtsb9ssy4nsKKWU42zNrprKRp/UhSUPjKR1U99A8LOC7hyorOHRmQm1fIRSSsVd2pcYgICgAK7lP68syHUgN0op5SxbgUFERonIGhEpFpGJFvtFRJ5x718qIoO89rURkekislpEVonI6bG8AKWUUrEVNjCISCbwHDAayAfGiUi+X7LRQJ7733hgste+vwGfGGP6ASeTYEt7KqWU8mWnxDAYKDbGbDDGVAHTgDF+acYArxqX74A2ItJFRFoBZwMvARhjqowxe2OXfaWUUrFmJzB0AzZ7vS91b7OTpjdQDvxTRBaJyIsiYjm0WETGi0iRiBSVl5fbvoB40qUalFLpyE5g8B/7BYGzVQdL0wgYBEw2xgwEKoCANgoAY8xUY0yBMaYgJ0fnLVJKKafYCQylQHev97nAVptpSoFSY8x89/bpuAJF0hp7WvfwiZRSKonZCQyFQJ6I9BKRbGAsMMMvzQzgOnfvpKHAPmPMNmPMdmCziPR1pxsOrIxV5p3w+OUDnM6CUkrFVdgBbsaYGhG5BZgFZAIvG2NWiMgE9/4pwEzgIqAYOATc4HWKW4E33EFlg9++pHZ+v458ubrM6WwopVRM2Rr5bIyZievm771titdrA9wc5NjFQEH0WXROiyauH8/Y07ozrfBY2/qSB0bSNCuTqtqj9H9gllPZU0qpuNCRzyFkZWZQ8vjFjBvcw2d766ZZZDfKsGxxV0qpZKeBwYa8Ti0AuO7040Kn69iiIbKjlFJxlfaT6NnRLLuR5eI9okUGpVQK0hJDPTTLbsQdI/L4+7iBTmdFKaViRgNDPd0xok9dVZOWIJRSqUADQwxJFM3RrZpobZ5SKrFoYFBKKeVDA4PDROuflFIJRgNDDOgsrEqpVKKBIYZEYPqE03n1l4N9tg/s0caZDCmlVBQ0MMRYQc92nN3Hd9rwX5zRM2h6rUlSSiUaDQwNIDtTf8xKqeShd6wY6NqmKQDXBykZeEoFHVs25tmrBjK6f+e6fRfmd/ZJOyC3dVzyqJRSdmlgiIHWTbMsJ9vzGP6jTlw9pAcf3nYWlwzoyuRrTq3b98hl/X3S3jWyr//hSinVoDQwxNnHtw8jKzODRy87iY4tm9Rtv3hAF8A1g6u3s/vk8MEtZ4U853l9delTpVT82AoMIjJKRNaISLGIBKzZ7F657Rn3/qUiMshvf6aILBKRD2OV8WTxoy6tLLc/M3Ygqx4aZbnvpNzWrHlkFM9dldSroCqlklTYwCAimcBzwGggHxgnIvl+yUYDee5/44HJfvtvB1bVO7cpJDNDaJqdGXR/40aZdG3TJOh+pZSKFzslhsFAsTFmgzGmCpgGjPFLMwZ41bh8B7QRkS4AIpILXAy8GMN8p4XmjeMzj9Kpx7WNy3mVUqnBTmDoBmz2el/q3mY3zdPA3cDR6LKYvvp0asmzVw2k6L4RPttP7t4GgJm3DSMzw95AiL/+/OS610N6tYtZHpVSqcfOI6nVncd/EgjLNCJyCVBmjFkgIueG/BCR8biqoejRw7p3Tzq6ZEBXjN+cG7een8fo/l3o27kl8yaez55D1Vz49DeWx7970xkU/rDb1me1a57N7oqqeudZKZXc7JQYSoHuXu9zga0205wJXCoiJbiqoM4XkdetPsQYM9UYU2CMKcjJSc9eN6/9arDldv+J9jIzhL6dWwLQsVWTutdWBvVoy2/OOT52mVRKpTw7gaEQyBORXiKSDYwFZvilmQFc5+6dNBTYZ4zZZoy51xiTa4zp6T7uS2PMNbG8gER19ZAeTIjwhpwRx/kxGjcK3tDt4V8yAZg4ul88sqOUSmBhq5KMMTUicgswC8gEXjbGrBCRCe79U4CZwEVAMXAIuCF+WU4Oj152kiOf26lVY3bsPxKw/cITO1ukDu3T355Nyc6KWGRLKZVEbI1jMMbMNMb0McYcb4x51L1tijso4O6NdLN7/0nGmCKLc8w2xlwS2+ynj3GDu4dPBIz4USfL7ZkZwq/O6lX32op3eeHPl59En07Bq6g8zjyhva18xcvqh63Hgiiloqcjn5PEYz8dUO9zeAbbDTquLc+MG8j4s3vX7Ts+p7lPVZbdBYTaN29c73zVR5Os8FVkSqnI6ILDKWLmbcNolCn869uSoGkuH9SNAbmt60oCl57clanfbABg8jWnctQYRj09x+eYcAHi+JwWPu+bZmVyuLo2iitQSiUKLTGkiPyurejTqWXIRmYRCVk91K/zsek7clq6SgKNG1n/irzzP2dw2cBuXD3Ut2vxiV19pwDp3q6p5fGf/fbsoPlQSjlLA0MC6Rei26ldd47sw/+cG3n31C6tfaffOK9vRwCG5XVg0kU/Ckh/6nFt+evPT6F10yyf7Y9f7lvlNefu8zmtZ+BI6y7uqcpbuEd3n9azLW2aZQWkU0o1PA0MCaR9i9D19c9eNZAPbw0982qLxo24Z1TkXUxbNrG+KYsIv/ZqixiW14GbvAJPVmYGJY9fXPe+SdaxX6mVD13oOofF+McWjRtR8vjF/OUKVyBp37wxD/zYfwquyGVlNuySeDqKXKUiDQxJ5JIBXenfzdmFfF771RDuthl4mmW7SgMX5Fv3lALwDJ3IyICmWdZNXh/cchb/ufnMuqlAQhlzSjee+tnJYdPFilPdkpWKJw0MCcK/SibRdAhTmvHwXnPC48ZhvVjgN9+Tx1F3ZAjVyH1SbmtO6d6Gdyaczr9+aT063GNo7/Z11VPhZGYIU689NXzCunMHlg5sTlUVM3++XAORij8NDAlg7j3n8fXvz3U6GyF9eOtZvBripjy6f2devK6AbIvGahEJWk1WFxhs5KFRZkbYqqIrTs31eb/u0dGMPc01BqR3TnOffb84o2fI0oy/318YuLqe3W69sZLfJb4lxn9POD2u51fJQburJoDcts3icl67T87ebj7veNpZjE3o3LoJnVsHXx/Ce7nSSJzTJ4deHZpzy/knULLzUETHWjVqw7HS1/WnH+ezQt4vz+zF05+vZedB10SB1w49zue4v1wxgE27DvHsV8VBPtG6raQhtW+RHdfzN3QJyI78Lq1YuW2/09lIKxoYUtQ7/3MGuW2tu4p6e+PGIXRrcyzd7y9s2LmR2jTL5qu7zgWIKDAM7d2OaeOtn26H9G7P5KsHcV6/jj7bMzOEovsuYO66nXyzrpyeHVwliJLHL6ayurZusJwnMDz443x+cWYvek78yH0Gw38nns+Zj39Zd05Pt95INcoQao4Gzk0VTtc24b/TVHPd6ccx8d1lTmcjrWhVUoo69bi2dGoVfgW4M0/oUHeDjJWzTuhQ73Pc6J6+w1+Pdq7S1cj8wLmfvMdGjD6pS92N3lPd43kYPiuvA3/w64JrdwR1N4sb8ze/Py9gzQx/b//GN4h9cscwW5+XDjw904JxusNFOtLAoGLuX78czNpHRgdsf+l6VxvEW+OHWh53QkdXgHr0sv7cd4l119Xcts1Y9uBIbjizZ9223h2ak52ZQV6QwXv3jOrL1UN68JOB/utLWbv+9OOC7LGuZ+nRvhkdWjT26Ur85JW+PaMiaYp45YbTQu5f/scLbZ+rV4yDvhMsJv1NWcGqRxuaBgYVc5kZYtkIPfxHnVj7yGiG9LaeeO+Eji1ZfP8FXDU49EJNLZtk+TT6fn7nOSEn02vTLJtHLzvJdqnAky7LfQ3NQ6zN7c3zZHtybmt+Oqgbr/9qCCfnurbZiQvtm7vaDwZ2D31ziKRdI7/LsZHow/LqX5ILpV/nlvx6WK+AwZJhhbnxHw0TGS49uWtkn5cgrKa0j2ZwajxoYFAJpU2zbJ+bvp0nqIwMISOGraa3Dc/jlvNO4MpTXb2Z+viNSA/VxXXZgyN5e8LpiAhn5XXgoTH9OaV7m7oJDD089zrvbsDTxg/ltvNPoFVT+zf+YFOOeFw2sBvn9c1h/h+G82NbN1D7P8efDvItgY0b3INJF+cz797hAWnDdcf2tIddZ1Fas3rI8PZklONWQnX9ffaqgVGdMxKe3nKJSAODSlhL7h/J6zcOafDPbd64EXdd2LfuhnTHiD5kN8qgTyfXhIHePZ38tWyS5TNf1cnd2/Cfm88MKK14noG9q5jyOrXkzpF9fQLj7cPzuGNEXtDPmz7hjIBto07szMAebQBo0aQR/7xhsK32Jju8p1l/6men8N29wzmvr2vFRauFnsD1ZPz9pMBg4W2wewT5gNw2PiWbJQ+MrAsq3do05eohgaXJUN9HMCWPX8zPT+vBnLvPs9wfi3aycKz6HiRKtZkGBpWwWjfLsrXyXLyd0yeHtY+Mrps2xISr+4ih317QhztG9AnY/u5NZ3Db+SfQqVUTmvoFnSnXnlp3s7S60YR7Ag/Ff3qTzq2b1HUICKZxowwaN8oMOhdYbohST+umWXRt05QXrivg4zuGRVRN5Wkr+q3Fz8+je5C81+cG7T+WJphmNqsonWDrN0RERonIGhEpFpGJFvtFRJ5x718qIoPc27uLyFciskpEVojI7bG+AKWSQWaG8Ohl/S33De7Zjod/4rvvtV8N5vM7zwl6vkE92nLnSNeAu1PcU4V4d521qhDybLtkQBeeuDJ49YvVfFy/8ZovK1qNg7TxnHF8B87p4yp1/KhLy7pSwVVepYML8jvRKsh8Xt5WWDTM+1fN5ftV68VaZ5ulsyZZmSy+/wKfbXkd6z+RZiyErcwUkUzgOeACoBQoFJEZxpiVXslGA3nuf0OAye7/a4DfGWMWikhLYIGIfOZ3rFJp4eohxzH1mw1s3HXI58b9tsVo42F5OT7vv/79uVTXHrU875RrTmXh5j0M7dWeiqqasPkQhCtOzeWufy+x3N+/W2s+uWMY68sq6N6uKZ+v3MFpvdrxvHvtjkjZabUYc0o3zuvXkVZNsjixa2ufiRkj0dzdMO9dOvF8ft9OLZl8zaCIx56semgUt765kM9XldlK/+thvdm0+xAzlmwNm7ZNs2y+u3c4Qx/7AnD1cEsEdkoMg4FiY8wGY0wVMA0Y45dmDPCqe4nP74A2ItLFGLPNGLMQwBhzAFgF2OszqFSCymkRfX39678awh8vPZHeOS04p08Ofx9nr5HzuPbNOSHI02TrZlmc17cjTbMz6xqzG9eN4Ygun/06t+LiAV0YkNumrmQSLU+bSS+/m173dk2Z7hUU7ZQIPMb7lWCG5XXgj5eeCLjGiLzlN27k+0nDee/mM+id0yLoTMLBNM3ODBj3EkrrZlk84/e93jY8eDtRIrLT/aEbsNnrfSmu0kC4NN2AbZ4NItITGAjMt/oQERkPjAfo0SN0d0WlnHRSbvQDrrq3a8b1Z/QECDshYH08ccUAXvm2hME9Ayf+828jWfT/LmDgw58B2Botf1rPtnUz5wL0dq/i181rapcmWRlUVvuWcP7005O4ZEBX1uw4wMZdFUy6KJ/WUa7BkZUp/Hfi+awvOwi4Zv318F5wysNqcseGFCo+N2SblV12AoPVNflfScg0ItICeAe4wxhjOemJMWYqMBWgoKAg8X5SSvlp4PnzItKxVZOA6dH9J/x7/+YzWbRpD22bH5t/yU7vpX/79YS67vTj6Ne5pc/4lO/uHc4pD33m/lzXtmbZjRiR34kREUxcGEq3Nk0tR6J73D6iD/sOV3NlQehuoVOuGcSE1xf6bPP+mUD9J0vs5g64D405kfvfX+Gzr1FG4vUBshMYSgHvn2wu4F95FjSNiGThCgpvGGPejT6rSiWOpQ+OJCORI4MNJ3dvY2uNi3BEJGDQYptm2Tz1s5O58+0ljjWotmuezdNjw1fVjerfpe51z/bNwgaSSD085kSuPDWXji0bc06fnIDAkNOyMWNP687PE2hcg53AUAjkiUgvYAswFrjKL80M4BYRmYarmmmfMWabuMLsS8AqY8xTMcy3Uo6KpD48UbRo7Gp3aNcs8hla27qPiWSKjZ8OyuW0nu2CdglNNBee2Innry2I6JhheR0YN7gHN73hKnH85pzA3lvXnt4TgHP7HpvU0X8ZW/8lcZ0WNjAYY2pE5BZgFpAJvGyMWSEiE9z7pwAzgYuAYuAQcIP78DOBa4FlIrLYve0PxpiZMb0KpVRYF57YmUd+0t92P3tv/bu15tVfDmaIxWJFoSRLULDqBSVC3RgR77Lh3HvO46lP1/Luoi2MP7s3w/JyfAbieRROGkHFkcBeYi9cV0B+1/h2ma0vW2Pv3TfymX7bpni9NsDNFsfNJZIx9kqpuBERrhkabILA8M7ukxM+UQpZ9VDg/FvHtW9GbttmPDjmRPK7tqobIW015UdOy8aWXWMjWRzKKYnX6qGUUkFcPMA139NPTol/r/cmWZl1U5lkuufiauweNd6qSRY3Duvd4Cv4NRRdqEcpxUe3nUVVjfUAukTSq0PzqAe/1Udu26b87oI+tqduT3YaGJRSnNhVF8MJRUS4NckGqdWHViUppZTyoYFBKaWUDw0MSimlfGhgUEop5UMDg1JKKR8aGJRSSvnQwKCUUsqHBgallFI+xNRn1es4EZFyYGOUh3cAdsYwO05LteuB1LumVLseSL1rSrXrgcBrOs4YE5MJrRIyMNSHiBQZYyKbOzeBpdr1QOpdU6pdD6TeNaXa9UB8r0mrkpRSSvnQwKCUUspHKgaGqU5nIMZS7Xog9a4p1a4HUu+aUu16II7XlHJtDEoppeonFUsMSiml6kEDg1JKKR8pExhEZJSIrBGRYhGZ6HR+QhGREhFZJiKLRaTIva2diHwmIuvc/7f1Sn+v+7rWiMiFXttPdZ+nWESekQZcZ1BEXhaRMhFZ7rUtZtcgIo1F5C339vki0tOha3pQRLa4v6vFInJRslyTiHQXka9EZJWIrBCR293bk/J7CnE9yfwdNRGR70Vkifua/uje7ux3ZIxJ+n9AJrAe6A1kA0uAfKfzFSK/JUAHv21/ASa6X08E/ux+ne++nsZAL/d1Zrr3fQ+cDgjwMTC6Aa/hbGAQsDwe1wDcBExxvx4LvOXQNT0I3GWRNuGvCegCDHK/bgmsdec7Kb+nENeTzN+RAC3cr7OA+cBQp7+jBrmJxPuf+4cxy+v9vcC9TucrRH5LCAwMa4Au7tddgDVW1wLMcl9vF2C11/ZxwPMNfB098b2JxuwaPGncrxvhGuEpDlxTsJtO0lyTV17eBy5Ihe/J73pS4jsCmgELgSFOf0epUpXUDdjs9b7UvS1RGeBTEVkgIuPd2zoZY7YBuP/v6N4e7Nq6uV/7b3dSLK+h7hhjTA2wD2gft5yHdouILHVXNXmK9El1Te7qg4G4nkiT/nvyux5I4u9IRDJFZDFQBnxmjHH8O0qVwGBVt57I/XDPNMYMAkYDN4vI2SHSBru2ZLrmaK4hUa5vMnA8cAqwDXjSvT1prklEWgDvAHcYY/aHSmqxLeGuyeJ6kvo7MsbUGmNOAXKBwSLSP0TyBrmmVAkMpUB3r/e5wFaH8hKWMWar+/8y4D1gMLBDRLoAuP8vcycPdm2l7tf+250Uy2uoO0ZEGgGtgd1xy3kQxpgd7j/co8ALuL4rn/y5JeQ1iUgWrpvoG8aYd92bk/Z7srqeZP+OPIwxe4HZwCgc/o5SJTAUAnki0ktEsnE1sMxwOE+WRKS5iLT0vAZGAstx5fd6d7LrcdWf4t4+1t2zoBeQB3zvLl4eEJGh7t4H13kd45RYXoP3ua4AvjTuStKG5PnjdLsM13cFSXBN7s9/CVhljHnKa1dSfk/BrifJv6McEWnjft0UGAGsxunvqCEaVRqo4eYiXL0U1gOTnM5PiHz2xtWrYAmwwpNXXHV+XwDr3P+38zpmkvu61uDV8wgowPVHsB54loZtyHwTV7G9GtcTya9ieQ1AE+DfQDGu3ha9Hbqm14BlwFL3H1iXZLkm4CxcVQZLgcXufxcl6/cU4nqS+TsaACxy5305cL97u6PfkU6JoZRSykeqVCUppZSKEQ0MSimlfGhgUEop5UMDg1JKKR8aGJRSSvnQwKCUUsqHBgallFI+/j+315AHo6LEMQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  7 Training Time 5747.030290603638 Best Test Acc:  0.8150718889439762\n",
      "test subjects:  ['./seg\\\\a08', './seg\\\\a13', './seg\\\\x20']\n",
      "*********\n",
      "32804 1509\n",
      "31386 1505\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.669914722442627\n",
      "Eval Loss:  0.7121812105178833\n",
      "Eval Loss:  0.7096965909004211\n",
      "[[  266 19088]\n",
      " [  325 11707]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.01      0.03     19354\n",
      "           1       0.38      0.97      0.55     12032\n",
      "\n",
      "    accuracy                           0.38     31386\n",
      "   macro avg       0.42      0.49      0.29     31386\n",
      "weighted avg       0.42      0.38      0.23     31386\n",
      "\n",
      "acc:  0.3814758172433569\n",
      "pre:  0.38015911673973046\n",
      "rec:  0.9729886968085106\n",
      "ma F1:  0.2866922692926598\n",
      "mi F1:  0.3814758172433569\n",
      "we F1:  0.2260327868964528\n",
      "[[ 29 781]\n",
      " [ 46 649]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.04      0.07       810\n",
      "           1       0.45      0.93      0.61       695\n",
      "\n",
      "    accuracy                           0.45      1505\n",
      "   macro avg       0.42      0.48      0.34      1505\n",
      "weighted avg       0.42      0.45      0.32      1505\n",
      "\n",
      "acc:  0.4504983388704319\n",
      "pre:  0.45384615384615384\n",
      "rec:  0.9338129496402877\n",
      "ma F1:  0.33818012628780325\n",
      "mi F1:  0.4504983388704319\n",
      "we F1:  0.31734690943779953\n",
      "Subject 8 Current Train Acc:  0.3814758172433569 Current Test Acc:  0.4504983388704319\n",
      "Loss:  0.17069049179553986\n",
      "Loss:  0.17305205762386322\n",
      "Loss:  0.16143950819969177\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.13653121888637543\n",
      "Loss:  0.1320076435804367\n",
      "Loss:  0.13125719130039215\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.12871716916561127\n",
      "Loss:  0.13250477612018585\n",
      "Loss:  0.10875394940376282\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  2.103543758392334\n",
      "Eval Loss:  0.034401655197143555\n",
      "Eval Loss:  0.20164692401885986\n",
      "[[17320  2034]\n",
      " [ 3331  8701]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.87     19354\n",
      "           1       0.81      0.72      0.76     12032\n",
      "\n",
      "    accuracy                           0.83     31386\n",
      "   macro avg       0.82      0.81      0.82     31386\n",
      "weighted avg       0.83      0.83      0.83     31386\n",
      "\n",
      "acc:  0.8290639138469381\n",
      "pre:  0.8105263157894737\n",
      "rec:  0.723154920212766\n",
      "ma F1:  0.815121838192943\n",
      "mi F1:  0.8290639138469381\n",
      "we F1:  0.8269658894678649\n",
      "[[743  67]\n",
      " [383 312]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.92      0.77       810\n",
      "           1       0.82      0.45      0.58       695\n",
      "\n",
      "    accuracy                           0.70      1505\n",
      "   macro avg       0.74      0.68      0.67      1505\n",
      "weighted avg       0.74      0.70      0.68      1505\n",
      "\n",
      "acc:  0.7009966777408638\n",
      "pre:  0.8232189973614775\n",
      "rec:  0.4489208633093525\n",
      "ma F1:  0.6742837850316266\n",
      "mi F1:  0.7009966777408638\n",
      "we F1:  0.6814113550120495\n",
      "Subject 8 Current Train Acc:  0.8290639138469381 Current Test Acc:  0.7009966777408638\n",
      "Loss:  0.12234517931938171\n",
      "Loss:  0.08989405632019043\n",
      "Loss:  0.08977688103914261\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.10713682323694229\n",
      "Loss:  0.07531767338514328\n",
      "Loss:  0.05723731964826584\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.06592071056365967\n",
      "Loss:  0.13000434637069702\n",
      "Loss:  0.10611005872488022\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  3.5325746536254883\n",
      "Eval Loss:  0.014633417129516602\n",
      "Eval Loss:  0.09633708000183105\n",
      "[[18863   491]\n",
      " [ 5092  6940]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.97      0.87     19354\n",
      "           1       0.93      0.58      0.71     12032\n",
      "\n",
      "    accuracy                           0.82     31386\n",
      "   macro avg       0.86      0.78      0.79     31386\n",
      "weighted avg       0.84      0.82      0.81     31386\n",
      "\n",
      "acc:  0.8221181418466832\n",
      "pre:  0.9339254474498722\n",
      "rec:  0.5767952127659575\n",
      "ma F1:  0.7921185872589247\n",
      "mi F1:  0.8221181418466832\n",
      "we F1:  0.810541529361715\n",
      "[[799  11]\n",
      " [579 116]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.99      0.73       810\n",
      "           1       0.91      0.17      0.28       695\n",
      "\n",
      "    accuracy                           0.61      1505\n",
      "   macro avg       0.75      0.58      0.51      1505\n",
      "weighted avg       0.73      0.61      0.52      1505\n",
      "\n",
      "acc:  0.6079734219269103\n",
      "pre:  0.9133858267716536\n",
      "rec:  0.1669064748201439\n",
      "ma F1:  0.5062928959998577\n",
      "mi F1:  0.6079734219269103\n",
      "we F1:  0.5234133359436512\n",
      "Loss:  0.12064774334430695\n",
      "Loss:  0.11539821326732635\n",
      "Loss:  0.055700790137052536\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.12653595209121704\n",
      "Loss:  0.0891503393650055\n",
      "Loss:  0.06845744699239731\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.07095105946063995\n",
      "Loss:  0.07863324135541916\n",
      "Loss:  0.10757149755954742\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  2.735764741897583\n",
      "Eval Loss:  0.010343670845031738\n",
      "Eval Loss:  0.04425835609436035\n",
      "[[18661   693]\n",
      " [ 3729  8303]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89     19354\n",
      "           1       0.92      0.69      0.79     12032\n",
      "\n",
      "    accuracy                           0.86     31386\n",
      "   macro avg       0.88      0.83      0.84     31386\n",
      "weighted avg       0.87      0.86      0.85     31386\n",
      "\n",
      "acc:  0.8591091569489582\n",
      "pre:  0.9229657625611383\n",
      "rec:  0.6900764627659575\n",
      "ma F1:  0.8418887840724587\n",
      "mi F1:  0.8591091569489582\n",
      "we F1:  0.854061748950015\n",
      "[[804   6]\n",
      " [567 128]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.99      0.74       810\n",
      "           1       0.96      0.18      0.31       695\n",
      "\n",
      "    accuracy                           0.62      1505\n",
      "   macro avg       0.77      0.59      0.52      1505\n",
      "weighted avg       0.76      0.62      0.54      1505\n",
      "\n",
      "acc:  0.6192691029900332\n",
      "pre:  0.9552238805970149\n",
      "rec:  0.1841726618705036\n",
      "ma F1:  0.5230411343940348\n",
      "mi F1:  0.6192691029900332\n",
      "we F1:  0.5394112769806322\n",
      "Loss:  0.09585109353065491\n",
      "Loss:  0.0833592340350151\n",
      "Loss:  0.06070037931203842\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.0911928340792656\n",
      "Loss:  0.07465504854917526\n",
      "Loss:  0.06866525858640671\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.0675709918141365\n",
      "Loss:  0.0786159560084343\n",
      "Loss:  0.07103786617517471\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  3.5111002922058105\n",
      "Eval Loss:  0.008435726165771484\n",
      "Eval Loss:  0.03406882286071777\n",
      "[[19001   353]\n",
      " [ 4666  7366]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.98      0.88     19354\n",
      "           1       0.95      0.61      0.75     12032\n",
      "\n",
      "    accuracy                           0.84     31386\n",
      "   macro avg       0.88      0.80      0.81     31386\n",
      "weighted avg       0.86      0.84      0.83     31386\n",
      "\n",
      "acc:  0.8400879372968839\n",
      "pre:  0.9542686876538412\n",
      "rec:  0.6122007978723404\n",
      "ma F1:  0.8146111647130034\n",
      "mi F1:  0.840087937296884\n",
      "we F1:  0.8306439046235469\n",
      "[[808   2]\n",
      " [578 117]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      1.00      0.74       810\n",
      "           1       0.98      0.17      0.29       695\n",
      "\n",
      "    accuracy                           0.61      1505\n",
      "   macro avg       0.78      0.58      0.51      1505\n",
      "weighted avg       0.77      0.61      0.53      1505\n",
      "\n",
      "acc:  0.6146179401993356\n",
      "pre:  0.9831932773109243\n",
      "rec:  0.1683453237410072\n",
      "ma F1:  0.511676355938651\n",
      "mi F1:  0.6146179401993356\n",
      "we F1:  0.5288084575160442\n",
      "Loss:  0.06396942585706711\n",
      "Loss:  0.0672249123454094\n",
      "Loss:  0.11920282244682312\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.09733113646507263\n",
      "Loss:  0.07696982473134995\n",
      "Loss:  0.06510932743549347\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.05933615192770958\n",
      "Loss:  0.043518926948308945\n",
      "Loss:  0.06297853589057922\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  1.9618873596191406\n",
      "Eval Loss:  0.007860422134399414\n",
      "Eval Loss:  0.028402209281921387\n",
      "[[18821   533]\n",
      " [ 3635  8397]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.97      0.90     19354\n",
      "           1       0.94      0.70      0.80     12032\n",
      "\n",
      "    accuracy                           0.87     31386\n",
      "   macro avg       0.89      0.84      0.85     31386\n",
      "weighted avg       0.88      0.87      0.86     31386\n",
      "\n",
      "acc:  0.8672019371694386\n",
      "pre:  0.9403135498320269\n",
      "rec:  0.6978889627659575\n",
      "ma F1:  0.8507374707335361\n",
      "mi F1:  0.8672019371694387\n",
      "we F1:  0.8623024000546872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[808   2]\n",
      " [568 127]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      1.00      0.74       810\n",
      "           1       0.98      0.18      0.31       695\n",
      "\n",
      "    accuracy                           0.62      1505\n",
      "   macro avg       0.79      0.59      0.52      1505\n",
      "weighted avg       0.77      0.62      0.54      1505\n",
      "\n",
      "acc:  0.6212624584717608\n",
      "pre:  0.9844961240310077\n",
      "rec:  0.18273381294964028\n",
      "ma F1:  0.5237510992280976\n",
      "mi F1:  0.6212624584717608\n",
      "we F1:  0.5402177751649865\n",
      "Loss:  0.0544617623090744\n",
      "Loss:  0.07156749814748764\n",
      "Loss:  0.07243527472019196\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.08165151625871658\n",
      "Loss:  0.05764869973063469\n",
      "Loss:  0.0430995337665081\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.05406460911035538\n",
      "Loss:  0.04622756689786911\n",
      "Loss:  0.04704783111810684\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.6057173013687134\n",
      "Eval Loss:  0.0058803558349609375\n",
      "Eval Loss:  0.019083261489868164\n",
      "[[18542   812]\n",
      " [ 2563  9469]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     19354\n",
      "           1       0.92      0.79      0.85     12032\n",
      "\n",
      "    accuracy                           0.89     31386\n",
      "   macro avg       0.90      0.87      0.88     31386\n",
      "weighted avg       0.89      0.89      0.89     31386\n",
      "\n",
      "acc:  0.892467979353852\n",
      "pre:  0.9210193560937652\n",
      "rec:  0.7869847074468085\n",
      "ma F1:  0.8826625521750249\n",
      "mi F1:  0.892467979353852\n",
      "we F1:  0.8905756280929542\n",
      "[[805   5]\n",
      " [499 196]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.99      0.76       810\n",
      "           1       0.98      0.28      0.44       695\n",
      "\n",
      "    accuracy                           0.67      1505\n",
      "   macro avg       0.80      0.64      0.60      1505\n",
      "weighted avg       0.78      0.67      0.61      1505\n",
      "\n",
      "acc:  0.6651162790697674\n",
      "pre:  0.9751243781094527\n",
      "rec:  0.2820143884892086\n",
      "ma F1:  0.599544701986755\n",
      "mi F1:  0.6651162790697674\n",
      "we F1:  0.611926855294713\n",
      "Loss:  0.06511962413787842\n",
      "Loss:  0.050499748438596725\n",
      "Loss:  0.08314448595046997\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.06870697438716888\n",
      "Loss:  0.08401411771774292\n",
      "Loss:  0.07369504868984222\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.07249541580677032\n",
      "Loss:  0.04736635461449623\n",
      "Loss:  0.06943733990192413\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  1.6019818782806396\n",
      "Eval Loss:  0.005354642868041992\n",
      "Eval Loss:  0.01516103744506836\n",
      "[[18914   440]\n",
      " [ 3566  8466]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.98      0.90     19354\n",
      "           1       0.95      0.70      0.81     12032\n",
      "\n",
      "    accuracy                           0.87     31386\n",
      "   macro avg       0.90      0.84      0.86     31386\n",
      "weighted avg       0.88      0.87      0.87     31386\n",
      "\n",
      "acc:  0.8723634741604537\n",
      "pre:  0.9505951044239839\n",
      "rec:  0.703623670212766\n",
      "ma F1:  0.8564568977927047\n",
      "mi F1:  0.8723634741604537\n",
      "we F1:  0.867604289845218\n",
      "[[808   2]\n",
      " [553 142]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      1.00      0.74       810\n",
      "           1       0.99      0.20      0.34       695\n",
      "\n",
      "    accuracy                           0.63      1505\n",
      "   macro avg       0.79      0.60      0.54      1505\n",
      "weighted avg       0.77      0.63      0.56      1505\n",
      "\n",
      "acc:  0.6312292358803987\n",
      "pre:  0.9861111111111112\n",
      "rec:  0.20431654676258992\n",
      "ma F1:  0.5414278255627738\n",
      "mi F1:  0.6312292358803987\n",
      "we F1:  0.5569340750920935\n",
      "Loss:  0.051465436816215515\n",
      "Loss:  0.08238808065652847\n",
      "Loss:  0.05600403621792793\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.0668882504105568\n",
      "Loss:  0.06600753962993622\n",
      "Loss:  0.05023651942610741\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.0672825500369072\n",
      "Loss:  0.09214534610509872\n",
      "Loss:  0.045017268508672714\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  1.0991199016571045\n",
      "Eval Loss:  0.007409334182739258\n",
      "Eval Loss:  0.012665271759033203\n",
      "[[18654   700]\n",
      " [ 2566  9466]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     19354\n",
      "           1       0.93      0.79      0.85     12032\n",
      "\n",
      "    accuracy                           0.90     31386\n",
      "   macro avg       0.91      0.88      0.89     31386\n",
      "weighted avg       0.90      0.90      0.89     31386\n",
      "\n",
      "acc:  0.8959408653539794\n",
      "pre:  0.9311430257721818\n",
      "rec:  0.7867353723404256\n",
      "ma F1:  0.8861873648418641\n",
      "mi F1:  0.8959408653539794\n",
      "we F1:  0.8939600172961206\n",
      "[[806   4]\n",
      " [518 177]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      1.00      0.76       810\n",
      "           1       0.98      0.25      0.40       695\n",
      "\n",
      "    accuracy                           0.65      1505\n",
      "   macro avg       0.79      0.62      0.58      1505\n",
      "weighted avg       0.78      0.65      0.59      1505\n",
      "\n",
      "acc:  0.653156146179402\n",
      "pre:  0.9779005524861878\n",
      "rec:  0.25467625899280577\n",
      "ma F1:  0.5797492649985234\n",
      "mi F1:  0.653156146179402\n",
      "we F1:  0.5931702369155362\n",
      "Loss:  0.05207870528101921\n",
      "Loss:  0.05894722044467926\n",
      "Loss:  0.04514516890048981\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.08746977895498276\n",
      "Loss:  0.047535791993141174\n",
      "Loss:  0.03889862447977066\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.05011263117194176\n",
      "Loss:  0.0688193291425705\n",
      "Loss:  0.08028050512075424\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  1.7928531169891357\n",
      "Eval Loss:  0.005097150802612305\n",
      "Eval Loss:  0.010037660598754883\n",
      "[[18641   713]\n",
      " [ 2341  9691]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92     19354\n",
      "           1       0.93      0.81      0.86     12032\n",
      "\n",
      "    accuracy                           0.90     31386\n",
      "   macro avg       0.91      0.88      0.89     31386\n",
      "weighted avg       0.90      0.90      0.90     31386\n",
      "\n",
      "acc:  0.9026954693175301\n",
      "pre:  0.9314686658977317\n",
      "rec:  0.805435505319149\n",
      "ma F1:  0.8940827385140432\n",
      "mi F1:  0.9026954693175301\n",
      "we F1:  0.9011288183959574\n",
      "[[806   4]\n",
      " [459 236]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      1.00      0.78       810\n",
      "           1       0.98      0.34      0.50       695\n",
      "\n",
      "    accuracy                           0.69      1505\n",
      "   macro avg       0.81      0.67      0.64      1505\n",
      "weighted avg       0.80      0.69      0.65      1505\n",
      "\n",
      "acc:  0.692358803986711\n",
      "pre:  0.9833333333333333\n",
      "rec:  0.339568345323741\n",
      "ma F1:  0.6408401520520585\n",
      "mi F1:  0.692358803986711\n",
      "we F1:  0.6512342660388742\n",
      "Loss:  0.058208685368299484\n",
      "Loss:  0.07781583815813065\n",
      "Loss:  0.08277577906847\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.05739887058734894\n",
      "Loss:  0.05049087852239609\n",
      "Loss:  0.045080386102199554\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.05242152139544487\n",
      "Loss:  0.06106928363442421\n",
      "Loss:  0.053544092923402786\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.6121360063552856\n",
      "Eval Loss:  0.0056574344635009766\n",
      "Eval Loss:  0.00980377197265625\n",
      "[[18480   874]\n",
      " [ 1915 10117]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19354\n",
      "           1       0.92      0.84      0.88     12032\n",
      "\n",
      "    accuracy                           0.91     31386\n",
      "   macro avg       0.91      0.90      0.90     31386\n",
      "weighted avg       0.91      0.91      0.91     31386\n",
      "\n",
      "acc:  0.9111387242719684\n",
      "pre:  0.9204803930488582\n",
      "rec:  0.8408410904255319\n",
      "ma F1:  0.904347491493791\n",
      "mi F1:  0.9111387242719684\n",
      "we F1:  0.9102933729241168\n",
      "[[800  10]\n",
      " [428 267]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.99      0.79       810\n",
      "           1       0.96      0.38      0.55       695\n",
      "\n",
      "    accuracy                           0.71      1505\n",
      "   macro avg       0.81      0.69      0.67      1505\n",
      "weighted avg       0.80      0.71      0.68      1505\n",
      "\n",
      "acc:  0.7089700996677741\n",
      "pre:  0.9638989169675091\n",
      "rec:  0.3841726618705036\n",
      "ma F1:  0.6672330655811193\n",
      "mi F1:  0.7089700996677741\n",
      "we F1:  0.6762382417911855\n",
      "Subject 8 Current Train Acc:  0.9111387242719684 Current Test Acc:  0.7089700996677741\n",
      "Loss:  0.03521359711885452\n",
      "Loss:  0.10732853412628174\n",
      "Loss:  0.04844611510634422\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.05852625519037247\n",
      "Loss:  0.039208751171827316\n",
      "Loss:  0.04583045095205307\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.05505757033824921\n",
      "Loss:  0.04671374708414078\n",
      "Loss:  0.04383872449398041\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.7103767395019531\n",
      "Eval Loss:  0.009432554244995117\n",
      "Eval Loss:  0.008060216903686523\n",
      "[[18142  1212]\n",
      " [ 1431 10601]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     19354\n",
      "           1       0.90      0.88      0.89     12032\n",
      "\n",
      "    accuracy                           0.92     31386\n",
      "   macro avg       0.91      0.91      0.91     31386\n",
      "weighted avg       0.92      0.92      0.92     31386\n",
      "\n",
      "acc:  0.9157904798317721\n",
      "pre:  0.8974011682045204\n",
      "rec:  0.8810671542553191\n",
      "ma F1:  0.9106314170558768\n",
      "mi F1:  0.9157904798317721\n",
      "we F1:  0.9156406542452556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[795  15]\n",
      " [422 273]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.98      0.78       810\n",
      "           1       0.95      0.39      0.56       695\n",
      "\n",
      "    accuracy                           0.71      1505\n",
      "   macro avg       0.80      0.69      0.67      1505\n",
      "weighted avg       0.79      0.71      0.68      1505\n",
      "\n",
      "acc:  0.7096345514950166\n",
      "pre:  0.9479166666666666\n",
      "rec:  0.39280575539568346\n",
      "ma F1:  0.6699264908476161\n",
      "mi F1:  0.7096345514950168\n",
      "we F1:  0.6786744352431163\n",
      "Subject 8 Current Train Acc:  0.9157904798317721 Current Test Acc:  0.7096345514950166\n",
      "Loss:  0.08167499303817749\n",
      "Loss:  0.02319999411702156\n",
      "Loss:  0.07637633383274078\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.03229626640677452\n",
      "Loss:  0.056432902812957764\n",
      "Loss:  0.03375932201743126\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.0692167654633522\n",
      "Loss:  0.03774062171578407\n",
      "Loss:  0.052123233675956726\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  2.7504093647003174\n",
      "Eval Loss:  0.00413823127746582\n",
      "Eval Loss:  0.007599830627441406\n",
      "[[18401   953]\n",
      " [ 1781 10251]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19354\n",
      "           1       0.91      0.85      0.88     12032\n",
      "\n",
      "    accuracy                           0.91     31386\n",
      "   macro avg       0.91      0.90      0.91     31386\n",
      "weighted avg       0.91      0.91      0.91     31386\n",
      "\n",
      "acc:  0.9128910979417575\n",
      "pre:  0.9149410924669761\n",
      "rec:  0.8519780585106383\n",
      "ma F1:  0.9065927933245826\n",
      "mi F1:  0.9128910979417575\n",
      "we F1:  0.912251221104577\n",
      "[[788  22]\n",
      " [362 333]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.97      0.80       810\n",
      "           1       0.94      0.48      0.63       695\n",
      "\n",
      "    accuracy                           0.74      1505\n",
      "   macro avg       0.81      0.73      0.72      1505\n",
      "weighted avg       0.80      0.74      0.73      1505\n",
      "\n",
      "acc:  0.7448504983388704\n",
      "pre:  0.9380281690140845\n",
      "rec:  0.479136690647482\n",
      "ma F1:  0.7191836734693877\n",
      "mi F1:  0.7448504983388703\n",
      "we F1:  0.7256708929418944\n",
      "Subject 8 Current Train Acc:  0.9128910979417575 Current Test Acc:  0.7448504983388704\n",
      "Loss:  0.04534304887056351\n",
      "Loss:  0.04663790389895439\n",
      "Loss:  0.058015525341033936\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.05387828126549721\n",
      "Loss:  0.07294195890426636\n",
      "Loss:  0.0875089094042778\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.05106951668858528\n",
      "Loss:  0.06002087518572807\n",
      "Loss:  0.05114499479532242\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.8097349405288696\n",
      "Eval Loss:  0.00668025016784668\n",
      "Eval Loss:  0.008056402206420898\n",
      "[[18270  1084]\n",
      " [ 1458 10574]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     19354\n",
      "           1       0.91      0.88      0.89     12032\n",
      "\n",
      "    accuracy                           0.92     31386\n",
      "   macro avg       0.92      0.91      0.91     31386\n",
      "weighted avg       0.92      0.92      0.92     31386\n",
      "\n",
      "acc:  0.9190084751162939\n",
      "pre:  0.9070166409332647\n",
      "rec:  0.8788231382978723\n",
      "ma F1:  0.9138273049906065\n",
      "mi F1:  0.9190084751162939\n",
      "we F1:  0.918756687482847\n",
      "[[803   7]\n",
      " [405 290]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.99      0.80       810\n",
      "           1       0.98      0.42      0.58       695\n",
      "\n",
      "    accuracy                           0.73      1505\n",
      "   macro avg       0.82      0.70      0.69      1505\n",
      "weighted avg       0.81      0.73      0.70      1505\n",
      "\n",
      "acc:  0.7262458471760798\n",
      "pre:  0.9764309764309764\n",
      "rec:  0.4172661870503597\n",
      "ma F1:  0.6902574410946641\n",
      "mi F1:  0.7262458471760798\n",
      "we F1:  0.6983250175066774\n",
      "Loss:  0.06804346293210983\n",
      "Loss:  0.03798641264438629\n",
      "Loss:  0.04204808920621872\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.055288996547460556\n",
      "Loss:  0.05430835112929344\n",
      "Loss:  0.07720982283353806\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.02570120245218277\n",
      "Loss:  0.045849353075027466\n",
      "Loss:  0.08317369967699051\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.4134102761745453\n",
      "Eval Loss:  0.006308317184448242\n",
      "Eval Loss:  0.006792783737182617\n",
      "[[17942  1412]\n",
      " [ 1070 10962]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94     19354\n",
      "           1       0.89      0.91      0.90     12032\n",
      "\n",
      "    accuracy                           0.92     31386\n",
      "   macro avg       0.91      0.92      0.92     31386\n",
      "weighted avg       0.92      0.92      0.92     31386\n",
      "\n",
      "acc:  0.9209201554833365\n",
      "pre:  0.8858897688702118\n",
      "rec:  0.9110704787234043\n",
      "ma F1:  0.9168054995770026\n",
      "mi F1:  0.9209201554833365\n",
      "we F1:  0.9211217618329018\n",
      "[[789  21]\n",
      " [352 343]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.97      0.81       810\n",
      "           1       0.94      0.49      0.65       695\n",
      "\n",
      "    accuracy                           0.75      1505\n",
      "   macro avg       0.82      0.73      0.73      1505\n",
      "weighted avg       0.81      0.75      0.73      1505\n",
      "\n",
      "acc:  0.7521594684385382\n",
      "pre:  0.9423076923076923\n",
      "rec:  0.4935251798561151\n",
      "ma F1:  0.7282984586001997\n",
      "mi F1:  0.7521594684385382\n",
      "we F1:  0.734450961136991\n",
      "Subject 8 Current Train Acc:  0.9209201554833365 Current Test Acc:  0.7521594684385382\n",
      "Loss:  0.03584093600511551\n",
      "Loss:  0.028989020735025406\n",
      "Loss:  0.04755694046616554\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.05472598969936371\n",
      "Loss:  0.04476401209831238\n",
      "Loss:  0.0819573923945427\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.04927706718444824\n",
      "Loss:  0.03519502282142639\n",
      "Loss:  0.0834183618426323\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.962914228439331\n",
      "Eval Loss:  0.005612373352050781\n",
      "Eval Loss:  0.00713348388671875\n",
      "[[18129  1225]\n",
      " [ 1165 10867]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94     19354\n",
      "           1       0.90      0.90      0.90     12032\n",
      "\n",
      "    accuracy                           0.92     31386\n",
      "   macro avg       0.92      0.92      0.92     31386\n",
      "weighted avg       0.92      0.92      0.92     31386\n",
      "\n",
      "acc:  0.9238513987128019\n",
      "pre:  0.8986933509758518\n",
      "rec:  0.9031748670212766\n",
      "ma F1:  0.9195441685906196\n",
      "mi F1:  0.9238513987128019\n",
      "we F1:  0.9238869858523406\n",
      "[[792  18]\n",
      " [349 346]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.98      0.81       810\n",
      "           1       0.95      0.50      0.65       695\n",
      "\n",
      "    accuracy                           0.76      1505\n",
      "   macro avg       0.82      0.74      0.73      1505\n",
      "weighted avg       0.81      0.76      0.74      1505\n",
      "\n",
      "acc:  0.7561461794019934\n",
      "pre:  0.9505494505494505\n",
      "rec:  0.497841726618705\n",
      "ma F1:  0.7326689927782126\n",
      "mi F1:  0.7561461794019934\n",
      "we F1:  0.7387225274457793\n",
      "Subject 8 Current Train Acc:  0.9238513987128019 Current Test Acc:  0.7561461794019934\n",
      "Loss:  0.06197905167937279\n",
      "Loss:  0.08270373195409775\n",
      "Loss:  0.056477561593055725\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.09440116584300995\n",
      "Loss:  0.05857028067111969\n",
      "Loss:  0.04534215107560158\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.035279009491205215\n",
      "Loss:  0.07207097113132477\n",
      "Loss:  0.04942450299859047\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.4721037447452545\n",
      "Eval Loss:  0.006052255630493164\n",
      "Eval Loss:  0.0059397220611572266\n",
      "[[18115  1239]\n",
      " [ 1205 10827]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94     19354\n",
      "           1       0.90      0.90      0.90     12032\n",
      "\n",
      "    accuracy                           0.92     31386\n",
      "   macro avg       0.92      0.92      0.92     31386\n",
      "weighted avg       0.92      0.92      0.92     31386\n",
      "\n",
      "acc:  0.9221308863824635\n",
      "pre:  0.8973147687717553\n",
      "rec:  0.8998503989361702\n",
      "ma F1:  0.9176929418884026\n",
      "mi F1:  0.9221308863824635\n",
      "we F1:  0.9221515902947575\n",
      "[[780  30]\n",
      " [350 345]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.96      0.80       810\n",
      "           1       0.92      0.50      0.64       695\n",
      "\n",
      "    accuracy                           0.75      1505\n",
      "   macro avg       0.81      0.73      0.72      1505\n",
      "weighted avg       0.80      0.75      0.73      1505\n",
      "\n",
      "acc:  0.7475083056478405\n",
      "pre:  0.92\n",
      "rec:  0.49640287769784175\n",
      "ma F1:  0.7244917622121592\n",
      "mi F1:  0.7475083056478405\n",
      "we F1:  0.7305765955342358\n",
      "Loss:  0.034892044961452484\n",
      "Loss:  0.022883057594299316\n",
      "Loss:  0.04367218166589737\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.09096454083919525\n",
      "Loss:  0.030416958034038544\n",
      "Loss:  0.04240061342716217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.045234061777591705\n",
      "Loss:  0.03724417835474014\n",
      "Loss:  0.03940116614103317\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.9907048940658569\n",
      "Eval Loss:  0.005406856536865234\n",
      "Eval Loss:  0.005460262298583984\n",
      "[[18039  1315]\n",
      " [ 1066 10966]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94     19354\n",
      "           1       0.89      0.91      0.90     12032\n",
      "\n",
      "    accuracy                           0.92     31386\n",
      "   macro avg       0.92      0.92      0.92     31386\n",
      "weighted avg       0.92      0.92      0.92     31386\n",
      "\n",
      "acc:  0.9241381507678583\n",
      "pre:  0.8929240289878675\n",
      "rec:  0.9114029255319149\n",
      "ma F1:  0.9200793829943881\n",
      "mi F1:  0.9241381507678583\n",
      "we F1:  0.9242810368382095\n",
      "[[781  29]\n",
      " [331 364]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.96      0.81       810\n",
      "           1       0.93      0.52      0.67       695\n",
      "\n",
      "    accuracy                           0.76      1505\n",
      "   macro avg       0.81      0.74      0.74      1505\n",
      "weighted avg       0.81      0.76      0.75      1505\n",
      "\n",
      "acc:  0.760797342192691\n",
      "pre:  0.926208651399491\n",
      "rec:  0.5237410071942447\n",
      "ma F1:  0.7409063781600048\n",
      "mi F1:  0.760797342192691\n",
      "we F1:  0.7463918958188991\n",
      "Subject 8 Current Train Acc:  0.9241381507678583 Current Test Acc:  0.760797342192691\n",
      "Loss:  0.08341287821531296\n",
      "Loss:  0.051973409950733185\n",
      "Loss:  0.04576656222343445\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.06039194390177727\n",
      "Loss:  0.05804049223661423\n",
      "Loss:  0.053865332156419754\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.07435405999422073\n",
      "Loss:  0.03549880161881447\n",
      "Loss:  0.05243367701768875\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.17816877365112305\n",
      "Eval Loss:  0.004878997802734375\n",
      "Eval Loss:  0.006232261657714844\n",
      "[[18199  1155]\n",
      " [ 1057 10975]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94     19354\n",
      "           1       0.90      0.91      0.91     12032\n",
      "\n",
      "    accuracy                           0.93     31386\n",
      "   macro avg       0.92      0.93      0.93     31386\n",
      "weighted avg       0.93      0.93      0.93     31386\n",
      "\n",
      "acc:  0.9295227171350283\n",
      "pre:  0.9047815333882935\n",
      "rec:  0.9121509308510638\n",
      "ma F1:  0.9255802149271233\n",
      "mi F1:  0.9295227171350282\n",
      "we F1:  0.9295762006921123\n",
      "[[773  37]\n",
      " [320 375]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.95      0.81       810\n",
      "           1       0.91      0.54      0.68       695\n",
      "\n",
      "    accuracy                           0.76      1505\n",
      "   macro avg       0.81      0.75      0.74      1505\n",
      "weighted avg       0.80      0.76      0.75      1505\n",
      "\n",
      "acc:  0.7627906976744186\n",
      "pre:  0.9101941747572816\n",
      "rec:  0.539568345323741\n",
      "ma F1:  0.7449541232143797\n",
      "mi F1:  0.7627906976744185\n",
      "we F1:  0.7501079072920291\n",
      "Subject 8 Current Train Acc:  0.9295227171350283 Current Test Acc:  0.7627906976744186\n",
      "Loss:  0.04761984199285507\n",
      "Loss:  0.058328207582235336\n",
      "Loss:  0.06933674961328506\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.04135105758905411\n",
      "Loss:  0.05689046531915665\n",
      "Loss:  0.0768553763628006\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.040355488657951355\n",
      "Loss:  0.028626203536987305\n",
      "Loss:  0.06356745958328247\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.09359228610992432\n",
      "Eval Loss:  0.006787538528442383\n",
      "Eval Loss:  0.006745338439941406\n",
      "[[17847  1507]\n",
      " [  879 11153]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.94     19354\n",
      "           1       0.88      0.93      0.90     12032\n",
      "\n",
      "    accuracy                           0.92     31386\n",
      "   macro avg       0.92      0.92      0.92     31386\n",
      "weighted avg       0.93      0.92      0.92     31386\n",
      "\n",
      "acc:  0.9239788440706047\n",
      "pre:  0.8809636650868878\n",
      "rec:  0.9269448138297872\n",
      "ma F1:  0.9203559746837338\n",
      "mi F1:  0.9239788440706047\n",
      "we F1:  0.9243187248556294\n",
      "[[727  83]\n",
      " [256 439]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.90      0.81       810\n",
      "           1       0.84      0.63      0.72       695\n",
      "\n",
      "    accuracy                           0.77      1505\n",
      "   macro avg       0.79      0.76      0.77      1505\n",
      "weighted avg       0.79      0.77      0.77      1505\n",
      "\n",
      "acc:  0.774750830564784\n",
      "pre:  0.8409961685823755\n",
      "rec:  0.6316546762589929\n",
      "ma F1:  0.7661887895087305\n",
      "mi F1:  0.774750830564784\n",
      "we F1:  0.7696076600693075\n",
      "Subject 8 Current Train Acc:  0.9239788440706047 Current Test Acc:  0.774750830564784\n",
      "Loss:  0.06117866188287735\n",
      "Loss:  0.03469180688261986\n",
      "Loss:  0.04965254291892052\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.029850181192159653\n",
      "Loss:  0.04490384832024574\n",
      "Loss:  0.0443478599190712\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.04914901405572891\n",
      "Loss:  0.0785316750407219\n",
      "Loss:  0.06348565220832825\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  8 Training Time 5750.187167644501 Best Test Acc:  0.774750830564784\n",
      "test subjects:  ['./seg\\\\a09', './seg\\\\a18']\n",
      "*********\n",
      "33329 984\n",
      "31907 984\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.6678330302238464\n",
      "Eval Loss:  0.6979125738143921\n",
      "Eval Loss:  0.6788997650146484\n",
      "[[19903    96]\n",
      " [11659   249]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.77     19999\n",
      "           1       0.72      0.02      0.04     11908\n",
      "\n",
      "    accuracy                           0.63     31907\n",
      "   macro avg       0.68      0.51      0.41     31907\n",
      "weighted avg       0.66      0.63      0.50     31907\n",
      "\n",
      "acc:  0.631585545491585\n",
      "pre:  0.7217391304347827\n",
      "rec:  0.020910312395028552\n",
      "ma F1:  0.40633035900979947\n",
      "mi F1:  0.631585545491585\n",
      "we F1:  0.49906159508510867\n",
      "[[164   1]\n",
      " [814   5]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.99      0.29       165\n",
      "           1       0.83      0.01      0.01       819\n",
      "\n",
      "    accuracy                           0.17       984\n",
      "   macro avg       0.50      0.50      0.15       984\n",
      "weighted avg       0.72      0.17      0.06       984\n",
      "\n",
      "acc:  0.1717479674796748\n",
      "pre:  0.8333333333333334\n",
      "rec:  0.006105006105006105\n",
      "ma F1:  0.14954267080251332\n",
      "mi F1:  0.1717479674796748\n",
      "we F1:  0.05820767692286801\n",
      "Subject 9 Current Train Acc:  0.631585545491585 Current Test Acc:  0.1717479674796748\n",
      "Loss:  0.1713525801897049\n",
      "Loss:  0.16441115736961365\n",
      "Loss:  0.1595725566148758\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.146437406539917\n",
      "Loss:  0.12553149461746216\n",
      "Loss:  0.13329163193702698\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.10585272312164307\n",
      "Loss:  0.11874925345182419\n",
      "Loss:  0.11391367018222809\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.01965951919555664\n",
      "Eval Loss:  0.1755446195602417\n",
      "Eval Loss:  0.017567157745361328\n",
      "[[17899  2100]\n",
      " [ 3543  8365]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.89      0.86     19999\n",
      "           1       0.80      0.70      0.75     11908\n",
      "\n",
      "    accuracy                           0.82     31907\n",
      "   macro avg       0.82      0.80      0.81     31907\n",
      "weighted avg       0.82      0.82      0.82     31907\n",
      "\n",
      "acc:  0.8231422571849437\n",
      "pre:  0.7993311036789298\n",
      "rec:  0.7024689284514612\n",
      "ma F1:  0.8058034217851664\n",
      "mi F1:  0.8231422571849438\n",
      "we F1:  0.8205179715250026\n",
      "[[143  22]\n",
      " [551 268]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.87      0.33       165\n",
      "           1       0.92      0.33      0.48       819\n",
      "\n",
      "    accuracy                           0.42       984\n",
      "   macro avg       0.57      0.60      0.41       984\n",
      "weighted avg       0.80      0.42      0.46       984\n",
      "\n",
      "acc:  0.4176829268292683\n",
      "pre:  0.9241379310344827\n",
      "rec:  0.32722832722832723\n",
      "ma F1:  0.4081317949972235\n",
      "mi F1:  0.4176829268292683\n",
      "we F1:  0.45810331674248195\n",
      "Subject 9 Current Train Acc:  0.8231422571849437 Current Test Acc:  0.4176829268292683\n",
      "Loss:  0.08981207013130188\n",
      "Loss:  0.1035180613398552\n",
      "Loss:  0.07732059061527252\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.08450931310653687\n",
      "Loss:  0.08065986633300781\n",
      "Loss:  0.11420665681362152\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.08471492677927017\n",
      "Loss:  0.09635797888040543\n",
      "Loss:  0.08000709861516953\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.01374506950378418\n",
      "Eval Loss:  0.08986413478851318\n",
      "Eval Loss:  0.010858535766601562\n",
      "[[18990  1009]\n",
      " [ 3892  8016]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.89     19999\n",
      "           1       0.89      0.67      0.77     11908\n",
      "\n",
      "    accuracy                           0.85     31907\n",
      "   macro avg       0.86      0.81      0.83     31907\n",
      "weighted avg       0.85      0.85      0.84     31907\n",
      "\n",
      "acc:  0.8463973422759896\n",
      "pre:  0.8881994459833795\n",
      "rec:  0.6731609002351361\n",
      "ma F1:  0.8257895122437321\n",
      "mi F1:  0.8463973422759896\n",
      "we F1:  0.8409834208268372\n",
      "[[158   7]\n",
      " [536 283]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.96      0.37       165\n",
      "           1       0.98      0.35      0.51       819\n",
      "\n",
      "    accuracy                           0.45       984\n",
      "   macro avg       0.60      0.65      0.44       984\n",
      "weighted avg       0.85      0.45      0.49       984\n",
      "\n",
      "acc:  0.4481707317073171\n",
      "pre:  0.9758620689655172\n",
      "rec:  0.34554334554334554\n",
      "ma F1:  0.4391196591334945\n",
      "mi F1:  0.4481707317073171\n",
      "we F1:  0.4864748708397342\n",
      "Subject 9 Current Train Acc:  0.8463973422759896 Current Test Acc:  0.4481707317073171\n",
      "Loss:  0.06855909526348114\n",
      "Loss:  0.12304654717445374\n",
      "Loss:  0.08802365511655807\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.10883377492427826\n",
      "Loss:  0.08912137895822525\n",
      "Loss:  0.0705103948712349\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.06402665376663208\n",
      "Loss:  0.1109333410859108\n",
      "Loss:  0.07171151041984558\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.009021997451782227\n",
      "Eval Loss:  0.06427443027496338\n",
      "Eval Loss:  0.007881879806518555\n",
      "[[19344   655]\n",
      " [ 3901  8007]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.89     19999\n",
      "           1       0.92      0.67      0.78     11908\n",
      "\n",
      "    accuracy                           0.86     31907\n",
      "   macro avg       0.88      0.82      0.84     31907\n",
      "weighted avg       0.87      0.86      0.85     31907\n",
      "\n",
      "acc:  0.8572100166107751\n",
      "pre:  0.9243823597321634\n",
      "rec:  0.6724051058112194\n",
      "ma F1:  0.8365783702090999\n",
      "mi F1:  0.8572100166107751\n",
      "we F1:  0.8513027815203774\n",
      "[[162   3]\n",
      " [590 229]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.98      0.35       165\n",
      "           1       0.99      0.28      0.44       819\n",
      "\n",
      "    accuracy                           0.40       984\n",
      "   macro avg       0.60      0.63      0.39       984\n",
      "weighted avg       0.86      0.40      0.42       984\n",
      "\n",
      "acc:  0.39735772357723576\n",
      "pre:  0.9870689655172413\n",
      "rec:  0.2796092796092796\n",
      "ma F1:  0.3945507576001253\n",
      "mi F1:  0.3973577235772357\n",
      "we F1:  0.4219500971378898\n",
      "Loss:  0.09570460021495819\n",
      "Loss:  0.09092836827039719\n",
      "Loss:  0.07964721322059631\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.05740354582667351\n",
      "Loss:  0.06151587888598442\n",
      "Loss:  0.05749732255935669\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.08888393640518188\n",
      "Loss:  0.06227078661322594\n",
      "Loss:  0.043690335005521774\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.007299661636352539\n",
      "Eval Loss:  0.05430591106414795\n",
      "Eval Loss:  0.006311655044555664\n",
      "[[19372   627]\n",
      " [ 3547  8361]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.90     19999\n",
      "           1       0.93      0.70      0.80     11908\n",
      "\n",
      "    accuracy                           0.87     31907\n",
      "   macro avg       0.89      0.84      0.85     31907\n",
      "weighted avg       0.88      0.87      0.86     31907\n",
      "\n",
      "acc:  0.86918231109161\n",
      "pre:  0.9302403204272364\n",
      "rec:  0.7021330198186093\n",
      "ma F1:  0.8514968102746883\n",
      "mi F1:  0.86918231109161\n",
      "we F1:  0.864492304517692\n",
      "[[161   4]\n",
      " [628 191]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.98      0.34       165\n",
      "           1       0.98      0.23      0.38       819\n",
      "\n",
      "    accuracy                           0.36       984\n",
      "   macro avg       0.59      0.60      0.36       984\n",
      "weighted avg       0.85      0.36      0.37       984\n",
      "\n",
      "acc:  0.35772357723577236\n",
      "pre:  0.9794871794871794\n",
      "rec:  0.23321123321123322\n",
      "ma F1:  0.35712602185751674\n",
      "mi F1:  0.35772357723577236\n",
      "we F1:  0.3701527291034884\n",
      "Loss:  0.09817912429571152\n",
      "Loss:  0.06842952966690063\n",
      "Loss:  0.07879488915205002\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.07522807270288467\n",
      "Loss:  0.06749773770570755\n",
      "Loss:  0.07606575638055801\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.11204872280359268\n",
      "Loss:  0.06469046324491501\n",
      "Loss:  0.06671106070280075\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.005524873733520508\n",
      "Eval Loss:  0.0519864559173584\n",
      "Eval Loss:  0.0051805973052978516\n",
      "[[19481   518]\n",
      " [ 3609  8299]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.97      0.90     19999\n",
      "           1       0.94      0.70      0.80     11908\n",
      "\n",
      "    accuracy                           0.87     31907\n",
      "   macro avg       0.89      0.84      0.85     31907\n",
      "weighted avg       0.88      0.87      0.87     31907\n",
      "\n",
      "acc:  0.870655342087943\n",
      "pre:  0.9412498582284223\n",
      "rec:  0.6969264360094054\n",
      "ma F1:  0.8525450056649098\n",
      "mi F1:  0.870655342087943\n",
      "we F1:  0.8656491669955092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[161   4]\n",
      " [619 200]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.98      0.34       165\n",
      "           1       0.98      0.24      0.39       819\n",
      "\n",
      "    accuracy                           0.37       984\n",
      "   macro avg       0.59      0.61      0.37       984\n",
      "weighted avg       0.85      0.37      0.38       984\n",
      "\n",
      "acc:  0.366869918699187\n",
      "pre:  0.9803921568627451\n",
      "rec:  0.2442002442002442\n",
      "ma F1:  0.36587379168024325\n",
      "mi F1:  0.36686991869918706\n",
      "we F1:  0.3825780755363761\n",
      "Loss:  0.0693397969007492\n",
      "Loss:  0.0795348584651947\n",
      "Loss:  0.051828376948833466\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.05455678328871727\n",
      "Loss:  0.08233578503131866\n",
      "Loss:  0.1057010069489479\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.10753563046455383\n",
      "Loss:  0.09889133274555206\n",
      "Loss:  0.06883988529443741\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.00409698486328125\n",
      "Eval Loss:  0.023157358169555664\n",
      "Eval Loss:  0.004034757614135742\n",
      "[[19263   736]\n",
      " [ 2777  9131]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.92     19999\n",
      "           1       0.93      0.77      0.84     11908\n",
      "\n",
      "    accuracy                           0.89     31907\n",
      "   macro avg       0.90      0.86      0.88     31907\n",
      "weighted avg       0.89      0.89      0.89     31907\n",
      "\n",
      "acc:  0.8898987682953584\n",
      "pre:  0.9254079254079254\n",
      "rec:  0.7667954316425932\n",
      "ma F1:  0.8775514683224688\n",
      "mi F1:  0.8898987682953584\n",
      "we F1:  0.887411516099872\n",
      "[[153  12]\n",
      " [494 325]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.93      0.38       165\n",
      "           1       0.96      0.40      0.56       819\n",
      "\n",
      "    accuracy                           0.49       984\n",
      "   macro avg       0.60      0.66      0.47       984\n",
      "weighted avg       0.84      0.49      0.53       984\n",
      "\n",
      "acc:  0.48577235772357724\n",
      "pre:  0.9643916913946587\n",
      "rec:  0.3968253968253968\n",
      "ma F1:  0.46956551383230777\n",
      "mi F1:  0.48577235772357724\n",
      "we F1:  0.5311892109537626\n",
      "Subject 9 Current Train Acc:  0.8898987682953584 Current Test Acc:  0.48577235772357724\n",
      "Loss:  0.05186829715967178\n",
      "Loss:  0.07038811594247818\n",
      "Loss:  0.0534868985414505\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.07653158903121948\n",
      "Loss:  0.0660323053598404\n",
      "Loss:  0.05679967999458313\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.04113906994462013\n",
      "Loss:  0.05438678711652756\n",
      "Loss:  0.05814462527632713\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.0031397342681884766\n",
      "Eval Loss:  0.018800020217895508\n",
      "Eval Loss:  0.0029931068420410156\n",
      "[[19259   740]\n",
      " [ 2654  9254]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     19999\n",
      "           1       0.93      0.78      0.85     11908\n",
      "\n",
      "    accuracy                           0.89     31907\n",
      "   macro avg       0.90      0.87      0.88     31907\n",
      "weighted avg       0.90      0.89      0.89     31907\n",
      "\n",
      "acc:  0.8936283574137337\n",
      "pre:  0.9259555733440064\n",
      "rec:  0.777124622102788\n",
      "ma F1:  0.8820288942105832\n",
      "mi F1:  0.8936283574137336\n",
      "we F1:  0.8914093296705222\n",
      "[[158   7]\n",
      " [547 272]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.96      0.36       165\n",
      "           1       0.97      0.33      0.50       819\n",
      "\n",
      "    accuracy                           0.44       984\n",
      "   macro avg       0.60      0.64      0.43       984\n",
      "weighted avg       0.85      0.44      0.47       984\n",
      "\n",
      "acc:  0.4369918699186992\n",
      "pre:  0.974910394265233\n",
      "rec:  0.3321123321123321\n",
      "ma F1:  0.4293323283713334\n",
      "mi F1:  0.4369918699186992\n",
      "we F1:  0.4732739088272736\n",
      "Loss:  0.041302796453237534\n",
      "Loss:  0.0803501158952713\n",
      "Loss:  0.037866927683353424\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.07444965094327927\n",
      "Loss:  0.05019694194197655\n",
      "Loss:  0.07103978097438812\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.09172257035970688\n",
      "Loss:  0.048948053270578384\n",
      "Loss:  0.048327989876270294\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.0029523372650146484\n",
      "Eval Loss:  0.02322995662689209\n",
      "Eval Loss:  0.0027403831481933594\n",
      "[[19364   635]\n",
      " [ 2691  9217]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92     19999\n",
      "           1       0.94      0.77      0.85     11908\n",
      "\n",
      "    accuracy                           0.90     31907\n",
      "   macro avg       0.91      0.87      0.88     31907\n",
      "weighted avg       0.90      0.90      0.89     31907\n",
      "\n",
      "acc:  0.8957595511956624\n",
      "pre:  0.9355460820138043\n",
      "rec:  0.7740174672489083\n",
      "ma F1:  0.8840309723457795\n",
      "mi F1:  0.8957595511956624\n",
      "we F1:  0.8933830893729208\n",
      "[[159   6]\n",
      " [532 287]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.96      0.37       165\n",
      "           1       0.98      0.35      0.52       819\n",
      "\n",
      "    accuracy                           0.45       984\n",
      "   macro avg       0.60      0.66      0.44       984\n",
      "weighted avg       0.85      0.45      0.49       984\n",
      "\n",
      "acc:  0.4532520325203252\n",
      "pre:  0.9795221843003413\n",
      "rec:  0.3504273504273504\n",
      "ma F1:  0.443841188731258\n",
      "mi F1:  0.4532520325203252\n",
      "we F1:  0.49192471871602333\n",
      "Loss:  0.055928200483322144\n",
      "Loss:  0.093915194272995\n",
      "Loss:  0.08376997709274292\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.04454085975885391\n",
      "Loss:  0.09062230587005615\n",
      "Loss:  0.0795179083943367\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.06558690220117569\n",
      "Loss:  0.05226771533489227\n",
      "Loss:  0.06406752020120621\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.0024077892303466797\n",
      "Eval Loss:  0.022919774055480957\n",
      "Eval Loss:  0.002003192901611328\n",
      "[[19215   784]\n",
      " [ 2249  9659]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     19999\n",
      "           1       0.92      0.81      0.86     11908\n",
      "\n",
      "    accuracy                           0.90     31907\n",
      "   macro avg       0.91      0.89      0.90     31907\n",
      "weighted avg       0.91      0.90      0.90     31907\n",
      "\n",
      "acc:  0.9049424891089729\n",
      "pre:  0.9249257876089246\n",
      "rec:  0.8111353711790393\n",
      "ma F1:  0.8955759092576859\n",
      "mi F1:  0.9049424891089729\n",
      "we F1:  0.9035065285101728\n",
      "[[157   8]\n",
      " [515 304]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.95      0.38       165\n",
      "           1       0.97      0.37      0.54       819\n",
      "\n",
      "    accuracy                           0.47       984\n",
      "   macro avg       0.60      0.66      0.46       984\n",
      "weighted avg       0.85      0.47      0.51       984\n",
      "\n",
      "acc:  0.4684959349593496\n",
      "pre:  0.9743589743589743\n",
      "rec:  0.3711843711843712\n",
      "ma F1:  0.4563633540274252\n",
      "mi F1:  0.4684959349593496\n",
      "we F1:  0.5103409589898642\n",
      "Loss:  0.06992804259061813\n",
      "Loss:  0.04315018281340599\n",
      "Loss:  0.05035921186208725\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.059189170598983765\n",
      "Loss:  0.06751345843076706\n",
      "Loss:  0.07029572129249573\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.08812224119901657\n",
      "Loss:  0.08516012132167816\n",
      "Loss:  0.04541299119591713\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.0024356842041015625\n",
      "Eval Loss:  0.012481689453125\n",
      "Eval Loss:  0.0026073455810546875\n",
      "[[19131   868]\n",
      " [ 2118  9790]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     19999\n",
      "           1       0.92      0.82      0.87     11908\n",
      "\n",
      "    accuracy                           0.91     31907\n",
      "   macro avg       0.91      0.89      0.90     31907\n",
      "weighted avg       0.91      0.91      0.91     31907\n",
      "\n",
      "acc:  0.906415520105306\n",
      "pre:  0.918558829048602\n",
      "rec:  0.8221363789049378\n",
      "ma F1:  0.897642823787917\n",
      "mi F1:  0.906415520105306\n",
      "we F1:  0.905241569736316\n",
      "[[156   9]\n",
      " [519 300]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.95      0.37       165\n",
      "           1       0.97      0.37      0.53       819\n",
      "\n",
      "    accuracy                           0.46       984\n",
      "   macro avg       0.60      0.66      0.45       984\n",
      "weighted avg       0.85      0.46      0.51       984\n",
      "\n",
      "acc:  0.4634146341463415\n",
      "pre:  0.970873786407767\n",
      "rec:  0.3663003663003663\n",
      "ma F1:  0.45167173252279635\n",
      "mi F1:  0.4634146341463415\n",
      "we F1:  0.5050040773963971\n",
      "Loss:  0.04965697228908539\n",
      "Loss:  0.07595037668943405\n",
      "Loss:  0.0364106260240078\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.05949477106332779\n",
      "Loss:  0.089170902967453\n",
      "Loss:  0.04437096416950226\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.051023781299591064\n",
      "Loss:  0.053397275507450104\n",
      "Loss:  0.037055604159832\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.002210378646850586\n",
      "Eval Loss:  0.017316579818725586\n",
      "Eval Loss:  0.0021414756774902344\n",
      "[[18934  1065]\n",
      " [ 1601 10307]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     19999\n",
      "           1       0.91      0.87      0.89     11908\n",
      "\n",
      "    accuracy                           0.92     31907\n",
      "   macro avg       0.91      0.91      0.91     31907\n",
      "weighted avg       0.92      0.92      0.92     31907\n",
      "\n",
      "acc:  0.9164446673143825\n",
      "pre:  0.9063489271895885\n",
      "rec:  0.8655525697010413\n",
      "ma F1:  0.9098545775579858\n",
      "mi F1:  0.9164446673143826\n",
      "we F1:  0.9160352216079458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[153  12]\n",
      " [463 356]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.93      0.39       165\n",
      "           1       0.97      0.43      0.60       819\n",
      "\n",
      "    accuracy                           0.52       984\n",
      "   macro avg       0.61      0.68      0.50       984\n",
      "weighted avg       0.85      0.52      0.56       984\n",
      "\n",
      "acc:  0.5172764227642277\n",
      "pre:  0.967391304347826\n",
      "rec:  0.4346764346764347\n",
      "ma F1:  0.4958184428621203\n",
      "mi F1:  0.5172764227642277\n",
      "we F1:  0.5649490776206337\n",
      "Subject 9 Current Train Acc:  0.9164446673143825 Current Test Acc:  0.5172764227642277\n",
      "Loss:  0.034010253846645355\n",
      "Loss:  0.0519820973277092\n",
      "Loss:  0.08139543235301971\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.07472902536392212\n",
      "Loss:  0.045469753444194794\n",
      "Loss:  0.06966609507799149\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.031166478991508484\n",
      "Loss:  0.08083006739616394\n",
      "Loss:  0.07290340214967728\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.0016453266143798828\n",
      "Eval Loss:  0.011515378952026367\n",
      "Eval Loss:  0.001644134521484375\n",
      "[[19052   947]\n",
      " [ 1783 10125]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19999\n",
      "           1       0.91      0.85      0.88     11908\n",
      "\n",
      "    accuracy                           0.91     31907\n",
      "   macro avg       0.91      0.90      0.91     31907\n",
      "weighted avg       0.91      0.91      0.91     31907\n",
      "\n",
      "acc:  0.9144388378725672\n",
      "pre:  0.9144689306358381\n",
      "rec:  0.8502687269062815\n",
      "ma F1:  0.9071724965283243\n",
      "mi F1:  0.9144388378725672\n",
      "we F1:  0.9137583560350196\n",
      "[[154  11]\n",
      " [455 364]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.93      0.40       165\n",
      "           1       0.97      0.44      0.61       819\n",
      "\n",
      "    accuracy                           0.53       984\n",
      "   macro avg       0.61      0.69      0.50       984\n",
      "weighted avg       0.85      0.53      0.57       984\n",
      "\n",
      "acc:  0.5264227642276422\n",
      "pre:  0.9706666666666667\n",
      "rec:  0.4444444444444444\n",
      "ma F1:  0.5038240297092699\n",
      "mi F1:  0.5264227642276422\n",
      "we F1:  0.5742029457807725\n",
      "Subject 9 Current Train Acc:  0.9144388378725672 Current Test Acc:  0.5264227642276422\n",
      "Loss:  0.04940725117921829\n",
      "Loss:  0.04273860156536102\n",
      "Loss:  0.03261305019259453\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.047492727637290955\n",
      "Loss:  0.03866029530763626\n",
      "Loss:  0.05631440877914429\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.04318304359912872\n",
      "Loss:  0.08460799604654312\n",
      "Loss:  0.04921083152294159\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.0018417835235595703\n",
      "Eval Loss:  0.011388540267944336\n",
      "Eval Loss:  0.0017900466918945312\n",
      "[[19243   756]\n",
      " [ 1962  9946]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93     19999\n",
      "           1       0.93      0.84      0.88     11908\n",
      "\n",
      "    accuracy                           0.91     31907\n",
      "   macro avg       0.92      0.90      0.91     31907\n",
      "weighted avg       0.92      0.91      0.91     31907\n",
      "\n",
      "acc:  0.9148149308929076\n",
      "pre:  0.9293589983180713\n",
      "rec:  0.8352368155861606\n",
      "ma F1:  0.9069116175432614\n",
      "mi F1:  0.9148149308929076\n",
      "we F1:  0.9137897188998266\n",
      "[[156   9]\n",
      " [496 323]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.95      0.38       165\n",
      "           1       0.97      0.39      0.56       819\n",
      "\n",
      "    accuracy                           0.49       984\n",
      "   macro avg       0.61      0.67      0.47       984\n",
      "weighted avg       0.85      0.49      0.53       984\n",
      "\n",
      "acc:  0.4867886178861789\n",
      "pre:  0.9728915662650602\n",
      "rec:  0.39438339438339437\n",
      "ma F1:  0.4715680154663019\n",
      "mi F1:  0.4867886178861789\n",
      "we F1:  0.5311744464998317\n",
      "Loss:  0.03713033348321915\n",
      "Loss:  0.052263293415308\n",
      "Loss:  0.04387451335787773\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.03426100313663483\n",
      "Loss:  0.056524209678173065\n",
      "Loss:  0.04841047525405884\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.10145119577646255\n",
      "Loss:  0.05309673026204109\n",
      "Loss:  0.06187698245048523\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.0017459392547607422\n",
      "Eval Loss:  0.013208866119384766\n",
      "Eval Loss:  0.0016100406646728516\n",
      "[[19105   894]\n",
      " [ 1994  9914]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93     19999\n",
      "           1       0.92      0.83      0.87     11908\n",
      "\n",
      "    accuracy                           0.91     31907\n",
      "   macro avg       0.91      0.89      0.90     31907\n",
      "weighted avg       0.91      0.91      0.91     31907\n",
      "\n",
      "acc:  0.9094869464380857\n",
      "pre:  0.9172834937083642\n",
      "rec:  0.8325495465233457\n",
      "ma F1:  0.9012969407958957\n",
      "mi F1:  0.9094869464380857\n",
      "we F1:  0.9085067477430133\n",
      "[[154  11]\n",
      " [476 343]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.93      0.39       165\n",
      "           1       0.97      0.42      0.58       819\n",
      "\n",
      "    accuracy                           0.51       984\n",
      "   macro avg       0.61      0.68      0.49       984\n",
      "weighted avg       0.85      0.51      0.55       984\n",
      "\n",
      "acc:  0.5050813008130082\n",
      "pre:  0.9689265536723164\n",
      "rec:  0.4188034188034188\n",
      "ma F1:  0.48612330904470075\n",
      "mi F1:  0.5050813008130082\n",
      "we F1:  0.5517239789731295\n",
      "Loss:  0.09134779870510101\n",
      "Loss:  0.04637540131807327\n",
      "Loss:  0.03401876240968704\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.06907037645578384\n",
      "Loss:  0.06015149876475334\n",
      "Loss:  0.08291442692279816\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.06743907928466797\n",
      "Loss:  0.05843915045261383\n",
      "Loss:  0.05522863194346428\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.0014996528625488281\n",
      "Eval Loss:  0.011534929275512695\n",
      "Eval Loss:  0.0014295578002929688\n",
      "[[19095   904]\n",
      " [ 1777 10131]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19999\n",
      "           1       0.92      0.85      0.88     11908\n",
      "\n",
      "    accuracy                           0.92     31907\n",
      "   macro avg       0.92      0.90      0.91     31907\n",
      "weighted avg       0.92      0.92      0.92     31907\n",
      "\n",
      "acc:  0.915974551038957\n",
      "pre:  0.9180788400543725\n",
      "rec:  0.8507725898555593\n",
      "ma F1:  0.9087742762928992\n",
      "mi F1:  0.915974551038957\n",
      "we F1:  0.9152733194622826\n",
      "[[158   7]\n",
      " [555 264]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.96      0.36       165\n",
      "           1       0.97      0.32      0.48       819\n",
      "\n",
      "    accuracy                           0.43       984\n",
      "   macro avg       0.60      0.64      0.42       984\n",
      "weighted avg       0.85      0.43      0.46       984\n",
      "\n",
      "acc:  0.42886178861788615\n",
      "pre:  0.974169741697417\n",
      "rec:  0.32234432234432236\n",
      "ma F1:  0.422156276775825\n",
      "mi F1:  0.42886178861788615\n",
      "we F1:  0.4635280196504291\n",
      "Loss:  0.02504030242562294\n",
      "Loss:  0.04921818524599075\n",
      "Loss:  0.0518370196223259\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.056925252079963684\n",
      "Loss:  0.04440021514892578\n",
      "Loss:  0.08210419118404388\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.05216064676642418\n",
      "Loss:  0.04331129789352417\n",
      "Loss:  0.0745626837015152\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.0017642974853515625\n",
      "Eval Loss:  0.007596492767333984\n",
      "Eval Loss:  0.001741170883178711\n",
      "[[19138   861]\n",
      " [ 2002  9906]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93     19999\n",
      "           1       0.92      0.83      0.87     11908\n",
      "\n",
      "    accuracy                           0.91     31907\n",
      "   macro avg       0.91      0.89      0.90     31907\n",
      "weighted avg       0.91      0.91      0.91     31907\n",
      "\n",
      "acc:  0.9102704735637948\n",
      "pre:  0.9200334354973531\n",
      "rec:  0.8318777292576419\n",
      "ma F1:  0.9020721332708244\n",
      "mi F1:  0.9102704735637948\n",
      "we F1:  0.9092572254838251\n",
      "[[156   9]\n",
      " [424 395]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.95      0.42       165\n",
      "           1       0.98      0.48      0.65       819\n",
      "\n",
      "    accuracy                           0.56       984\n",
      "   macro avg       0.62      0.71      0.53       984\n",
      "weighted avg       0.86      0.56      0.61       984\n",
      "\n",
      "acc:  0.5599593495934959\n",
      "pre:  0.9777227722772277\n",
      "rec:  0.4822954822954823\n",
      "ma F1:  0.5323722609712062\n",
      "mi F1:  0.5599593495934959\n",
      "we F1:  0.6078616164480992\n",
      "Subject 9 Current Train Acc:  0.9102704735637948 Current Test Acc:  0.5599593495934959\n",
      "Loss:  0.052241601049900055\n",
      "Loss:  0.041557785123586655\n",
      "Loss:  0.0653478279709816\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.042480453848838806\n",
      "Loss:  0.05532015115022659\n",
      "Loss:  0.06166095286607742\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.0742468535900116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.05413363128900528\n",
      "Loss:  0.04737109690904617\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.0016667842864990234\n",
      "Eval Loss:  0.01122426986694336\n",
      "Eval Loss:  0.001590728759765625\n",
      "[[19186   813]\n",
      " [ 1781 10127]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94     19999\n",
      "           1       0.93      0.85      0.89     11908\n",
      "\n",
      "    accuracy                           0.92     31907\n",
      "   macro avg       0.92      0.90      0.91     31907\n",
      "weighted avg       0.92      0.92      0.92     31907\n",
      "\n",
      "acc:  0.9187012254364246\n",
      "pre:  0.9256855575868373\n",
      "rec:  0.8504366812227074\n",
      "ma F1:  0.9115731421089649\n",
      "mi F1:  0.9187012254364246\n",
      "we F1:  0.9179395536557666\n",
      "[[156   9]\n",
      " [489 330]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.95      0.39       165\n",
      "           1       0.97      0.40      0.57       819\n",
      "\n",
      "    accuracy                           0.49       984\n",
      "   macro avg       0.61      0.67      0.48       984\n",
      "weighted avg       0.85      0.49      0.54       984\n",
      "\n",
      "acc:  0.49390243902439024\n",
      "pre:  0.9734513274336283\n",
      "rec:  0.40293040293040294\n",
      "ma F1:  0.47756668585684126\n",
      "mi F1:  0.49390243902439024\n",
      "we F1:  0.5389665856934907\n",
      "Loss:  0.05210309103131294\n",
      "Loss:  0.039084918797016144\n",
      "Loss:  0.07448097318410873\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.07691086828708649\n",
      "Loss:  0.03597109764814377\n",
      "Loss:  0.06362715363502502\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.049040354788303375\n",
      "Loss:  0.05377431586384773\n",
      "Loss:  0.03153466805815697\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.0011951923370361328\n",
      "Eval Loss:  0.017033100128173828\n",
      "Eval Loss:  0.0011188983917236328\n",
      "[[18889  1110]\n",
      " [ 1413 10495]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     19999\n",
      "           1       0.90      0.88      0.89     11908\n",
      "\n",
      "    accuracy                           0.92     31907\n",
      "   macro avg       0.92      0.91      0.92     31907\n",
      "weighted avg       0.92      0.92      0.92     31907\n",
      "\n",
      "acc:  0.9209264424734385\n",
      "pre:  0.9043515725980181\n",
      "rec:  0.881340275445079\n",
      "ma F1:  0.9150468755026036\n",
      "mi F1:  0.9209264424734385\n",
      "we F1:  0.9207142064962927\n",
      "[[156   9]\n",
      " [526 293]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.95      0.37       165\n",
      "           1       0.97      0.36      0.52       819\n",
      "\n",
      "    accuracy                           0.46       984\n",
      "   macro avg       0.60      0.65      0.45       984\n",
      "weighted avg       0.85      0.46      0.50       984\n",
      "\n",
      "acc:  0.4563008130081301\n",
      "pre:  0.9701986754966887\n",
      "rec:  0.35775335775335776\n",
      "ma F1:  0.44555323032332195\n",
      "mi F1:  0.4563008130081301\n",
      "we F1:  0.4968592089792673\n",
      "Loss:  0.04687564820051193\n",
      "Loss:  0.04618453234434128\n",
      "Loss:  0.05149538069963455\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.045766640454530716\n",
      "Loss:  0.06507374346256256\n",
      "Loss:  0.07772806286811829\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.04272890463471413\n",
      "Loss:  0.047456759959459305\n",
      "Loss:  0.03280191868543625\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.0014998912811279297\n",
      "Eval Loss:  0.017047643661499023\n",
      "Eval Loss:  0.0014617443084716797\n",
      "[[18912  1087]\n",
      " [ 1231 10677]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94     19999\n",
      "           1       0.91      0.90      0.90     11908\n",
      "\n",
      "    accuracy                           0.93     31907\n",
      "   macro avg       0.92      0.92      0.92     31907\n",
      "weighted avg       0.93      0.93      0.93     31907\n",
      "\n",
      "acc:  0.927351364904253\n",
      "pre:  0.907599455967358\n",
      "rec:  0.8966241182398388\n",
      "ma F1:  0.9221666998175403\n",
      "mi F1:  0.927351364904253\n",
      "we F1:  0.9272607040939935\n",
      "[[153  12]\n",
      " [460 359]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.93      0.39       165\n",
      "           1       0.97      0.44      0.60       819\n",
      "\n",
      "    accuracy                           0.52       984\n",
      "   macro avg       0.61      0.68      0.50       984\n",
      "weighted avg       0.85      0.52      0.57       984\n",
      "\n",
      "acc:  0.5203252032520326\n",
      "pre:  0.967654986522911\n",
      "rec:  0.43833943833943834\n",
      "ma F1:  0.4983387699552828\n",
      "mi F1:  0.5203252032520326\n",
      "we F1:  0.5681403591595268\n",
      "Loss:  0.04136073589324951\n",
      "Loss:  0.05503304302692413\n",
      "Loss:  0.06874891370534897\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.07364839315414429\n",
      "Loss:  0.0404997318983078\n",
      "Loss:  0.045478902757167816\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.03179097920656204\n",
      "Loss:  0.04603762924671173\n",
      "Loss:  0.0468815341591835\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  9 Training Time 5791.822843074799 Best Test Acc:  0.5599593495934959\n",
      "test subjects:  ['./seg\\\\a11']\n",
      "*********\n",
      "33847 466\n",
      "32425 466\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.6226849555969238\n",
      "Eval Loss:  0.7825818061828613\n",
      "Eval Loss:  0.6049126982688904\n",
      "[[   16 19904]\n",
      " [   48 12457]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.00      0.00     19920\n",
      "           1       0.38      1.00      0.56     12505\n",
      "\n",
      "    accuracy                           0.38     32425\n",
      "   macro avg       0.32      0.50      0.28     32425\n",
      "weighted avg       0.30      0.38      0.22     32425\n",
      "\n",
      "acc:  0.3846723207401696\n",
      "pre:  0.38493866073359906\n",
      "rec:  0.9961615353858456\n",
      "ma F1:  0.27844963975459774\n",
      "mi F1:  0.3846723207401696\n",
      "we F1:  0.21513952163643885\n",
      "[[  0 244]\n",
      " [  0 222]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       244\n",
      "           1       0.48      1.00      0.65       222\n",
      "\n",
      "    accuracy                           0.48       466\n",
      "   macro avg       0.24      0.50      0.32       466\n",
      "weighted avg       0.23      0.48      0.31       466\n",
      "\n",
      "acc:  0.47639484978540775\n",
      "pre:  0.47639484978540775\n",
      "rec:  1.0\n",
      "ma F1:  0.3226744186046512\n",
      "mi F1:  0.47639484978540775\n",
      "we F1:  0.30744086236151313\n",
      "Subject 10 Current Train Acc:  0.3846723207401696 Current Test Acc:  0.47639484978540775\n",
      "Loss:  0.16871829330921173\n",
      "Loss:  0.1673286259174347\n",
      "Loss:  0.15274028480052948\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.12711457908153534\n",
      "Loss:  0.1349540799856186\n",
      "Loss:  0.13202518224716187\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.10174554586410522\n",
      "Loss:  0.09664792567491531\n",
      "Loss:  0.09678724408149719\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.42643803358078003\n",
      "Eval Loss:  0.04100501537322998\n",
      "Eval Loss:  2.608961582183838\n",
      "[[17307  2613]\n",
      " [ 3053  9452]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86     19920\n",
      "           1       0.78      0.76      0.77     12505\n",
      "\n",
      "    accuracy                           0.83     32425\n",
      "   macro avg       0.82      0.81      0.81     32425\n",
      "weighted avg       0.82      0.83      0.82     32425\n",
      "\n",
      "acc:  0.8252582883577486\n",
      "pre:  0.7834231247409863\n",
      "rec:  0.7558576569372251\n",
      "ma F1:  0.814364113395891\n",
      "mi F1:  0.8252582883577486\n",
      "we F1:  0.8246480481307318\n",
      "[[228  16]\n",
      " [155  67]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.93      0.73       244\n",
      "           1       0.81      0.30      0.44       222\n",
      "\n",
      "    accuracy                           0.63       466\n",
      "   macro avg       0.70      0.62      0.58       466\n",
      "weighted avg       0.70      0.63      0.59       466\n",
      "\n",
      "acc:  0.6330472103004292\n",
      "pre:  0.8072289156626506\n",
      "rec:  0.30180180180180183\n",
      "ma F1:  0.5833084947839047\n",
      "mi F1:  0.6330472103004292\n",
      "we F1:  0.590105089450759\n",
      "Subject 10 Current Train Acc:  0.8252582883577486 Current Test Acc:  0.6330472103004292\n",
      "Loss:  0.09499979019165039\n",
      "Loss:  0.1207275316119194\n",
      "Loss:  0.0827064961194992\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.07848192751407623\n",
      "Loss:  0.05517669394612312\n",
      "Loss:  0.08343692123889923\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.08147355914115906\n",
      "Loss:  0.09950591623783112\n",
      "Loss:  0.10659606754779816\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.16106760501861572\n",
      "Eval Loss:  0.01524496078491211\n",
      "Eval Loss:  2.5852999687194824\n",
      "[[18128  1792]\n",
      " [ 2593  9912]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.91      0.89     19920\n",
      "           1       0.85      0.79      0.82     12505\n",
      "\n",
      "    accuracy                           0.86     32425\n",
      "   macro avg       0.86      0.85      0.86     32425\n",
      "weighted avg       0.86      0.86      0.86     32425\n",
      "\n",
      "acc:  0.8647648419429452\n",
      "pre:  0.84688995215311\n",
      "rec:  0.7926429428228708\n",
      "ma F1:  0.8554865242642709\n",
      "mi F1:  0.8647648419429452\n",
      "we F1:  0.8638602737271932\n",
      "[[218  26]\n",
      " [135  87]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.89      0.73       244\n",
      "           1       0.77      0.39      0.52       222\n",
      "\n",
      "    accuracy                           0.65       466\n",
      "   macro avg       0.69      0.64      0.62       466\n",
      "weighted avg       0.69      0.65      0.63       466\n",
      "\n",
      "acc:  0.6545064377682404\n",
      "pre:  0.7699115044247787\n",
      "rec:  0.3918918918918919\n",
      "ma F1:  0.6248606215155379\n",
      "mi F1:  0.6545064377682404\n",
      "we F1:  0.629839308214465\n",
      "Subject 10 Current Train Acc:  0.8647648419429452 Current Test Acc:  0.6545064377682404\n",
      "Loss:  0.1080147922039032\n",
      "Loss:  0.061455730348825455\n",
      "Loss:  0.07551068812608719\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.1153925210237503\n",
      "Loss:  0.09224149584770203\n",
      "Loss:  0.06942335516214371\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.09118301421403885\n",
      "Loss:  0.08824867755174637\n",
      "Loss:  0.06255372613668442\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.06860852241516113\n",
      "Eval Loss:  0.01233530044555664\n",
      "Eval Loss:  3.263002872467041\n",
      "[[17618  2302]\n",
      " [ 1835 10670]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.89     19920\n",
      "           1       0.82      0.85      0.84     12505\n",
      "\n",
      "    accuracy                           0.87     32425\n",
      "   macro avg       0.86      0.87      0.87     32425\n",
      "weighted avg       0.87      0.87      0.87     32425\n",
      "\n",
      "acc:  0.8724132613723978\n",
      "pre:  0.822540857230959\n",
      "rec:  0.8532586965213914\n",
      "ma F1:  0.86627312012447\n",
      "mi F1:  0.8724132613723979\n",
      "we F1:  0.8728259622881697\n",
      "[[202  42]\n",
      " [115 107]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.83      0.72       244\n",
      "           1       0.72      0.48      0.58       222\n",
      "\n",
      "    accuracy                           0.66       466\n",
      "   macro avg       0.68      0.65      0.65       466\n",
      "weighted avg       0.68      0.66      0.65       466\n",
      "\n",
      "acc:  0.6630901287553648\n",
      "pre:  0.7181208053691275\n",
      "rec:  0.481981981981982\n",
      "ma F1:  0.648481004751815\n",
      "mi F1:  0.6630901287553648\n",
      "we F1:  0.6518641703105319\n",
      "Subject 10 Current Train Acc:  0.8724132613723978 Current Test Acc:  0.6630901287553648\n",
      "Loss:  0.08181306719779968\n",
      "Loss:  0.04623791575431824\n",
      "Loss:  0.09081906825304031\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.08183157444000244\n",
      "Loss:  0.06176183745265007\n",
      "Loss:  0.10951289534568787\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.052059024572372437\n",
      "Loss:  0.07055114954710007\n",
      "Loss:  0.065958671271801\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.15005254745483398\n",
      "Eval Loss:  0.0065114498138427734\n",
      "Eval Loss:  3.677499294281006\n",
      "[[19010   910]\n",
      " [ 2994  9511]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.91     19920\n",
      "           1       0.91      0.76      0.83     12505\n",
      "\n",
      "    accuracy                           0.88     32425\n",
      "   macro avg       0.89      0.86      0.87     32425\n",
      "weighted avg       0.88      0.88      0.88     32425\n",
      "\n",
      "acc:  0.8795990747879723\n",
      "pre:  0.9126763266481144\n",
      "rec:  0.7605757696921231\n",
      "ma F1:  0.8682960521035381\n",
      "mi F1:  0.8795990747879724\n",
      "we F1:  0.8771192875183269\n",
      "[[231  13]\n",
      " [159  63]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.95      0.73       244\n",
      "           1       0.83      0.28      0.42       222\n",
      "\n",
      "    accuracy                           0.63       466\n",
      "   macro avg       0.71      0.62      0.58       466\n",
      "weighted avg       0.71      0.63      0.58       466\n",
      "\n",
      "acc:  0.630901287553648\n",
      "pre:  0.8289473684210527\n",
      "rec:  0.28378378378378377\n",
      "ma F1:  0.5757627082759935\n",
      "mi F1:  0.630901287553648\n",
      "we F1:  0.5829832365147339\n",
      "Loss:  0.1129460483789444\n",
      "Loss:  0.047758571803569794\n",
      "Loss:  0.05562968552112579\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.07940497994422913\n",
      "Loss:  0.06735832244157791\n",
      "Loss:  0.10273018479347229\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.06375060230493546\n",
      "Loss:  0.08463618159294128\n",
      "Loss:  0.06381052732467651\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.09633421897888184\n",
      "Eval Loss:  0.003968000411987305\n",
      "Eval Loss:  2.996284008026123\n",
      "[[18972   948]\n",
      " [ 2666  9839]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91     19920\n",
      "           1       0.91      0.79      0.84     12505\n",
      "\n",
      "    accuracy                           0.89     32425\n",
      "   macro avg       0.89      0.87      0.88     32425\n",
      "weighted avg       0.89      0.89      0.89     32425\n",
      "\n",
      "acc:  0.8885427910562838\n",
      "pre:  0.9121164364512839\n",
      "rec:  0.7868052778888445\n",
      "ma F1:  0.8789383154337094\n",
      "mi F1:  0.8885427910562838\n",
      "we F1:  0.8867361022224304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[218  26]\n",
      " [139  83]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.89      0.73       244\n",
      "           1       0.76      0.37      0.50       222\n",
      "\n",
      "    accuracy                           0.65       466\n",
      "   macro avg       0.69      0.63      0.61       466\n",
      "weighted avg       0.68      0.65      0.62       466\n",
      "\n",
      "acc:  0.6459227467811158\n",
      "pre:  0.7614678899082569\n",
      "rec:  0.3738738738738739\n",
      "ma F1:  0.6134840723668006\n",
      "mi F1:  0.6459227467811158\n",
      "we F1:  0.6187703748639483\n",
      "Loss:  0.05596291646361351\n",
      "Loss:  0.08929218351840973\n",
      "Loss:  0.0985250174999237\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.08605203032493591\n",
      "Loss:  0.06562570482492447\n",
      "Loss:  0.05724967643618584\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.07344672083854675\n",
      "Loss:  0.04668257758021355\n",
      "Loss:  0.07552367448806763\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.10963302850723267\n",
      "Eval Loss:  0.0038328170776367188\n",
      "Eval Loss:  2.003368854522705\n",
      "[[19047   873]\n",
      " [ 2590  9915]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     19920\n",
      "           1       0.92      0.79      0.85     12505\n",
      "\n",
      "    accuracy                           0.89     32425\n",
      "   macro avg       0.90      0.87      0.88     32425\n",
      "weighted avg       0.90      0.89      0.89     32425\n",
      "\n",
      "acc:  0.8931996915959908\n",
      "pre:  0.9190767519466073\n",
      "rec:  0.7928828468612555\n",
      "ma F1:  0.8839986986563472\n",
      "mi F1:  0.8931996915959909\n",
      "we F1:  0.891469719533204\n",
      "[[228  16]\n",
      " [150  72]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.93      0.73       244\n",
      "           1       0.82      0.32      0.46       222\n",
      "\n",
      "    accuracy                           0.64       466\n",
      "   macro avg       0.71      0.63      0.60       466\n",
      "weighted avg       0.71      0.64      0.61       466\n",
      "\n",
      "acc:  0.6437768240343348\n",
      "pre:  0.8181818181818182\n",
      "rec:  0.32432432432432434\n",
      "ma F1:  0.5988175500466757\n",
      "mi F1:  0.6437768240343348\n",
      "we F1:  0.6051579604808327\n",
      "Loss:  0.03250143304467201\n",
      "Loss:  0.0646471306681633\n",
      "Loss:  0.05483177304267883\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.033472925424575806\n",
      "Loss:  0.03310044854879379\n",
      "Loss:  0.05009300261735916\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.07881888747215271\n",
      "Loss:  0.05626103654503822\n",
      "Loss:  0.03752291575074196\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.195087730884552\n",
      "Eval Loss:  0.0026454925537109375\n",
      "Eval Loss:  2.686554431915283\n",
      "[[19175   745]\n",
      " [ 2693  9812]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     19920\n",
      "           1       0.93      0.78      0.85     12505\n",
      "\n",
      "    accuracy                           0.89     32425\n",
      "   macro avg       0.90      0.87      0.88     32425\n",
      "weighted avg       0.90      0.89      0.89     32425\n",
      "\n",
      "acc:  0.8939707016191211\n",
      "pre:  0.9294307094818604\n",
      "rec:  0.7846461415433826\n",
      "ma F1:  0.8843255872772391\n",
      "mi F1:  0.8939707016191211\n",
      "we F1:  0.8919640068911507\n",
      "[[223  21]\n",
      " [154  68]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.91      0.72       244\n",
      "           1       0.76      0.31      0.44       222\n",
      "\n",
      "    accuracy                           0.62       466\n",
      "   macro avg       0.68      0.61      0.58       466\n",
      "weighted avg       0.67      0.62      0.58       466\n",
      "\n",
      "acc:  0.6244635193133047\n",
      "pre:  0.7640449438202247\n",
      "rec:  0.3063063063063063\n",
      "ma F1:  0.5777477463483335\n",
      "mi F1:  0.6244635193133047\n",
      "we F1:  0.584378372188523\n",
      "Loss:  0.06531362980604172\n",
      "Loss:  0.0879346951842308\n",
      "Loss:  0.06212567165493965\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.03970971703529358\n",
      "Loss:  0.0474395714700222\n",
      "Loss:  0.05397772789001465\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.0745067298412323\n",
      "Loss:  0.06285993754863739\n",
      "Loss:  0.07005933672189713\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.1336606740951538\n",
      "Eval Loss:  0.002876758575439453\n",
      "Eval Loss:  1.5998954772949219\n",
      "[[19234   686]\n",
      " [ 2676  9829]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92     19920\n",
      "           1       0.93      0.79      0.85     12505\n",
      "\n",
      "    accuracy                           0.90     32425\n",
      "   macro avg       0.91      0.88      0.89     32425\n",
      "weighted avg       0.90      0.90      0.89     32425\n",
      "\n",
      "acc:  0.8963145720894372\n",
      "pre:  0.9347598668568712\n",
      "rec:  0.7860055977608956\n",
      "ma F1:  0.8867900730959141\n",
      "mi F1:  0.8963145720894372\n",
      "we F1:  0.8942992873475859\n",
      "[[219  25]\n",
      " [150  72]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.90      0.71       244\n",
      "           1       0.74      0.32      0.45       222\n",
      "\n",
      "    accuracy                           0.62       466\n",
      "   macro avg       0.67      0.61      0.58       466\n",
      "weighted avg       0.66      0.62      0.59       466\n",
      "\n",
      "acc:  0.6244635193133047\n",
      "pre:  0.7422680412371134\n",
      "rec:  0.32432432432432434\n",
      "ma F1:  0.5829647092514842\n",
      "mi F1:  0.6244635193133047\n",
      "we F1:  0.5891754155192397\n",
      "Loss:  0.056034155189991\n",
      "Loss:  0.05839292332530022\n",
      "Loss:  0.07105951756238937\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.08350154012441635\n",
      "Loss:  0.06230276823043823\n",
      "Loss:  0.051671456545591354\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.08350673317909241\n",
      "Loss:  0.04666493833065033\n",
      "Loss:  0.05091531574726105\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.09071493148803711\n",
      "Eval Loss:  0.002607107162475586\n",
      "Eval Loss:  1.3285179138183594\n",
      "[[19061   859]\n",
      " [ 2169 10336]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     19920\n",
      "           1       0.92      0.83      0.87     12505\n",
      "\n",
      "    accuracy                           0.91     32425\n",
      "   macro avg       0.91      0.89      0.90     32425\n",
      "weighted avg       0.91      0.91      0.91     32425\n",
      "\n",
      "acc:  0.906615265998458\n",
      "pre:  0.9232693166592228\n",
      "rec:  0.8265493802479008\n",
      "ma F1:  0.8993259198876191\n",
      "mi F1:  0.906615265998458\n",
      "we F1:  0.905520819762905\n",
      "[[215  29]\n",
      " [135  87]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.88      0.72       244\n",
      "           1       0.75      0.39      0.51       222\n",
      "\n",
      "    accuracy                           0.65       466\n",
      "   macro avg       0.68      0.64      0.62       466\n",
      "weighted avg       0.68      0.65      0.62       466\n",
      "\n",
      "acc:  0.648068669527897\n",
      "pre:  0.75\n",
      "rec:  0.3918918918918919\n",
      "ma F1:  0.6193493116570039\n",
      "mi F1:  0.648068669527897\n",
      "we F1:  0.6242854512910637\n",
      "Loss:  0.08382250368595123\n",
      "Loss:  0.033505894243717194\n",
      "Loss:  0.05073526129126549\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.06429742276668549\n",
      "Loss:  0.061118606477975845\n",
      "Loss:  0.08409073203802109\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.09231583774089813\n",
      "Loss:  0.04729079455137253\n",
      "Loss:  0.08489717543125153\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.08953416347503662\n",
      "Eval Loss:  0.001958608627319336\n",
      "Eval Loss:  1.9713828563690186\n",
      "[[19162   758]\n",
      " [ 2316 10189]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.93     19920\n",
      "           1       0.93      0.81      0.87     12505\n",
      "\n",
      "    accuracy                           0.91     32425\n",
      "   macro avg       0.91      0.89      0.90     32425\n",
      "weighted avg       0.91      0.91      0.90     32425\n",
      "\n",
      "acc:  0.9051966075558983\n",
      "pre:  0.9307572851009409\n",
      "rec:  0.8147940823670532\n",
      "ma F1:  0.8973344821250164\n",
      "mi F1:  0.9051966075558983\n",
      "we F1:  0.9038314909370067\n",
      "[[223  21]\n",
      " [157  65]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.91      0.71       244\n",
      "           1       0.76      0.29      0.42       222\n",
      "\n",
      "    accuracy                           0.62       466\n",
      "   macro avg       0.67      0.60      0.57       466\n",
      "weighted avg       0.67      0.62      0.58       466\n",
      "\n",
      "acc:  0.6180257510729614\n",
      "pre:  0.7558139534883721\n",
      "rec:  0.2927927927927928\n",
      "ma F1:  0.568410755910756\n",
      "mi F1:  0.6180257510729614\n",
      "we F1:  0.5753191729586581\n",
      "Loss:  0.05294721946120262\n",
      "Loss:  0.04066132754087448\n",
      "Loss:  0.043337076902389526\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.06020265072584152\n",
      "Loss:  0.06497430056333542\n",
      "Loss:  0.06927211582660675\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.07249818742275238\n",
      "Loss:  0.04278803989291191\n",
      "Loss:  0.05060835927724838\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.13015216588974\n",
      "Eval Loss:  0.0016551017761230469\n",
      "Eval Loss:  1.8649166822433472\n",
      "[[19199   721]\n",
      " [ 2336 10169]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.93     19920\n",
      "           1       0.93      0.81      0.87     12505\n",
      "\n",
      "    accuracy                           0.91     32425\n",
      "   macro avg       0.91      0.89      0.90     32425\n",
      "weighted avg       0.91      0.91      0.90     32425\n",
      "\n",
      "acc:  0.9057208943716268\n",
      "pre:  0.9337924701561066\n",
      "rec:  0.8131947221111555\n",
      "ma F1:  0.8977942205862932\n",
      "mi F1:  0.9057208943716268\n",
      "we F1:  0.9043032223712598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[225  19]\n",
      " [156  66]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.92      0.72       244\n",
      "           1       0.78      0.30      0.43       222\n",
      "\n",
      "    accuracy                           0.62       466\n",
      "   macro avg       0.68      0.61      0.57       466\n",
      "weighted avg       0.68      0.62      0.58       466\n",
      "\n",
      "acc:  0.6244635193133047\n",
      "pre:  0.7764705882352941\n",
      "rec:  0.2972972972972973\n",
      "ma F1:  0.5749837133550488\n",
      "mi F1:  0.6244635193133047\n",
      "we F1:  0.5818299758146818\n",
      "Loss:  0.040975600481033325\n",
      "Loss:  0.06981456279754639\n",
      "Loss:  0.059003181755542755\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.04610959812998772\n",
      "Loss:  0.036597803235054016\n",
      "Loss:  0.07967790216207504\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.048131927847862244\n",
      "Loss:  0.05030049756169319\n",
      "Loss:  0.046463243663311005\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.08238101005554199\n",
      "Eval Loss:  0.0016813278198242188\n",
      "Eval Loss:  2.8022687435150146\n",
      "[[18971   949]\n",
      " [ 1805 10700]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19920\n",
      "           1       0.92      0.86      0.89     12505\n",
      "\n",
      "    accuracy                           0.92     32425\n",
      "   macro avg       0.92      0.90      0.91     32425\n",
      "weighted avg       0.92      0.92      0.91     32425\n",
      "\n",
      "acc:  0.915065535851966\n",
      "pre:  0.9185337797235814\n",
      "rec:  0.8556577369052379\n",
      "ma F1:  0.9091545597129123\n",
      "mi F1:  0.915065535851966\n",
      "we F1:  0.9144537844827205\n",
      "[[221  23]\n",
      " [130  92]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.91      0.74       244\n",
      "           1       0.80      0.41      0.55       222\n",
      "\n",
      "    accuracy                           0.67       466\n",
      "   macro avg       0.71      0.66      0.64       466\n",
      "weighted avg       0.71      0.67      0.65       466\n",
      "\n",
      "acc:  0.6716738197424893\n",
      "pre:  0.8\n",
      "rec:  0.4144144144144144\n",
      "ma F1:  0.6444256040695209\n",
      "mi F1:  0.6716738197424893\n",
      "we F1:  0.6490725865873914\n",
      "Subject 10 Current Train Acc:  0.915065535851966 Current Test Acc:  0.6716738197424893\n",
      "Loss:  0.05720534548163414\n",
      "Loss:  0.04707667976617813\n",
      "Loss:  0.0387958362698555\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.06848848611116409\n",
      "Loss:  0.04354706034064293\n",
      "Loss:  0.03968150168657303\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.04120268672704697\n",
      "Loss:  0.023474665358662605\n",
      "Loss:  0.06748103350400925\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.05992460250854492\n",
      "Eval Loss:  0.0017621517181396484\n",
      "Eval Loss:  2.738192081451416\n",
      "[[19066   854]\n",
      " [ 1762 10743]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94     19920\n",
      "           1       0.93      0.86      0.89     12505\n",
      "\n",
      "    accuracy                           0.92     32425\n",
      "   macro avg       0.92      0.91      0.91     32425\n",
      "weighted avg       0.92      0.92      0.92     32425\n",
      "\n",
      "acc:  0.9193215111796453\n",
      "pre:  0.9263602655859274\n",
      "rec:  0.8590963614554178\n",
      "ma F1:  0.9136309098034541\n",
      "mi F1:  0.9193215111796453\n",
      "we F1:  0.9187006934396981\n",
      "[[231  13]\n",
      " [175  47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.95      0.71       244\n",
      "           1       0.78      0.21      0.33       222\n",
      "\n",
      "    accuracy                           0.60       466\n",
      "   macro avg       0.68      0.58      0.52       466\n",
      "weighted avg       0.67      0.60      0.53       466\n",
      "\n",
      "acc:  0.5965665236051502\n",
      "pre:  0.7833333333333333\n",
      "rec:  0.21171171171171171\n",
      "ma F1:  0.522051282051282\n",
      "mi F1:  0.5965665236051502\n",
      "we F1:  0.5309607131066358\n",
      "Loss:  0.055038534104824066\n",
      "Loss:  0.09169520437717438\n",
      "Loss:  0.06818316876888275\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.07043760269880295\n",
      "Loss:  0.061055801808834076\n",
      "Loss:  0.11947847902774811\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.02895718440413475\n",
      "Loss:  0.0737377181649208\n",
      "Loss:  0.07099741697311401\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.08241915702819824\n",
      "Eval Loss:  0.0014467239379882812\n",
      "Eval Loss:  3.015255928039551\n",
      "[[18713  1207]\n",
      " [ 1380 11125]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     19920\n",
      "           1       0.90      0.89      0.90     12505\n",
      "\n",
      "    accuracy                           0.92     32425\n",
      "   macro avg       0.92      0.91      0.92     32425\n",
      "weighted avg       0.92      0.92      0.92     32425\n",
      "\n",
      "acc:  0.9202158828064765\n",
      "pre:  0.9021245540058385\n",
      "rec:  0.8896441423430628\n",
      "ma F1:  0.9155934475500882\n",
      "mi F1:  0.9202158828064765\n",
      "we F1:  0.9201104951813638\n",
      "[[218  26]\n",
      " [138  84]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.89      0.73       244\n",
      "           1       0.76      0.38      0.51       222\n",
      "\n",
      "    accuracy                           0.65       466\n",
      "   macro avg       0.69      0.64      0.62       466\n",
      "weighted avg       0.68      0.65      0.62       466\n",
      "\n",
      "acc:  0.648068669527897\n",
      "pre:  0.7636363636363637\n",
      "rec:  0.3783783783783784\n",
      "ma F1:  0.6163453815261044\n",
      "mi F1:  0.648068669527897\n",
      "we F1:  0.621553682541324\n",
      "Loss:  0.03321888670325279\n",
      "Loss:  0.061467766761779785\n",
      "Loss:  0.03827929124236107\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.052153330296278\n",
      "Loss:  0.08150791376829147\n",
      "Loss:  0.05710116773843765\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.03635537251830101\n",
      "Loss:  0.03594692423939705\n",
      "Loss:  0.05257096886634827\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.12478059530258179\n",
      "Eval Loss:  0.0015978813171386719\n",
      "Eval Loss:  3.0493509769439697\n",
      "[[19027   893]\n",
      " [ 1629 10876]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94     19920\n",
      "           1       0.92      0.87      0.90     12505\n",
      "\n",
      "    accuracy                           0.92     32425\n",
      "   macro avg       0.92      0.91      0.92     32425\n",
      "weighted avg       0.92      0.92      0.92     32425\n",
      "\n",
      "acc:  0.9222205088666152\n",
      "pre:  0.924122695216246\n",
      "rec:  0.8697321071571371\n",
      "ma F1:  0.9169739288073931\n",
      "mi F1:  0.9222205088666152\n",
      "we F1:  0.9217467654089305\n",
      "[[228  16]\n",
      " [157  65]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.93      0.72       244\n",
      "           1       0.80      0.29      0.43       222\n",
      "\n",
      "    accuracy                           0.63       466\n",
      "   macro avg       0.70      0.61      0.58       466\n",
      "weighted avg       0.69      0.63      0.58       466\n",
      "\n",
      "acc:  0.628755364806867\n",
      "pre:  0.8024691358024691\n",
      "rec:  0.2927927927927928\n",
      "ma F1:  0.5770015793312241\n",
      "mi F1:  0.628755364806867\n",
      "we F1:  0.5839867528310042\n",
      "Loss:  0.05384911969304085\n",
      "Loss:  0.027613425627350807\n",
      "Loss:  0.03209971636533737\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.05685700848698616\n",
      "Loss:  0.05708802863955498\n",
      "Loss:  0.06461125612258911\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.051666259765625\n",
      "Loss:  0.04307812452316284\n",
      "Loss:  0.08082114160060883\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.05681180953979492\n",
      "Eval Loss:  0.0013210773468017578\n",
      "Eval Loss:  1.1025804281234741\n",
      "[[18750  1170]\n",
      " [ 1248 11257]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94     19920\n",
      "           1       0.91      0.90      0.90     12505\n",
      "\n",
      "    accuracy                           0.93     32425\n",
      "   macro avg       0.92      0.92      0.92     32425\n",
      "weighted avg       0.93      0.93      0.93     32425\n",
      "\n",
      "acc:  0.9254279105628374\n",
      "pre:  0.9058501649633862\n",
      "rec:  0.9001999200319872\n",
      "ma F1:  0.9212210135060526\n",
      "mi F1:  0.9254279105628374\n",
      "we F1:  0.9253841179603511\n",
      "[[226  18]\n",
      " [146  76]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.93      0.73       244\n",
      "           1       0.81      0.34      0.48       222\n",
      "\n",
      "    accuracy                           0.65       466\n",
      "   macro avg       0.71      0.63      0.61       466\n",
      "weighted avg       0.70      0.65      0.61       466\n",
      "\n",
      "acc:  0.648068669527897\n",
      "pre:  0.8085106382978723\n",
      "rec:  0.34234234234234234\n",
      "ma F1:  0.607389445997041\n",
      "mi F1:  0.648068669527897\n",
      "we F1:  0.6133557321148999\n",
      "Loss:  0.07566012442111969\n",
      "Loss:  0.04620552808046341\n",
      "Loss:  0.06333832442760468\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.04741685092449188\n",
      "Loss:  0.03687511384487152\n",
      "Loss:  0.036025695502758026\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.0427086167037487\n",
      "Loss:  0.039548661559820175\n",
      "Loss:  0.052572984248399734\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.044670939445495605\n",
      "Eval Loss:  0.0012695789337158203\n",
      "Eval Loss:  1.1342167854309082\n",
      "[[18735  1185]\n",
      " [ 1250 11255]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94     19920\n",
      "           1       0.90      0.90      0.90     12505\n",
      "\n",
      "    accuracy                           0.92     32425\n",
      "   macro avg       0.92      0.92      0.92     32425\n",
      "weighted avg       0.92      0.92      0.92     32425\n",
      "\n",
      "acc:  0.9249036237471087\n",
      "pre:  0.9047427652733119\n",
      "rec:  0.9000399840063974\n",
      "ma F1:  0.9206826626145495\n",
      "mi F1:  0.9249036237471087\n",
      "we F1:  0.9248669442720262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[226  18]\n",
      " [150  72]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.93      0.73       244\n",
      "           1       0.80      0.32      0.46       222\n",
      "\n",
      "    accuracy                           0.64       466\n",
      "   macro avg       0.70      0.63      0.60       466\n",
      "weighted avg       0.70      0.64      0.60       466\n",
      "\n",
      "acc:  0.6394849785407726\n",
      "pre:  0.8\n",
      "rec:  0.32432432432432434\n",
      "ma F1:  0.5952853598014889\n",
      "mi F1:  0.6394849785407726\n",
      "we F1:  0.601599591049958\n",
      "Loss:  0.025851719081401825\n",
      "Loss:  0.046293750405311584\n",
      "Loss:  0.045942556113004684\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.05819883942604065\n",
      "Loss:  0.04012175276875496\n",
      "Loss:  0.05056732892990112\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.050330743193626404\n",
      "Loss:  0.05895489081740379\n",
      "Loss:  0.061257414519786835\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.10503649711608887\n",
      "Eval Loss:  0.0014934539794921875\n",
      "Eval Loss:  1.9183416366577148\n",
      "[[18884  1036]\n",
      " [ 1289 11216]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94     19920\n",
      "           1       0.92      0.90      0.91     12505\n",
      "\n",
      "    accuracy                           0.93     32425\n",
      "   macro avg       0.93      0.92      0.92     32425\n",
      "weighted avg       0.93      0.93      0.93     32425\n",
      "\n",
      "acc:  0.9282960678488821\n",
      "pre:  0.9154423767548155\n",
      "rec:  0.896921231507397\n",
      "ma F1:  0.9240484972088479\n",
      "mi F1:  0.9282960678488821\n",
      "we F1:  0.928155922390884\n",
      "[[224  20]\n",
      " [152  70]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.92      0.72       244\n",
      "           1       0.78      0.32      0.45       222\n",
      "\n",
      "    accuracy                           0.63       466\n",
      "   macro avg       0.69      0.62      0.59       466\n",
      "weighted avg       0.68      0.63      0.59       466\n",
      "\n",
      "acc:  0.630901287553648\n",
      "pre:  0.7777777777777778\n",
      "rec:  0.3153153153153153\n",
      "ma F1:  0.5856492969396194\n",
      "mi F1:  0.630901287553648\n",
      "we F1:  0.5921138670273378\n",
      "Loss:  0.05186045914888382\n",
      "Loss:  0.0510077141225338\n",
      "Loss:  0.043424252420663834\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.026861967518925667\n",
      "Loss:  0.05605282261967659\n",
      "Loss:  0.05074755847454071\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.055326689034700394\n",
      "Loss:  0.06764237582683563\n",
      "Loss:  0.07026699930429459\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.06551170349121094\n",
      "Eval Loss:  0.001581430435180664\n",
      "Eval Loss:  2.757432460784912\n",
      "[[18835  1085]\n",
      " [ 1282 11223]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.94     19920\n",
      "           1       0.91      0.90      0.90     12505\n",
      "\n",
      "    accuracy                           0.93     32425\n",
      "   macro avg       0.92      0.92      0.92     32425\n",
      "weighted avg       0.93      0.93      0.93     32425\n",
      "\n",
      "acc:  0.9270007710100231\n",
      "pre:  0.9118459538511537\n",
      "rec:  0.8974810075969613\n",
      "ma F1:  0.9227430712916269\n",
      "mi F1:  0.9270007710100231\n",
      "we F1:  0.9268905809358609\n",
      "[[233  11]\n",
      " [175  47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.95      0.71       244\n",
      "           1       0.81      0.21      0.34       222\n",
      "\n",
      "    accuracy                           0.60       466\n",
      "   macro avg       0.69      0.58      0.53       466\n",
      "weighted avg       0.69      0.60      0.53       466\n",
      "\n",
      "acc:  0.6008583690987125\n",
      "pre:  0.8103448275862069\n",
      "rec:  0.21171171171171171\n",
      "ma F1:  0.5252191060473269\n",
      "mi F1:  0.6008583690987125\n",
      "we F1:  0.5341656855480285\n",
      "Loss:  0.047193534672260284\n",
      "Loss:  0.04011160880327225\n",
      "Loss:  0.05754359811544418\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.04704967141151428\n",
      "Loss:  0.05781156197190285\n",
      "Loss:  0.043338388204574585\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.06658194214105606\n",
      "Loss:  0.05796036869287491\n",
      "Loss:  0.07730787247419357\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAA0nUlEQVR4nO3deXwU5f3A8c83Cfd9hDNIAFFAlFPEiggKCliLtrUFf6K2VcSqFbWteLVatdrWq1YrXni0nq1aURAEFAFFINwgV8AA4QwohEMISZ7fHzsbZjezu7P3Lvt9v155ZXbmmdlnstn5zjynGGNQSimVebKSnQGllFLJoQFAKaUylAYApZTKUBoAlFIqQ2kAUEqpDJWT7AyEo3nz5iY/Pz/Z2VBKqbSyePHiPcaYXP/1aRUA8vPzKSgoSHY2lFIqrYjIZqf1WgSklFIZSgOAUkplKA0ASimVoTQAKKVUhtIAoJRSGUoDgFJKZSgNAEoplaEyIgBs/fYwc9aXJDsbSimVUtKqI1ikzvvbZ1QaKHrk4mRnRSmlUkZGPAFU6pw3SilVjasAICLDRGSdiBSKyASH7V1EZL6IHBWR39rWnyoiy2w/pSIy3tp2n4hss20bEbOzCuBwWXm830IppdJGyCIgEckGngGGAsXAIhGZbIz52pbsW+A3wKX2fY0x64CetuNsA963JXnCGPNoFPkPS0HRdww8pdp4SEoplZHcPAH0AwqNMZuMMWXAW8BIewJjzG5jzCLgWJDjXABsNMY4DkqUCDWyM6LESymlXHFzRWwLbLW9LrbWhWsU8KbfuptEZIWITBKRJhEcMyylR4LFJ6WUyixuAoA4rAurWlVEagI/Av5jW/0s0AlPEdEO4LEA+44VkQIRKSgpia4p5/X/WgzA5+tLWFG8L6pjKaVUunMTAIqBdrbXecD2MN9nOLDEGLPLu8IYs8sYU2GMqQRewFPUVI0x5nljTF9jTN/c3OjL75+cuZ6rJy3kR09/EfWxlFIqnbkJAIuAziLSwbqTHwVMDvN9RuNX/CMirW0vLwNWhXlM187v0qJq+cmZG6qWd+4/Eq+3VEqplBcyABhjyoGbgOnAGuAdY8xqERknIuMARKSViBQDtwH3iEixiDS0ttXF04LoPb9D/1VEVorICmAwcGvMzsrPs1f2dlw/9InPq5ZH/H0uby3cEq8sKKVUynHVE9gYMxWY6rduom15J56iIad9DwPNHNaPCSunUaiVk+24/sCRch7/ZB23XXgqX+8oZcJ7KxnV76REZUsppZIq49tFPvVpYbKzoJRSSZHxAQCgvKIy2VlQSqmEy5gAMG38uQG3PTFzfQJzopRSqSFjAkB+s3oBt63dcSCBOVFKqdSQMQFAKaWULw0ASimVoTImANTUgeCUUspHxlwVs7KchjSqbvPeQ/ynYGvohEopleYyJgAAjDqzXcg0I5/5gt/9d0UCcqOUUsmVUQGgU279kGn2HdYho5VSmSGjAkC3Ng2TnQWllEoZGRUAzjm5ebKzoJRSKSOjAoBSSqnjNAAopVSG0gAAfHe4LNlZUEqphNMAACzZsi/ZWVBKqYTTAKCUUhlKA4BSSmWojAsAN59/crKzoJRSKSHjAsDQbi2TnQWllEoJGRcAmtarmewsKKVUSnAVAERkmIisE5FCEZngsL2LiMwXkaMi8lu/bUUislJElolIgW19UxGZISIbrN9Noj+d2DntD9Mor6ikrLySikqT7OwopVTMhQwAIpINPAMMB7oBo0Wkm1+yb4HfAI8GOMxgY0xPY0xf27oJwCxjTGdglvU67tzOC3CorIKPVuzglHs+5ufPzee2t5exdmdpVO/9fVkFxmgwUUqlBjdXw35AoTFmkzGmDHgLGGlPYIzZbYxZBIQzlOZI4FVr+VXg0jD2jVjjuu6LgMa/vQyAgs3f8d7Sbdz8xtKI33fPwaN0/cM0nv18Y8THUEqpWHITANoC9hlSiq11bhngExFZLCJjbetbGmN2AFi/WzjtLCJjRaRARApKSkrCeNtAmUnOHfiu0iMAfLh8R1LeXyml/LkJAE5TaYVzFT3HGNMbTxHSjSIyMIx9McY8b4zpa4zpm5ubG86ujsTxdJRSKvO4CQDFgH0qrTxgu9s3MMZst37vBt7HU6QEsEtEWgNYv3e7PWY0auYkt+HT92XlzFkf/ZOMUkpFy83VcBHQWUQ6iEhNYBQw2c3BRaSeiDTwLgMXAquszZOBq63lq4EPwsl4MsSi8Kho72GumrSQoj2HYnA0pZSKXE6oBMaYchG5CZgOZAOTjDGrRWSctX2iiLQCCoCGQKWIjMfTYqg58L6IeN/rDWPMNOvQjwDviMivgC3A5TE9syR6c+EWduw/wm1DT6la51/0dPBoeaKzpZRSPkIGAABjzFRgqt+6ibblnXiKhvyVAj0CHHMvcIHrnKaRO99bCeATAJzsPXiU749VkNekbiKypZRSPjKuJzBAnRrZUe1vjCF/whQe/2Sd633Eoe6570MzGfCXz6LKi1JKRSojA0DX1g0i2s8YQ98HZ9DhTs/D0D8+K4wqH9onTEXrjx+s4ovCPcnOhkpTGRkAOuXWj3jfPQfTa/awHfu/Z82O6Howq9T16vzN/N+LC5KdDZWmMjIARHrjLU7lOK73jXjXqJz98KcM//vc5Ly5OqGVV1Syevv+ZGdDRSEjA8CQrskfElqLf1S6+9v0dVz81Dw27DqQ7KyoCGVkABjWvVVE++3Y932Mc6JU+lpevA+AkoNHk5sRFbGMDACROlRWEXDb/sPH2Prt4YDbdQgKpVSq0QAQI0Oe+Jxz/xq4SWey6gCUUioQDQBRsF/TSw54HoM/W+tuSKMpK3VUUKVOdNNW7eC7Q6nbclADQIz94pVFVcv5E6awseSgY7qJOi+AUie03QeOMO7fS7j+34uTnZWANABEodLA0i3fBU0z1xr5U0uAlMosxyo8Tf2Kg9QNJpsGgChd9s8vXaXTOgClVKrRABBn3ub+2/YdSWo+lFLKnwaAGDhyLHDzUK+rJy1MQE6UUso9DQAx0OXeaQG3GQPTVu0M+5gVlYYDR45Fk62EOnKsgm+DtHaorDQcq6hMYI6UUqFoAIiztTtLGRdBK4B7/reS0+/7hPI0uWiOev4rej8wI+D237+7gs53f5zAHMWfMYbvg3QOzBg6rElQqfzn0QAQZ25m/vpg2bZq695d7FlXEWDQoFS7o162dV/Q7f9dXBzRcf+7uJivNu2NaN94e2neN3T9wzR2lWZm/Y72bg8uHf46GRsATm4R+ZDQsXbLW8vCvphf+dKCqjvq3g/M4KfPumuNlG5++5/ljHr+q2Rnw5G3M1/xdzpGlEpPGRsAop0VLNa8dwt3vb+SJ2eup8wKCIW7D5I/YQozvt7lk/7Ljcfvir89VEbB5uD9EU5klZWGispUftB29vn6En78zy/SMu/KvVQe+TdjA0DH3HoJeZ/S78Ob/P2NBVt4cuaGqtfeopWPV50YQ0fM2+A7e9VDU74mf8IUKm0XwVvfXkaP+z9xfcyL/zGPTnd5Zmk7Wl7BO4u2YlL5W2cZ/9ZSlmzZx/7v06eyX7mXDn1/MjYAJMq8GE3X996Sba5bBa3dWcpfpq1NyYvgvu99WwpN+qIIgEpbXt9fui3oRfHku6Zy53srql7bZzx7atYGfv/uCh1rSSkXXAUAERkmIutEpFBEJjhs7yIi80XkqIj81ra+nYh8JiJrRGS1iNxi23afiGwTkWXWz4jYnJI7WSkWngPNNvb4J+urlkNVtA5+dDY79n/PsCfn8uzsjZQeCe/pI12UVxreXLjVcdtea8rOAyfguX+9vZTlIf4HVOoxKdwOKGQAEJFs4BlgONANGC0i3fySfQv8BnjUb305cLsxpivQH7jRb98njDE9rZ+pkZ5EJO65uGsi3y5ie0OMJLjb1gLlmz2HOPvhT6teJzPGlVdUcrS8ehPJT9fspv+fZzlui9b3ZRW8tcg5MMRKWXklf/xgVUxHeLzsn1+4SjfiqbmMfMZdWpV86dBKys0TQD+g0BizyRhTBrwFjLQnMMbsNsYsAo75rd9hjFliLR8A1gBtY5LzKDWrXyvZWfDh5l/FqUSn359nhZU+Wg9PXcPgR2eHTHfZP7/k1Huqd5B7b+k2dpYeYXdp7GeR8i9eiofJy7fz6vzNPPLx2qiP5f14Nu9NjcHC9n9/zFWvdhWeFCyJreImALQF7LdVxURwEReRfKAXsMC2+iYRWSEik0SkSYD9xopIgYgUlJSUhPu2aSMed+vrbXO1+g9LHWoGs0Cem7OJb/YcCplu5bbkThYer3svb12Fb/+MFP6Gh6HH/Z8w4qm5yc7GCcPbP2T3gdSdMtNNAHD6LoX1Hy8i9YF3gfHGGG+N3bNAJ6AnsAN4zGlfY8zzxpi+xpi+ubm54bztCWde4R7yJ0xh7c7S0ImBnz03v2p56grfStELnzw+g9n6XQd4f2lkHbVCmb/RXSeuWFxCE90rN/Uf8MO3qSR0cE+mYMONpJp06B/iJgAUA+1sr/OA7W7fQERq4Ln4v26Mec+73hizyxhTYYypBF7AU9SkgvA2BV1U5K7Nv/0m9bEZ63l70RY2lhxk9fb97LIVwVz4xBxufXt5wON8vb2Unfsj6+06+oXgnbiCXUTD/bKf/9jnYaVXsZGo559lW/fR+4EZjj3nk+mDZdvInzCF3QfSr0d4jos0i4DOItIB2AaMAq5wc3DxNG15CVhjjHncb1trY4z3tvQyYJXrXGeoaMsS73h3ZUT7eYsFih65OLoMhCnY2ELRevjjNZ4K3UtOA+D8R2fTsmFt3hzb39X+W749zJIt++KWv3SQ6EYG3ua+8zfuZWTPlKhKBODNhVsAT6fNFg1qJzk34QkZAIwx5SJyEzAdyAYmGWNWi8g4a/tEEWkFFAANgUoRGY+nxdAZwBhgpYgssw55l9Xi568i0hPPDUQRcH0Mz0vF2bGKSj5cvp3LerWt1oT1xbmbkpQr95773JNHbwDYtOcQm1zUbXgti8HFP5UrB1X0UqyluSM3TwBYF+ypfusm2pZ34ika8jePAE/5xpgx7rOpUs0znxXy5MwN1MjO4pIebXy2PThlTcj9w734GWMC9pUIZvrqnfQ6qXHs7sysfJf5jN2UBt/0CM1et5um9WpyRl7jZGdFA2YcaE/gFNHhzqmMfHpe0DTJ+AIUFH3ruL7EatmwL0bDGHy6dnfYTRDX7izlnYLA7f6PHKvg+n8t5soXFwRMkwxvL9rCyXdNDToG0Iyvd/H1dneV/fF0zcuL+NHTqdX3IB3urP0dOVYR8dDuB4+WM+HdFa5GFg6XBoAUsrw4uU0nvSorDWNfK2D+xr38dOL80DvEwPX/WswDH30dcLtT8Bv25Fx+/98V1TdYvE021+86yIrifdFmMYjwIvODH62hvNJwuCzwF/q61wp8mmRusDXpVcdVVBqfcaRSVZd7p3HlS5HdiLw4dxNvLdrKS3O/iXGuNACklW37PM3K4n0DdLCsnE++3sXY1wri/E6eYR28tgTplxDuV1zEtydmqt3FuvXa/CIOHS1n6BNzIj7G7gNHWBWjfhn7Dx/jZxPnV/0vJlunu6Zyw+vhT7jkxorifWzy6z8TDv+blq82OT9Nh+L9isRjSAlXdQAqtfh36kqU9Um8Cy3cfZBKY2jRILV6cMfDRyuOt7L+wwerWbvT3d/dGIMxkJXle4sw5LHPKT1SHpNWXJNXbGdh0bf887PCqI6zq/QIlcbQulEd1/sEugBOX73LcX20vDcNiW79lkj6BJCGXrZG0Iy3A35ljhdGcRcarYuenMPwv8/lk6/df9lTtaw41H3cTW8s9Xm9/7C7epab31xKx7uqD6kV6aCA+ROmhL3PC3M2ubpROOvPszj74U/ZvPcQp983nS1hDYeRoh+sn1T9/7PTAKCqTJrnKWMMa4iIDGuaEcvH8Fj/5T6yenvvKj3CHz9YFXGl45FjFREPJf7Q1DVc8o/gjRns3l1czIEj5by/NDadu/Z/f4zxby2l1OXQ6ZlOA4Cq8ierEvafszeGTBvo7mb2ut2u3mtRgNZFsXLgSHnKTrQS7xvDO99byavzNzM3wrkoutw7jcdsw5A7CRYejpYnb67q5+ds5H/LtvPal0VJy4NXOtwbaQBQMfXRCncTsdz+n+XVihi2x7Bi8cEpazgryEipifLo9HV8uta32MrpurCp5CCT5n0Tkwnm7c1LIx0f6b0lzmNDOQ4MFucL3dHyCsa8tMBn4h/v+kC27z/CJ6t3xjdjJwCtBM5wjhXKUXyh/7s48kHlNqb4QGSReNqqLHWqSBSO/6m94xg9NDV0J7pwOM2MtrJ4P93bNoyoY51dosq4VxTvZ+6GPczd4Lv+ziBDm7yxYAtvLNgS9wrcfYfLAvbn0DoAlfIuiHIAtRfnfUP3P04PmuaOIG31/Q34y6dBt3vrKSLxZeEeuv9xeszLhyO9A3bazfFiEuGFpMyhKObLwj1c8vS8uDQkSPQUpHM2JG94+IpKw5FjFfT80wzXgzOmoowOALN/OyjZWUg5m/ceYs/B0OOXT12500p/OGQPxbeD9Nb1F2oI3Q27I28C+8TM9Rw8Ws7aHenVqSrScYeu/9fiau3Yr7B6Ra8L0bQ01NOB/Vr/7eEy7n5/pU/Z/+x1ux0DkD//J8a9B4+GLLYKlrVEzMJVcuAone6aWjWelNd6F8118ydMoXB36vz/ZXQAyG9ej9PbNkp2NlLKeX+bzYJvQlfQptO47KG4bWYJznf78X7Uj6bTVagLfbiczvUvH6/l9QVbmLzseP+Fa15exKOfrAt5vO3WMOPe4/Z5cCY/nfilTxqnv3llpUna/+DW7zyt5P7nNyx1ud/T24oAPfsXb06dJwatA1AZw+lC8unaXcxZH1lrmVRlv0gHC07HKioxBmrmVL8P3LHffdD5PsAYTkVhjK5qt9pvDKS3Fm2pluaJmetxKi2L9wTs+ROm0CPP3U3jwm/cTYaUTBn9BKAy04R3j9dJ/PKVAl7xazJYWWn4+8wNru8w9x0+Rv6EKeRPmMKFT8R/Upoz7pse9UBxIp6Ofafc87Hj9kDD6xwfd+d4Au/fKV4X32mrqrfm+SROvX/diHbMrojrjOLw59UAoDJOqHH/v9y4lydmruf8x2a7Ot6na4/3fVi/y30dRaRf6NIj5bzypW9leODmo4EfAdzM7Wz36dpd3PvBatfpo7leFe05FLBSWaj+ZDPob5/xyhffJKQOwK1YXa/jeUYZHwAevbxHsrOgEsTtF/JYpafyct/hY1w+8UuufTX8QfFmfL2Lv0xbW/U63tMFjno++NSb/iKpt5i15nige3vRVg4e9S36iWZOZv/sDHp0dlhNiov2Hua+DwOPJpsMoQL84bLypI6vBRoAOLVVg2RnQaWwRUXfMXPNLv63dBuHArR28q/Uy58wheteK+BZW4/qP4Zx5xyJbQFaT9kv9L6tcqK7r6w0sHzrPp91by5039rLn1NAClSJmmiR1mUE4o0L1/9rMRc+MYdjEQ7ZEQsZHwAAnr6iV7KzoBIgmkve+LeXMealBew+UL2JrJvROo3xXEgqK01C28vbz/mfs6MbwTMU/zoA+2mWlVdWCxiuj+v353p9wRbXI6TGwqBHZ0e0X6inrAXW8NDeeSv+78WvIhqALxoaAIA6NbKTnQWVANFedpds2cfjM4KPkRPIrLW7GPTo7Ij3D8U+RaX9urPOVsSw9+DxSm37xenJmetjUkQVKK7t3H+EU+75mJHPfBFWEHp70VaMMQFbGTkJt2jryLEKNu+N7A7feViMyP/LvihMfKshDQBK+YlHpduxCs+F4ekox9EP12bbMMtFAS50T87cwO3vLA96nDERzWblOecZa4632PnrtMB9A/wvnmUVlXzocmwpr6KwhpWGW99exnl/mx32dKROHpyyhsG2p4VAsSCVBolzFQBEZJiIrBORQhGZ4LC9i4jMF5GjIvJbN/uKSFMRmSEiG6zfTaI/ncik0gei4ifRQxUEciiKylK7cCun52443t/BP8iFugDa9w3EvyNUuN5YWL29f1hDkwMfLt8eOpHlu0NlfL7eM5xEtHn3CjcAJVvIACAi2cAzwHCgGzBaRLr5JfsW+A3waBj7TgBmGWM6A7Os10opl2auibwtfCStgELtE6gi2t+TM6sXg4kIBQ5j6kTbyzpQT+g1O0rp9cAMDscoGDtJjduN4Nw8AfQDCo0xm4wxZcBbwEh7AmPMbmPMIsC/T32wfUcCr1rLrwKXRnYK0evRrnGy3lol0BKXY+ocOZa8Vhmp7N9fVb9DtwtUCex/DX9qlt+wnnjmPnZ6Qou2Xf91rxWwdmdp1RNO0Z5DPPzxmrg3v3zms8KAld6b9x7iqVkbfOptPnYYtTUR3ASAtoC9fVextc6NYPu2NMbsALB+t3A6gIiMFZECESkoKYnP6H+5DWpx1dnt43JslV5WFu9n3L/jM8l4orgZ5jnUxTwS3noOr1lrd3PAYeRVpzvjXaVHmeNQzBTtE8CBI8cY9uRcutw7ja827eVXry7iuc83UbTHt6jGGMPSLd/x4EfOfQnCnVxoYpBJlZ6bs8mnMYAg3PD6krCOHytuAoBjZbfL40ezryexMc8bY/oaY/rm5uaGs2tY6tfSYZEULCvel+wsuBKsOsPeM9mteA1pbG955BUo74fLqvezyIoyANhbEN03eXXAsfsBLvvnl7wYYLhxe+VulRi1FnAKkoniJgAUA+1sr/MAtzUtwfbdJSKtAazf4f/XKpWh/hPFxDupynGk1TgNhBDuuEWO40IFO0QY2b7jXXfzZcSjTsFNAFgEdBaRDiJSExgFTHZ5/GD7TgautpavBj5wn+3YS4fZe1T83fu/VcnOwglFxP3Q4U5zCcf6exmoeOz0+z6pWnZ6Eoknp86FdvG8NoUMAMaYcuAmYDqwBnjHGLNaRMaJyDhPBqWViBQDtwH3iEixiDQMtK916EeAoSKyARhqvVZKnWCiaX0b7bSVgSr031kUeNiKf83fHNV7QnwHcIslVwXfxpipwFS/dRNtyzvxFO+42tdavxe4IJzMxlO7JnWTnQWllJ94XUi9E9FEJUjmwol5JX5PACUHjlKrRhYNa9eILF9h0J7Alp+f2S50IqVUWFJpeGZIzTvzHX7B6MyHZjLgkeBzY8eKNn2xRPuoqZSq7vfvLuerTaGnGA0kll9LtwPIRVvZuu9wWdSBpvRIYuohNAAopeImmos/QFYa3pj1/NOMqJuvJooWASmlUtbGEvczrMVKLIaMitHQQnGnAUAplbJei0GLnHDtPejcLPPWt5clNiMJoAFAKaVsXpz3DYuKqhddvb90m8/rRJTyHC4rZ0MY80yHS+sAlFLKz8JvQtddJGLQwBv+vaRqyOp40CcAm6JHLubczs2TnQ2lVJL9bXrgiWu8yivjHwDmFYaehyEaGgD8aHNQpZQb8a7oPf2P04MOXhcLGgD86OVfKeWGfw/eWDtw1K8vQBxmtNMAoJRSGUoDgFJKpYF4lAZpAPBz98Vdk50FpZSqJpo5oAPRAODnlJYNkp0FpZSqpsxhvoRoaQBQSqkMpQFAKaUylAYABz/o1CzZWVBKKR/JmhNYKaXUCUgDgFJKZSgNAA50NAilVCbQAOAg1eYxVUqpeHAVAERkmIisE5FCEZngsF1E5Clr+woR6W2tP1VEltl+SkVkvLXtPhHZZts2IqZnppRSJxATh7GAQs4HICLZwDPAUKAYWCQik40xX9uSDQc6Wz9nAc8CZxlj1gE9bcfZBrxv2+8JY8yjMTgPpZRSYXLzBNAPKDTGbDLGlAFvASP90owEXjMeXwGNRaS1X5oLgI3GmMTP8RYmrQNQSmUCNwGgLbDV9rrYWhdumlHAm37rbrKKjCaJSBOnNxeRsSJSICIFJSXxmxlHKaUyjZsA4HQ/7F8YFTSNiNQEfgT8x7b9WaATniKiHcBjTm9ujHneGNPXGNM3NzfXRXaVUurEE4/JqtwEgGKgne11HrA9zDTDgSXGmKrh7Iwxu4wxFcaYSuAFPEVNKcH7h65XMzvJOVFKKY94VAK7CQCLgM4i0sG6kx8FTPZLMxm4ymoN1B/Yb4zZYds+Gr/iH786gsuAVWHnPs4e/3nPZGdBKaXiJmQrIGNMuYjcBEwHsoFJxpjVIjLO2j4RmAqMAAqBw8AvvPuLSF08LYiu9zv0X0WkJ56ioiKH7UmndcFKqVRxqKwi5scMGQAAjDFT8Vzk7esm2pYNcGOAfQ8D1UZXM8aMCSunCXTrkM6s3VHKWR10UDil1IlLewI76HVSExbePYRGdWtwWpuGyc6OUkrFpURCA4BSSmUoDQAhBKp4f+iy7onNiFIqo+l8AEngnSS+cd0aPut1wDilVLrTABBC+2Z1Aahbw7dPgA4XoZRKpCNxaAWkASAEbxGQiPDBjeckNzNKqYx14Gh5zI+pASAMPdo1TnYWlFIqZjQAuJST7VvmoyVASql056ojWCbLa1KHGwZ14vI+eT7ru7dtlKQcKaVUbOgTQAgiwh3DutAxt77P+u5tG/H1ny5KUq6UUip6GgCiULemPkAppdKXBgCllMpQegsbpvO7tKD0+2PJzoZSSkVNA0CYJl1zpuP6sQM78vycTQnOjVJKRU6LgGIkJ0sbhiql0osGgBj55YAOyc6CUkqFRQNAjDSvX4vR/Y5Pi3xmfpMk5kYppULTOoAo3TWiC+WVngGD7hjWhTcXbk1yjpRSyh0NAFEaO7BT1XLjujWrlgPNI6CUUqlCi4BibEjXFgBce67WCSilUpurJwARGQb8HcgGXjTGPOK3XaztI4DDwDXGmCXWtiLgAFABlBtj+lrrmwJvA/lAEfAzY8x3UZ9Rkr14taeZ6NZvDyc5J0opFVzIJwARyQaeAYYD3YDRItLNL9lwoLP1MxZ41m/7YGNMT+/F3zIBmGWM6QzMsl6fcBrW1lI2pVRqclME1A8oNMZsMsaUAW8BI/3SjAReMx5fAY1FpHWI444EXrWWXwUudZ/t9FEj2/dPfFqbhknKiVJK+XITANoC9qYtxdY6t2kM8ImILBaRsbY0LY0xOwCs3y2c3lxExopIgYgUlJSUuMhuavGfOrJmjla7KKVSg5urkVMXV/82LsHSnGOM6Y2nmOhGERkYRv4wxjxvjOlrjOmbm5sbzq4pQieSUUqlJjcBoBhoZ3udB2x3m8YY4/29G3gfT5ESwC5vMZH1e3e4mU8PvrHygq4tk5QPpZTy5SYALAI6i0gHEakJjAIm+6WZDFwlHv2B/caYHSJST0QaAIhIPeBCYJVtn6ut5auBD6I8l5Ti1A9gTP/2jB3YkWt+kJ/w/CillL+QAcAYUw7cBEwH1gDvGGNWi8g4ERlnJZsKbAIKgReAX1vrWwLzRGQ5sBCYYoyZZm17BBgqIhuAodbrE5Cn0KdWThYPXNrdp1L41iGnUEvrBJRSSeKqjaIxZiqei7x93UTbsgFudNhvE9AjwDH3AheEk9l05F8JbNegdg4f3TyAoU/Mieo96tbM5nBZRVTHUEplHr39jBNTrZ78uOsGdqRHu8Zc2qstnVs2qFp/ac82Eb3XW2P7R7SfUiqzaQCIM6cHgLaN6/DBjefQtF5Nh63H/XpQp6DbARrVqcEZeY1ZcNcJ/zCllIoxDQBxItalv37tHM47JZfnxvQJuc842wW/6JGLue7cjgHTtm5UG6CqDqFZiGCilFL+NADESbumdbj5/JN5+ZozefWX/Rh0qmM/Nx9dWvn2Em5SryZFj1zsmPb2C0/1eZ2TnRUwrQ5HoZRyoleGOBGRahfpWHj5F2dSVl5Jj7zG1vuE3mfh3UPocu+00AmVUhlFnwDSTO2cbC46rVV4+9TIDrp98Knp2MNaKRUtDQBpKifbc+vfsmFtx+2X9fIfrslZl1YNePjHZ8QsX0qp9KFFQGnGW+TTvH4tnvx5T845ubnP9usHduS5OZtcHWvdg8PIEqFSpy9TKiPpE0AK8LboCdelvdqS26CWzzpvQGjXpI7jPhv/PKJquVZONjWys6iVk03hQ8MjyoNSKn1pAEgB797wA56+oldMjjXwlFxevuZMfnNBZ8ft2VnOtcY52Vl0aF7PZ10LW3CpX0sfFpU60WgASAFtGtfhh2dE1gvYyeAuLcjJDv+jnXnbeay6/yLHbdcPPN4n4XcXxb51k1Iq8TQApJizOjQNuG1ot5b0OqlxRMedPn4g//pVv6BpsrOEHNsTwtUBRi391QCd8F6pROuUWy90ojDpc32Kef3asyiv9K2UfeUXZ9K4bk16tmsc8XFPbdWAU1s1CJ3QJrd+LW4c3IlnPtvosz5QMVIgbRvXYdu+78PaRynlK9ANWTT0CSDF5GRnVWu3P+jUFhFd/McP6UyY1+qA7B3O/Oc5Ticf3jQg2VlQKiLxaKyXvt9kFdL4Iaew6WHn4SHcqpntCUaR1Ck4GTuwIz926KNw4+DQA9/FQpfW4T0FKXUi0wCgAjo9rxFjB3Zk3Hmdwp7F7PGfeaaBaFSnhs/6u0Z05fGf96yW/pYLTok0m2HJdjN2hk1egOa0SiVa55b1Y35MrQNQjkSga2vP4HQThncJa9+5vx9Mu6Z16dq6IbkNajHy6S+Cph90ai41c7J4/dqzKNp7iCwR7nxvZcR5D8b/+j+ka0tmrtkVMP0zV/Rm5DPB869UInjH/4olfQLIQB/fci6zfzvIcZu3nLFmFEU+7ZrWBTwBpHn9Wgzt1jJg2s9/N4iJV3qGyj7n5Ob831ntqRNi7CKvszs2Cztv4hcBfjkgP2j6Hu0a07ax71PAvT/sFvb7KhWtenHoi6MBIAN1bd2Q/ObOTcq8M5k5lZQ8cGl3mtT1LdK5MMjF3euei7sG3Na+Wb1qld7BSmk+vGkAD1zanXNObsbdQY4L8L8bz3Fcf/vQ48VN4jhlj683rjuravm1X/bjVwM6cPEZrUPup1Sq0wCgHDldGMf0b8/SP1zos+4k624/mJzsrIhaI53fpfocCqfnNWJM//a8fm3/qgHxArG3nGrVsHZVPcbNtl7SbqoE2jfzBMtaOVkMPCV5I6fWq+nuyUgpt7QOQPmoUyObkT3bcEW/k2J63E9vH8S6XQfC2qd+rRw2PDSc/y4udqwTyHJx9a5fK4eDR8s5s0NT7vvRaWG9v12gyXaCvWesdcytz8pt+2N+XJW5XD0BiMgwEVknIoUiMsFhu4jIU9b2FSLS21rfTkQ+E5E1IrJaRG6x7XOfiGwTkWXWzwj/46rEExH+PqoXZ4UoX/cfhA5gym8G8PdRPR3T5zevF/Y8BgZPn4PGfi2JvDq3CN0q4qHLunuOFeNG1MO7e85l1u3n+aw/v0uLgMNpuNWuqXPLo0Dx7md981wdN9Qc1CrzhAwAIpINPAMMB7oBo0XEvxZsONDZ+hkLPGutLwduN8Z0BfoDN/rt+4Qxpqf1MzW6U1GJ5K0IbWOrID2tTSNG9nQ3D0Es2Ct0L+nRhtwGtfjqzgt80ngrlAMNZud/TX34x6e7eu8fntGGwoeG0ynXNwhNuuZMV/t7jexZfQyoz24fxIjTqwfLXgE6A7qNbbHqFHgiefkX4X1eJxo3TwD9gEJjzCZjTBnwFjDSL81I4DXj8RXQWERaG2N2GGOWABhjDgBrgMRdIVTcXHJGayZd07eqXN2/vX8sBSvp8dZB/GN0LxbdPYRWfkNrD+nakt8POzVghbF/q6BmYdwlx6pznNNx//KT6pP0XHtuR4fU7oXzEDTxyt4Btw1K0gxyHeMwFs5gF3N1n8jc/Ae3BbbaXhdT/SIeMo2I5AO9gAW21TdZRUaTRKSJ05uLyFgRKRCRgpKSEhfZVYkgIpzfpSVZWcI/Rvfio5vDG2JhdL92Abd5m5Ge3rZhyON8ePMAPgvQpBUgK0v49aCTaVDbN0B9OeF85t95vrvMRuHN6/rzx0uOP/QG61D36e3nMePWgQDV8guBA+HZndw1h33d1prJzr+ZK8Cw7uG3clru10Ag1pL9AHNLgCHWo9Gwtu+Tab8gg0HGg5sA4PR397+XCJpGROoD7wLjjTGl1upngU5AT2AH8JjTmxtjnjfG9DXG9M3N1blrU9ElPdpUXbTdevjHZwSsWO19UhOmjT+Xawd473gDf/Ub1alRbR6DBy/tznu//kHQ92/TuA6tG9WpdlFtVj825eTv3nA2tw45hbM7NeMX5wQePdXetrtjbn06twx/qIo+7X3vne67pHo/hXUPDqNLq9AB1Y1AfUQa1a3BdecGPlf/i52T6wd2rNb6a8m9Q/nvuLMd09fKSUxDxhGnt+LWobHvrf7zM31vhN65/mzH0XZ/3Ds+BSdu/nrFgD2XecB2t2lEpAaei//rxpj3vAmMMbuMMRXGmErgBTxFTUoB0KVVQ7KsQmvvRTpYhzK7K/u3p/dJjg+UQeU2qEWf9k15JQblwn3aN+WWIcHvGO8Y1oW7RnSlX4emUXUua+JXbNXQKo5rYAsutXJi14S0lkNHvQ+sPhe3X3gqfxp5vLWVfdwnp6caf71OasIDl3avet2+WV2a1qtJ3/ym1YrrAN64rn9YeYfIZuAL9Pe76uz2YR/LzqlYzul/IV5FVW4CwCKgs4h0EJGawChgsl+aycBVVmug/sB+Y8wO8XxiLwFrjDGP23cQEfsz5mXAqojPQqkIdW3dkGb1avLGdWex6O4hgGf01Uj434mHcsOgTtSvlRPwrs+f/wWwW+uGrH9wOA0dLqyPXt6DD0MUy110WksevbxHtfXeJ7NAeRrsVwfQtF5NelgV1LVrZHPV2flV2+zjPl0epLWSdx6KC7q2oEGAJ4VLbJMm/aR3Hqvvv4g+7Zsw747BAY/rZP6dF7D4niFVn/c9F3etyj94hjIZ7xe8A7Uiu/9Hp/HEz6v/Dd24YVAn6gZonHC/X5PleNWxhQwAxphy4CZgOp5K3HeMMatFZJyIjLOSTQU2AYV47uZ/ba0/BxgDnO/Q3POvIrJSRFYAg4FbY3ZW6oQUj+Fw69fKYfG9Q/lBp+YRH2OANQ/zuzcEL3YC+GEYPYj/fNnpPq2S2jSq7VMOPfWWc6lpFYHM+d1gn5ZOP+2TR37zejw3pg83n3+y4/GfG9OXn/bJ44Wr+latG23r/3HvD7sx8zbfZq6/u+hUftw7j/xmx4v83A5VfvP5nbnmB/lVdSLjzgs9Aqy9OOg3F5zM9ed5igVbN6pdVXyW18R98eN/rKKkZvVrVTVlvvbcjrxn++zaNa1LXRed7qaPH4iIcFmvvIBNf3sHmcDpjmFdAv5TX/2DfN4a259V91/E01f04tzOkf9/BuOqI5jVRHOq37qJtmUD3Oiw3zwCFOAaY8aElVOVsZJd+RfKy784k7LySldpu7XxlMO7Ge/oirM8F2NvJzgR4dahp/D3WRuqpT2pWV2GdmvJ+0u3+ay/6LRWjv0v1j4wzCdP6x4cxp+nrKlWzu3fdNR7YfxRz7Y8ZeXjH6ODz2d97YAOlBw8SnaWcN+PTsMYQ36zepx3Si5Lt3zH9ed1ZOxrix33vXvE8dZbIsJPe+fx3Oeb+GGP6oG0VcPanJ7XiBlfBx7c78x850pW/0mOruzfnk0lh2jbuA6PzVhfdac+7LRWTFu9E8BngqX6tXKYedt57Nx/hG8Pl1EzWzitTSM27z3MlS8twJ+3wjzYPU1/qy9OLKeL9ac9gZWKUo3srJCT5Dx9RS+a16/eec6N6eMH0rhu6CKA09o05P2l21xVyPuPv1QrJ5v7R3avlq5D83rcMKgTa3eU8tm6463w7JfLUIOU3eNXpi0iDLbu7N++3nNHfn6XFnzy9S6yRKp6eLdvVrdaU9vOLRs4Nh54bkwfzshrRHaWVAWAjs3rsWnPoaB5C6RuzRwe+ckZVFYacrKzGGOV9U8c04f8CVMc9zm5RX1O9uucuPXbw45pG1mf5xVnncQ/Pi2MKI+xoAFApZE4lAEFcO8Pu9GtdWxazcDxu7jDZeEPEeE/lWdOllSbNhQ8ZfbnnNy8ahjvWBAR7hjWhfsmrwaOB4BYfxJPje7FHuspoX6tHB758emcG8a4S96nHGMMNw7uxGW98ji5RX2fi7VT7/VQsrKEGwbFbrKij24eUFVsB9C6UXLnm9AAoFKeU+uPeEvlie8X3zOUsorqRU4iEtOLv91P++TxypdFjgP0xULtGtk+ZfmjIhyLSkT43UXhzV+RKB2a16N720bV1p+R14gVxft9Wm0ligYAlfK8xbPJCASx5i0q+kmfyNt1N3JRHBTIrwZ0cDWCq7/ubRuFNSCe8vA+KbVtXCdgh8UPbjyHnaVHaFI38WM1aQBQKe+8U3K55gf5/DpB8wbHU43sLFbdf5HrSW9iTSezCczt36Zv+yZhFye1bxY46IpI0oqCNAColJeTnRXVUM6pJtDAdOnEO1dyuNOFJsvofifxf2cFL1ZyW+z3XxfNfb28rYv8K91TRfr/JyqlEu7yPnm0blS7qg+Ev8cu70HbJsmt4LRzO8prrPXLb8pvLujMmP7R9RiOFw0ASqmwiQjndg7cSucnfdzNUXCiy8oSbovDGEKxolNCKqVUhtInAKXUCevVX/bjwJFjyc5GytIAoJQ6YZ0XRmeyTKRFQEoplaE0ACilVIbSAKCUUhlKA4BSSmUoDQBKKZWhNAAopVSG0gCglFIZSgOAUkplKAk0230qEpESYHOEuzcH9sQwO8mi55Fa9DxSi56Hs/bGmGq94tIqAERDRAqMMX2TnY9o6XmkFj2P1KLnER4tAlJKqQylAUAppTJUJgWA55OdgRjR80gteh6pRc8jDBlTB6CUUspXJj0BKKWUstEAoJRSGSojAoCIDBORdSJSKCITkp0ffyJSJCIrRWSZiBRY65qKyAwR2WD9bmJLf6d1LutE5CLb+j7WcQpF5CkRkTjne5KI7BaRVbZ1Mcu3iNQSkbet9QtEJD+B53GfiGyzPpNlIjIiDc6jnYh8JiJrRGS1iNxirU+rzyTIeaTVZyIitUVkoYgst87jfmt96nwexpgT+gfIBjYCHYGawHKgW7Lz5ZfHIqC537q/AhOs5QnAX6zlbtY51AI6WOeWbW1bCJwNCPAxMDzO+R4I9AZWxSPfwK+BidbyKODtBJ7HfcBvHdKm8nm0Bnpbyw2A9VZ+0+ozCXIeafWZWO9Z31quASwA+qfS5xG3i0Oq/Fh/tOm213cCdyY7X355LKJ6AFgHtLaWWwPrnPIPTLfOsTWw1rZ+NPBcAvKej++FM2b59qaxlnPw9IyUBJ1HoItNSp+HX14/AIam62ficB5p+5kAdYElwFmp9HlkQhFQW2Cr7XWxtS6VGOATEVksImOtdS2NMTsArN8trPWBzqettey/PtFime+qfYwx5cB+oFnccl7dTSKywioi8j6mp8V5WEUBvfDcdabtZ+J3HpBmn4mIZIvIMmA3MMMYk1KfRyYEAKdy8FRr+3qOMaY3MBy4UUQGBkkb6HxS/TwjyXcyz+lZoBPQE9gBPBYiTylzHiJSH3gXGG+MKQ2W1GFdypyLw3mk3WdijKkwxvQE8oB+ItI9SPKEn0cmBIBioJ3tdR6wPUl5cWSM2W793g28D/QDdolIawDr924reaDzKbaW/dcnWizzXbWPiOQAjYBv45ZzG2PMLuvLWwm8gOcz8cmTX35T4jxEpAaei+brxpj3rNVp95k4nUe6fiZW3vcBs4FhpNDnkQkBYBHQWUQ6iEhNPBUlk5OcpyoiUk9EGniXgQuBVXjyeLWV7Go85aBY60dZtf8dgM7AQutR8oCI9LdaCFxl2yeRYplv+7F+CnxqrMLOePN+QS2X4flMvHlKyfOw3vclYI0x5nHbprT6TAKdR7p9JiKSKyKNreU6wBBgLan0ecSz8iZVfoAReFoSbATuTnZ+/PLWEU/N/3JgtTd/eMrxZgEbrN9NbfvcbZ3LOmwtfYC+eL4UG4GniX/l3Jt4HsWP4bkT+VUs8w3UBv4DFOJpBdExgefxL2AlsML6krVOg/MYgOfxfwWwzPoZkW6fSZDzSKvPBDgDWGrldxXwB2t9ynweOhSEUkplqEwoAlJKKeVAA4BSSmUoDQBKKZWhNAAopVSG0gCglFIZSgOAUkplKA0ASimVof4fU//UaUClKEIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  10 Training Time 5824.007786273956 Best Test Acc:  0.6716738197424893\n",
      "test subjects:  ['./seg\\\\a15', './seg\\\\x27', './seg\\\\x28']\n",
      "*********\n",
      "32810 1503\n",
      "31397 1494\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.6738974452018738\n",
      "Eval Loss:  0.6561837792396545\n",
      "Eval Loss:  0.7223235368728638\n",
      "[[ 1731 18218]\n",
      " [ 1536  9912]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.09      0.15     19949\n",
      "           1       0.35      0.87      0.50     11448\n",
      "\n",
      "    accuracy                           0.37     31397\n",
      "   macro avg       0.44      0.48      0.33     31397\n",
      "weighted avg       0.47      0.37      0.28     31397\n",
      "\n",
      "acc:  0.37083160811542504\n",
      "pre:  0.3523640241734803\n",
      "rec:  0.8658280922431866\n",
      "ma F1:  0.3250028126681354\n",
      "mi F1:  0.37083160811542504\n",
      "we F1:  0.2773814228505814\n",
      "[[  25  190]\n",
      " [ 246 1033]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.12      0.10       215\n",
      "           1       0.84      0.81      0.83      1279\n",
      "\n",
      "    accuracy                           0.71      1494\n",
      "   macro avg       0.47      0.46      0.46      1494\n",
      "weighted avg       0.74      0.71      0.72      1494\n",
      "\n",
      "acc:  0.7081659973226239\n",
      "pre:  0.8446443172526574\n",
      "rec:  0.8076622361219703\n",
      "ma F1:  0.4643100334547177\n",
      "mi F1:  0.7081659973226239\n",
      "we F1:  0.721713550870841\n",
      "Subject 11 Current Train Acc:  0.37083160811542504 Current Test Acc:  0.7081659973226239\n",
      "Loss:  0.16657845675945282\n",
      "Loss:  0.16458803415298462\n",
      "Loss:  0.15935373306274414\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.15329903364181519\n",
      "Loss:  0.14728623628616333\n",
      "Loss:  0.13731372356414795\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.11303193122148514\n",
      "Loss:  0.09863504767417908\n",
      "Loss:  0.11021433025598526\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.25606727600097656\n",
      "Eval Loss:  2.5356407165527344\n",
      "Eval Loss:  0.03823280334472656\n",
      "[[17534  2415]\n",
      " [ 3466  7982]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.86     19949\n",
      "           1       0.77      0.70      0.73     11448\n",
      "\n",
      "    accuracy                           0.81     31397\n",
      "   macro avg       0.80      0.79      0.79     31397\n",
      "weighted avg       0.81      0.81      0.81     31397\n",
      "\n",
      "acc:  0.8126891104245628\n",
      "pre:  0.7677214581129171\n",
      "rec:  0.6972396925227113\n",
      "ma F1:  0.7935837029576971\n",
      "mi F1:  0.8126891104245629\n",
      "we F1:  0.8105869555619502\n",
      "[[ 125   90]\n",
      " [ 187 1092]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.58      0.47       215\n",
      "           1       0.92      0.85      0.89      1279\n",
      "\n",
      "    accuracy                           0.81      1494\n",
      "   macro avg       0.66      0.72      0.68      1494\n",
      "weighted avg       0.85      0.81      0.83      1494\n",
      "\n",
      "acc:  0.8145917001338688\n",
      "pre:  0.9238578680203046\n",
      "rec:  0.8537920250195465\n",
      "ma F1:  0.680913715055434\n",
      "mi F1:  0.8145917001338688\n",
      "we F1:  0.828000970612264\n",
      "Subject 11 Current Train Acc:  0.8126891104245628 Current Test Acc:  0.8145917001338688\n",
      "Loss:  0.12206573039293289\n",
      "Loss:  0.07094219326972961\n",
      "Loss:  0.10819096863269806\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.09189368039369583\n",
      "Loss:  0.11061172932386398\n",
      "Loss:  0.08997043967247009\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.1122216284275055\n",
      "Loss:  0.10178593546152115\n",
      "Loss:  0.07776853442192078\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.165982186794281\n",
      "Eval Loss:  1.2696996927261353\n",
      "Eval Loss:  0.016281843185424805\n",
      "[[18641  1308]\n",
      " [ 3331  8117]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.93      0.89     19949\n",
      "           1       0.86      0.71      0.78     11448\n",
      "\n",
      "    accuracy                           0.85     31397\n",
      "   macro avg       0.85      0.82      0.83     31397\n",
      "weighted avg       0.85      0.85      0.85     31397\n",
      "\n",
      "acc:  0.8522470299710163\n",
      "pre:  0.8612201591511937\n",
      "rec:  0.7090321453529\n",
      "ma F1:  0.8335453168258498\n",
      "mi F1:  0.8522470299710163\n",
      "we F1:  0.8486520503346926\n",
      "[[ 143   72]\n",
      " [ 224 1055]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.67      0.49       215\n",
      "           1       0.94      0.82      0.88      1279\n",
      "\n",
      "    accuracy                           0.80      1494\n",
      "   macro avg       0.66      0.74      0.68      1494\n",
      "weighted avg       0.86      0.80      0.82      1494\n",
      "\n",
      "acc:  0.8018741633199464\n",
      "pre:  0.9361135758651287\n",
      "rec:  0.8248631743549648\n",
      "ma F1:  0.684191582898424\n",
      "mi F1:  0.8018741633199464\n",
      "we F1:  0.8214879267235334\n",
      "Loss:  0.07666657865047455\n",
      "Loss:  0.0754900723695755\n",
      "Loss:  0.05660085007548332\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.09040910005569458\n",
      "Loss:  0.0989832952618599\n",
      "Loss:  0.0751141607761383\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.08080992847681046\n",
      "Loss:  0.05742139369249344\n",
      "Loss:  0.09319960325956345\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.14238286018371582\n",
      "Eval Loss:  0.9224002361297607\n",
      "Eval Loss:  0.009866714477539062\n",
      "[[19481   468]\n",
      " [ 4632  6816]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.98      0.88     19949\n",
      "           1       0.94      0.60      0.73     11448\n",
      "\n",
      "    accuracy                           0.84     31397\n",
      "   macro avg       0.87      0.79      0.81     31397\n",
      "weighted avg       0.85      0.84      0.83     31397\n",
      "\n",
      "acc:  0.8375640984807465\n",
      "pre:  0.9357495881383855\n",
      "rec:  0.5953878406708596\n",
      "ma F1:  0.8059963174015672\n",
      "mi F1:  0.8375640984807465\n",
      "we F1:  0.8271852401772564\n",
      "[[172  43]\n",
      " [430 849]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.80      0.42       215\n",
      "           1       0.95      0.66      0.78      1279\n",
      "\n",
      "    accuracy                           0.68      1494\n",
      "   macro avg       0.62      0.73      0.60      1494\n",
      "weighted avg       0.86      0.68      0.73      1494\n",
      "\n",
      "acc:  0.6834002677376171\n",
      "pre:  0.9517937219730942\n",
      "rec:  0.6637998436278343\n",
      "ma F1:  0.6015903415840383\n",
      "mi F1:  0.6834002677376171\n",
      "we F1:  0.7301660600883335\n",
      "Loss:  0.07199802249670029\n",
      "Loss:  0.0733252689242363\n",
      "Loss:  0.0874863862991333\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.08272293955087662\n",
      "Loss:  0.08868906646966934\n",
      "Loss:  0.08419796079397202\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.09879475831985474\n",
      "Loss:  0.059676527976989746\n",
      "Loss:  0.08698731660842896\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.07834291458129883\n",
      "Eval Loss:  0.24009019136428833\n",
      "Eval Loss:  0.007253408432006836\n",
      "[[19139   810]\n",
      " [ 3096  8352]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91     19949\n",
      "           1       0.91      0.73      0.81     11448\n",
      "\n",
      "    accuracy                           0.88     31397\n",
      "   macro avg       0.89      0.84      0.86     31397\n",
      "weighted avg       0.88      0.88      0.87     31397\n",
      "\n",
      "acc:  0.875593209542313\n",
      "pre:  0.9115913555992141\n",
      "rec:  0.7295597484276729\n",
      "ma F1:  0.8589430003884003\n",
      "mi F1:  0.875593209542313\n",
      "we F1:  0.8720646679528214\n",
      "[[154  61]\n",
      " [356 923]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.72      0.42       215\n",
      "           1       0.94      0.72      0.82      1279\n",
      "\n",
      "    accuracy                           0.72      1494\n",
      "   macro avg       0.62      0.72      0.62      1494\n",
      "weighted avg       0.85      0.72      0.76      1494\n",
      "\n",
      "acc:  0.7208835341365462\n",
      "pre:  0.9380081300813008\n",
      "rec:  0.7216575449569976\n",
      "ma F1:  0.6202794581498469\n",
      "mi F1:  0.7208835341365462\n",
      "we F1:  0.7594767752497794\n",
      "Loss:  0.05962410196661949\n",
      "Loss:  0.06695729494094849\n",
      "Loss:  0.07385819405317307\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.07634573429822922\n",
      "Loss:  0.07782521843910217\n",
      "Loss:  0.057775888592004776\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.0713314563035965\n",
      "Loss:  0.09741872549057007\n",
      "Loss:  0.07186917215585709\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.03506505489349365\n",
      "Eval Loss:  0.1536346673965454\n",
      "Eval Loss:  0.013239860534667969\n",
      "[[18450  1499]\n",
      " [ 2062  9386]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91     19949\n",
      "           1       0.86      0.82      0.84     11448\n",
      "\n",
      "    accuracy                           0.89     31397\n",
      "   macro avg       0.88      0.87      0.88     31397\n",
      "weighted avg       0.89      0.89      0.89     31397\n",
      "\n",
      "acc:  0.8865815205274389\n",
      "pre:  0.8622875516766192\n",
      "rec:  0.8198812019566737\n",
      "ma F1:  0.8762695910024731\n",
      "mi F1:  0.8865815205274388\n",
      "we F1:  0.8859410068113582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 111  104]\n",
      " [ 217 1062]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.52      0.41       215\n",
      "           1       0.91      0.83      0.87      1279\n",
      "\n",
      "    accuracy                           0.79      1494\n",
      "   macro avg       0.62      0.67      0.64      1494\n",
      "weighted avg       0.83      0.79      0.80      1494\n",
      "\n",
      "acc:  0.785140562248996\n",
      "pre:  0.9108061749571184\n",
      "rec:  0.8303362001563722\n",
      "ma F1:  0.6387757177236213\n",
      "mi F1:  0.7851405622489959\n",
      "we F1:  0.8025319685911279\n",
      "Loss:  0.05795615166425705\n",
      "Loss:  0.06351885944604874\n",
      "Loss:  0.0652237981557846\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.08910255134105682\n",
      "Loss:  0.08267147839069366\n",
      "Loss:  0.07798921316862106\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.06023390591144562\n",
      "Loss:  0.06672366708517075\n",
      "Loss:  0.09407109767198563\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.03101193904876709\n",
      "Eval Loss:  0.17230117321014404\n",
      "Eval Loss:  0.010164499282836914\n",
      "[[19163   786]\n",
      " [ 2726  8722]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     19949\n",
      "           1       0.92      0.76      0.83     11448\n",
      "\n",
      "    accuracy                           0.89     31397\n",
      "   macro avg       0.90      0.86      0.87     31397\n",
      "weighted avg       0.89      0.89      0.89     31397\n",
      "\n",
      "acc:  0.8881421791890945\n",
      "pre:  0.9173327724021876\n",
      "rec:  0.7618798043326345\n",
      "ma F1:  0.8742339691591308\n",
      "mi F1:  0.8881421791890945\n",
      "we F1:  0.8855579509103731\n",
      "[[145  70]\n",
      " [421 858]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.67      0.37       215\n",
      "           1       0.92      0.67      0.78      1279\n",
      "\n",
      "    accuracy                           0.67      1494\n",
      "   macro avg       0.59      0.67      0.57      1494\n",
      "weighted avg       0.83      0.67      0.72      1494\n",
      "\n",
      "acc:  0.6713520749665328\n",
      "pre:  0.9245689655172413\n",
      "rec:  0.6708365910867865\n",
      "ma F1:  0.5744224377446455\n",
      "mi F1:  0.6713520749665328\n",
      "we F1:  0.7190691895035346\n",
      "Loss:  0.06402642279863358\n",
      "Loss:  0.07999469339847565\n",
      "Loss:  0.05748068541288376\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.07492783665657043\n",
      "Loss:  0.08750279992818832\n",
      "Loss:  0.05919447913765907\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.06240353733301163\n",
      "Loss:  0.06289172917604446\n",
      "Loss:  0.04047626256942749\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.025510311126708984\n",
      "Eval Loss:  0.20307165384292603\n",
      "Eval Loss:  0.00974273681640625\n",
      "[[19398   551]\n",
      " [ 3302  8146]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91     19949\n",
      "           1       0.94      0.71      0.81     11448\n",
      "\n",
      "    accuracy                           0.88     31397\n",
      "   macro avg       0.90      0.84      0.86     31397\n",
      "weighted avg       0.88      0.88      0.87     31397\n",
      "\n",
      "acc:  0.8772812689110424\n",
      "pre:  0.9366448200528918\n",
      "rec:  0.7115653389238294\n",
      "ma F1:  0.8591972822235164\n",
      "mi F1:  0.8772812689110424\n",
      "we F1:  0.8728599173844351\n",
      "[[151  64]\n",
      " [489 790]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.70      0.35       215\n",
      "           1       0.93      0.62      0.74      1279\n",
      "\n",
      "    accuracy                           0.63      1494\n",
      "   macro avg       0.58      0.66      0.55      1494\n",
      "weighted avg       0.83      0.63      0.68      1494\n",
      "\n",
      "acc:  0.6298527443105756\n",
      "pre:  0.9250585480093677\n",
      "rec:  0.617670054730258\n",
      "ma F1:  0.5469785575048733\n",
      "mi F1:  0.6298527443105756\n",
      "we F1:  0.6849725086179678\n",
      "Loss:  0.03703320771455765\n",
      "Loss:  0.07513280212879181\n",
      "Loss:  0.0838668942451477\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.051154278218746185\n",
      "Loss:  0.05891629680991173\n",
      "Loss:  0.07240180671215057\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.055438704788684845\n",
      "Loss:  0.09081819653511047\n",
      "Loss:  0.0520651638507843\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.01802229881286621\n",
      "Eval Loss:  0.14169526100158691\n",
      "Eval Loss:  0.012622952461242676\n",
      "[[19274   675]\n",
      " [ 2749  8699]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92     19949\n",
      "           1       0.93      0.76      0.84     11448\n",
      "\n",
      "    accuracy                           0.89     31397\n",
      "   macro avg       0.90      0.86      0.88     31397\n",
      "weighted avg       0.89      0.89      0.89     31397\n",
      "\n",
      "acc:  0.8909449947447209\n",
      "pre:  0.9279923191807126\n",
      "rec:  0.7598707197763801\n",
      "ma F1:  0.8769901744303594\n",
      "mi F1:  0.8909449947447209\n",
      "we F1:  0.8882081344769207\n",
      "[[142  73]\n",
      " [468 811]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.66      0.34       215\n",
      "           1       0.92      0.63      0.75      1279\n",
      "\n",
      "    accuracy                           0.64      1494\n",
      "   macro avg       0.58      0.65      0.55      1494\n",
      "weighted avg       0.82      0.64      0.69      1494\n",
      "\n",
      "acc:  0.6378848728246319\n",
      "pre:  0.917420814479638\n",
      "rec:  0.63408913213448\n",
      "ma F1:  0.5470634220148783\n",
      "mi F1:  0.6378848728246319\n",
      "we F1:  0.6915088983401068\n",
      "Loss:  0.04570435360074043\n",
      "Loss:  0.04385444149374962\n",
      "Loss:  0.07063347101211548\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.06667584925889969\n",
      "Loss:  0.060013849288225174\n",
      "Loss:  0.05208501219749451\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.05458468571305275\n",
      "Loss:  0.05819735676050186\n",
      "Loss:  0.10244835913181305\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.017994403839111328\n",
      "Eval Loss:  0.1986650824546814\n",
      "Eval Loss:  0.011776208877563477\n",
      "[[19395   554]\n",
      " [ 2881  8567]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92     19949\n",
      "           1       0.94      0.75      0.83     11448\n",
      "\n",
      "    accuracy                           0.89     31397\n",
      "   macro avg       0.90      0.86      0.88     31397\n",
      "weighted avg       0.90      0.89      0.89     31397\n",
      "\n",
      "acc:  0.8905946428002676\n",
      "pre:  0.9392610459379453\n",
      "rec:  0.748340321453529\n",
      "ma F1:  0.8758256034987553\n",
      "mi F1:  0.8905946428002677\n",
      "we F1:  0.8874206905972183\n",
      "[[149  66]\n",
      " [479 800]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.69      0.35       215\n",
      "           1       0.92      0.63      0.75      1279\n",
      "\n",
      "    accuracy                           0.64      1494\n",
      "   macro avg       0.58      0.66      0.55      1494\n",
      "weighted avg       0.82      0.64      0.69      1494\n",
      "\n",
      "acc:  0.6352074966532798\n",
      "pre:  0.9237875288683602\n",
      "rec:  0.6254886630179828\n",
      "ma F1:  0.5497100764004679\n",
      "mi F1:  0.6352074966532798\n",
      "we F1:  0.6894477955233432\n",
      "Loss:  0.06774511188268661\n",
      "Loss:  0.06899773329496384\n",
      "Loss:  0.07269352674484253\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.07674960792064667\n",
      "Loss:  0.06070966273546219\n",
      "Loss:  0.0631924495100975\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.056773778051137924\n",
      "Loss:  0.05248109996318817\n",
      "Loss:  0.06305906176567078\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.014574766159057617\n",
      "Eval Loss:  0.2578887939453125\n",
      "Eval Loss:  0.010087013244628906\n",
      "[[19273   676]\n",
      " [ 2416  9032]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93     19949\n",
      "           1       0.93      0.79      0.85     11448\n",
      "\n",
      "    accuracy                           0.90     31397\n",
      "   macro avg       0.91      0.88      0.89     31397\n",
      "weighted avg       0.90      0.90      0.90     31397\n",
      "\n",
      "acc:  0.9015192534318566\n",
      "pre:  0.9303667078697981\n",
      "rec:  0.7889587700908456\n",
      "ma F1:  0.8897942589947148\n",
      "mi F1:  0.9015192534318567\n",
      "we F1:  0.8995271149375077\n",
      "[[139  76]\n",
      " [415 864]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.65      0.36       215\n",
      "           1       0.92      0.68      0.78      1279\n",
      "\n",
      "    accuracy                           0.67      1494\n",
      "   macro avg       0.59      0.66      0.57      1494\n",
      "weighted avg       0.82      0.67      0.72      1494\n",
      "\n",
      "acc:  0.6713520749665328\n",
      "pre:  0.9191489361702128\n",
      "rec:  0.6755277560594214\n",
      "ma F1:  0.5701188049069069\n",
      "mi F1:  0.6713520749665328\n",
      "we F1:  0.7186873557254474\n",
      "Loss:  0.06774706393480301\n",
      "Loss:  0.050560470670461655\n",
      "Loss:  0.07587143778800964\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.050274595618247986\n",
      "Loss:  0.04892519861459732\n",
      "Loss:  0.04691591113805771\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.09879525750875473\n",
      "Loss:  0.042192183434963226\n",
      "Loss:  0.08143897354602814\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.013535022735595703\n",
      "Eval Loss:  0.19082331657409668\n",
      "Eval Loss:  0.0131988525390625\n",
      "[[19138   811]\n",
      " [ 2062  9386]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     19949\n",
      "           1       0.92      0.82      0.87     11448\n",
      "\n",
      "    accuracy                           0.91     31397\n",
      "   macro avg       0.91      0.89      0.90     31397\n",
      "weighted avg       0.91      0.91      0.91     31397\n",
      "\n",
      "acc:  0.9084944421441539\n",
      "pre:  0.9204668039619496\n",
      "rec:  0.8198812019566737\n",
      "ma F1:  0.8987239152929691\n",
      "mi F1:  0.9084944421441539\n",
      "we F1:  0.9072410654941504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[121  94]\n",
      " [418 861]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.56      0.32       215\n",
      "           1       0.90      0.67      0.77      1279\n",
      "\n",
      "    accuracy                           0.66      1494\n",
      "   macro avg       0.56      0.62      0.55      1494\n",
      "weighted avg       0.80      0.66      0.71      1494\n",
      "\n",
      "acc:  0.6572958500669344\n",
      "pre:  0.9015706806282723\n",
      "rec:  0.673182173573104\n",
      "ma F1:  0.5458847946731131\n",
      "mi F1:  0.6572958500669344\n",
      "we F1:  0.7060758256717967\n",
      "Loss:  0.0874018520116806\n",
      "Loss:  0.07559148222208023\n",
      "Loss:  0.04703260585665703\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.06538764387369156\n",
      "Loss:  0.053849153220653534\n",
      "Loss:  0.049087829887866974\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.07858777046203613\n",
      "Loss:  0.04460807144641876\n",
      "Loss:  0.0720185860991478\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.014473438262939453\n",
      "Eval Loss:  0.47731223702430725\n",
      "Eval Loss:  0.009962081909179688\n",
      "[[19252   697]\n",
      " [ 2392  9056]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93     19949\n",
      "           1       0.93      0.79      0.85     11448\n",
      "\n",
      "    accuracy                           0.90     31397\n",
      "   macro avg       0.91      0.88      0.89     31397\n",
      "weighted avg       0.90      0.90      0.90     31397\n",
      "\n",
      "acc:  0.901614803962162\n",
      "pre:  0.9285348098021121\n",
      "rec:  0.7910552061495457\n",
      "ma F1:  0.8900160104544954\n",
      "mi F1:  0.901614803962162\n",
      "we F1:  0.8996866012360444\n",
      "[[139  76]\n",
      " [451 828]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.65      0.35       215\n",
      "           1       0.92      0.65      0.76      1279\n",
      "\n",
      "    accuracy                           0.65      1494\n",
      "   macro avg       0.58      0.65      0.55      1494\n",
      "weighted avg       0.82      0.65      0.70      1494\n",
      "\n",
      "acc:  0.6472556894243642\n",
      "pre:  0.915929203539823\n",
      "rec:  0.6473807662236122\n",
      "ma F1:  0.5519653562394904\n",
      "mi F1:  0.6472556894243642\n",
      "we F1:  0.6991190783130835\n",
      "Loss:  0.030692866072058678\n",
      "Loss:  0.0778273418545723\n",
      "Loss:  0.04841959476470947\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.06406228244304657\n",
      "Loss:  0.059745535254478455\n",
      "Loss:  0.06091007590293884\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.05335554853081703\n",
      "Loss:  0.09181603789329529\n",
      "Loss:  0.09784728288650513\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.010412931442260742\n",
      "Eval Loss:  0.2035771608352661\n",
      "Eval Loss:  0.007950782775878906\n",
      "[[19238   711]\n",
      " [ 2214  9234]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     19949\n",
      "           1       0.93      0.81      0.86     11448\n",
      "\n",
      "    accuracy                           0.91     31397\n",
      "   macro avg       0.91      0.89      0.90     31397\n",
      "weighted avg       0.91      0.91      0.91     31397\n",
      "\n",
      "acc:  0.9068382329521929\n",
      "pre:  0.9285067873303168\n",
      "rec:  0.8066037735849056\n",
      "ma F1:  0.8963112829275777\n",
      "mi F1:  0.9068382329521929\n",
      "we F1:  0.9052566649906777\n",
      "[[137  78]\n",
      " [490 789]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.64      0.33       215\n",
      "           1       0.91      0.62      0.74      1279\n",
      "\n",
      "    accuracy                           0.62      1494\n",
      "   macro avg       0.56      0.63      0.53      1494\n",
      "weighted avg       0.81      0.62      0.68      1494\n",
      "\n",
      "acc:  0.6198125836680054\n",
      "pre:  0.9100346020761245\n",
      "rec:  0.6168881939014855\n",
      "ma F1:  0.5303686026922984\n",
      "mi F1:  0.6198125836680054\n",
      "we F1:  0.6763323998673786\n",
      "Loss:  0.05158423259854317\n",
      "Loss:  0.0609734021127224\n",
      "Loss:  0.04865800216794014\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.058029815554618835\n",
      "Loss:  0.07190542668104172\n",
      "Loss:  0.09134875237941742\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.07863229513168335\n",
      "Loss:  0.049123626202344894\n",
      "Loss:  0.0326988622546196\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.008732318878173828\n",
      "Eval Loss:  0.2657804489135742\n",
      "Eval Loss:  0.007830381393432617\n",
      "[[19274   675]\n",
      " [ 2274  9174]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93     19949\n",
      "           1       0.93      0.80      0.86     11448\n",
      "\n",
      "    accuracy                           0.91     31397\n",
      "   macro avg       0.91      0.88      0.90     31397\n",
      "weighted avg       0.91      0.91      0.90     31397\n",
      "\n",
      "acc:  0.9060738287097493\n",
      "pre:  0.931465123362778\n",
      "rec:  0.8013626834381551\n",
      "ma F1:  0.8952322073542347\n",
      "mi F1:  0.9060738287097493\n",
      "we F1:  0.9043574175664357\n",
      "[[121  94]\n",
      " [550 729]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.56      0.27       215\n",
      "           1       0.89      0.57      0.69      1279\n",
      "\n",
      "    accuracy                           0.57      1494\n",
      "   macro avg       0.53      0.57      0.48      1494\n",
      "weighted avg       0.78      0.57      0.63      1494\n",
      "\n",
      "acc:  0.5689424364123159\n",
      "pre:  0.8857837181044957\n",
      "rec:  0.5699765441751369\n",
      "ma F1:  0.4833814082256392\n",
      "mi F1:  0.5689424364123159\n",
      "we F1:  0.6331132075523236\n",
      "Loss:  0.06750641018152237\n",
      "Loss:  0.04869040474295616\n",
      "Loss:  0.07264990359544754\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.0627124086022377\n",
      "Loss:  0.048257067799568176\n",
      "Loss:  0.04216352105140686\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.10452236235141754\n",
      "Loss:  0.05162782967090607\n",
      "Loss:  0.0654197484254837\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.008941411972045898\n",
      "Eval Loss:  0.18561094999313354\n",
      "Eval Loss:  0.010980367660522461\n",
      "[[18824  1125]\n",
      " [ 1560  9888]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19949\n",
      "           1       0.90      0.86      0.88     11448\n",
      "\n",
      "    accuracy                           0.91     31397\n",
      "   macro avg       0.91      0.90      0.91     31397\n",
      "weighted avg       0.91      0.91      0.91     31397\n",
      "\n",
      "acc:  0.9144822753766283\n",
      "pre:  0.8978479978207573\n",
      "rec:  0.8637316561844863\n",
      "ma F1:  0.9069443324817754\n",
      "mi F1:  0.9144822753766283\n",
      "we F1:  0.914115332095601\n",
      "[[ 97 118]\n",
      " [384 895]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.45      0.28       215\n",
      "           1       0.88      0.70      0.78      1279\n",
      "\n",
      "    accuracy                           0.66      1494\n",
      "   macro avg       0.54      0.58      0.53      1494\n",
      "weighted avg       0.79      0.66      0.71      1494\n",
      "\n",
      "acc:  0.6639892904953146\n",
      "pre:  0.8835143139190523\n",
      "rec:  0.6997654417513682\n",
      "ma F1:  0.5298564722874165\n",
      "mi F1:  0.6639892904953146\n",
      "we F1:  0.7087002298979472\n",
      "Loss:  0.04508404806256294\n",
      "Loss:  0.043390847742557526\n",
      "Loss:  0.08740712702274323\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.042367320507764816\n",
      "Loss:  0.04604761302471161\n",
      "Loss:  0.059754978865385056\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.06359576433897018\n",
      "Loss:  0.045721545815467834\n",
      "Loss:  0.03728463500738144\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.009889602661132812\n",
      "Eval Loss:  0.16239506006240845\n",
      "Eval Loss:  0.013908147811889648\n",
      "[[18953   996]\n",
      " [ 1570  9878]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94     19949\n",
      "           1       0.91      0.86      0.89     11448\n",
      "\n",
      "    accuracy                           0.92     31397\n",
      "   macro avg       0.92      0.91      0.91     31397\n",
      "weighted avg       0.92      0.92      0.92     31397\n",
      "\n",
      "acc:  0.9182724464120776\n",
      "pre:  0.9084053706087916\n",
      "rec:  0.8628581411600279\n",
      "ma F1:  0.9108221423720128\n",
      "mi F1:  0.9182724464120776\n",
      "we F1:  0.9178012095504801\n",
      "[[106 109]\n",
      " [437 842]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.49      0.28       215\n",
      "           1       0.89      0.66      0.76      1279\n",
      "\n",
      "    accuracy                           0.63      1494\n",
      "   macro avg       0.54      0.58      0.52      1494\n",
      "weighted avg       0.79      0.63      0.69      1494\n",
      "\n",
      "acc:  0.6345381526104418\n",
      "pre:  0.8853838065194533\n",
      "rec:  0.6583268178264269\n",
      "ma F1:  0.5174201639906765\n",
      "mi F1:  0.6345381526104418\n",
      "we F1:  0.6867320388431633\n",
      "Loss:  0.0572575144469738\n",
      "Loss:  0.09680064022541046\n",
      "Loss:  0.06779658794403076\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.07067074626684189\n",
      "Loss:  0.039649322628974915\n",
      "Loss:  0.07461615651845932\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.02966476045548916\n",
      "Loss:  0.034501124173402786\n",
      "Loss:  0.05678367614746094\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.00849008560180664\n",
      "Eval Loss:  0.1259596347808838\n",
      "Eval Loss:  0.011948108673095703\n",
      "[[19099   850]\n",
      " [ 1794  9654]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.94     19949\n",
      "           1       0.92      0.84      0.88     11448\n",
      "\n",
      "    accuracy                           0.92     31397\n",
      "   macro avg       0.92      0.90      0.91     31397\n",
      "weighted avg       0.92      0.92      0.91     31397\n",
      "\n",
      "acc:  0.915788132624136\n",
      "pre:  0.9190784463061691\n",
      "rec:  0.8432914046121593\n",
      "ma F1:  0.9074090566676418\n",
      "mi F1:  0.915788132624136\n",
      "we F1:  0.914950668600533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[124  91]\n",
      " [533 746]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.19      0.58      0.28       215\n",
      "           1       0.89      0.58      0.71      1279\n",
      "\n",
      "    accuracy                           0.58      1494\n",
      "   macro avg       0.54      0.58      0.49      1494\n",
      "weighted avg       0.79      0.58      0.64      1494\n",
      "\n",
      "acc:  0.5823293172690763\n",
      "pre:  0.8912783751493429\n",
      "rec:  0.5832681782642689\n",
      "ma F1:  0.49475381973951194\n",
      "mi F1:  0.5823293172690763\n",
      "we F1:  0.6445614232305994\n",
      "Loss:  0.04487209767103195\n",
      "Loss:  0.07793769985437393\n",
      "Loss:  0.04713854566216469\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.039512068033218384\n",
      "Loss:  0.0852784737944603\n",
      "Loss:  0.06409326195716858\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.05522563308477402\n",
      "Loss:  0.05346935614943504\n",
      "Loss:  0.05369246378540993\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.009222269058227539\n",
      "Eval Loss:  0.24771004915237427\n",
      "Eval Loss:  0.011503219604492188\n",
      "[[19337   612]\n",
      " [ 2034  9414]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.97      0.94     19949\n",
      "           1       0.94      0.82      0.88     11448\n",
      "\n",
      "    accuracy                           0.92     31397\n",
      "   macro avg       0.92      0.90      0.91     31397\n",
      "weighted avg       0.92      0.92      0.91     31397\n",
      "\n",
      "acc:  0.9157244322705991\n",
      "pre:  0.9389587073608617\n",
      "rec:  0.8223270440251572\n",
      "ma F1:  0.9063722188727564\n",
      "mi F1:  0.9157244322705991\n",
      "we F1:  0.9143842279521741\n",
      "[[150  65]\n",
      " [591 688]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.70      0.31       215\n",
      "           1       0.91      0.54      0.68      1279\n",
      "\n",
      "    accuracy                           0.56      1494\n",
      "   macro avg       0.56      0.62      0.50      1494\n",
      "weighted avg       0.81      0.56      0.62      1494\n",
      "\n",
      "acc:  0.5609103078982597\n",
      "pre:  0.9136786188579017\n",
      "rec:  0.5379202501954652\n",
      "ma F1:  0.4954864428557309\n",
      "mi F1:  0.5609103078982597\n",
      "we F1:  0.6248749045755276\n",
      "Loss:  0.0495641753077507\n",
      "Loss:  0.05213829502463341\n",
      "Loss:  0.06289323419332504\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.07320262491703033\n",
      "Loss:  0.06288804113864899\n",
      "Loss:  0.04954317957162857\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.05216529965400696\n",
      "Loss:  0.03879119083285332\n",
      "Loss:  0.06140821799635887\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.010270833969116211\n",
      "Eval Loss:  0.1764504313468933\n",
      "Eval Loss:  0.01057744026184082\n",
      "[[19272   677]\n",
      " [ 1913  9535]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.97      0.94     19949\n",
      "           1       0.93      0.83      0.88     11448\n",
      "\n",
      "    accuracy                           0.92     31397\n",
      "   macro avg       0.92      0.90      0.91     31397\n",
      "weighted avg       0.92      0.92      0.92     31397\n",
      "\n",
      "acc:  0.917508042169634\n",
      "pre:  0.9337054445750098\n",
      "rec:  0.8328965758211041\n",
      "ma F1:  0.9087299011168192\n",
      "mi F1:  0.917508042169634\n",
      "we F1:  0.9163937582689172\n",
      "[[130  85]\n",
      " [451 828]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.60      0.33       215\n",
      "           1       0.91      0.65      0.76      1279\n",
      "\n",
      "    accuracy                           0.64      1494\n",
      "   macro avg       0.57      0.63      0.54      1494\n",
      "weighted avg       0.81      0.64      0.69      1494\n",
      "\n",
      "acc:  0.6412315930388219\n",
      "pre:  0.9069003285870756\n",
      "rec:  0.6473807662236122\n",
      "ma F1:  0.5410538091919451\n",
      "mi F1:  0.6412315930388219\n",
      "we F1:  0.6937603450272989\n",
      "Loss:  0.06108500808477402\n",
      "Loss:  0.04143107682466507\n",
      "Loss:  0.042471110820770264\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.09178774058818817\n",
      "Loss:  0.04287593066692352\n",
      "Loss:  0.0619644820690155\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.05239307880401611\n",
      "Loss:  0.03889085352420807\n",
      "Loss:  0.038131341338157654\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  11 Training Time 5819.479893922806 Best Test Acc:  0.8145917001338688\n",
      "test subjects:  ['./seg\\\\a17', './seg\\\\x12']\n",
      "*********\n",
      "33301 1012\n",
      "31880 1011\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.8569623231887817\n",
      "Eval Loss:  0.8455221652984619\n",
      "Eval Loss:  0.5813891291618347\n",
      "[[    0 19367]\n",
      " [    0 12513]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     19367\n",
      "           1       0.39      1.00      0.56     12513\n",
      "\n",
      "    accuracy                           0.39     31880\n",
      "   macro avg       0.20      0.50      0.28     31880\n",
      "weighted avg       0.15      0.39      0.22     31880\n",
      "\n",
      "acc:  0.39250313676286075\n",
      "pre:  0.39250313676286075\n",
      "rec:  1.0\n",
      "ma F1:  0.2818687630932805\n",
      "mi F1:  0.3925031367628607\n",
      "we F1:  0.22126874733916055\n",
      "[[  0 797]\n",
      " [  0 214]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       797\n",
      "           1       0.21      1.00      0.35       214\n",
      "\n",
      "    accuracy                           0.21      1011\n",
      "   macro avg       0.11      0.50      0.17      1011\n",
      "weighted avg       0.04      0.21      0.07      1011\n",
      "\n",
      "acc:  0.21167161226508407\n",
      "pre:  0.21167161226508407\n",
      "rec:  1.0\n",
      "ma F1:  0.1746938775510204\n",
      "mi F1:  0.21167161226508407\n",
      "we F1:  0.07395546942812732\n",
      "Subject 12 Current Train Acc:  0.39250313676286075 Current Test Acc:  0.21167161226508407\n",
      "Loss:  0.16976439952850342\n",
      "Loss:  0.15716202557086945\n",
      "Loss:  0.1591440737247467\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.14946773648262024\n",
      "Loss:  0.11751352250576019\n",
      "Loss:  0.11030208319425583\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.1225474402308464\n",
      "Loss:  0.09474135935306549\n",
      "Loss:  0.08981184661388397\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.06413674354553223\n",
      "Eval Loss:  0.031169891357421875\n",
      "Eval Loss:  0.33851808309555054\n",
      "[[18087  1280]\n",
      " [ 4957  7556]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85     19367\n",
      "           1       0.86      0.60      0.71     12513\n",
      "\n",
      "    accuracy                           0.80     31880\n",
      "   macro avg       0.82      0.77      0.78     31880\n",
      "weighted avg       0.81      0.80      0.80     31880\n",
      "\n",
      "acc:  0.8043601003764116\n",
      "pre:  0.8551380715255772\n",
      "rec:  0.6038519939263166\n",
      "ma F1:  0.7803971324248081\n",
      "mi F1:  0.8043601003764116\n",
      "we F1:  0.7959931994972885\n",
      "[[772  25]\n",
      " [166  48]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89       797\n",
      "           1       0.66      0.22      0.33       214\n",
      "\n",
      "    accuracy                           0.81      1011\n",
      "   macro avg       0.74      0.60      0.61      1011\n",
      "weighted avg       0.79      0.81      0.77      1011\n",
      "\n",
      "acc:  0.811078140454995\n",
      "pre:  0.6575342465753424\n",
      "rec:  0.22429906542056074\n",
      "ma F1:  0.6122041590938758\n",
      "mi F1:  0.811078140454995\n",
      "we F1:  0.7723471578970975\n",
      "Subject 12 Current Train Acc:  0.8043601003764116 Current Test Acc:  0.811078140454995\n",
      "Loss:  0.10140614211559296\n",
      "Loss:  0.09690303355455399\n",
      "Loss:  0.12834177911281586\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.08501196652650833\n",
      "Loss:  0.09539107978343964\n",
      "Loss:  0.1110333576798439\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.07692614197731018\n",
      "Loss:  0.07910388708114624\n",
      "Loss:  0.11138965934515\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.05507373809814453\n",
      "Eval Loss:  0.011942863464355469\n",
      "Eval Loss:  0.21421372890472412\n",
      "[[18669   698]\n",
      " [ 4636  7877]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.96      0.88     19367\n",
      "           1       0.92      0.63      0.75     12513\n",
      "\n",
      "    accuracy                           0.83     31880\n",
      "   macro avg       0.86      0.80      0.81     31880\n",
      "weighted avg       0.85      0.83      0.82     31880\n",
      "\n",
      "acc:  0.8326850690087829\n",
      "pre:  0.9186005830903791\n",
      "rec:  0.6295053144729481\n",
      "ma F1:  0.8110299696509863\n",
      "mi F1:  0.8326850690087829\n",
      "we F1:  0.8247831248583936\n",
      "[[784  13]\n",
      " [146  68]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.98      0.91       797\n",
      "           1       0.84      0.32      0.46       214\n",
      "\n",
      "    accuracy                           0.84      1011\n",
      "   macro avg       0.84      0.65      0.68      1011\n",
      "weighted avg       0.84      0.84      0.81      1011\n",
      "\n",
      "acc:  0.8427299703264095\n",
      "pre:  0.8395061728395061\n",
      "rec:  0.3177570093457944\n",
      "ma F1:  0.6844748903261264\n",
      "mi F1:  0.8427299703264095\n",
      "we F1:  0.8133334261364128\n",
      "Subject 12 Current Train Acc:  0.8326850690087829 Current Test Acc:  0.8427299703264095\n",
      "Loss:  0.11766061186790466\n",
      "Loss:  0.08642055094242096\n",
      "Loss:  0.08898520469665527\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.07474128901958466\n",
      "Loss:  0.09074162691831589\n",
      "Loss:  0.08410042524337769\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.0843958705663681\n",
      "Loss:  0.07271745800971985\n",
      "Loss:  0.08667334169149399\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.0600581169128418\n",
      "Eval Loss:  0.008627653121948242\n",
      "Eval Loss:  0.24295932054519653\n",
      "[[18756   611]\n",
      " [ 4292  8221]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.97      0.88     19367\n",
      "           1       0.93      0.66      0.77     12513\n",
      "\n",
      "    accuracy                           0.85     31880\n",
      "   macro avg       0.87      0.81      0.83     31880\n",
      "weighted avg       0.86      0.85      0.84     31880\n",
      "\n",
      "acc:  0.8462045169385195\n",
      "pre:  0.9308197463768116\n",
      "rec:  0.656996723407656\n",
      "ma F1:  0.827350797940251\n",
      "mi F1:  0.8462045169385195\n",
      "we F1:  0.8396169004570172\n",
      "[[784  13]\n",
      " [135  79]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91       797\n",
      "           1       0.86      0.37      0.52       214\n",
      "\n",
      "    accuracy                           0.85      1011\n",
      "   macro avg       0.86      0.68      0.72      1011\n",
      "weighted avg       0.85      0.85      0.83      1011\n",
      "\n",
      "acc:  0.8536102868447082\n",
      "pre:  0.8586956521739131\n",
      "rec:  0.3691588785046729\n",
      "ma F1:  0.7150463915169798\n",
      "mi F1:  0.8536102868447082\n",
      "we F1:  0.8296318538943779\n",
      "Subject 12 Current Train Acc:  0.8462045169385195 Current Test Acc:  0.8536102868447082\n",
      "Loss:  0.07049977779388428\n",
      "Loss:  0.07088037580251694\n",
      "Loss:  0.09542777389287949\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.060178276151418686\n",
      "Loss:  0.061454784125089645\n",
      "Loss:  0.04756808653473854\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.094204381108284\n",
      "Loss:  0.08225685358047485\n",
      "Loss:  0.11018632352352142\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.03715026378631592\n",
      "Eval Loss:  0.008137226104736328\n",
      "Eval Loss:  0.2616632580757141\n",
      "[[18803   564]\n",
      " [ 4124  8389]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89     19367\n",
      "           1       0.94      0.67      0.78     12513\n",
      "\n",
      "    accuracy                           0.85     31880\n",
      "   macro avg       0.88      0.82      0.84     31880\n",
      "weighted avg       0.87      0.85      0.85     31880\n",
      "\n",
      "acc:  0.852948557089084\n",
      "pre:  0.9370043560817604\n",
      "rec:  0.6704227603292576\n",
      "ma F1:  0.8353824894380334\n",
      "mi F1:  0.852948557089084\n",
      "we F1:  0.846943640550027\n",
      "[[782  15]\n",
      " [127  87]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.98      0.92       797\n",
      "           1       0.85      0.41      0.55       214\n",
      "\n",
      "    accuracy                           0.86      1011\n",
      "   macro avg       0.86      0.69      0.73      1011\n",
      "weighted avg       0.86      0.86      0.84      1011\n",
      "\n",
      "acc:  0.8595450049455984\n",
      "pre:  0.8529411764705882\n",
      "rec:  0.40654205607476634\n",
      "ma F1:  0.7336986362354756\n",
      "mi F1:  0.8595450049455984\n",
      "we F1:  0.8392647268225282\n",
      "Subject 12 Current Train Acc:  0.852948557089084 Current Test Acc:  0.8595450049455984\n",
      "Loss:  0.0769517794251442\n",
      "Loss:  0.09843984246253967\n",
      "Loss:  0.057643428444862366\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.06364540755748749\n",
      "Loss:  0.07026754319667816\n",
      "Loss:  0.09216083586215973\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.0756300613284111\n",
      "Loss:  0.09372760355472565\n",
      "Loss:  0.06188805028796196\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.07365775108337402\n",
      "Eval Loss:  0.007471799850463867\n",
      "Eval Loss:  0.3302907347679138\n",
      "[[18540   827]\n",
      " [ 3152  9361]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90     19367\n",
      "           1       0.92      0.75      0.82     12513\n",
      "\n",
      "    accuracy                           0.88     31880\n",
      "   macro avg       0.89      0.85      0.86     31880\n",
      "weighted avg       0.88      0.88      0.87     31880\n",
      "\n",
      "acc:  0.8751882057716437\n",
      "pre:  0.9188260698861406\n",
      "rec:  0.748101973947095\n",
      "ma F1:  0.8639060261541247\n",
      "mi F1:  0.8751882057716437\n",
      "we F1:  0.8723304797000965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[777  20]\n",
      " [108 106]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92       797\n",
      "           1       0.84      0.50      0.62       214\n",
      "\n",
      "    accuracy                           0.87      1011\n",
      "   macro avg       0.86      0.74      0.77      1011\n",
      "weighted avg       0.87      0.87      0.86      1011\n",
      "\n",
      "acc:  0.8733926805143423\n",
      "pre:  0.8412698412698413\n",
      "rec:  0.4953271028037383\n",
      "ma F1:  0.7737147653353851\n",
      "mi F1:  0.8733926805143423\n",
      "we F1:  0.8603201670482495\n",
      "Subject 12 Current Train Acc:  0.8751882057716437 Current Test Acc:  0.8733926805143423\n",
      "Loss:  0.06810156255960464\n",
      "Loss:  0.05422763153910637\n",
      "Loss:  0.08470087498426437\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.06537051498889923\n",
      "Loss:  0.045416153967380524\n",
      "Loss:  0.07487471401691437\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.07553337514400482\n",
      "Loss:  0.09082544595003128\n",
      "Loss:  0.0915982574224472\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.08614718914031982\n",
      "Eval Loss:  0.008637666702270508\n",
      "Eval Loss:  0.4178217351436615\n",
      "[[18518   849]\n",
      " [ 3131  9382]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.90     19367\n",
      "           1       0.92      0.75      0.83     12513\n",
      "\n",
      "    accuracy                           0.88     31880\n",
      "   macro avg       0.89      0.85      0.86     31880\n",
      "weighted avg       0.88      0.88      0.87     31880\n",
      "\n",
      "acc:  0.8751568381430364\n",
      "pre:  0.9170169093930212\n",
      "rec:  0.7497802285622952\n",
      "ma F1:  0.8639867451158444\n",
      "mi F1:  0.8751568381430364\n",
      "we F1:  0.8723667601780569\n",
      "[[782  15]\n",
      " [114 100]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92       797\n",
      "           1       0.87      0.47      0.61       214\n",
      "\n",
      "    accuracy                           0.87      1011\n",
      "   macro avg       0.87      0.72      0.77      1011\n",
      "weighted avg       0.87      0.87      0.86      1011\n",
      "\n",
      "acc:  0.8724035608308606\n",
      "pre:  0.8695652173913043\n",
      "rec:  0.4672897196261682\n",
      "ma F1:  0.7658533169837539\n",
      "mi F1:  0.8724035608308606\n",
      "we F1:  0.8569365899498288\n",
      "Loss:  0.0809260681271553\n",
      "Loss:  0.038739606738090515\n",
      "Loss:  0.06772873550653458\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.07630905508995056\n",
      "Loss:  0.09741687029600143\n",
      "Loss:  0.07617249339818954\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.059496838599443436\n",
      "Loss:  0.06298608332872391\n",
      "Loss:  0.07505559176206589\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.03415358066558838\n",
      "Eval Loss:  0.011687040328979492\n",
      "Eval Loss:  0.66029953956604\n",
      "[[18787   580]\n",
      " [ 3600  8913]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.97      0.90     19367\n",
      "           1       0.94      0.71      0.81     12513\n",
      "\n",
      "    accuracy                           0.87     31880\n",
      "   macro avg       0.89      0.84      0.85     31880\n",
      "weighted avg       0.88      0.87      0.86     31880\n",
      "\n",
      "acc:  0.8688833124215809\n",
      "pre:  0.9389023490993363\n",
      "rec:  0.7122992088228243\n",
      "ma F1:  0.8549708174839217\n",
      "mi F1:  0.8688833124215809\n",
      "we F1:  0.8646281235708891\n",
      "[[779  18]\n",
      " [126  88]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.98      0.92       797\n",
      "           1       0.83      0.41      0.55       214\n",
      "\n",
      "    accuracy                           0.86      1011\n",
      "   macro avg       0.85      0.69      0.73      1011\n",
      "weighted avg       0.85      0.86      0.84      1011\n",
      "\n",
      "acc:  0.857566765578635\n",
      "pre:  0.8301886792452831\n",
      "rec:  0.411214953271028\n",
      "ma F1:  0.7326968272620447\n",
      "mi F1:  0.857566765578635\n",
      "we F1:  0.8380501905595442\n",
      "Loss:  0.06242384761571884\n",
      "Loss:  0.0871523767709732\n",
      "Loss:  0.05397998169064522\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.0706135481595993\n",
      "Loss:  0.06279004365205765\n",
      "Loss:  0.04097771272063255\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.0704168751835823\n",
      "Loss:  0.0916120633482933\n",
      "Loss:  0.06174737960100174\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.052919626235961914\n",
      "Eval Loss:  0.012610197067260742\n",
      "Eval Loss:  0.19767606258392334\n",
      "[[18133  1234]\n",
      " [ 2022 10491]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     19367\n",
      "           1       0.89      0.84      0.87     12513\n",
      "\n",
      "    accuracy                           0.90     31880\n",
      "   macro avg       0.90      0.89      0.89     31880\n",
      "weighted avg       0.90      0.90      0.90     31880\n",
      "\n",
      "acc:  0.8978670012547052\n",
      "pre:  0.8947547974413647\n",
      "rec:  0.8384080556221529\n",
      "ma F1:  0.8916404946195073\n",
      "mi F1:  0.8978670012547052\n",
      "we F1:  0.8972249589583775\n",
      "[[732  65]\n",
      " [ 70 144]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92       797\n",
      "           1       0.69      0.67      0.68       214\n",
      "\n",
      "    accuracy                           0.87      1011\n",
      "   macro avg       0.80      0.80      0.80      1011\n",
      "weighted avg       0.87      0.87      0.87      1011\n",
      "\n",
      "acc:  0.8664688427299704\n",
      "pre:  0.6889952153110048\n",
      "rec:  0.6728971962616822\n",
      "ma F1:  0.7982116482375954\n",
      "mi F1:  0.8664688427299704\n",
      "we F1:  0.8658884244094569\n",
      "Loss:  0.06947621703147888\n",
      "Loss:  0.04863275587558746\n",
      "Loss:  0.07200956344604492\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.046599697321653366\n",
      "Loss:  0.0952536016702652\n",
      "Loss:  0.07083002477884293\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.06112983822822571\n",
      "Loss:  0.09046237915754318\n",
      "Loss:  0.08122263103723526\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.02279365062713623\n",
      "Eval Loss:  0.012744426727294922\n",
      "Eval Loss:  0.26922905445098877\n",
      "[[18305  1062]\n",
      " [ 2085 10428]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     19367\n",
      "           1       0.91      0.83      0.87     12513\n",
      "\n",
      "    accuracy                           0.90     31880\n",
      "   macro avg       0.90      0.89      0.89     31880\n",
      "weighted avg       0.90      0.90      0.90     31880\n",
      "\n",
      "acc:  0.9012860727728984\n",
      "pre:  0.9075718015665797\n",
      "rec:  0.8333732917765524\n",
      "ma F1:  0.8948677583272511\n",
      "mi F1:  0.9012860727728984\n",
      "we F1:  0.9004525148602542\n",
      "[[751  46]\n",
      " [ 80 134]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92       797\n",
      "           1       0.74      0.63      0.68       214\n",
      "\n",
      "    accuracy                           0.88      1011\n",
      "   macro avg       0.82      0.78      0.80      1011\n",
      "weighted avg       0.87      0.88      0.87      1011\n",
      "\n",
      "acc:  0.8753709198813057\n",
      "pre:  0.7444444444444445\n",
      "rec:  0.6261682242990654\n",
      "ma F1:  0.8014037341448508\n",
      "mi F1:  0.8753709198813057\n",
      "we F1:  0.8712949323366711\n",
      "Subject 12 Current Train Acc:  0.9012860727728984 Current Test Acc:  0.8753709198813057\n",
      "Loss:  0.06785327196121216\n",
      "Loss:  0.059081029146909714\n",
      "Loss:  0.09602632373571396\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.0652657300233841\n",
      "Loss:  0.07147300243377686\n",
      "Loss:  0.0705394595861435\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.06769438087940216\n",
      "Loss:  0.043096788227558136\n",
      "Loss:  0.05459229648113251\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.03093576431274414\n",
      "Eval Loss:  0.012969970703125\n",
      "Eval Loss:  0.3922731876373291\n",
      "[[18635   732]\n",
      " [ 2760  9753]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91     19367\n",
      "           1       0.93      0.78      0.85     12513\n",
      "\n",
      "    accuracy                           0.89     31880\n",
      "   macro avg       0.90      0.87      0.88     31880\n",
      "weighted avg       0.89      0.89      0.89     31880\n",
      "\n",
      "acc:  0.8904642409033877\n",
      "pre:  0.9301859799713877\n",
      "rec:  0.779429393430832\n",
      "ma F1:  0.8812463427433967\n",
      "mi F1:  0.8904642409033877\n",
      "we F1:  0.888359546299868\n",
      "[[763  34]\n",
      " [ 92 122]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92       797\n",
      "           1       0.78      0.57      0.66       214\n",
      "\n",
      "    accuracy                           0.88      1011\n",
      "   macro avg       0.84      0.76      0.79      1011\n",
      "weighted avg       0.87      0.88      0.87      1011\n",
      "\n",
      "acc:  0.8753709198813057\n",
      "pre:  0.782051282051282\n",
      "rec:  0.5700934579439252\n",
      "ma F1:  0.7915941365093907\n",
      "mi F1:  0.8753709198813057\n",
      "we F1:  0.8677904933047518\n",
      "Loss:  0.0759928971529007\n",
      "Loss:  0.061262812465429306\n",
      "Loss:  0.051940951496362686\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.05657871812582016\n",
      "Loss:  0.05134120583534241\n",
      "Loss:  0.08372975885868073\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.0667835995554924\n",
      "Loss:  0.09294059872627258\n",
      "Loss:  0.07797661423683167\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.018924474716186523\n",
      "Eval Loss:  0.01617264747619629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss:  0.2076776623725891\n",
      "[[18566   801]\n",
      " [ 2209 10304]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.93     19367\n",
      "           1       0.93      0.82      0.87     12513\n",
      "\n",
      "    accuracy                           0.91     31880\n",
      "   macro avg       0.91      0.89      0.90     31880\n",
      "weighted avg       0.91      0.91      0.90     31880\n",
      "\n",
      "acc:  0.9055834378920954\n",
      "pre:  0.9278703286807745\n",
      "rec:  0.8234635978582274\n",
      "ma F1:  0.8987855117888107\n",
      "mi F1:  0.9055834378920954\n",
      "we F1:  0.9044249435864279\n",
      "[[737  60]\n",
      " [ 68 146]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       797\n",
      "           1       0.71      0.68      0.70       214\n",
      "\n",
      "    accuracy                           0.87      1011\n",
      "   macro avg       0.81      0.80      0.81      1011\n",
      "weighted avg       0.87      0.87      0.87      1011\n",
      "\n",
      "acc:  0.8733926805143423\n",
      "pre:  0.7087378640776699\n",
      "rec:  0.6822429906542056\n",
      "ma F1:  0.8076689851970751\n",
      "mi F1:  0.8733926805143423\n",
      "we F1:  0.8725030196640239\n",
      "Loss:  0.054565366357564926\n",
      "Loss:  0.09334874153137207\n",
      "Loss:  0.062344226986169815\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.07476656883955002\n",
      "Loss:  0.07298646867275238\n",
      "Loss:  0.06132980436086655\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.06160397455096245\n",
      "Loss:  0.07082604616880417\n",
      "Loss:  0.06409050524234772\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.018570423126220703\n",
      "Eval Loss:  0.019422173500061035\n",
      "Eval Loss:  0.17455244064331055\n",
      "[[18354  1013]\n",
      " [ 1795 10718]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19367\n",
      "           1       0.91      0.86      0.88     12513\n",
      "\n",
      "    accuracy                           0.91     31880\n",
      "   macro avg       0.91      0.90      0.91     31880\n",
      "weighted avg       0.91      0.91      0.91     31880\n",
      "\n",
      "acc:  0.9119196988707654\n",
      "pre:  0.9136476003750746\n",
      "rec:  0.8565491888436026\n",
      "ma F1:  0.9065588522959187\n",
      "mi F1:  0.9119196988707654\n",
      "we F1:  0.9113706965106907\n",
      "[[740  57]\n",
      " [ 68 146]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92       797\n",
      "           1       0.72      0.68      0.70       214\n",
      "\n",
      "    accuracy                           0.88      1011\n",
      "   macro avg       0.82      0.81      0.81      1011\n",
      "weighted avg       0.87      0.88      0.88      1011\n",
      "\n",
      "acc:  0.8763600395647874\n",
      "pre:  0.7192118226600985\n",
      "rec:  0.6822429906542056\n",
      "ma F1:  0.8111790941078912\n",
      "mi F1:  0.8763600395647874\n",
      "we F1:  0.8751529850192893\n",
      "Subject 12 Current Train Acc:  0.9119196988707654 Current Test Acc:  0.8763600395647874\n",
      "Loss:  0.0605144277215004\n",
      "Loss:  0.05125341936945915\n",
      "Loss:  0.06815430521965027\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.08947103470563889\n",
      "Loss:  0.04084053635597229\n",
      "Loss:  0.05887977033853531\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.06613025069236755\n",
      "Loss:  0.04974941536784172\n",
      "Loss:  0.06933405250310898\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.020648717880249023\n",
      "Eval Loss:  0.022486329078674316\n",
      "Eval Loss:  0.1339552402496338\n",
      "[[18219  1148]\n",
      " [ 1581 10932]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19367\n",
      "           1       0.90      0.87      0.89     12513\n",
      "\n",
      "    accuracy                           0.91     31880\n",
      "   macro avg       0.91      0.91      0.91     31880\n",
      "weighted avg       0.91      0.91      0.91     31880\n",
      "\n",
      "acc:  0.9143977415307403\n",
      "pre:  0.9049668874172185\n",
      "rec:  0.873651402541357\n",
      "ma F1:  0.9096787310248178\n",
      "mi F1:  0.9143977415307403\n",
      "we F1:  0.9141173337430274\n",
      "[[729  68]\n",
      " [ 61 153]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92       797\n",
      "           1       0.69      0.71      0.70       214\n",
      "\n",
      "    accuracy                           0.87      1011\n",
      "   macro avg       0.81      0.81      0.81      1011\n",
      "weighted avg       0.87      0.87      0.87      1011\n",
      "\n",
      "acc:  0.8724035608308606\n",
      "pre:  0.6923076923076923\n",
      "rec:  0.7149532710280374\n",
      "ma F1:  0.8110814158138322\n",
      "mi F1:  0.8724035608308606\n",
      "we F1:  0.8731487952321091\n",
      "Loss:  0.04989154264330864\n",
      "Loss:  0.06282062083482742\n",
      "Loss:  0.07121579349040985\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.038331806659698486\n",
      "Loss:  0.06579912453889847\n",
      "Loss:  0.07083608955144882\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.07340092957019806\n",
      "Loss:  0.04041336104273796\n",
      "Loss:  0.061371639370918274\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.018281936645507812\n",
      "Eval Loss:  0.03323197364807129\n",
      "Eval Loss:  0.08711457252502441\n",
      "[[18094  1273]\n",
      " [ 1515 10998]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93     19367\n",
      "           1       0.90      0.88      0.89     12513\n",
      "\n",
      "    accuracy                           0.91     31880\n",
      "   macro avg       0.91      0.91      0.91     31880\n",
      "weighted avg       0.91      0.91      0.91     31880\n",
      "\n",
      "acc:  0.9125470514429109\n",
      "pre:  0.8962594735555375\n",
      "rec:  0.8789259170462719\n",
      "ma F1:  0.9079884355180152\n",
      "mi F1:  0.9125470514429109\n",
      "we F1:  0.9123915856799705\n",
      "[[726  71]\n",
      " [ 63 151]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92       797\n",
      "           1       0.68      0.71      0.69       214\n",
      "\n",
      "    accuracy                           0.87      1011\n",
      "   macro avg       0.80      0.81      0.80      1011\n",
      "weighted avg       0.87      0.87      0.87      1011\n",
      "\n",
      "acc:  0.8674579624134521\n",
      "pre:  0.6801801801801802\n",
      "rec:  0.705607476635514\n",
      "ma F1:  0.8040856346240615\n",
      "mi F1:  0.8674579624134521\n",
      "we F1:  0.8683396643653044\n",
      "Loss:  0.052717987447977066\n",
      "Loss:  0.07509800046682358\n",
      "Loss:  0.08787467330694199\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.057213712483644485\n",
      "Loss:  0.06942662596702576\n",
      "Loss:  0.04223942011594772\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.04863898083567619\n",
      "Loss:  0.03061676397919655\n",
      "Loss:  0.04725015163421631\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.029724597930908203\n",
      "Eval Loss:  0.029223084449768066\n",
      "Eval Loss:  0.3694048523902893\n",
      "[[18246  1121]\n",
      " [ 1593 10920]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19367\n",
      "           1       0.91      0.87      0.89     12513\n",
      "\n",
      "    accuracy                           0.91     31880\n",
      "   macro avg       0.91      0.91      0.91     31880\n",
      "weighted avg       0.91      0.91      0.91     31880\n",
      "\n",
      "acc:  0.9148682559598494\n",
      "pre:  0.9069014201478283\n",
      "rec:  0.8726923999040997\n",
      "ma F1:  0.9101220063748838\n",
      "mi F1:  0.9148682559598494\n",
      "we F1:  0.9145624642857976\n",
      "[[736  61]\n",
      " [ 66 148]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92       797\n",
      "           1       0.71      0.69      0.70       214\n",
      "\n",
      "    accuracy                           0.87      1011\n",
      "   macro avg       0.81      0.81      0.81      1011\n",
      "weighted avg       0.87      0.87      0.87      1011\n",
      "\n",
      "acc:  0.874381800197824\n",
      "pre:  0.7081339712918661\n",
      "rec:  0.6915887850467289\n",
      "ma F1:  0.8101694764901822\n",
      "mi F1:  0.874381800197824\n",
      "we F1:  0.8738357770370446\n",
      "Loss:  0.07591801881790161\n",
      "Loss:  0.05940026417374611\n",
      "Loss:  0.058537330478429794\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.0740109235048294\n",
      "Loss:  0.07999821752309799\n",
      "Loss:  0.0435432605445385\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.07110639661550522\n",
      "Loss:  0.06771437078714371\n",
      "Loss:  0.07027718424797058\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.03318226337432861\n",
      "Eval Loss:  0.01997244358062744\n",
      "Eval Loss:  0.06580102443695068\n",
      "[[17893  1474]\n",
      " [ 1319 11194]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93     19367\n",
      "           1       0.88      0.89      0.89     12513\n",
      "\n",
      "    accuracy                           0.91     31880\n",
      "   macro avg       0.91      0.91      0.91     31880\n",
      "weighted avg       0.91      0.91      0.91     31880\n",
      "\n",
      "acc:  0.9123902132998746\n",
      "pre:  0.8836438269655825\n",
      "rec:  0.8945896267881404\n",
      "ma F1:  0.9083430694656454\n",
      "mi F1:  0.9123902132998746\n",
      "we F1:  0.912483855230656\n",
      "[[751  46]\n",
      " [ 72 142]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93       797\n",
      "           1       0.76      0.66      0.71       214\n",
      "\n",
      "    accuracy                           0.88      1011\n",
      "   macro avg       0.83      0.80      0.82      1011\n",
      "weighted avg       0.88      0.88      0.88      1011\n",
      "\n",
      "acc:  0.8832838773491593\n",
      "pre:  0.7553191489361702\n",
      "rec:  0.6635514018691588\n",
      "ma F1:  0.8168140777593514\n",
      "mi F1:  0.8832838773491593\n",
      "we F1:  0.8804460862336667\n",
      "Subject 12 Current Train Acc:  0.9123902132998746 Current Test Acc:  0.8832838773491593\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.037976041436195374\n",
      "Loss:  0.06389830261468887\n",
      "Loss:  0.04482043907046318\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.06438886374235153\n",
      "Loss:  0.033090218901634216\n",
      "Loss:  0.059394821524620056\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.07138019055128098\n",
      "Loss:  0.03927168622612953\n",
      "Loss:  0.0944070890545845\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.02177739143371582\n",
      "Eval Loss:  0.021825194358825684\n",
      "Eval Loss:  0.19016969203948975\n",
      "[[18496   871]\n",
      " [ 1751 10762]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93     19367\n",
      "           1       0.93      0.86      0.89     12513\n",
      "\n",
      "    accuracy                           0.92     31880\n",
      "   macro avg       0.92      0.91      0.91     31880\n",
      "weighted avg       0.92      0.92      0.92     31880\n",
      "\n",
      "acc:  0.9177540777917189\n",
      "pre:  0.9251267944640248\n",
      "rec:  0.8600655318468793\n",
      "ma F1:  0.9126109322225273\n",
      "mi F1:  0.9177540777917189\n",
      "we F1:  0.917168873744539\n",
      "[[745  52]\n",
      " [ 73 141]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92       797\n",
      "           1       0.73      0.66      0.69       214\n",
      "\n",
      "    accuracy                           0.88      1011\n",
      "   macro avg       0.82      0.80      0.81      1011\n",
      "weighted avg       0.87      0.88      0.87      1011\n",
      "\n",
      "acc:  0.8763600395647874\n",
      "pre:  0.7305699481865285\n",
      "rec:  0.6588785046728972\n",
      "ma F1:  0.8077376560348697\n",
      "mi F1:  0.8763600395647874\n",
      "we F1:  0.8739741619917274\n",
      "Loss:  0.06420078128576279\n",
      "Loss:  0.07323209941387177\n",
      "Loss:  0.07298171520233154\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.04389418289065361\n",
      "Loss:  0.040479183197021484\n",
      "Loss:  0.04801933839917183\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.03482078015804291\n",
      "Loss:  0.0538754016160965\n",
      "Loss:  0.07262270152568817\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.020935416221618652\n",
      "Eval Loss:  0.02960371971130371\n",
      "Eval Loss:  0.050133347511291504\n",
      "[[18213  1154]\n",
      " [ 1578 10935]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19367\n",
      "           1       0.90      0.87      0.89     12513\n",
      "\n",
      "    accuracy                           0.91     31880\n",
      "   macro avg       0.91      0.91      0.91     31880\n",
      "weighted avg       0.91      0.91      0.91     31880\n",
      "\n",
      "acc:  0.9143036386449185\n",
      "pre:  0.9045413185540574\n",
      "rec:  0.8738911532006713\n",
      "ma F1:  0.9095917440298928\n",
      "mi F1:  0.9143036386449185\n",
      "we F1:  0.9140291342045818\n",
      "[[746  51]\n",
      " [ 81 133]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92       797\n",
      "           1       0.72      0.62      0.67       214\n",
      "\n",
      "    accuracy                           0.87      1011\n",
      "   macro avg       0.81      0.78      0.79      1011\n",
      "weighted avg       0.86      0.87      0.87      1011\n",
      "\n",
      "acc:  0.8694362017804155\n",
      "pre:  0.7228260869565217\n",
      "rec:  0.6214953271028038\n",
      "ma F1:  0.7935304601826868\n",
      "mi F1:  0.8694362017804155\n",
      "we F1:  0.8657214020284875\n",
      "Loss:  0.06819813698530197\n",
      "Loss:  0.05323541536927223\n",
      "Loss:  0.043438225984573364\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.08853518962860107\n",
      "Loss:  0.0763210728764534\n",
      "Loss:  0.04530343413352966\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.06447675824165344\n",
      "Loss:  0.03909455984830856\n",
      "Loss:  0.03870081901550293\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.09866797924041748\n",
      "Eval Loss:  0.02962625026702881\n",
      "Eval Loss:  0.10353422164916992\n",
      "[[17802  1565]\n",
      " [ 1017 11496]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.93     19367\n",
      "           1       0.88      0.92      0.90     12513\n",
      "\n",
      "    accuracy                           0.92     31880\n",
      "   macro avg       0.91      0.92      0.92     31880\n",
      "weighted avg       0.92      0.92      0.92     31880\n",
      "\n",
      "acc:  0.9190087829360101\n",
      "pre:  0.8801776280529822\n",
      "rec:  0.9187245264924478\n",
      "ma F1:  0.9157108408189882\n",
      "mi F1:  0.9190087829360101\n",
      "we F1:  0.9192953786036484\n",
      "[[685 112]\n",
      " [ 55 159]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.86      0.89       797\n",
      "           1       0.59      0.74      0.66       214\n",
      "\n",
      "    accuracy                           0.83      1011\n",
      "   macro avg       0.76      0.80      0.77      1011\n",
      "weighted avg       0.85      0.83      0.84      1011\n",
      "\n",
      "acc:  0.8348170128585559\n",
      "pre:  0.5867158671586716\n",
      "rec:  0.7429906542056075\n",
      "ma F1:  0.773508441266626\n",
      "mi F1:  0.8348170128585559\n",
      "we F1:  0.8414607173846776\n",
      "Loss:  0.07536017149686813\n",
      "Loss:  0.06839796155691147\n",
      "Loss:  0.057178936898708344\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.047945477068424225\n",
      "Loss:  0.028195064514875412\n",
      "Loss:  0.06773952394723892\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.032020773738622665\n",
      "Loss:  0.07326779514551163\n",
      "Loss:  0.057628847658634186\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  12 Training Time 5823.405604839325 Best Test Acc:  0.8832838773491593\n",
      "test subjects:  ['./seg\\\\a19', './seg\\\\x05', './seg\\\\x08', './seg\\\\x25']\n",
      "*********\n",
      "32279 2034\n",
      "30857 2034\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.798627495765686\n",
      "Eval Loss:  0.7970882654190063\n",
      "Eval Loss:  0.7846294045448303\n",
      "[[    0 19266]\n",
      " [    0 11591]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     19266\n",
      "           1       0.38      1.00      0.55     11591\n",
      "\n",
      "    accuracy                           0.38     30857\n",
      "   macro avg       0.19      0.50      0.27     30857\n",
      "weighted avg       0.14      0.38      0.21     30857\n",
      "\n",
      "acc:  0.37563599831480704\n",
      "pre:  0.37563599831480704\n",
      "rec:  1.0\n",
      "ma F1:  0.2730635130041463\n",
      "mi F1:  0.3756359983148071\n",
      "we F1:  0.20514497062132153\n",
      "[[   0  898]\n",
      " [   0 1136]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       898\n",
      "           1       0.56      1.00      0.72      1136\n",
      "\n",
      "    accuracy                           0.56      2034\n",
      "   macro avg       0.28      0.50      0.36      2034\n",
      "weighted avg       0.31      0.56      0.40      2034\n",
      "\n",
      "acc:  0.5585054080629301\n",
      "pre:  0.5585054080629301\n",
      "rec:  1.0\n",
      "ma F1:  0.35835962145110406\n",
      "mi F1:  0.5585054080629301\n",
      "we F1:  0.4002915732236521\n",
      "Subject 13 Current Train Acc:  0.37563599831480704 Current Test Acc:  0.5585054080629301\n",
      "Loss:  0.1717795431613922\n",
      "Loss:  0.16511940956115723\n",
      "Loss:  0.16288061439990997\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.14051401615142822\n",
      "Loss:  0.13488194346427917\n",
      "Loss:  0.1180691197514534\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.107870914041996\n",
      "Loss:  0.10538594424724579\n",
      "Loss:  0.12636040151119232\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.02662980556488037\n",
      "Eval Loss:  0.055403709411621094\n",
      "Eval Loss:  1.205906867980957\n",
      "[[17522  1744]\n",
      " [ 3631  7960]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.91      0.87     19266\n",
      "           1       0.82      0.69      0.75     11591\n",
      "\n",
      "    accuracy                           0.83     30857\n",
      "   macro avg       0.82      0.80      0.81     30857\n",
      "weighted avg       0.83      0.83      0.82     30857\n",
      "\n",
      "acc:  0.8258093787471238\n",
      "pre:  0.8202802967848309\n",
      "rec:  0.6867397118453973\n",
      "ma F1:  0.8073056591792428\n",
      "mi F1:  0.8258093787471238\n",
      "we F1:  0.8221577871528348\n",
      "[[724 174]\n",
      " [189 947]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.81      0.80       898\n",
      "           1       0.84      0.83      0.84      1136\n",
      "\n",
      "    accuracy                           0.82      2034\n",
      "   macro avg       0.82      0.82      0.82      2034\n",
      "weighted avg       0.82      0.82      0.82      2034\n",
      "\n",
      "acc:  0.8215339233038348\n",
      "pre:  0.8447814451382694\n",
      "rec:  0.8336267605633803\n",
      "ma F1:  0.8193626454980114\n",
      "mi F1:  0.8215339233038348\n",
      "we F1:  0.8216799733804597\n",
      "Subject 13 Current Train Acc:  0.8258093787471238 Current Test Acc:  0.8215339233038348\n",
      "Loss:  0.08308370411396027\n",
      "Loss:  0.1302790641784668\n",
      "Loss:  0.072611503303051\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.11001194268465042\n",
      "Loss:  0.09682320803403854\n",
      "Loss:  0.09253579378128052\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.12237909436225891\n",
      "Loss:  0.0699608102440834\n",
      "Loss:  0.07735240459442139\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.017311453819274902\n",
      "Eval Loss:  0.05100655555725098\n",
      "Eval Loss:  0.9832900166511536\n",
      "[[18109  1157]\n",
      " [ 3245  8346]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89     19266\n",
      "           1       0.88      0.72      0.79     11591\n",
      "\n",
      "    accuracy                           0.86     30857\n",
      "   macro avg       0.86      0.83      0.84     30857\n",
      "weighted avg       0.86      0.86      0.85     30857\n",
      "\n",
      "acc:  0.8573419321385747\n",
      "pre:  0.8782489740082079\n",
      "rec:  0.7200414114399103\n",
      "ma F1:  0.8414724024701604\n",
      "mi F1:  0.8573419321385747\n",
      "we F1:  0.8539479366507484\n",
      "[[725 173]\n",
      " [183 953]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.81      0.80       898\n",
      "           1       0.85      0.84      0.84      1136\n",
      "\n",
      "    accuracy                           0.82      2034\n",
      "   macro avg       0.82      0.82      0.82      2034\n",
      "weighted avg       0.83      0.82      0.83      2034\n",
      "\n",
      "acc:  0.8249754178957719\n",
      "pre:  0.8463587921847247\n",
      "rec:  0.8389084507042254\n",
      "ma F1:  0.8227482221066824\n",
      "mi F1:  0.8249754178957719\n",
      "we F1:  0.8250731019216092\n",
      "Subject 13 Current Train Acc:  0.8573419321385747 Current Test Acc:  0.8249754178957719\n",
      "Loss:  0.06698356568813324\n",
      "Loss:  0.09631022810935974\n",
      "Loss:  0.13933980464935303\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.0875985324382782\n",
      "Loss:  0.11625679582357407\n",
      "Loss:  0.08441413938999176\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.11725422739982605\n",
      "Loss:  0.10259872674942017\n",
      "Loss:  0.052260663360357285\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.014818072319030762\n",
      "Eval Loss:  0.05243849754333496\n",
      "Eval Loss:  0.49733319878578186\n",
      "[[17981  1285]\n",
      " [ 2763  8828]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90     19266\n",
      "           1       0.87      0.76      0.81     11591\n",
      "\n",
      "    accuracy                           0.87     30857\n",
      "   macro avg       0.87      0.85      0.86     30857\n",
      "weighted avg       0.87      0.87      0.87     30857\n",
      "\n",
      "acc:  0.8688142074731827\n",
      "pre:  0.8729358251755167\n",
      "rec:  0.7616253990164783\n",
      "ma F1:  0.8561579472437456\n",
      "mi F1:  0.8688142074731827\n",
      "we F1:  0.8667705111310973\n",
      "[[696 202]\n",
      " [191 945]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.78      0.78       898\n",
      "           1       0.82      0.83      0.83      1136\n",
      "\n",
      "    accuracy                           0.81      2034\n",
      "   macro avg       0.80      0.80      0.80      2034\n",
      "weighted avg       0.81      0.81      0.81      2034\n",
      "\n",
      "acc:  0.8067846607669616\n",
      "pre:  0.8238884045335658\n",
      "rec:  0.8318661971830986\n",
      "ma F1:  0.8038450071224286\n",
      "mi F1:  0.8067846607669616\n",
      "we F1:  0.8066547965497333\n",
      "Loss:  0.08504408597946167\n",
      "Loss:  0.07686511427164078\n",
      "Loss:  0.11355581134557724\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.06455448269844055\n",
      "Loss:  0.10071007907390594\n",
      "Loss:  0.06999604403972626\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.0690503716468811\n",
      "Loss:  0.07658258080482483\n",
      "Loss:  0.0867394506931305\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.011090397834777832\n",
      "Eval Loss:  0.037238240242004395\n",
      "Eval Loss:  0.3216894865036011\n",
      "[[18368   898]\n",
      " [ 3182  8409]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90     19266\n",
      "           1       0.90      0.73      0.80     11591\n",
      "\n",
      "    accuracy                           0.87     30857\n",
      "   macro avg       0.88      0.84      0.85     30857\n",
      "weighted avg       0.87      0.87      0.86     30857\n",
      "\n",
      "acc:  0.867777165635026\n",
      "pre:  0.9035134844740518\n",
      "rec:  0.7254766629281338\n",
      "ma F1:  0.8524026033149983\n",
      "mi F1:  0.867777165635026\n",
      "we F1:  0.8642511589738209\n",
      "[[720 178]\n",
      " [196 940]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.80      0.79       898\n",
      "           1       0.84      0.83      0.83      1136\n",
      "\n",
      "    accuracy                           0.82      2034\n",
      "   macro avg       0.81      0.81      0.81      2034\n",
      "weighted avg       0.82      0.82      0.82      2034\n",
      "\n",
      "acc:  0.816125860373648\n",
      "pre:  0.8407871198568873\n",
      "rec:  0.8274647887323944\n",
      "ma F1:  0.8139492794385381\n",
      "mi F1:  0.816125860373648\n",
      "we F1:  0.8163039442683386\n",
      "Loss:  0.10426005721092224\n",
      "Loss:  0.09203126281499863\n",
      "Loss:  0.09856529533863068\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.06513416767120361\n",
      "Loss:  0.07872578501701355\n",
      "Loss:  0.045570652931928635\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.0850214883685112\n",
      "Loss:  0.08677848428487778\n",
      "Loss:  0.08067488670349121\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.009865045547485352\n",
      "Eval Loss:  0.01828145980834961\n",
      "Eval Loss:  0.2521766424179077\n",
      "[[18359   907]\n",
      " [ 2891  8700]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.91     19266\n",
      "           1       0.91      0.75      0.82     11591\n",
      "\n",
      "    accuracy                           0.88     30857\n",
      "   macro avg       0.88      0.85      0.86     30857\n",
      "weighted avg       0.88      0.88      0.87     30857\n",
      "\n",
      "acc:  0.8769160968337817\n",
      "pre:  0.9055896741958989\n",
      "rec:  0.7505823483737383\n",
      "ma F1:  0.8635457047897577\n",
      "mi F1:  0.8769160968337817\n",
      "we F1:  0.8741697610003265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[711 187]\n",
      " [173 963]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80       898\n",
      "           1       0.84      0.85      0.84      1136\n",
      "\n",
      "    accuracy                           0.82      2034\n",
      "   macro avg       0.82      0.82      0.82      2034\n",
      "weighted avg       0.82      0.82      0.82      2034\n",
      "\n",
      "acc:  0.8230088495575221\n",
      "pre:  0.837391304347826\n",
      "rec:  0.8477112676056338\n",
      "ma F1:  0.820249741509584\n",
      "mi F1:  0.8230088495575222\n",
      "we F1:  0.8228555657770812\n",
      "Loss:  0.04874477535486221\n",
      "Loss:  0.06494338810443878\n",
      "Loss:  0.07237252593040466\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.041194312274456024\n",
      "Loss:  0.06281191855669022\n",
      "Loss:  0.10241502523422241\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.08360596001148224\n",
      "Loss:  0.07939021289348602\n",
      "Loss:  0.08838541060686111\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.00803685188293457\n",
      "Eval Loss:  0.008223295211791992\n",
      "Eval Loss:  0.14154183864593506\n",
      "[[18600   666]\n",
      " [ 3147  8444]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.91     19266\n",
      "           1       0.93      0.73      0.82     11591\n",
      "\n",
      "    accuracy                           0.88     30857\n",
      "   macro avg       0.89      0.85      0.86     30857\n",
      "weighted avg       0.88      0.88      0.87     30857\n",
      "\n",
      "acc:  0.8764299834721457\n",
      "pre:  0.9268935236004391\n",
      "rec:  0.7284962470882581\n",
      "ma F1:  0.8614177390841044\n",
      "mi F1:  0.8764299834721456\n",
      "we F1:  0.8727626559488363\n",
      "[[750 148]\n",
      " [226 910]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.84      0.80       898\n",
      "           1       0.86      0.80      0.83      1136\n",
      "\n",
      "    accuracy                           0.82      2034\n",
      "   macro avg       0.81      0.82      0.81      2034\n",
      "weighted avg       0.82      0.82      0.82      2034\n",
      "\n",
      "acc:  0.816125860373648\n",
      "pre:  0.8601134215500945\n",
      "rec:  0.801056338028169\n",
      "ma F1:  0.814980995029619\n",
      "mi F1:  0.816125860373648\n",
      "we F1:  0.8166839822288621\n",
      "Loss:  0.10420039296150208\n",
      "Loss:  0.04664170369505882\n",
      "Loss:  0.03748525679111481\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.06150541082024574\n",
      "Loss:  0.10395453125238419\n",
      "Loss:  0.089093878865242\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.07960003614425659\n",
      "Loss:  0.06862886250019073\n",
      "Loss:  0.0559028759598732\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.01011800765991211\n",
      "Eval Loss:  0.009060859680175781\n",
      "Eval Loss:  0.2723684310913086\n",
      "[[17786  1480]\n",
      " [ 1686  9905]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92     19266\n",
      "           1       0.87      0.85      0.86     11591\n",
      "\n",
      "    accuracy                           0.90     30857\n",
      "   macro avg       0.89      0.89      0.89     30857\n",
      "weighted avg       0.90      0.90      0.90     30857\n",
      "\n",
      "acc:  0.8973976731373756\n",
      "pre:  0.8700043917435222\n",
      "rec:  0.8545423173151583\n",
      "ma F1:  0.8902377518544335\n",
      "mi F1:  0.8973976731373756\n",
      "we F1:  0.8972105212804684\n",
      "[[ 688  210]\n",
      " [ 136 1000]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.77      0.80       898\n",
      "           1       0.83      0.88      0.85      1136\n",
      "\n",
      "    accuracy                           0.83      2034\n",
      "   macro avg       0.83      0.82      0.83      2034\n",
      "weighted avg       0.83      0.83      0.83      2034\n",
      "\n",
      "acc:  0.8298918387413963\n",
      "pre:  0.8264462809917356\n",
      "rec:  0.8802816901408451\n",
      "ma F1:  0.8257928834312092\n",
      "mi F1:  0.8298918387413963\n",
      "we F1:  0.8289196506229546\n",
      "Subject 13 Current Train Acc:  0.8973976731373756 Current Test Acc:  0.8298918387413963\n",
      "Loss:  0.06661158800125122\n",
      "Loss:  0.04843105003237724\n",
      "Loss:  0.06543927639722824\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.055626045912504196\n",
      "Loss:  0.048632875084877014\n",
      "Loss:  0.05793353542685509\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.07714477926492691\n",
      "Loss:  0.09217749536037445\n",
      "Loss:  0.08422821760177612\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.0061626434326171875\n",
      "Eval Loss:  0.00516057014465332\n",
      "Eval Loss:  0.09905755519866943\n",
      "[[18468   798]\n",
      " [ 2444  9147]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     19266\n",
      "           1       0.92      0.79      0.85     11591\n",
      "\n",
      "    accuracy                           0.89     30857\n",
      "   macro avg       0.90      0.87      0.88     30857\n",
      "weighted avg       0.90      0.89      0.89     30857\n",
      "\n",
      "acc:  0.8949346987717536\n",
      "pre:  0.9197586726998491\n",
      "rec:  0.789146751790182\n",
      "ma F1:  0.8843852208156615\n",
      "mi F1:  0.8949346987717535\n",
      "we F1:  0.893071761241904\n",
      "[[708 190]\n",
      " [179 957]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.79       898\n",
      "           1       0.83      0.84      0.84      1136\n",
      "\n",
      "    accuracy                           0.82      2034\n",
      "   macro avg       0.82      0.82      0.82      2034\n",
      "weighted avg       0.82      0.82      0.82      2034\n",
      "\n",
      "acc:  0.8185840707964602\n",
      "pre:  0.8343504795117699\n",
      "rec:  0.8424295774647887\n",
      "ma F1:  0.815823937985181\n",
      "mi F1:  0.8185840707964602\n",
      "we F1:  0.818462137218452\n",
      "Loss:  0.04998411983251572\n",
      "Loss:  0.06424630433320999\n",
      "Loss:  0.06555239856243134\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.09173350036144257\n",
      "Loss:  0.04899078607559204\n",
      "Loss:  0.06224453076720238\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.05256923288106918\n",
      "Loss:  0.07079455256462097\n",
      "Loss:  0.08010616898536682\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.005667924880981445\n",
      "Eval Loss:  0.004505634307861328\n",
      "Eval Loss:  0.07016754150390625\n",
      "[[18621   645]\n",
      " [ 2679  8912]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92     19266\n",
      "           1       0.93      0.77      0.84     11591\n",
      "\n",
      "    accuracy                           0.89     30857\n",
      "   macro avg       0.90      0.87      0.88     30857\n",
      "weighted avg       0.90      0.89      0.89     30857\n",
      "\n",
      "acc:  0.8922772790614771\n",
      "pre:  0.9325102019462175\n",
      "rec:  0.7688724010007765\n",
      "ma F1:  0.8804407374631376\n",
      "mi F1:  0.8922772790614771\n",
      "we F1:  0.8897975668757708\n",
      "[[769 129]\n",
      " [210 926]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82       898\n",
      "           1       0.88      0.82      0.85      1136\n",
      "\n",
      "    accuracy                           0.83      2034\n",
      "   macro avg       0.83      0.84      0.83      2034\n",
      "weighted avg       0.84      0.83      0.83      2034\n",
      "\n",
      "acc:  0.8333333333333334\n",
      "pre:  0.8777251184834123\n",
      "rec:  0.8151408450704225\n",
      "ma F1:  0.8323343887317396\n",
      "mi F1:  0.8333333333333334\n",
      "we F1:  0.833848712395302\n",
      "Subject 13 Current Train Acc:  0.8922772790614771 Current Test Acc:  0.8333333333333334\n",
      "Loss:  0.061527982354164124\n",
      "Loss:  0.05976661294698715\n",
      "Loss:  0.030781615525484085\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.07097121328115463\n",
      "Loss:  0.07404005527496338\n",
      "Loss:  0.07175739854574203\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.05870961397886276\n",
      "Loss:  0.09027761965990067\n",
      "Loss:  0.09728489071130753\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.0043659210205078125\n",
      "Eval Loss:  0.004116535186767578\n",
      "Eval Loss:  0.027214407920837402\n",
      "[[18651   615]\n",
      " [ 2871  8720]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.91     19266\n",
      "           1       0.93      0.75      0.83     11591\n",
      "\n",
      "    accuracy                           0.89     30857\n",
      "   macro avg       0.90      0.86      0.87     30857\n",
      "weighted avg       0.89      0.89      0.88     30857\n",
      "\n",
      "acc:  0.887027254755809\n",
      "pre:  0.9341189073379753\n",
      "rec:  0.7523078250366664\n",
      "ma F1:  0.8739733327237236\n",
      "mi F1:  0.887027254755809\n",
      "we F1:  0.8840618285042346\n",
      "[[773 125]\n",
      " [221 915]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82       898\n",
      "           1       0.88      0.81      0.84      1136\n",
      "\n",
      "    accuracy                           0.83      2034\n",
      "   macro avg       0.83      0.83      0.83      2034\n",
      "weighted avg       0.83      0.83      0.83      2034\n",
      "\n",
      "acc:  0.8298918387413963\n",
      "pre:  0.8798076923076923\n",
      "rec:  0.8054577464788732\n",
      "ma F1:  0.8290586913941053\n",
      "mi F1:  0.8298918387413963\n",
      "we F1:  0.8304550932860438\n",
      "Loss:  0.08197740465402603\n",
      "Loss:  0.035659950226545334\n",
      "Loss:  0.06390160322189331\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.07572630047798157\n",
      "Loss:  0.03865876421332359\n",
      "Loss:  0.0518532395362854\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.059402935206890106\n",
      "Loss:  0.04703077673912048\n",
      "Loss:  0.05109395086765289\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.0035924911499023438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss:  0.004823446273803711\n",
      "Eval Loss:  0.03623497486114502\n",
      "[[18200  1066]\n",
      " [ 1898  9693]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92     19266\n",
      "           1       0.90      0.84      0.87     11591\n",
      "\n",
      "    accuracy                           0.90     30857\n",
      "   macro avg       0.90      0.89      0.90     30857\n",
      "weighted avg       0.90      0.90      0.90     30857\n",
      "\n",
      "acc:  0.9039439997407396\n",
      "pre:  0.9009201598661586\n",
      "rec:  0.8362522646881201\n",
      "ma F1:  0.8960426622219464\n",
      "mi F1:  0.9039439997407396\n",
      "we F1:  0.9031712346278166\n",
      "[[782 116]\n",
      " [265 871]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.87      0.80       898\n",
      "           1       0.88      0.77      0.82      1136\n",
      "\n",
      "    accuracy                           0.81      2034\n",
      "   macro avg       0.81      0.82      0.81      2034\n",
      "weighted avg       0.82      0.81      0.81      2034\n",
      "\n",
      "acc:  0.8126843657817109\n",
      "pre:  0.8824721377912867\n",
      "rec:  0.766725352112676\n",
      "ma F1:  0.8123250432586182\n",
      "mi F1:  0.8126843657817108\n",
      "we F1:  0.8132859282080124\n",
      "Loss:  0.05884627252817154\n",
      "Loss:  0.08797096461057663\n",
      "Loss:  0.07444532960653305\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.053651485592126846\n",
      "Loss:  0.05152934789657593\n",
      "Loss:  0.048401348292827606\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.07521802932024002\n",
      "Loss:  0.05746999755501747\n",
      "Loss:  0.060536790639162064\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.0030608177185058594\n",
      "Eval Loss:  0.0035927295684814453\n",
      "Eval Loss:  0.04270219802856445\n",
      "[[18250  1016]\n",
      " [ 1864  9727]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19266\n",
      "           1       0.91      0.84      0.87     11591\n",
      "\n",
      "    accuracy                           0.91     30857\n",
      "   macro avg       0.91      0.89      0.90     30857\n",
      "weighted avg       0.91      0.91      0.91     30857\n",
      "\n",
      "acc:  0.9066662345659008\n",
      "pre:  0.9054267895373732\n",
      "rec:  0.8391855750150979\n",
      "ma F1:  0.8989575275369462\n",
      "mi F1:  0.9066662345659008\n",
      "we F1:  0.9058992530382046\n",
      "[[754 144]\n",
      " [196 940]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.84      0.82       898\n",
      "           1       0.87      0.83      0.85      1136\n",
      "\n",
      "    accuracy                           0.83      2034\n",
      "   macro avg       0.83      0.83      0.83      2034\n",
      "weighted avg       0.83      0.83      0.83      2034\n",
      "\n",
      "acc:  0.8328416912487709\n",
      "pre:  0.8671586715867159\n",
      "rec:  0.8274647887323944\n",
      "ma F1:  0.8314320814320814\n",
      "mi F1:  0.8328416912487709\n",
      "we F1:  0.8332357757136517\n",
      "Loss:  0.06662886589765549\n",
      "Loss:  0.06250576674938202\n",
      "Loss:  0.0798531025648117\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.08337275683879852\n",
      "Loss:  0.05454389378428459\n",
      "Loss:  0.03921031951904297\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.04754561558365822\n",
      "Loss:  0.04814929887652397\n",
      "Loss:  0.03849149867892265\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.0026848316192626953\n",
      "Eval Loss:  0.0034410953521728516\n",
      "Eval Loss:  0.011517524719238281\n",
      "[[18749   517]\n",
      " [ 2951  8640]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.97      0.92     19266\n",
      "           1       0.94      0.75      0.83     11591\n",
      "\n",
      "    accuracy                           0.89     30857\n",
      "   macro avg       0.90      0.86      0.87     30857\n",
      "weighted avg       0.89      0.89      0.88     30857\n",
      "\n",
      "acc:  0.8876105907897722\n",
      "pre:  0.9435404608496233\n",
      "rec:  0.7454059183849538\n",
      "ma F1:  0.8740978955675611\n",
      "mi F1:  0.8876105907897722\n",
      "we F1:  0.8843570642123796\n",
      "[[842  56]\n",
      " [462 674]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.94      0.76       898\n",
      "           1       0.92      0.59      0.72      1136\n",
      "\n",
      "    accuracy                           0.75      2034\n",
      "   macro avg       0.78      0.77      0.74      2034\n",
      "weighted avg       0.80      0.75      0.74      2034\n",
      "\n",
      "acc:  0.7453294001966568\n",
      "pre:  0.9232876712328767\n",
      "rec:  0.5933098591549296\n",
      "ma F1:  0.7435800835837634\n",
      "mi F1:  0.7453294001966569\n",
      "we F1:  0.741101885048831\n",
      "Loss:  0.08885593712329865\n",
      "Loss:  0.034514810889959335\n",
      "Loss:  0.06396640837192535\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.07109946757555008\n",
      "Loss:  0.05833160877227783\n",
      "Loss:  0.022970927879214287\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.04560764133930206\n",
      "Loss:  0.05012815073132515\n",
      "Loss:  0.06758961826562881\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.0030744075775146484\n",
      "Eval Loss:  0.006815433502197266\n",
      "Eval Loss:  0.033843278884887695\n",
      "[[18290   976]\n",
      " [ 1695  9896]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     19266\n",
      "           1       0.91      0.85      0.88     11591\n",
      "\n",
      "    accuracy                           0.91     30857\n",
      "   macro avg       0.91      0.90      0.91     30857\n",
      "weighted avg       0.91      0.91      0.91     30857\n",
      "\n",
      "acc:  0.9134394140713614\n",
      "pre:  0.9102281089036056\n",
      "rec:  0.8537658528168407\n",
      "ma F1:  0.9065220659193939\n",
      "mi F1:  0.9134394140713614\n",
      "we F1:  0.912846898784101\n",
      "[[740 158]\n",
      " [227 909]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.82      0.79       898\n",
      "           1       0.85      0.80      0.83      1136\n",
      "\n",
      "    accuracy                           0.81      2034\n",
      "   macro avg       0.81      0.81      0.81      2034\n",
      "weighted avg       0.81      0.81      0.81      2034\n",
      "\n",
      "acc:  0.8107177974434612\n",
      "pre:  0.851921274601687\n",
      "rec:  0.8001760563380281\n",
      "ma F1:  0.8094019975198334\n",
      "mi F1:  0.8107177974434612\n",
      "we F1:  0.8112550175306227\n",
      "Loss:  0.06646747887134552\n",
      "Loss:  0.05422727391123772\n",
      "Loss:  0.06467220187187195\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.034749168902635574\n",
      "Loss:  0.0486011765897274\n",
      "Loss:  0.07357223331928253\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.07363606989383698\n",
      "Loss:  0.057700686156749725\n",
      "Loss:  0.05288959667086601\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.002115964889526367\n",
      "Eval Loss:  0.004234790802001953\n",
      "Eval Loss:  0.027696847915649414\n",
      "[[18286   980]\n",
      " [ 1761  9830]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19266\n",
      "           1       0.91      0.85      0.88     11591\n",
      "\n",
      "    accuracy                           0.91     30857\n",
      "   macro avg       0.91      0.90      0.90     30857\n",
      "weighted avg       0.91      0.91      0.91     30857\n",
      "\n",
      "acc:  0.9111708850503938\n",
      "pre:  0.9093432007400555\n",
      "rec:  0.8480717798291778\n",
      "ma F1:  0.9039584537209031\n",
      "mi F1:  0.9111708850503938\n",
      "we F1:  0.9105047416175257\n",
      "[[792 106]\n",
      " [235 901]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82       898\n",
      "           1       0.89      0.79      0.84      1136\n",
      "\n",
      "    accuracy                           0.83      2034\n",
      "   macro avg       0.83      0.84      0.83      2034\n",
      "weighted avg       0.84      0.83      0.83      2034\n",
      "\n",
      "acc:  0.8323500491642084\n",
      "pre:  0.8947368421052632\n",
      "rec:  0.7931338028169014\n",
      "ma F1:  0.8318672088527431\n",
      "mi F1:  0.8323500491642084\n",
      "we F1:  0.8329214840282362\n",
      "Loss:  0.0608304999768734\n",
      "Loss:  0.026301134377717972\n",
      "Loss:  0.06237538158893585\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.06396068632602692\n",
      "Loss:  0.0507754348218441\n",
      "Loss:  0.05953952670097351\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.07250983268022537\n",
      "Loss:  0.05690084770321846\n",
      "Loss:  0.05901650711894035\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.002006053924560547\n",
      "Eval Loss:  0.003355741500854492\n",
      "Eval Loss:  0.013382673263549805\n",
      "[[18427   839]\n",
      " [ 1719  9872]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.94     19266\n",
      "           1       0.92      0.85      0.89     11591\n",
      "\n",
      "    accuracy                           0.92     30857\n",
      "   macro avg       0.92      0.90      0.91     30857\n",
      "weighted avg       0.92      0.92      0.92     30857\n",
      "\n",
      "acc:  0.9171014680623522\n",
      "pre:  0.9216693119223228\n",
      "rec:  0.8516952808213268\n",
      "ma F1:  0.9101988382664319\n",
      "mi F1:  0.9171014680623522\n",
      "we F1:  0.9163914371774416\n",
      "[[789 109]\n",
      " [233 903]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.88      0.82       898\n",
      "           1       0.89      0.79      0.84      1136\n",
      "\n",
      "    accuracy                           0.83      2034\n",
      "   macro avg       0.83      0.84      0.83      2034\n",
      "weighted avg       0.84      0.83      0.83      2034\n",
      "\n",
      "acc:  0.831858407079646\n",
      "pre:  0.892292490118577\n",
      "rec:  0.7948943661971831\n",
      "ma F1:  0.831328561452514\n",
      "mi F1:  0.831858407079646\n",
      "we F1:  0.8324347303933687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.06164541095495224\n",
      "Loss:  0.03895897418260574\n",
      "Loss:  0.06325441598892212\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.05567669868469238\n",
      "Loss:  0.08651964366436005\n",
      "Loss:  0.023141300305724144\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.04531587287783623\n",
      "Loss:  0.0613587349653244\n",
      "Loss:  0.04639894887804985\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.002239227294921875\n",
      "Eval Loss:  0.006242036819458008\n",
      "Eval Loss:  0.015443801879882812\n",
      "[[18361   905]\n",
      " [ 1534 10057]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94     19266\n",
      "           1       0.92      0.87      0.89     11591\n",
      "\n",
      "    accuracy                           0.92     30857\n",
      "   macro avg       0.92      0.91      0.91     30857\n",
      "weighted avg       0.92      0.92      0.92     30857\n",
      "\n",
      "acc:  0.9209579673979972\n",
      "pre:  0.9174420726144864\n",
      "rec:  0.8676559399534122\n",
      "ma F1:  0.9147866954092277\n",
      "mi F1:  0.9209579673979972\n",
      "we F1:  0.9204905143535687\n",
      "[[740 158]\n",
      " [212 924]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.82      0.80       898\n",
      "           1       0.85      0.81      0.83      1136\n",
      "\n",
      "    accuracy                           0.82      2034\n",
      "   macro avg       0.82      0.82      0.82      2034\n",
      "weighted avg       0.82      0.82      0.82      2034\n",
      "\n",
      "acc:  0.8180924287118977\n",
      "pre:  0.8539741219963032\n",
      "rec:  0.8133802816901409\n",
      "ma F1:  0.8165915238954013\n",
      "mi F1:  0.8180924287118977\n",
      "we F1:  0.8185329116471739\n",
      "Loss:  0.039120614528656006\n",
      "Loss:  0.07709749042987823\n",
      "Loss:  0.04646892100572586\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.06364738196134567\n",
      "Loss:  0.08533114194869995\n",
      "Loss:  0.03196550905704498\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.07013831287622452\n",
      "Loss:  0.05540850758552551\n",
      "Loss:  0.0491354763507843\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.0025026798248291016\n",
      "Eval Loss:  0.0035851001739501953\n",
      "Eval Loss:  0.02384769916534424\n",
      "[[18331   935]\n",
      " [ 1510 10081]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94     19266\n",
      "           1       0.92      0.87      0.89     11591\n",
      "\n",
      "    accuracy                           0.92     30857\n",
      "   macro avg       0.92      0.91      0.91     30857\n",
      "weighted avg       0.92      0.92      0.92     30857\n",
      "\n",
      "acc:  0.9207635220533429\n",
      "pre:  0.9151234567901234\n",
      "rec:  0.8697265119489259\n",
      "ma F1:  0.9146634407367509\n",
      "mi F1:  0.9207635220533429\n",
      "we F1:  0.9203383648706713\n",
      "[[ 729  169]\n",
      " [ 126 1010]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.81      0.83       898\n",
      "           1       0.86      0.89      0.87      1136\n",
      "\n",
      "    accuracy                           0.85      2034\n",
      "   macro avg       0.85      0.85      0.85      2034\n",
      "weighted avg       0.85      0.85      0.85      2034\n",
      "\n",
      "acc:  0.8549655850540806\n",
      "pre:  0.8566581849024597\n",
      "rec:  0.8890845070422535\n",
      "ma F1:  0.852143625429532\n",
      "mi F1:  0.8549655850540806\n",
      "we F1:  0.8545337549335981\n",
      "Subject 13 Current Train Acc:  0.9207635220533429 Current Test Acc:  0.8549655850540806\n",
      "Loss:  0.05716323107481003\n",
      "Loss:  0.0839855745434761\n",
      "Loss:  0.05523746460676193\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.05763412266969681\n",
      "Loss:  0.06265413761138916\n",
      "Loss:  0.04689390957355499\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.04942738637328148\n",
      "Loss:  0.03135024011135101\n",
      "Loss:  0.05531604588031769\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.0010485649108886719\n",
      "Eval Loss:  0.002198934555053711\n",
      "Eval Loss:  0.006085395812988281\n",
      "[[18432   834]\n",
      " [ 1781  9810]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.96      0.93     19266\n",
      "           1       0.92      0.85      0.88     11591\n",
      "\n",
      "    accuracy                           0.92     30857\n",
      "   macro avg       0.92      0.90      0.91     30857\n",
      "weighted avg       0.92      0.92      0.91     30857\n",
      "\n",
      "acc:  0.9152542372881356\n",
      "pre:  0.9216459977452086\n",
      "rec:  0.8463463031662497\n",
      "ma F1:  0.9080774387954718\n",
      "mi F1:  0.9152542372881356\n",
      "we F1:  0.914465971436529\n",
      "[[803  95]\n",
      " [261 875]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.89      0.82       898\n",
      "           1       0.90      0.77      0.83      1136\n",
      "\n",
      "    accuracy                           0.82      2034\n",
      "   macro avg       0.83      0.83      0.82      2034\n",
      "weighted avg       0.84      0.82      0.83      2034\n",
      "\n",
      "acc:  0.8249754178957719\n",
      "pre:  0.9020618556701031\n",
      "rec:  0.7702464788732394\n",
      "ma F1:  0.8247558308720389\n",
      "mi F1:  0.8249754178957719\n",
      "we F1:  0.8254816879782675\n",
      "Loss:  0.06340403109788895\n",
      "Loss:  0.03435630723834038\n",
      "Loss:  0.05959131568670273\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.04279743880033493\n",
      "Loss:  0.05264341086149216\n",
      "Loss:  0.047758448868989944\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.045115210115909576\n",
      "Loss:  0.0467904731631279\n",
      "Loss:  0.0654563456773758\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  13 Training Time 5734.7265067100525 Best Test Acc:  0.8549655850540806\n",
      "test subjects:  ['./seg\\\\b01', './seg\\\\x03']\n",
      "*********\n",
      "33361 952\n",
      "31939 952\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.8272640705108643\n",
      "Eval Loss:  0.7926221489906311\n",
      "Eval Loss:  0.8202564716339111\n",
      "[[    0 19243]\n",
      " [    0 12696]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     19243\n",
      "           1       0.40      1.00      0.57     12696\n",
      "\n",
      "    accuracy                           0.40     31939\n",
      "   macro avg       0.20      0.50      0.28     31939\n",
      "weighted avg       0.16      0.40      0.23     31939\n",
      "\n",
      "acc:  0.3975077491468111\n",
      "pre:  0.3975077491468111\n",
      "rec:  1.0\n",
      "ma F1:  0.2844404615212277\n",
      "mi F1:  0.39750774914681103\n",
      "we F1:  0.2261345752511667\n",
      "[[  0 921]\n",
      " [  0  31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       921\n",
      "           1       0.03      1.00      0.06        31\n",
      "\n",
      "    accuracy                           0.03       952\n",
      "   macro avg       0.02      0.50      0.03       952\n",
      "weighted avg       0.00      0.03      0.00       952\n",
      "\n",
      "acc:  0.032563025210084036\n",
      "pre:  0.032563025210084036\n",
      "rec:  1.0\n",
      "ma F1:  0.03153611393692777\n",
      "mi F1:  0.032563025210084036\n",
      "we F1:  0.002053822546312523\n",
      "Subject 14 Current Train Acc:  0.3975077491468111 Current Test Acc:  0.032563025210084036\n",
      "Loss:  0.17153695225715637\n",
      "Loss:  0.16647091507911682\n",
      "Loss:  0.16409754753112793\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.1419505774974823\n",
      "Loss:  0.1362534910440445\n",
      "Loss:  0.13792426884174347\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.10487856715917587\n",
      "Loss:  0.11145458370447159\n",
      "Loss:  0.08117897808551788\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.028841733932495117\n",
      "Eval Loss:  0.035068631172180176\n",
      "Eval Loss:  0.028884172439575195\n",
      "[[17135  2108]\n",
      " [ 4835  7861]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.89      0.83     19243\n",
      "           1       0.79      0.62      0.69     12696\n",
      "\n",
      "    accuracy                           0.78     31939\n",
      "   macro avg       0.78      0.75      0.76     31939\n",
      "weighted avg       0.78      0.78      0.78     31939\n",
      "\n",
      "acc:  0.7826168633958484\n",
      "pre:  0.7885444879125288\n",
      "rec:  0.6191713925645873\n",
      "ma F1:  0.76260119573043\n",
      "mi F1:  0.7826168633958484\n",
      "we F1:  0.7767312988364786\n",
      "[[889  32]\n",
      " [ 29   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       921\n",
      "           1       0.06      0.06      0.06        31\n",
      "\n",
      "    accuracy                           0.94       952\n",
      "   macro avg       0.51      0.51      0.51       952\n",
      "weighted avg       0.94      0.94      0.94       952\n",
      "\n",
      "acc:  0.9359243697478992\n",
      "pre:  0.058823529411764705\n",
      "rec:  0.06451612903225806\n",
      "ma F1:  0.5141841301710796\n",
      "mi F1:  0.9359243697478992\n",
      "we F1:  0.9373507741658591\n",
      "Subject 14 Current Train Acc:  0.7826168633958484 Current Test Acc:  0.9359243697478992\n",
      "Loss:  0.1333683580160141\n",
      "Loss:  0.12164036929607391\n",
      "Loss:  0.10666224360466003\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.10476060956716537\n",
      "Loss:  0.07482371479272842\n",
      "Loss:  0.09649179875850677\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.0788469985127449\n",
      "Loss:  0.08652746677398682\n",
      "Loss:  0.11182677745819092\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.01490163803100586\n",
      "Eval Loss:  0.01802206039428711\n",
      "Eval Loss:  0.013807058334350586\n",
      "[[18293   950]\n",
      " [ 4508  8188]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.95      0.87     19243\n",
      "           1       0.90      0.64      0.75     12696\n",
      "\n",
      "    accuracy                           0.83     31939\n",
      "   macro avg       0.85      0.80      0.81     31939\n",
      "weighted avg       0.84      0.83      0.82     31939\n",
      "\n",
      "acc:  0.8291117442625003\n",
      "pre:  0.8960385204639965\n",
      "rec:  0.644927536231884\n",
      "ma F1:  0.8101032586135462\n",
      "mi F1:  0.8291117442625003\n",
      "we F1:  0.8224188009731407\n",
      "[[896  25]\n",
      " [ 27   4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       921\n",
      "           1       0.14      0.13      0.13        31\n",
      "\n",
      "    accuracy                           0.95       952\n",
      "   macro avg       0.55      0.55      0.55       952\n",
      "weighted avg       0.94      0.95      0.94       952\n",
      "\n",
      "acc:  0.9453781512605042\n",
      "pre:  0.13793103448275862\n",
      "rec:  0.12903225806451613\n",
      "ma F1:  0.5525668835864064\n",
      "mi F1:  0.9453781512605042\n",
      "we F1:  0.9444974085078716\n",
      "Subject 14 Current Train Acc:  0.8291117442625003 Current Test Acc:  0.9453781512605042\n",
      "Loss:  0.07653175294399261\n",
      "Loss:  0.11070261150598526\n",
      "Loss:  0.09331092238426208\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.11781847476959229\n",
      "Loss:  0.07907071709632874\n",
      "Loss:  0.10665523260831833\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.0665445402264595\n",
      "Loss:  0.07669619470834732\n",
      "Loss:  0.07855914533138275\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.013010025024414062\n",
      "Eval Loss:  0.01730656623840332\n",
      "Eval Loss:  0.011372089385986328\n",
      "[[18691   552]\n",
      " [ 4963  7733]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.97      0.87     19243\n",
      "           1       0.93      0.61      0.74     12696\n",
      "\n",
      "    accuracy                           0.83     31939\n",
      "   macro avg       0.86      0.79      0.80     31939\n",
      "weighted avg       0.85      0.83      0.82     31939\n",
      "\n",
      "acc:  0.827327092269639\n",
      "pre:  0.9333735666867834\n",
      "rec:  0.6090894770006301\n",
      "ma F1:  0.8042896802349901\n",
      "mi F1:  0.8273270922696391\n",
      "we F1:  0.81805368247909\n",
      "[[914   7]\n",
      " [ 30   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       921\n",
      "           1       0.12      0.03      0.05        31\n",
      "\n",
      "    accuracy                           0.96       952\n",
      "   macro avg       0.55      0.51      0.52       952\n",
      "weighted avg       0.94      0.96      0.95       952\n",
      "\n",
      "acc:  0.9611344537815126\n",
      "pre:  0.125\n",
      "rec:  0.03225806451612903\n",
      "ma F1:  0.5157214545954492\n",
      "mi F1:  0.9611344537815126\n",
      "we F1:  0.9499137539115459\n",
      "Subject 14 Current Train Acc:  0.827327092269639 Current Test Acc:  0.9611344537815126\n",
      "Loss:  0.05229847878217697\n",
      "Loss:  0.10249845683574677\n",
      "Loss:  0.11482615768909454\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.10873956978321075\n",
      "Loss:  0.07997852563858032\n",
      "Loss:  0.07139206677675247\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.08623767644166946\n",
      "Loss:  0.06545136868953705\n",
      "Loss:  0.08504088968038559\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.012282371520996094\n",
      "Eval Loss:  0.0424579381942749\n",
      "Eval Loss:  0.012758493423461914\n",
      "[[18271   972]\n",
      " [ 3363  9333]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89     19243\n",
      "           1       0.91      0.74      0.81     12696\n",
      "\n",
      "    accuracy                           0.86     31939\n",
      "   macro avg       0.88      0.84      0.85     31939\n",
      "weighted avg       0.87      0.86      0.86     31939\n",
      "\n",
      "acc:  0.8642725194902784\n",
      "pre:  0.9056768558951965\n",
      "rec:  0.7351134215500945\n",
      "ma F1:  0.8527400382967083\n",
      "mi F1:  0.8642725194902784\n",
      "we F1:  0.8611874710975925\n",
      "[[906  15]\n",
      " [ 30   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       921\n",
      "           1       0.06      0.03      0.04        31\n",
      "\n",
      "    accuracy                           0.95       952\n",
      "   macro avg       0.52      0.51      0.51       952\n",
      "weighted avg       0.94      0.95      0.95       952\n",
      "\n",
      "acc:  0.9527310924369747\n",
      "pre:  0.0625\n",
      "rec:  0.03225806451612903\n",
      "ma F1:  0.5091602791049393\n",
      "mi F1:  0.9527310924369747\n",
      "we F1:  0.945379090006057\n",
      "Loss:  0.0624333918094635\n",
      "Loss:  0.07398778945207596\n",
      "Loss:  0.06317481398582458\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.09266282618045807\n",
      "Loss:  0.05586012080311775\n",
      "Loss:  0.09719599038362503\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.08521610498428345\n",
      "Loss:  0.09719952195882797\n",
      "Loss:  0.053404778242111206\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.012062311172485352\n",
      "Eval Loss:  0.009074687957763672\n",
      "Eval Loss:  0.0056056976318359375\n",
      "[[18690   553]\n",
      " [ 4528  8168]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.88     19243\n",
      "           1       0.94      0.64      0.76     12696\n",
      "\n",
      "    accuracy                           0.84     31939\n",
      "   macro avg       0.87      0.81      0.82     31939\n",
      "weighted avg       0.86      0.84      0.83     31939\n",
      "\n",
      "acc:  0.8409154951626538\n",
      "pre:  0.9365898406146084\n",
      "rec:  0.6433522369250158\n",
      "ma F1:  0.8215479034703013\n",
      "mi F1:  0.8409154951626537\n",
      "we F1:  0.833598808508301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[914   7]\n",
      " [ 31   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       921\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.96       952\n",
      "   macro avg       0.48      0.50      0.49       952\n",
      "weighted avg       0.94      0.96      0.95       952\n",
      "\n",
      "acc:  0.9600840336134454\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48981779206859594\n",
      "mi F1:  0.9600840336134454\n",
      "we F1:  0.9477356859142371\n",
      "Loss:  0.06006040796637535\n",
      "Loss:  0.09360269457101822\n",
      "Loss:  0.08171959966421127\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.06575257331132889\n",
      "Loss:  0.07003612071275711\n",
      "Loss:  0.07695487141609192\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.08210255205631256\n",
      "Loss:  0.1133553683757782\n",
      "Loss:  0.07414521276950836\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.015779972076416016\n",
      "Eval Loss:  0.03323507308959961\n",
      "Eval Loss:  0.009575366973876953\n",
      "[[17923  1320]\n",
      " [ 2556 10140]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.93      0.90     19243\n",
      "           1       0.88      0.80      0.84     12696\n",
      "\n",
      "    accuracy                           0.88     31939\n",
      "   macro avg       0.88      0.87      0.87     31939\n",
      "weighted avg       0.88      0.88      0.88     31939\n",
      "\n",
      "acc:  0.8786436644854253\n",
      "pre:  0.8848167539267016\n",
      "rec:  0.7986767485822306\n",
      "ma F1:  0.8709824012105194\n",
      "mi F1:  0.8786436644854253\n",
      "we F1:  0.8774269972095955\n",
      "[[900  21]\n",
      " [ 29   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       921\n",
      "           1       0.09      0.06      0.07        31\n",
      "\n",
      "    accuracy                           0.95       952\n",
      "   macro avg       0.53      0.52      0.52       952\n",
      "weighted avg       0.94      0.95      0.94       952\n",
      "\n",
      "acc:  0.9474789915966386\n",
      "pre:  0.08695652173913043\n",
      "rec:  0.06451612903225806\n",
      "ma F1:  0.5235235235235236\n",
      "mi F1:  0.9474789915966386\n",
      "we F1:  0.9437021054668113\n",
      "Loss:  0.08603525161743164\n",
      "Loss:  0.07374114543199539\n",
      "Loss:  0.06564812362194061\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.06921300292015076\n",
      "Loss:  0.07975023239850998\n",
      "Loss:  0.05925879627466202\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.0752251148223877\n",
      "Loss:  0.09243463724851608\n",
      "Loss:  0.0906766802072525\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.010725021362304688\n",
      "Eval Loss:  0.0206449031829834\n",
      "Eval Loss:  0.007150888442993164\n",
      "[[18444   799]\n",
      " [ 3117  9579]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.90     19243\n",
      "           1       0.92      0.75      0.83     12696\n",
      "\n",
      "    accuracy                           0.88     31939\n",
      "   macro avg       0.89      0.86      0.87     31939\n",
      "weighted avg       0.88      0.88      0.87     31939\n",
      "\n",
      "acc:  0.8773912771220138\n",
      "pre:  0.9230102139140489\n",
      "rec:  0.7544896030245747\n",
      "ma F1:  0.8671570931089301\n",
      "mi F1:  0.8773912771220139\n",
      "we F1:  0.874715266006128\n",
      "[[912   9]\n",
      " [ 31   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       921\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.96       952\n",
      "   macro avg       0.48      0.50      0.49       952\n",
      "weighted avg       0.94      0.96      0.95       952\n",
      "\n",
      "acc:  0.957983193277311\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48927038626609437\n",
      "mi F1:  0.957983193277311\n",
      "we F1:  0.946676524687128\n",
      "Loss:  0.0929955393075943\n",
      "Loss:  0.09157589823007584\n",
      "Loss:  0.055711910128593445\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.05702460929751396\n",
      "Loss:  0.0589081309735775\n",
      "Loss:  0.08024323731660843\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.04955407977104187\n",
      "Loss:  0.08026900142431259\n",
      "Loss:  0.05649256333708763\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.02072882652282715\n",
      "Eval Loss:  0.043381571769714355\n",
      "Eval Loss:  0.015848875045776367\n",
      "[[17861  1382]\n",
      " [ 2259 10437]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91     19243\n",
      "           1       0.88      0.82      0.85     12696\n",
      "\n",
      "    accuracy                           0.89     31939\n",
      "   macro avg       0.89      0.88      0.88     31939\n",
      "weighted avg       0.89      0.89      0.89     31939\n",
      "\n",
      "acc:  0.8860014402454679\n",
      "pre:  0.883069633640748\n",
      "rec:  0.822069943289225\n",
      "ma F1:  0.879490327686229\n",
      "mi F1:  0.8860014402454679\n",
      "we F1:  0.8852322799929824\n",
      "[[909  12]\n",
      " [ 29   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       921\n",
      "           1       0.14      0.06      0.09        31\n",
      "\n",
      "    accuracy                           0.96       952\n",
      "   macro avg       0.56      0.53      0.53       952\n",
      "weighted avg       0.94      0.96      0.95       952\n",
      "\n",
      "acc:  0.9569327731092437\n",
      "pre:  0.14285714285714285\n",
      "rec:  0.06451612903225806\n",
      "ma F1:  0.5334170103400873\n",
      "mi F1:  0.9569327731092437\n",
      "we F1:  0.9489947709404722\n",
      "Loss:  0.057873453944921494\n",
      "Loss:  0.08371855318546295\n",
      "Loss:  0.09422330558300018\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.055965423583984375\n",
      "Loss:  0.05656399205327034\n",
      "Loss:  0.09383544325828552\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.04964231327176094\n",
      "Loss:  0.04418366402387619\n",
      "Loss:  0.05885294824838638\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.01233220100402832\n",
      "Eval Loss:  0.1562514305114746\n",
      "Eval Loss:  0.13184595108032227\n",
      "[[17812  1431]\n",
      " [ 1728 10968]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92     19243\n",
      "           1       0.88      0.86      0.87     12696\n",
      "\n",
      "    accuracy                           0.90     31939\n",
      "   macro avg       0.90      0.89      0.90     31939\n",
      "weighted avg       0.90      0.90      0.90     31939\n",
      "\n",
      "acc:  0.9010927079745765\n",
      "pre:  0.884587466731188\n",
      "rec:  0.8638941398865785\n",
      "ma F1:  0.89633256811595\n",
      "mi F1:  0.9010927079745766\n",
      "we F1:  0.9008861384921084\n",
      "[[905  16]\n",
      " [ 29   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       921\n",
      "           1       0.11      0.06      0.08        31\n",
      "\n",
      "    accuracy                           0.95       952\n",
      "   macro avg       0.54      0.52      0.53       952\n",
      "weighted avg       0.94      0.95      0.95       952\n",
      "\n",
      "acc:  0.9527310924369747\n",
      "pre:  0.1111111111111111\n",
      "rec:  0.06451612903225806\n",
      "ma F1:  0.5286869464767039\n",
      "mi F1:  0.9527310924369747\n",
      "we F1:  0.9466263594386543\n",
      "Loss:  0.07377608120441437\n",
      "Loss:  0.061843253672122955\n",
      "Loss:  0.026286838576197624\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.0641862004995346\n",
      "Loss:  0.07639523595571518\n",
      "Loss:  0.06507003307342529\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.03972291573882103\n",
      "Loss:  0.07102611660957336\n",
      "Loss:  0.07137967646121979\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.017997264862060547\n",
      "Eval Loss:  0.02619647979736328\n",
      "Eval Loss:  0.020284056663513184\n",
      "[[18331   912]\n",
      " [ 2461 10235]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.92     19243\n",
      "           1       0.92      0.81      0.86     12696\n",
      "\n",
      "    accuracy                           0.89     31939\n",
      "   macro avg       0.90      0.88      0.89     31939\n",
      "weighted avg       0.90      0.89      0.89     31939\n",
      "\n",
      "acc:  0.894392435580325\n",
      "pre:  0.9181842648246165\n",
      "rec:  0.8061594202898551\n",
      "ma F1:  0.8871408113044315\n",
      "mi F1:  0.894392435580325\n",
      "we F1:  0.8930049891866294\n",
      "[[912   9]\n",
      " [ 31   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       921\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.96       952\n",
      "   macro avg       0.48      0.50      0.49       952\n",
      "weighted avg       0.94      0.96      0.95       952\n",
      "\n",
      "acc:  0.957983193277311\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48927038626609437\n",
      "mi F1:  0.957983193277311\n",
      "we F1:  0.946676524687128\n",
      "Loss:  0.0704667940735817\n",
      "Loss:  0.0500669851899147\n",
      "Loss:  0.06153963506221771\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.06952227652072906\n",
      "Loss:  0.03862801194190979\n",
      "Loss:  0.06578328460454941\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.08533740043640137\n",
      "Loss:  0.06470786780118942\n",
      "Loss:  0.037161264568567276\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.026050806045532227\n",
      "Eval Loss:  0.05600893497467041\n",
      "Eval Loss:  0.12735682725906372\n",
      "[[17988  1255]\n",
      " [ 1885 10811]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92     19243\n",
      "           1       0.90      0.85      0.87     12696\n",
      "\n",
      "    accuracy                           0.90     31939\n",
      "   macro avg       0.90      0.89      0.90     31939\n",
      "weighted avg       0.90      0.90      0.90     31939\n",
      "\n",
      "acc:  0.9016875919721971\n",
      "pre:  0.8959887286590419\n",
      "rec:  0.8515280403276623\n",
      "ma F1:  0.896459369380158\n",
      "mi F1:  0.9016875919721971\n",
      "we F1:  0.9012286564513687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[914   7]\n",
      " [ 31   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       921\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.96       952\n",
      "   macro avg       0.48      0.50      0.49       952\n",
      "weighted avg       0.94      0.96      0.95       952\n",
      "\n",
      "acc:  0.9600840336134454\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48981779206859594\n",
      "mi F1:  0.9600840336134454\n",
      "we F1:  0.9477356859142371\n",
      "Loss:  0.06974293291568756\n",
      "Loss:  0.06739531457424164\n",
      "Loss:  0.04666699096560478\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.06846042722463608\n",
      "Loss:  0.06529901176691055\n",
      "Loss:  0.0630737766623497\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.04005248472094536\n",
      "Loss:  0.04690182954072952\n",
      "Loss:  0.05757568031549454\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.03244125843048096\n",
      "Eval Loss:  0.033170104026794434\n",
      "Eval Loss:  0.041472434997558594\n",
      "[[18181  1062]\n",
      " [ 2057 10639]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     19243\n",
      "           1       0.91      0.84      0.87     12696\n",
      "\n",
      "    accuracy                           0.90     31939\n",
      "   macro avg       0.90      0.89      0.90     31939\n",
      "weighted avg       0.90      0.90      0.90     31939\n",
      "\n",
      "acc:  0.902345095337988\n",
      "pre:  0.9092385266216563\n",
      "rec:  0.8379804662885948\n",
      "ma F1:  0.8965781936725252\n",
      "mi F1:  0.902345095337988\n",
      "we F1:  0.9015842802813538\n",
      "[[915   6]\n",
      " [ 31   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       921\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.96       952\n",
      "   macro avg       0.48      0.50      0.49       952\n",
      "weighted avg       0.94      0.96      0.95       952\n",
      "\n",
      "acc:  0.9611344537815126\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49009105516871987\n",
      "mi F1:  0.9611344537815126\n",
      "we F1:  0.9482644155680484\n",
      "Loss:  0.07424971461296082\n",
      "Loss:  0.0794358029961586\n",
      "Loss:  0.06253810971975327\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.05178818851709366\n",
      "Loss:  0.061089083552360535\n",
      "Loss:  0.0768318846821785\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.07127435505390167\n",
      "Loss:  0.08867061883211136\n",
      "Loss:  0.056574746966362\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.05948352813720703\n",
      "Eval Loss:  0.04723513126373291\n",
      "Eval Loss:  0.05003094673156738\n",
      "[[17684  1559]\n",
      " [ 1398 11298]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92     19243\n",
      "           1       0.88      0.89      0.88     12696\n",
      "\n",
      "    accuracy                           0.91     31939\n",
      "   macro avg       0.90      0.90      0.90     31939\n",
      "weighted avg       0.91      0.91      0.91     31939\n",
      "\n",
      "acc:  0.9074172641598046\n",
      "pre:  0.8787430971455238\n",
      "rec:  0.8898865784499055\n",
      "ma F1:  0.9035619144318925\n",
      "mi F1:  0.9074172641598046\n",
      "we F1:  0.9075144629236934\n",
      "[[909  12]\n",
      " [ 30   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       921\n",
      "           1       0.08      0.03      0.05        31\n",
      "\n",
      "    accuracy                           0.96       952\n",
      "   macro avg       0.52      0.51      0.51       952\n",
      "weighted avg       0.94      0.96      0.95       952\n",
      "\n",
      "acc:  0.9558823529411765\n",
      "pre:  0.07692307692307693\n",
      "rec:  0.03225806451612903\n",
      "ma F1:  0.5114369501466276\n",
      "mi F1:  0.9558823529411765\n",
      "we F1:  0.947071761255822\n",
      "Loss:  0.04940103366971016\n",
      "Loss:  0.04696233943104744\n",
      "Loss:  0.06990426778793335\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.08533898741006851\n",
      "Loss:  0.05974968895316124\n",
      "Loss:  0.09027838706970215\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.06259971112012863\n",
      "Loss:  0.07724931836128235\n",
      "Loss:  0.08373303711414337\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.12122762203216553\n",
      "Eval Loss:  0.02138340473175049\n",
      "Eval Loss:  0.007915973663330078\n",
      "[[17669  1574]\n",
      " [ 1599 11097]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92     19243\n",
      "           1       0.88      0.87      0.87     12696\n",
      "\n",
      "    accuracy                           0.90     31939\n",
      "   macro avg       0.90      0.90      0.90     31939\n",
      "weighted avg       0.90      0.90      0.90     31939\n",
      "\n",
      "acc:  0.9006543723973826\n",
      "pre:  0.8757793386473048\n",
      "rec:  0.874054820415879\n",
      "ma F1:  0.8962620929578384\n",
      "mi F1:  0.9006543723973826\n",
      "we F1:  0.9006376640915412\n",
      "[[909  12]\n",
      " [ 31   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       921\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.95       952\n",
      "   macro avg       0.48      0.49      0.49       952\n",
      "weighted avg       0.94      0.95      0.95       952\n",
      "\n",
      "acc:  0.9548319327731093\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48844707146695326\n",
      "mi F1:  0.9548319327731093\n",
      "we F1:  0.9450835143299663\n",
      "Loss:  0.050563372671604156\n",
      "Loss:  0.05208244174718857\n",
      "Loss:  0.04956603795289993\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.06987830996513367\n",
      "Loss:  0.06842752546072006\n",
      "Loss:  0.05501075088977814\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.08733882009983063\n",
      "Loss:  0.04727501794695854\n",
      "Loss:  0.06167548522353172\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.046982526779174805\n",
      "Eval Loss:  0.0411529541015625\n",
      "Eval Loss:  0.018700599670410156\n",
      "[[18152  1091]\n",
      " [ 1782 10914]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93     19243\n",
      "           1       0.91      0.86      0.88     12696\n",
      "\n",
      "    accuracy                           0.91     31939\n",
      "   macro avg       0.91      0.90      0.91     31939\n",
      "weighted avg       0.91      0.91      0.91     31939\n",
      "\n",
      "acc:  0.9100472776229688\n",
      "pre:  0.9091211995002082\n",
      "rec:  0.8596408317580341\n",
      "ma F1:  0.9051775378199995\n",
      "mi F1:  0.9100472776229688\n",
      "we F1:  0.9095823715434094\n",
      "[[911  10]\n",
      " [ 31   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       921\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.96       952\n",
      "   macro avg       0.48      0.49      0.49       952\n",
      "weighted avg       0.94      0.96      0.95       952\n",
      "\n",
      "acc:  0.9569327731092437\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48899624261943103\n",
      "mi F1:  0.9569327731092437\n",
      "we F1:  0.9461460912867562\n",
      "Loss:  0.04864242672920227\n",
      "Loss:  0.03713476285338402\n",
      "Loss:  0.06630698591470718\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.059579554945230484\n",
      "Loss:  0.04467666149139404\n",
      "Loss:  0.04858044534921646\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.04008404538035393\n",
      "Loss:  0.034894756972789764\n",
      "Loss:  0.062413640320301056\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.0348820686340332\n",
      "Eval Loss:  0.01511240005493164\n",
      "Eval Loss:  0.005636692047119141\n",
      "[[18490   753]\n",
      " [ 2386 10310]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92     19243\n",
      "           1       0.93      0.81      0.87     12696\n",
      "\n",
      "    accuracy                           0.90     31939\n",
      "   macro avg       0.91      0.89      0.89     31939\n",
      "weighted avg       0.90      0.90      0.90     31939\n",
      "\n",
      "acc:  0.9017189016562823\n",
      "pre:  0.9319352797613667\n",
      "rec:  0.8120667926906112\n",
      "ma F1:  0.8948197077413707\n",
      "mi F1:  0.9017189016562823\n",
      "we F1:  0.9003415931400168\n",
      "[[917   4]\n",
      " [ 31   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       921\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.96       952\n",
      "   macro avg       0.48      0.50      0.49       952\n",
      "weighted avg       0.94      0.96      0.95       952\n",
      "\n",
      "acc:  0.9632352941176471\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49063670411985016\n",
      "mi F1:  0.9632352941176471\n",
      "we F1:  0.9493201775092059\n",
      "Subject 14 Current Train Acc:  0.9017189016562823 Current Test Acc:  0.9632352941176471\n",
      "Loss:  0.08079203963279724\n",
      "Loss:  0.07292968034744263\n",
      "Loss:  0.07588288187980652\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.04387003183364868\n",
      "Loss:  0.07856100797653198\n",
      "Loss:  0.03862592577934265\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.07503864169120789\n",
      "Loss:  0.08931414783000946\n",
      "Loss:  0.06423872709274292\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.08723032474517822\n",
      "Eval Loss:  0.08447039127349854\n",
      "Eval Loss:  0.037459611892700195\n",
      "[[17933  1310]\n",
      " [ 1458 11238]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93     19243\n",
      "           1       0.90      0.89      0.89     12696\n",
      "\n",
      "    accuracy                           0.91     31939\n",
      "   macro avg       0.91      0.91      0.91     31939\n",
      "weighted avg       0.91      0.91      0.91     31939\n",
      "\n",
      "acc:  0.913334794451924\n",
      "pre:  0.8956008925725215\n",
      "rec:  0.8851606805293005\n",
      "ma F1:  0.9093517230929508\n",
      "mi F1:  0.913334794451924\n",
      "we F1:  0.9132467444801349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[914   7]\n",
      " [ 31   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       921\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.96       952\n",
      "   macro avg       0.48      0.50      0.49       952\n",
      "weighted avg       0.94      0.96      0.95       952\n",
      "\n",
      "acc:  0.9600840336134454\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48981779206859594\n",
      "mi F1:  0.9600840336134454\n",
      "we F1:  0.9477356859142371\n",
      "Loss:  0.05677051469683647\n",
      "Loss:  0.03406853973865509\n",
      "Loss:  0.041218411177396774\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.04833154007792473\n",
      "Loss:  0.06227772682905197\n",
      "Loss:  0.0843675434589386\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.07119690626859665\n",
      "Loss:  0.04671226441860199\n",
      "Loss:  0.06781791895627975\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.060117483139038086\n",
      "Eval Loss:  0.04297327995300293\n",
      "Eval Loss:  0.07543063163757324\n",
      "[[17917  1326]\n",
      " [ 1289 11407]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19243\n",
      "           1       0.90      0.90      0.90     12696\n",
      "\n",
      "    accuracy                           0.92     31939\n",
      "   macro avg       0.91      0.91      0.91     31939\n",
      "weighted avg       0.92      0.92      0.92     31939\n",
      "\n",
      "acc:  0.918125176116973\n",
      "pre:  0.8958611481975968\n",
      "rec:  0.8984719596723377\n",
      "ma F1:  0.9145762412801164\n",
      "mi F1:  0.918125176116973\n",
      "we F1:  0.9181453467143561\n",
      "[[915   6]\n",
      " [ 31   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       921\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.96       952\n",
      "   macro avg       0.48      0.50      0.49       952\n",
      "weighted avg       0.94      0.96      0.95       952\n",
      "\n",
      "acc:  0.9611344537815126\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49009105516871987\n",
      "mi F1:  0.9611344537815126\n",
      "we F1:  0.9482644155680484\n",
      "Loss:  0.038168277591466904\n",
      "Loss:  0.036029182374477386\n",
      "Loss:  0.05767694488167763\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.04280145838856697\n",
      "Loss:  0.07022206485271454\n",
      "Loss:  0.036337509751319885\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.03236306458711624\n",
      "Loss:  0.08531318604946136\n",
      "Loss:  0.041607566177845\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.08005142211914062\n",
      "Eval Loss:  0.04959559440612793\n",
      "Eval Loss:  0.032059669494628906\n",
      "[[17789  1454]\n",
      " [ 1187 11509]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     19243\n",
      "           1       0.89      0.91      0.90     12696\n",
      "\n",
      "    accuracy                           0.92     31939\n",
      "   macro avg       0.91      0.92      0.91     31939\n",
      "weighted avg       0.92      0.92      0.92     31939\n",
      "\n",
      "acc:  0.9173111243307555\n",
      "pre:  0.8878346061868395\n",
      "rec:  0.9065059861373661\n",
      "ma F1:  0.9139856980246903\n",
      "mi F1:  0.9173111243307555\n",
      "we F1:  0.9174525079014114\n",
      "[[905  16]\n",
      " [ 31   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       921\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.95       952\n",
      "   macro avg       0.48      0.49      0.49       952\n",
      "weighted avg       0.94      0.95      0.94       952\n",
      "\n",
      "acc:  0.9506302521008403\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4873451803984922\n",
      "mi F1:  0.9506302521008403\n",
      "we F1:  0.9429514940063263\n",
      "Loss:  0.050445083528757095\n",
      "Loss:  0.06098317354917526\n",
      "Loss:  0.08474849909543991\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.06667622178792953\n",
      "Loss:  0.06945443898439407\n",
      "Loss:  0.060278795659542084\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.023009110242128372\n",
      "Loss:  0.0682094469666481\n",
      "Loss:  0.06926309317350388\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  14 Training Time 5814.32467675209 Best Test Acc:  0.9632352941176471\n",
      "test subjects:  ['./seg\\\\b02', './seg\\\\b03', './seg\\\\x16', './seg\\\\x21']\n",
      "*********\n",
      "32330 1983\n",
      "30908 1983\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.821864128112793\n",
      "Eval Loss:  0.8026704788208008\n",
      "Eval Loss:  0.6015551090240479\n",
      "[[18532     0]\n",
      " [12376     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75     18532\n",
      "           1       0.00      0.00      0.00     12376\n",
      "\n",
      "    accuracy                           0.60     30908\n",
      "   macro avg       0.30      0.50      0.37     30908\n",
      "weighted avg       0.36      0.60      0.45     30908\n",
      "\n",
      "acc:  0.5995858677365083\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.3748381877022654\n",
      "mi F1:  0.5995858677365083\n",
      "we F1:  0.449495360068486\n",
      "[[1632    0]\n",
      " [ 351    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      1.00      0.90      1632\n",
      "           1       0.00      0.00      0.00       351\n",
      "\n",
      "    accuracy                           0.82      1983\n",
      "   macro avg       0.41      0.50      0.45      1983\n",
      "weighted avg       0.68      0.82      0.74      1983\n",
      "\n",
      "acc:  0.8229954614220878\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.45145228215767635\n",
      "mi F1:  0.8229954614220878\n",
      "we F1:  0.7430863585288228\n",
      "Subject 15 Current Train Acc:  0.5995858677365083 Current Test Acc:  0.8229954614220878\n",
      "Loss:  0.15587443113327026\n",
      "Loss:  0.16392964124679565\n",
      "Loss:  0.1489371806383133\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.148399218916893\n",
      "Loss:  0.12149837613105774\n",
      "Loss:  0.12116008996963501\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.12772642076015472\n",
      "Loss:  0.1548052728176117\n",
      "Loss:  0.08696313202381134\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.45928600430488586\n",
      "Eval Loss:  0.2973986864089966\n",
      "Eval Loss:  0.14771676063537598\n",
      "[[15132  3400]\n",
      " [ 2359 10017]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.82      0.84     18532\n",
      "           1       0.75      0.81      0.78     12376\n",
      "\n",
      "    accuracy                           0.81     30908\n",
      "   macro avg       0.81      0.81      0.81     30908\n",
      "weighted avg       0.82      0.81      0.81     30908\n",
      "\n",
      "acc:  0.8136728355118416\n",
      "pre:  0.7465901468286502\n",
      "rec:  0.8093891402714932\n",
      "ma F1:  0.8084261417655147\n",
      "mi F1:  0.8136728355118416\n",
      "we F1:  0.8147406376995106\n",
      "[[1372  260]\n",
      " [ 209  142]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85      1632\n",
      "           1       0.35      0.40      0.38       351\n",
      "\n",
      "    accuracy                           0.76      1983\n",
      "   macro avg       0.61      0.62      0.62      1983\n",
      "weighted avg       0.78      0.76      0.77      1983\n",
      "\n",
      "acc:  0.7634896621280888\n",
      "pre:  0.35323383084577115\n",
      "rec:  0.4045584045584046\n",
      "ma F1:  0.6155942678089386\n",
      "mi F1:  0.7634896621280888\n",
      "we F1:  0.7696219101852243\n",
      "Loss:  0.09788026660680771\n",
      "Loss:  0.11490307748317719\n",
      "Loss:  0.10250194370746613\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.08807434141635895\n",
      "Loss:  0.09493611007928848\n",
      "Loss:  0.10180383920669556\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.09647940844297409\n",
      "Loss:  0.11988340318202972\n",
      "Loss:  0.08980333060026169\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  1.3565447330474854\n",
      "Eval Loss:  1.1340463161468506\n",
      "Eval Loss:  0.009000062942504883\n",
      "[[17794   738]\n",
      " [ 4951  7425]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.96      0.86     18532\n",
      "           1       0.91      0.60      0.72     12376\n",
      "\n",
      "    accuracy                           0.82     30908\n",
      "   macro avg       0.85      0.78      0.79     30908\n",
      "weighted avg       0.83      0.82      0.81     30908\n",
      "\n",
      "acc:  0.8159376213278116\n",
      "pre:  0.9095920617420066\n",
      "rec:  0.5999515190691661\n",
      "ma F1:  0.7925949067971512\n",
      "mi F1:  0.8159376213278116\n",
      "we F1:  0.8064533068984864\n",
      "[[1627    5]\n",
      " [ 331   20]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91      1632\n",
      "           1       0.80      0.06      0.11       351\n",
      "\n",
      "    accuracy                           0.83      1983\n",
      "   macro avg       0.82      0.53      0.51      1983\n",
      "weighted avg       0.83      0.83      0.76      1983\n",
      "\n",
      "acc:  0.8305597579425114\n",
      "pre:  0.8\n",
      "rec:  0.05698005698005698\n",
      "ma F1:  0.5063948319800865\n",
      "mi F1:  0.8305597579425112\n",
      "we F1:  0.7647988582139796\n",
      "Subject 15 Current Train Acc:  0.8159376213278116 Current Test Acc:  0.8305597579425114\n",
      "Loss:  0.05460518226027489\n",
      "Loss:  0.08285285532474518\n",
      "Loss:  0.0887695774435997\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.07327456772327423\n",
      "Loss:  0.0681900754570961\n",
      "Loss:  0.06638306379318237\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.06299369037151337\n",
      "Loss:  0.09283565729856491\n",
      "Loss:  0.07789003103971481\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  1.386523723602295\n",
      "Eval Loss:  1.0820358991622925\n",
      "Eval Loss:  0.007976770401000977\n",
      "[[17918   614]\n",
      " [ 4503  7873]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.88     18532\n",
      "           1       0.93      0.64      0.75     12376\n",
      "\n",
      "    accuracy                           0.83     30908\n",
      "   macro avg       0.86      0.80      0.81     30908\n",
      "weighted avg       0.85      0.83      0.83     30908\n",
      "\n",
      "acc:  0.8344441568525948\n",
      "pre:  0.9276540591492871\n",
      "rec:  0.6361506140917905\n",
      "ma F1:  0.8148925742942548\n",
      "mi F1:  0.8344441568525948\n",
      "we F1:  0.8268746093593758\n",
      "[[1628    4]\n",
      " [ 327   24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91      1632\n",
      "           1       0.86      0.07      0.13       351\n",
      "\n",
      "    accuracy                           0.83      1983\n",
      "   macro avg       0.84      0.53      0.52      1983\n",
      "weighted avg       0.84      0.83      0.77      1983\n",
      "\n",
      "acc:  0.8330811901159859\n",
      "pre:  0.8571428571428571\n",
      "rec:  0.06837606837606838\n",
      "ma F1:  0.5171857035777835\n",
      "mi F1:  0.8330811901159859\n",
      "we F1:  0.7694688196971335\n",
      "Subject 15 Current Train Acc:  0.8344441568525948 Current Test Acc:  0.8330811901159859\n",
      "Loss:  0.07789025455713272\n",
      "Loss:  0.11869882047176361\n",
      "Loss:  0.08105140924453735\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.04449068009853363\n",
      "Loss:  0.08278676867485046\n",
      "Loss:  0.09118855744600296\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.0686236098408699\n",
      "Loss:  0.06169959902763367\n",
      "Loss:  0.09985890984535217\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  1.5476152896881104\n",
      "Eval Loss:  1.0839056968688965\n",
      "Eval Loss:  0.006400346755981445\n",
      "[[18025   507]\n",
      " [ 4250  8126]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.97      0.88     18532\n",
      "           1       0.94      0.66      0.77     12376\n",
      "\n",
      "    accuracy                           0.85     30908\n",
      "   macro avg       0.88      0.81      0.83     30908\n",
      "weighted avg       0.86      0.85      0.84     30908\n",
      "\n",
      "acc:  0.8460916267632975\n",
      "pre:  0.9412718637785242\n",
      "rec:  0.6565934065934066\n",
      "ma F1:  0.8285000467792287\n",
      "mi F1:  0.8460916267632975\n",
      "we F1:  0.8394399160975363\n",
      "[[1627    5]\n",
      " [ 325   26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91      1632\n",
      "           1       0.84      0.07      0.14       351\n",
      "\n",
      "    accuracy                           0.83      1983\n",
      "   macro avg       0.84      0.54      0.52      1983\n",
      "weighted avg       0.83      0.83      0.77      1983\n",
      "\n",
      "acc:  0.8335854765506808\n",
      "pre:  0.8387096774193549\n",
      "rec:  0.07407407407407407\n",
      "ma F1:  0.5220248807965594\n",
      "mi F1:  0.8335854765506808\n",
      "we F1:  0.7713122781488577\n",
      "Subject 15 Current Train Acc:  0.8460916267632975 Current Test Acc:  0.8335854765506808\n",
      "Loss:  0.09312234818935394\n",
      "Loss:  0.06732504814863205\n",
      "Loss:  0.06863577663898468\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.08098053187131882\n",
      "Loss:  0.07334303855895996\n",
      "Loss:  0.07500030845403671\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.09423140436410904\n",
      "Loss:  0.07703695446252823\n",
      "Loss:  0.05939067155122757\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  1.1439827680587769\n",
      "Eval Loss:  0.8481069207191467\n",
      "Eval Loss:  0.007947921752929688\n",
      "[[17785   747]\n",
      " [ 3056  9320]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90     18532\n",
      "           1       0.93      0.75      0.83     12376\n",
      "\n",
      "    accuracy                           0.88     30908\n",
      "   macro avg       0.89      0.86      0.87     30908\n",
      "weighted avg       0.88      0.88      0.87     30908\n",
      "\n",
      "acc:  0.8769574220266597\n",
      "pre:  0.925797159034469\n",
      "rec:  0.7530704589528119\n",
      "ma F1:  0.8669797337761684\n",
      "mi F1:  0.8769574220266597\n",
      "we F1:  0.87423580570411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1608   24]\n",
      " [ 299   52]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.99      0.91      1632\n",
      "           1       0.68      0.15      0.24       351\n",
      "\n",
      "    accuracy                           0.84      1983\n",
      "   macro avg       0.76      0.57      0.58      1983\n",
      "weighted avg       0.82      0.84      0.79      1983\n",
      "\n",
      "acc:  0.8371154815935451\n",
      "pre:  0.6842105263157895\n",
      "rec:  0.14814814814814814\n",
      "ma F1:  0.5761454994960802\n",
      "mi F1:  0.8371154815935451\n",
      "we F1:  0.7909928947832605\n",
      "Subject 15 Current Train Acc:  0.8769574220266597 Current Test Acc:  0.8371154815935451\n",
      "Loss:  0.08558283746242523\n",
      "Loss:  0.09682188928127289\n",
      "Loss:  0.06053856015205383\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.09647662937641144\n",
      "Loss:  0.06579550355672836\n",
      "Loss:  0.051308032125234604\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.08406215906143188\n",
      "Loss:  0.055067919194698334\n",
      "Loss:  0.08955351263284683\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  1.370928406715393\n",
      "Eval Loss:  0.6774848103523254\n",
      "Eval Loss:  0.009785890579223633\n",
      "[[17403  1129]\n",
      " [ 2146 10230]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91     18532\n",
      "           1       0.90      0.83      0.86     12376\n",
      "\n",
      "    accuracy                           0.89     30908\n",
      "   macro avg       0.90      0.88      0.89     30908\n",
      "weighted avg       0.89      0.89      0.89     30908\n",
      "\n",
      "acc:  0.8940403778956905\n",
      "pre:  0.9006074478387182\n",
      "rec:  0.8265998707175177\n",
      "ma F1:  0.8880086119357955\n",
      "mi F1:  0.8940403778956906\n",
      "we F1:  0.8931851839766589\n",
      "[[1426  206]\n",
      " [ 229  122]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.87      1632\n",
      "           1       0.37      0.35      0.36       351\n",
      "\n",
      "    accuracy                           0.78      1983\n",
      "   macro avg       0.62      0.61      0.61      1983\n",
      "weighted avg       0.77      0.78      0.78      1983\n",
      "\n",
      "acc:  0.7806354009077155\n",
      "pre:  0.3719512195121951\n",
      "rec:  0.3475783475783476\n",
      "ma F1:  0.61350623444972\n",
      "mi F1:  0.7806354009077157\n",
      "we F1:  0.7776875705177356\n",
      "Loss:  0.06786026805639267\n",
      "Loss:  0.08765639364719391\n",
      "Loss:  0.049054693430662155\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.07904835790395737\n",
      "Loss:  0.05067586153745651\n",
      "Loss:  0.06130639463663101\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.05832923576235771\n",
      "Loss:  0.05883544683456421\n",
      "Loss:  0.0800337940454483\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  1.1723389625549316\n",
      "Eval Loss:  0.7876614928245544\n",
      "Eval Loss:  0.005307197570800781\n",
      "[[17611   921]\n",
      " [ 2336 10040]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.92     18532\n",
      "           1       0.92      0.81      0.86     12376\n",
      "\n",
      "    accuracy                           0.89     30908\n",
      "   macro avg       0.90      0.88      0.89     30908\n",
      "weighted avg       0.90      0.89      0.89     30908\n",
      "\n",
      "acc:  0.8946227513912256\n",
      "pre:  0.9159748198157103\n",
      "rec:  0.8112475759534583\n",
      "ma F1:  0.8878963226737397\n",
      "mi F1:  0.8946227513912256\n",
      "we F1:  0.8933655995440134\n",
      "[[1561   71]\n",
      " [ 270   81]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90      1632\n",
      "           1       0.53      0.23      0.32       351\n",
      "\n",
      "    accuracy                           0.83      1983\n",
      "   macro avg       0.69      0.59      0.61      1983\n",
      "weighted avg       0.80      0.83      0.80      1983\n",
      "\n",
      "acc:  0.8280383257690368\n",
      "pre:  0.5328947368421053\n",
      "rec:  0.23076923076923078\n",
      "ma F1:  0.6117990296741067\n",
      "mi F1:  0.8280383257690368\n",
      "we F1:  0.7989629069022185\n",
      "Loss:  0.05814949795603752\n",
      "Loss:  0.0663192868232727\n",
      "Loss:  0.04940440505743027\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.03687985986471176\n",
      "Loss:  0.04758113622665405\n",
      "Loss:  0.061900585889816284\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.1057247668504715\n",
      "Loss:  0.03679182380437851\n",
      "Loss:  0.03259100392460823\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  1.4124531745910645\n",
      "Eval Loss:  0.523479700088501\n",
      "Eval Loss:  0.009899616241455078\n",
      "[[17039  1493]\n",
      " [ 1654 10722]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92     18532\n",
      "           1       0.88      0.87      0.87     12376\n",
      "\n",
      "    accuracy                           0.90     30908\n",
      "   macro avg       0.89      0.89      0.89     30908\n",
      "weighted avg       0.90      0.90      0.90     30908\n",
      "\n",
      "acc:  0.898181700530607\n",
      "pre:  0.8777732296356938\n",
      "rec:  0.8663542340012929\n",
      "ma F1:  0.8937431956997814\n",
      "mi F1:  0.898181700530607\n",
      "we F1:  0.8980685773269085\n",
      "[[1507  125]\n",
      " [ 213  138]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90      1632\n",
      "           1       0.52      0.39      0.45       351\n",
      "\n",
      "    accuracy                           0.83      1983\n",
      "   macro avg       0.70      0.66      0.67      1983\n",
      "weighted avg       0.81      0.83      0.82      1983\n",
      "\n",
      "acc:  0.8295511850731215\n",
      "pre:  0.5247148288973384\n",
      "rec:  0.39316239316239315\n",
      "ma F1:  0.6743380392278809\n",
      "mi F1:  0.8295511850731214\n",
      "we F1:  0.8195740069618132\n",
      "Loss:  0.08394280076026917\n",
      "Loss:  0.10010942071676254\n",
      "Loss:  0.0870201513171196\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.06410539150238037\n",
      "Loss:  0.059572163969278336\n",
      "Loss:  0.05757668614387512\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.04929590970277786\n",
      "Loss:  0.05392045900225639\n",
      "Loss:  0.04805896803736687\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  1.2757632732391357\n",
      "Eval Loss:  0.404838502407074\n",
      "Eval Loss:  0.00467371940612793\n",
      "[[17734   798]\n",
      " [ 2271 10105]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92     18532\n",
      "           1       0.93      0.82      0.87     12376\n",
      "\n",
      "    accuracy                           0.90     30908\n",
      "   macro avg       0.91      0.89      0.89     30908\n",
      "weighted avg       0.90      0.90      0.90     30908\n",
      "\n",
      "acc:  0.9007053190112593\n",
      "pre:  0.9268091351004311\n",
      "rec:  0.8164996767937944\n",
      "ma F1:  0.8942633446601026\n",
      "mi F1:  0.9007053190112592\n",
      "we F1:  0.8994615087845909\n",
      "[[1571   61]\n",
      " [ 272   79]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90      1632\n",
      "           1       0.56      0.23      0.32       351\n",
      "\n",
      "    accuracy                           0.83      1983\n",
      "   macro avg       0.71      0.59      0.61      1983\n",
      "weighted avg       0.80      0.83      0.80      1983\n",
      "\n",
      "acc:  0.8320726172465961\n",
      "pre:  0.5642857142857143\n",
      "rec:  0.22507122507122507\n",
      "ma F1:  0.612982461281484\n",
      "mi F1:  0.8320726172465961\n",
      "we F1:  0.8010886876831653\n",
      "Loss:  0.0584137961268425\n",
      "Loss:  0.08373142778873444\n",
      "Loss:  0.06114867702126503\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.06000465154647827\n",
      "Loss:  0.0680122822523117\n",
      "Loss:  0.059628766030073166\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.07800773531198502\n",
      "Loss:  0.07146581262350082\n",
      "Loss:  0.06504534929990768\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  1.475950002670288\n",
      "Eval Loss:  0.3248082995414734\n",
      "Eval Loss:  0.006128787994384766\n",
      "[[17256  1276]\n",
      " [ 1596 10780]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92     18532\n",
      "           1       0.89      0.87      0.88     12376\n",
      "\n",
      "    accuracy                           0.91     30908\n",
      "   macro avg       0.90      0.90      0.90     30908\n",
      "weighted avg       0.91      0.91      0.91     30908\n",
      "\n",
      "acc:  0.9070790733790605\n",
      "pre:  0.8941605839416058\n",
      "rec:  0.8710407239819005\n",
      "ma F1:  0.9028124685120713\n",
      "mi F1:  0.9070790733790605\n",
      "we F1:  0.9068682467024952\n",
      "[[1445  187]\n",
      " [ 232  119]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.89      0.87      1632\n",
      "           1       0.39      0.34      0.36       351\n",
      "\n",
      "    accuracy                           0.79      1983\n",
      "   macro avg       0.63      0.61      0.62      1983\n",
      "weighted avg       0.78      0.79      0.78      1983\n",
      "\n",
      "acc:  0.7887039838628341\n",
      "pre:  0.3888888888888889\n",
      "rec:  0.33903133903133903\n",
      "ma F1:  0.6178141529052494\n",
      "mi F1:  0.7887039838628341\n",
      "we F1:  0.7829045552104273\n",
      "Loss:  0.0423763245344162\n",
      "Loss:  0.049779292196035385\n",
      "Loss:  0.037149179726839066\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.04725715145468712\n",
      "Loss:  0.08186373114585876\n",
      "Loss:  0.05834672972559929\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.04501760005950928\n",
      "Loss:  0.08734636008739471\n",
      "Loss:  0.06519521027803421\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  1.309194803237915\n",
      "Eval Loss:  0.35223686695098877\n",
      "Eval Loss:  0.00506591796875\n",
      "[[17430  1102]\n",
      " [ 1745 10631]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92     18532\n",
      "           1       0.91      0.86      0.88     12376\n",
      "\n",
      "    accuracy                           0.91     30908\n",
      "   macro avg       0.91      0.90      0.90     30908\n",
      "weighted avg       0.91      0.91      0.91     30908\n",
      "\n",
      "acc:  0.9078879254561926\n",
      "pre:  0.9060768771840109\n",
      "rec:  0.8590012928248222\n",
      "ma F1:  0.9032040486054496\n",
      "mi F1:  0.9078879254561926\n",
      "we F1:  0.9074449584294199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1474  158]\n",
      " [ 243  108]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88      1632\n",
      "           1       0.41      0.31      0.35       351\n",
      "\n",
      "    accuracy                           0.80      1983\n",
      "   macro avg       0.63      0.61      0.62      1983\n",
      "weighted avg       0.78      0.80      0.79      1983\n",
      "\n",
      "acc:  0.7977811396873424\n",
      "pre:  0.40601503759398494\n",
      "rec:  0.3076923076923077\n",
      "ma F1:  0.6151719011408132\n",
      "mi F1:  0.7977811396873425\n",
      "we F1:  0.7864181929256623\n",
      "Loss:  0.03958167880773544\n",
      "Loss:  0.06358294188976288\n",
      "Loss:  0.05501016229391098\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.05058477446436882\n",
      "Loss:  0.05885345861315727\n",
      "Loss:  0.060454413294792175\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.06389914453029633\n",
      "Loss:  0.026352841407060623\n",
      "Loss:  0.049822255969047546\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  1.2857818603515625\n",
      "Eval Loss:  0.15470701456069946\n",
      "Eval Loss:  0.0073468685150146484\n",
      "[[17005  1527]\n",
      " [ 1265 11111]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92     18532\n",
      "           1       0.88      0.90      0.89     12376\n",
      "\n",
      "    accuracy                           0.91     30908\n",
      "   macro avg       0.90      0.91      0.91     30908\n",
      "weighted avg       0.91      0.91      0.91     30908\n",
      "\n",
      "acc:  0.9096674000258833\n",
      "pre:  0.8791739199240386\n",
      "rec:  0.8977860374919199\n",
      "ma F1:  0.9062585318506078\n",
      "mi F1:  0.9096674000258833\n",
      "we F1:  0.9098189309831147\n",
      "[[1463  169]\n",
      " [ 236  115]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88      1632\n",
      "           1       0.40      0.33      0.36       351\n",
      "\n",
      "    accuracy                           0.80      1983\n",
      "   macro avg       0.63      0.61      0.62      1983\n",
      "weighted avg       0.78      0.80      0.79      1983\n",
      "\n",
      "acc:  0.7957639939485628\n",
      "pre:  0.40492957746478875\n",
      "rec:  0.32763532763532766\n",
      "ma F1:  0.6203098074163725\n",
      "mi F1:  0.7957639939485628\n",
      "we F1:  0.7870433481787877\n",
      "Loss:  0.04015156254172325\n",
      "Loss:  0.04424560070037842\n",
      "Loss:  0.04077184200286865\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.057921383529901505\n",
      "Loss:  0.06174977868795395\n",
      "Loss:  0.07977106422185898\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.057099901139736176\n",
      "Loss:  0.0805082619190216\n",
      "Loss:  0.1014905720949173\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  1.3316080570220947\n",
      "Eval Loss:  0.3628479838371277\n",
      "Eval Loss:  0.004208564758300781\n",
      "[[17682   850]\n",
      " [ 2052 10324]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     18532\n",
      "           1       0.92      0.83      0.88     12376\n",
      "\n",
      "    accuracy                           0.91     30908\n",
      "   macro avg       0.91      0.89      0.90     30908\n",
      "weighted avg       0.91      0.91      0.91     30908\n",
      "\n",
      "acc:  0.9061084508865018\n",
      "pre:  0.9239305530696259\n",
      "rec:  0.8341952165481578\n",
      "ma F1:  0.9004676328167904\n",
      "mi F1:  0.9061084508865018\n",
      "we F1:  0.9051869690545105\n",
      "[[1575   57]\n",
      " [ 287   64]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.90      1632\n",
      "           1       0.53      0.18      0.27       351\n",
      "\n",
      "    accuracy                           0.83      1983\n",
      "   macro avg       0.69      0.57      0.59      1983\n",
      "weighted avg       0.79      0.83      0.79      1983\n",
      "\n",
      "acc:  0.8265254664649521\n",
      "pre:  0.5289256198347108\n",
      "rec:  0.18233618233618235\n",
      "ma F1:  0.5863659736303397\n",
      "mi F1:  0.8265254664649521\n",
      "we F1:  0.7899690909838397\n",
      "Loss:  0.10899292677640915\n",
      "Loss:  0.061926476657390594\n",
      "Loss:  0.05836820602416992\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.0513324961066246\n",
      "Loss:  0.06335428357124329\n",
      "Loss:  0.043528273701667786\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.06812722235918045\n",
      "Loss:  0.044125188142061234\n",
      "Loss:  0.07544602453708649\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  1.470451831817627\n",
      "Eval Loss:  0.2869027853012085\n",
      "Eval Loss:  0.007094621658325195\n",
      "[[16828  1704]\n",
      " [ 1048 11328]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92     18532\n",
      "           1       0.87      0.92      0.89     12376\n",
      "\n",
      "    accuracy                           0.91     30908\n",
      "   macro avg       0.91      0.91      0.91     30908\n",
      "weighted avg       0.91      0.91      0.91     30908\n",
      "\n",
      "acc:  0.9109615633492947\n",
      "pre:  0.8692449355432781\n",
      "rec:  0.9153199741435035\n",
      "ma F1:  0.9080499372629371\n",
      "mi F1:  0.9109615633492947\n",
      "we F1:  0.911308840933413\n",
      "[[1418  214]\n",
      " [ 218  133]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87      1632\n",
      "           1       0.38      0.38      0.38       351\n",
      "\n",
      "    accuracy                           0.78      1983\n",
      "   macro avg       0.63      0.62      0.62      1983\n",
      "weighted avg       0.78      0.78      0.78      1983\n",
      "\n",
      "acc:  0.7821482602118003\n",
      "pre:  0.38328530259365995\n",
      "rec:  0.3789173789173789\n",
      "ma F1:  0.6244489413712198\n",
      "mi F1:  0.7821482602118002\n",
      "we F1:  0.7816573673905066\n",
      "Loss:  0.05160679668188095\n",
      "Loss:  0.05358900874853134\n",
      "Loss:  0.06764353811740875\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.06710872054100037\n",
      "Loss:  0.04034734517335892\n",
      "Loss:  0.03443692624568939\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.08792760223150253\n",
      "Loss:  0.05789874121546745\n",
      "Loss:  0.05569932982325554\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  1.5170550346374512\n",
      "Eval Loss:  0.10346972942352295\n",
      "Eval Loss:  0.0059795379638671875\n",
      "[[16985  1547]\n",
      " [ 1037 11339]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     18532\n",
      "           1       0.88      0.92      0.90     12376\n",
      "\n",
      "    accuracy                           0.92     30908\n",
      "   macro avg       0.91      0.92      0.91     30908\n",
      "weighted avg       0.92      0.92      0.92     30908\n",
      "\n",
      "acc:  0.9163970493076227\n",
      "pre:  0.8799472295514512\n",
      "rec:  0.9162087912087912\n",
      "ma F1:  0.9135110201460095\n",
      "mi F1:  0.9163970493076227\n",
      "we F1:  0.9166577426962913\n",
      "[[1389  243]\n",
      " [ 213  138]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86      1632\n",
      "           1       0.36      0.39      0.38       351\n",
      "\n",
      "    accuracy                           0.77      1983\n",
      "   macro avg       0.61      0.62      0.62      1983\n",
      "weighted avg       0.78      0.77      0.77      1983\n",
      "\n",
      "acc:  0.7700453857791225\n",
      "pre:  0.36220472440944884\n",
      "rec:  0.39316239316239315\n",
      "ma F1:  0.6180236625201496\n",
      "mi F1:  0.7700453857791225\n",
      "we F1:  0.7736909906534385\n",
      "Loss:  0.06890275329351425\n",
      "Loss:  0.036284834146499634\n",
      "Loss:  0.09142950922250748\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.047060996294021606\n",
      "Loss:  0.03769049793481827\n",
      "Loss:  0.09931711107492447\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.05708446726202965\n",
      "Loss:  0.06207740306854248\n",
      "Loss:  0.05416443943977356\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  1.3930552005767822\n",
      "Eval Loss:  0.13766080141067505\n",
      "Eval Loss:  0.004133939743041992\n",
      "[[17471  1061]\n",
      " [ 1344 11032]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     18532\n",
      "           1       0.91      0.89      0.90     12376\n",
      "\n",
      "    accuracy                           0.92     30908\n",
      "   macro avg       0.92      0.92      0.92     30908\n",
      "weighted avg       0.92      0.92      0.92     30908\n",
      "\n",
      "acc:  0.9221884301798887\n",
      "pre:  0.9122632928140246\n",
      "rec:  0.8914027149321267\n",
      "ma F1:  0.9186581507293856\n",
      "mi F1:  0.9221884301798887\n",
      "we F1:  0.9220332711358613\n",
      "[[1488  144]\n",
      " [ 266   85]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88      1632\n",
      "           1       0.37      0.24      0.29       351\n",
      "\n",
      "    accuracy                           0.79      1983\n",
      "   macro avg       0.61      0.58      0.59      1983\n",
      "weighted avg       0.76      0.79      0.78      1983\n",
      "\n",
      "acc:  0.7932425617750882\n",
      "pre:  0.37117903930131\n",
      "rec:  0.24216524216524216\n",
      "ma F1:  0.5860083100800456\n",
      "mi F1:  0.7932425617750882\n",
      "we F1:  0.7752221920624759\n",
      "Loss:  0.07033449411392212\n",
      "Loss:  0.06479094922542572\n",
      "Loss:  0.05956432223320007\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.07398280501365662\n",
      "Loss:  0.05329351872205734\n",
      "Loss:  0.03155793994665146\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.03686617687344551\n",
      "Loss:  0.037555720657110214\n",
      "Loss:  0.06324609369039536\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  1.1986746788024902\n",
      "Eval Loss:  0.15606218576431274\n",
      "Eval Loss:  0.004643440246582031\n",
      "[[17417  1115]\n",
      " [ 1266 11110]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     18532\n",
      "           1       0.91      0.90      0.90     12376\n",
      "\n",
      "    accuracy                           0.92     30908\n",
      "   macro avg       0.92      0.92      0.92     30908\n",
      "weighted avg       0.92      0.92      0.92     30908\n",
      "\n",
      "acc:  0.9229649281739356\n",
      "pre:  0.9087934560327199\n",
      "rec:  0.8977052359405301\n",
      "ma F1:  0.9196178691617256\n",
      "mi F1:  0.9229649281739356\n",
      "we F1:  0.9228847940513981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1405  227]\n",
      " [ 242  109]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.86      0.86      1632\n",
      "           1       0.32      0.31      0.32       351\n",
      "\n",
      "    accuracy                           0.76      1983\n",
      "   macro avg       0.59      0.59      0.59      1983\n",
      "weighted avg       0.76      0.76      0.76      1983\n",
      "\n",
      "acc:  0.7634896621280888\n",
      "pre:  0.3244047619047619\n",
      "rec:  0.31054131054131057\n",
      "ma F1:  0.5871451382424346\n",
      "mi F1:  0.7634896621280888\n",
      "we F1:  0.7614486375460789\n",
      "Loss:  0.05027206614613533\n",
      "Loss:  0.030543137341737747\n",
      "Loss:  0.03960505127906799\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.06521883606910706\n",
      "Loss:  0.019644442945718765\n",
      "Loss:  0.03156251832842827\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.060534678399562836\n",
      "Loss:  0.041274294257164\n",
      "Loss:  0.05228165164589882\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  1.0035953521728516\n",
      "Eval Loss:  0.16195178031921387\n",
      "Eval Loss:  0.0035490989685058594\n",
      "[[17224  1308]\n",
      " [ 1157 11219]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93     18532\n",
      "           1       0.90      0.91      0.90     12376\n",
      "\n",
      "    accuracy                           0.92     30908\n",
      "   macro avg       0.92      0.92      0.92     30908\n",
      "weighted avg       0.92      0.92      0.92     30908\n",
      "\n",
      "acc:  0.9202471851947716\n",
      "pre:  0.8955855352438732\n",
      "rec:  0.9065126050420168\n",
      "ma F1:  0.9171186500917194\n",
      "mi F1:  0.9202471851947714\n",
      "we F1:  0.9203258544371632\n",
      "[[1372  260]\n",
      " [ 227  124]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85      1632\n",
      "           1       0.32      0.35      0.34       351\n",
      "\n",
      "    accuracy                           0.75      1983\n",
      "   macro avg       0.59      0.60      0.59      1983\n",
      "weighted avg       0.76      0.75      0.76      1983\n",
      "\n",
      "acc:  0.7544125063035805\n",
      "pre:  0.3229166666666667\n",
      "rec:  0.35327635327635326\n",
      "ma F1:  0.5933438184930425\n",
      "mi F1:  0.7544125063035804\n",
      "we F1:  0.758671534106263\n",
      "Loss:  0.07114733010530472\n",
      "Loss:  0.052045922726392746\n",
      "Loss:  0.040818504989147186\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.05540221557021141\n",
      "Loss:  0.06987734138965607\n",
      "Loss:  0.05454567074775696\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.0531817227602005\n",
      "Loss:  0.047697681933641434\n",
      "Loss:  0.07967768609523773\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  1.0074260234832764\n",
      "Eval Loss:  0.1820325255393982\n",
      "Eval Loss:  0.008170843124389648\n",
      "[[16657  1875]\n",
      " [  770 11606]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.90      0.93     18532\n",
      "           1       0.86      0.94      0.90     12376\n",
      "\n",
      "    accuracy                           0.91     30908\n",
      "   macro avg       0.91      0.92      0.91     30908\n",
      "weighted avg       0.92      0.91      0.91     30908\n",
      "\n",
      "acc:  0.9144234502394202\n",
      "pre:  0.8609153623618426\n",
      "rec:  0.9377828054298643\n",
      "ma F1:  0.9120753114183792\n",
      "mi F1:  0.9144234502394202\n",
      "we F1:  0.9149371491895786\n",
      "[[1382  250]\n",
      " [ 209  142]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86      1632\n",
      "           1       0.36      0.40      0.38       351\n",
      "\n",
      "    accuracy                           0.77      1983\n",
      "   macro avg       0.62      0.63      0.62      1983\n",
      "weighted avg       0.78      0.77      0.77      1983\n",
      "\n",
      "acc:  0.7685325264750378\n",
      "pre:  0.3622448979591837\n",
      "rec:  0.4045584045584046\n",
      "ma F1:  0.619910142820216\n",
      "mi F1:  0.7685325264750379\n",
      "we F1:  0.773446653676528\n",
      "Loss:  0.05004476010799408\n",
      "Loss:  0.045795902609825134\n",
      "Loss:  0.07101376354694366\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.07288806140422821\n",
      "Loss:  0.042820826172828674\n",
      "Loss:  0.04139365255832672\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.06354813277721405\n",
      "Loss:  0.025975413620471954\n",
      "Loss:  0.0400545559823513\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  15 Training Time 5975.5635578632355 Best Test Acc:  0.8371154815935451\n",
      "test subjects:  ['./seg\\\\b04', './seg\\\\c08']\n",
      "*********\n",
      "33371 942\n",
      "32061 830\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.7757585048675537\n",
      "Eval Loss:  0.7598429322242737\n",
      "Eval Loss:  0.6669012308120728\n",
      "[[   82 19262]\n",
      " [   32 12685]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.00      0.01     19344\n",
      "           1       0.40      1.00      0.57     12717\n",
      "\n",
      "    accuracy                           0.40     32061\n",
      "   macro avg       0.56      0.50      0.29     32061\n",
      "weighted avg       0.59      0.40      0.23     32061\n",
      "\n",
      "acc:  0.39820966283022985\n",
      "pre:  0.397063887062948\n",
      "rec:  0.9974836832586301\n",
      "ma F1:  0.28822369805832637\n",
      "mi F1:  0.39820966283022985\n",
      "we F1:  0.23039008792237428\n",
      "[[  2 818]\n",
      " [  0  10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.00      0.00       820\n",
      "           1       0.01      1.00      0.02        10\n",
      "\n",
      "    accuracy                           0.01       830\n",
      "   macro avg       0.51      0.50      0.01       830\n",
      "weighted avg       0.99      0.01      0.01       830\n",
      "\n",
      "acc:  0.014457831325301205\n",
      "pre:  0.012077294685990338\n",
      "rec:  1.0\n",
      "ma F1:  0.014366264248674576\n",
      "mi F1:  0.014457831325301207\n",
      "we F1:  0.005095097740228373\n",
      "Subject 16 Current Train Acc:  0.39820966283022985 Current Test Acc:  0.014457831325301205\n",
      "Loss:  0.17078696191310883\n",
      "Loss:  0.16778279840946198\n",
      "Loss:  0.15335848927497864\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.1361963301897049\n",
      "Loss:  0.13119642436504364\n",
      "Loss:  0.10344578325748444\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.11450795084238052\n",
      "Loss:  0.0826277956366539\n",
      "Loss:  0.10424381494522095\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.044489502906799316\n",
      "Eval Loss:  0.05863344669342041\n",
      "Eval Loss:  0.27393221855163574\n",
      "[[18162  1182]\n",
      " [ 4983  7734]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.94      0.85     19344\n",
      "           1       0.87      0.61      0.72     12717\n",
      "\n",
      "    accuracy                           0.81     32061\n",
      "   macro avg       0.83      0.77      0.78     32061\n",
      "weighted avg       0.82      0.81      0.80     32061\n",
      "\n",
      "acc:  0.807710302236362\n",
      "pre:  0.8674293405114402\n",
      "rec:  0.6081623024298184\n",
      "ma F1:  0.7849611717559117\n",
      "mi F1:  0.8077103022363621\n",
      "we F1:  0.7994182572654959\n",
      "[[736  84]\n",
      " [  8   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94       820\n",
      "           1       0.02      0.20      0.04        10\n",
      "\n",
      "    accuracy                           0.89       830\n",
      "   macro avg       0.51      0.55      0.49       830\n",
      "weighted avg       0.98      0.89      0.93       830\n",
      "\n",
      "acc:  0.8891566265060241\n",
      "pre:  0.023255813953488372\n",
      "rec:  0.2\n",
      "ma F1:  0.491421568627451\n",
      "mi F1:  0.8891566265060241\n",
      "we F1:  0.9303390030711081\n",
      "Subject 16 Current Train Acc:  0.807710302236362 Current Test Acc:  0.8891566265060241\n",
      "Loss:  0.08990886807441711\n",
      "Loss:  0.10822786390781403\n",
      "Loss:  0.07995013147592545\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.10302340984344482\n",
      "Loss:  0.06611134856939316\n",
      "Loss:  0.11889465153217316\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.09903494268655777\n",
      "Loss:  0.07174593955278397\n",
      "Loss:  0.08104915916919708\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.058907389640808105\n",
      "Eval Loss:  0.02784717082977295\n",
      "Eval Loss:  0.23370391130447388\n",
      "[[18416   928]\n",
      " [ 4113  8604]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.95      0.88     19344\n",
      "           1       0.90      0.68      0.77     12717\n",
      "\n",
      "    accuracy                           0.84     32061\n",
      "   macro avg       0.86      0.81      0.83     32061\n",
      "weighted avg       0.85      0.84      0.84     32061\n",
      "\n",
      "acc:  0.8427684725991079\n",
      "pre:  0.9026437263953\n",
      "rec:  0.6765746638358103\n",
      "ma F1:  0.8265200899088117\n",
      "mi F1:  0.842768472599108\n",
      "we F1:  0.837494206509769\n",
      "[[767  53]\n",
      " [  9   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.96       820\n",
      "           1       0.02      0.10      0.03        10\n",
      "\n",
      "    accuracy                           0.93       830\n",
      "   macro avg       0.50      0.52      0.50       830\n",
      "weighted avg       0.98      0.93      0.95       830\n",
      "\n",
      "acc:  0.9253012048192771\n",
      "pre:  0.018518518518518517\n",
      "rec:  0.1\n",
      "ma F1:  0.4962014411027569\n",
      "mi F1:  0.9253012048192772\n",
      "we F1:  0.9499492330223147\n",
      "Subject 16 Current Train Acc:  0.8427684725991079 Current Test Acc:  0.9253012048192771\n",
      "Loss:  0.07885626703500748\n",
      "Loss:  0.07540198415517807\n",
      "Loss:  0.0808500200510025\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.07007714360952377\n",
      "Loss:  0.07737721502780914\n",
      "Loss:  0.09239214658737183\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.11328352987766266\n",
      "Loss:  0.10310468822717667\n",
      "Loss:  0.09037723392248154\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.09566247463226318\n",
      "Eval Loss:  0.03550541400909424\n",
      "Eval Loss:  0.2605109214782715\n",
      "[[18322  1022]\n",
      " [ 3610  9107]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89     19344\n",
      "           1       0.90      0.72      0.80     12717\n",
      "\n",
      "    accuracy                           0.86     32061\n",
      "   macro avg       0.87      0.83      0.84     32061\n",
      "weighted avg       0.86      0.86      0.85     32061\n",
      "\n",
      "acc:  0.8555254046972958\n",
      "pre:  0.899101589495508\n",
      "rec:  0.7161280176142172\n",
      "ma F1:  0.8425154917833957\n",
      "mi F1:  0.8555254046972958\n",
      "we F1:  0.8518716168925021\n",
      "[[771  49]\n",
      " [  9   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.96       820\n",
      "           1       0.02      0.10      0.03        10\n",
      "\n",
      "    accuracy                           0.93       830\n",
      "   macro avg       0.50      0.52      0.50       830\n",
      "weighted avg       0.98      0.93      0.95       830\n",
      "\n",
      "acc:  0.9301204819277108\n",
      "pre:  0.02\n",
      "rec:  0.1\n",
      "ma F1:  0.49854166666666666\n",
      "mi F1:  0.9301204819277109\n",
      "we F1:  0.9525401606425703\n",
      "Subject 16 Current Train Acc:  0.8555254046972958 Current Test Acc:  0.9301204819277108\n",
      "Loss:  0.07592818886041641\n",
      "Loss:  0.061430927366018295\n",
      "Loss:  0.07784605026245117\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.10021260380744934\n",
      "Loss:  0.06668315082788467\n",
      "Loss:  0.10245049744844437\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.0636570006608963\n",
      "Loss:  0.06794384121894836\n",
      "Loss:  0.06397060304880142\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.03345358371734619\n",
      "Eval Loss:  0.027073144912719727\n",
      "Eval Loss:  0.24226480722427368\n",
      "[[18789   555]\n",
      " [ 4687  8030]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.88     19344\n",
      "           1       0.94      0.63      0.75     12717\n",
      "\n",
      "    accuracy                           0.84     32061\n",
      "   macro avg       0.87      0.80      0.82     32061\n",
      "weighted avg       0.85      0.84      0.83     32061\n",
      "\n",
      "acc:  0.8364991734506098\n",
      "pre:  0.9353523587652883\n",
      "rec:  0.6314382322874892\n",
      "ma F1:  0.8157501947812098\n",
      "mi F1:  0.8364991734506098\n",
      "we F1:  0.8285305165250627\n",
      "[[796  24]\n",
      " [  9   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       820\n",
      "           1       0.04      0.10      0.06        10\n",
      "\n",
      "    accuracy                           0.96       830\n",
      "   macro avg       0.51      0.54      0.52       830\n",
      "weighted avg       0.98      0.96      0.97       830\n",
      "\n",
      "acc:  0.9602409638554217\n",
      "pre:  0.04\n",
      "rec:  0.1\n",
      "ma F1:  0.5184175824175824\n",
      "mi F1:  0.9602409638554217\n",
      "we F1:  0.96857725407123\n",
      "Subject 16 Current Train Acc:  0.8364991734506098 Current Test Acc:  0.9602409638554217\n",
      "Loss:  0.060226455330848694\n",
      "Loss:  0.07780014723539352\n",
      "Loss:  0.07564785331487656\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.10411551594734192\n",
      "Loss:  0.07292340695858002\n",
      "Loss:  0.0838809683918953\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.07086574286222458\n",
      "Loss:  0.07455205917358398\n",
      "Loss:  0.06302444636821747\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.05390024185180664\n",
      "Eval Loss:  0.03349173069000244\n",
      "Eval Loss:  0.23309266567230225\n",
      "[[18645   699]\n",
      " [ 4097  8620]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.96      0.89     19344\n",
      "           1       0.92      0.68      0.78     12717\n",
      "\n",
      "    accuracy                           0.85     32061\n",
      "   macro avg       0.87      0.82      0.83     32061\n",
      "weighted avg       0.86      0.85      0.84     32061\n",
      "\n",
      "acc:  0.8504101556408097\n",
      "pre:  0.9249919519261723\n",
      "rec:  0.6778328222064952\n",
      "ma F1:  0.8341995045506962\n",
      "mi F1:  0.8504101556408097\n",
      "we F1:  0.8449155130069738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[795  25]\n",
      " [  9   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       820\n",
      "           1       0.04      0.10      0.06        10\n",
      "\n",
      "    accuracy                           0.96       830\n",
      "   macro avg       0.51      0.53      0.52       830\n",
      "weighted avg       0.98      0.96      0.97       830\n",
      "\n",
      "acc:  0.9590361445783132\n",
      "pre:  0.038461538461538464\n",
      "rec:  0.1\n",
      "ma F1:  0.5173097974822113\n",
      "mi F1:  0.9590361445783132\n",
      "we F1:  0.9679374311696705\n",
      "Loss:  0.05954712629318237\n",
      "Loss:  0.05324253812432289\n",
      "Loss:  0.09381464123725891\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.06631499528884888\n",
      "Loss:  0.051660649478435516\n",
      "Loss:  0.08723731338977814\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.08587897568941116\n",
      "Loss:  0.05326174944639206\n",
      "Loss:  0.06366615742444992\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.03792846202850342\n",
      "Eval Loss:  0.016227006912231445\n",
      "Eval Loss:  0.24830323457717896\n",
      "[[18973   371]\n",
      " [ 5152  7565]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.98      0.87     19344\n",
      "           1       0.95      0.59      0.73     12717\n",
      "\n",
      "    accuracy                           0.83     32061\n",
      "   macro avg       0.87      0.79      0.80     32061\n",
      "weighted avg       0.85      0.83      0.82     32061\n",
      "\n",
      "acc:  0.8277346308599233\n",
      "pre:  0.9532510080645161\n",
      "rec:  0.594873004639459\n",
      "ma F1:  0.80276258006273\n",
      "mi F1:  0.8277346308599232\n",
      "we F1:  0.8172690475095217\n",
      "[[808  12]\n",
      " [  9   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       820\n",
      "           1       0.08      0.10      0.09        10\n",
      "\n",
      "    accuracy                           0.97       830\n",
      "   macro avg       0.53      0.54      0.54       830\n",
      "weighted avg       0.98      0.97      0.98       830\n",
      "\n",
      "acc:  0.9746987951807229\n",
      "pre:  0.07692307692307693\n",
      "rec:  0.1\n",
      "ma F1:  0.5370640886032243\n",
      "mi F1:  0.9746987951807229\n",
      "we F1:  0.9763256900007135\n",
      "Subject 16 Current Train Acc:  0.8277346308599233 Current Test Acc:  0.9746987951807229\n",
      "Loss:  0.0525849349796772\n",
      "Loss:  0.05376908928155899\n",
      "Loss:  0.05534563213586807\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.09343187510967255\n",
      "Loss:  0.0791705921292305\n",
      "Loss:  0.05264632776379585\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.07329611480236053\n",
      "Loss:  0.07459914684295654\n",
      "Loss:  0.07033784687519073\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.19410347938537598\n",
      "Eval Loss:  0.03987777233123779\n",
      "Eval Loss:  0.10837972164154053\n",
      "[[18318  1026]\n",
      " [ 2706 10011]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91     19344\n",
      "           1       0.91      0.79      0.84     12717\n",
      "\n",
      "    accuracy                           0.88     32061\n",
      "   macro avg       0.89      0.87      0.88     32061\n",
      "weighted avg       0.89      0.88      0.88     32061\n",
      "\n",
      "acc:  0.8835968934219145\n",
      "pre:  0.9070399565099212\n",
      "rec:  0.7872139655579146\n",
      "ma F1:  0.8752200768339211\n",
      "mi F1:  0.8835968934219145\n",
      "we F1:  0.8819027737797056\n",
      "[[783  37]\n",
      " [  9   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97       820\n",
      "           1       0.03      0.10      0.04        10\n",
      "\n",
      "    accuracy                           0.94       830\n",
      "   macro avg       0.51      0.53      0.51       830\n",
      "weighted avg       0.98      0.94      0.96       830\n",
      "\n",
      "acc:  0.944578313253012\n",
      "pre:  0.02631578947368421\n",
      "rec:  0.1\n",
      "ma F1:  0.5065653432588917\n",
      "mi F1:  0.944578313253012\n",
      "we F1:  0.9602616421019063\n",
      "Loss:  0.07317415624856949\n",
      "Loss:  0.0645294114947319\n",
      "Loss:  0.11004391312599182\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.052252065390348434\n",
      "Loss:  0.07754173874855042\n",
      "Loss:  0.06365983933210373\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.1081683561205864\n",
      "Loss:  0.08550222218036652\n",
      "Loss:  0.0816950798034668\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.10126090049743652\n",
      "Eval Loss:  0.05718827247619629\n",
      "Eval Loss:  0.07644414901733398\n",
      "[[18217  1127]\n",
      " [ 2475 10242]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91     19344\n",
      "           1       0.90      0.81      0.85     12717\n",
      "\n",
      "    accuracy                           0.89     32061\n",
      "   macro avg       0.89      0.87      0.88     32061\n",
      "weighted avg       0.89      0.89      0.89     32061\n",
      "\n",
      "acc:  0.8876516640154705\n",
      "pre:  0.9008707889875979\n",
      "rec:  0.805378627034678\n",
      "ma F1:  0.8802417585860013\n",
      "mi F1:  0.8876516640154705\n",
      "we F1:  0.8863991809409972\n",
      "[[788  32]\n",
      " [  9   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97       820\n",
      "           1       0.03      0.10      0.05        10\n",
      "\n",
      "    accuracy                           0.95       830\n",
      "   macro avg       0.51      0.53      0.51       830\n",
      "weighted avg       0.98      0.95      0.96       830\n",
      "\n",
      "acc:  0.9506024096385542\n",
      "pre:  0.030303030303030304\n",
      "rec:  0.1\n",
      "ma F1:  0.5105780155614043\n",
      "mi F1:  0.9506024096385542\n",
      "we F1:  0.9634620806217492\n",
      "Loss:  0.0924459770321846\n",
      "Loss:  0.07421834021806717\n",
      "Loss:  0.05800690874457359\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.05849936977028847\n",
      "Loss:  0.05999468266963959\n",
      "Loss:  0.05452011898159981\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.08385542780160904\n",
      "Loss:  0.09142804145812988\n",
      "Loss:  0.08426932990550995\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.13579100370407104\n",
      "Eval Loss:  0.03941452503204346\n",
      "Eval Loss:  0.1168661117553711\n",
      "[[18550   794]\n",
      " [ 3107  9610]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.90     19344\n",
      "           1       0.92      0.76      0.83     12717\n",
      "\n",
      "    accuracy                           0.88     32061\n",
      "   macro avg       0.89      0.86      0.87     32061\n",
      "weighted avg       0.88      0.88      0.88     32061\n",
      "\n",
      "acc:  0.8783256916502916\n",
      "pre:  0.923683198769704\n",
      "rec:  0.755681371392624\n",
      "ma F1:  0.868067451522218\n",
      "mi F1:  0.8783256916502916\n",
      "we F1:  0.875671630194337\n",
      "[[802  18]\n",
      " [  9   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       820\n",
      "           1       0.05      0.10      0.07        10\n",
      "\n",
      "    accuracy                           0.97       830\n",
      "   macro avg       0.52      0.54      0.53       830\n",
      "weighted avg       0.98      0.97      0.97       830\n",
      "\n",
      "acc:  0.9674698795180723\n",
      "pre:  0.05263157894736842\n",
      "rec:  0.1\n",
      "ma F1:  0.5262056280259625\n",
      "mi F1:  0.9674698795180723\n",
      "we F1:  0.9724279048157365\n",
      "Loss:  0.05783112347126007\n",
      "Loss:  0.12259062379598618\n",
      "Loss:  0.06333103775978088\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.0808371901512146\n",
      "Loss:  0.0695793479681015\n",
      "Loss:  0.047503601759672165\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.08690138161182404\n",
      "Loss:  0.10378934442996979\n",
      "Loss:  0.0613885223865509\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.18090003728866577\n",
      "Eval Loss:  0.08713257312774658\n",
      "Eval Loss:  0.06470608711242676\n",
      "[[18033  1311]\n",
      " [ 1871 10846]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92     19344\n",
      "           1       0.89      0.85      0.87     12717\n",
      "\n",
      "    accuracy                           0.90     32061\n",
      "   macro avg       0.90      0.89      0.90     32061\n",
      "weighted avg       0.90      0.90      0.90     32061\n",
      "\n",
      "acc:  0.9007516920869593\n",
      "pre:  0.8921608949576376\n",
      "rec:  0.8528741055280333\n",
      "ma F1:  0.8955005322217372\n",
      "mi F1:  0.9007516920869594\n",
      "we F1:  0.900342529776604\n",
      "[[769  51]\n",
      " [  8   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.96       820\n",
      "           1       0.04      0.20      0.06        10\n",
      "\n",
      "    accuracy                           0.93       830\n",
      "   macro avg       0.51      0.57      0.51       830\n",
      "weighted avg       0.98      0.93      0.95       830\n",
      "\n",
      "acc:  0.9289156626506024\n",
      "pre:  0.03773584905660377\n",
      "rec:  0.2\n",
      "ma F1:  0.5132738964924313\n",
      "mi F1:  0.9289156626506024\n",
      "we F1:  0.9522176130349587\n",
      "Loss:  0.03303219750523567\n",
      "Loss:  0.05173652246594429\n",
      "Loss:  0.12881068885326385\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.07023140788078308\n",
      "Loss:  0.06137099489569664\n",
      "Loss:  0.06826886534690857\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.06088429316878319\n",
      "Loss:  0.05055835470557213\n",
      "Loss:  0.06356542557477951\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.18121862411499023\n",
      "Eval Loss:  0.060328006744384766\n",
      "Eval Loss:  0.06754124164581299\n",
      "[[18196  1148]\n",
      " [ 2026 10691]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     19344\n",
      "           1       0.90      0.84      0.87     12717\n",
      "\n",
      "    accuracy                           0.90     32061\n",
      "   macro avg       0.90      0.89      0.90     32061\n",
      "weighted avg       0.90      0.90      0.90     32061\n",
      "\n",
      "acc:  0.901001216431178\n",
      "pre:  0.9030323507052961\n",
      "rec:  0.8406856963120233\n",
      "ma F1:  0.895262014835225\n",
      "mi F1:  0.901001216431178\n",
      "we F1:  0.9003297948454024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[788  32]\n",
      " [  8   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       820\n",
      "           1       0.06      0.20      0.09        10\n",
      "\n",
      "    accuracy                           0.95       830\n",
      "   macro avg       0.52      0.58      0.53       830\n",
      "weighted avg       0.98      0.95      0.96       830\n",
      "\n",
      "acc:  0.9518072289156626\n",
      "pre:  0.058823529411764705\n",
      "rec:  0.2\n",
      "ma F1:  0.533078307830783\n",
      "mi F1:  0.9518072289156626\n",
      "we F1:  0.9645928448266512\n",
      "Loss:  0.03888043761253357\n",
      "Loss:  0.05183129012584686\n",
      "Loss:  0.04466358944773674\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.07287293672561646\n",
      "Loss:  0.11159880459308624\n",
      "Loss:  0.08101827651262283\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.0462479367852211\n",
      "Loss:  0.10150180011987686\n",
      "Loss:  0.06221785023808479\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.08861291408538818\n",
      "Eval Loss:  0.06187403202056885\n",
      "Eval Loss:  0.06169450283050537\n",
      "[[18369   975]\n",
      " [ 2269 10448]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     19344\n",
      "           1       0.91      0.82      0.87     12717\n",
      "\n",
      "    accuracy                           0.90     32061\n",
      "   macro avg       0.90      0.89      0.89     32061\n",
      "weighted avg       0.90      0.90      0.90     32061\n",
      "\n",
      "acc:  0.8988178784192633\n",
      "pre:  0.9146458898713122\n",
      "rec:  0.8215774160572462\n",
      "ma F1:  0.8922403606892364\n",
      "mi F1:  0.8988178784192634\n",
      "we F1:  0.8977433539977692\n",
      "[[790  30]\n",
      " [  8   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       820\n",
      "           1       0.06      0.20      0.10        10\n",
      "\n",
      "    accuracy                           0.95       830\n",
      "   macro avg       0.53      0.58      0.54       830\n",
      "weighted avg       0.98      0.95      0.97       830\n",
      "\n",
      "acc:  0.9542168674698795\n",
      "pre:  0.0625\n",
      "rec:  0.2\n",
      "ma F1:  0.5358761551592206\n",
      "mi F1:  0.9542168674698795\n",
      "we F1:  0.9658964305039335\n",
      "Loss:  0.045066092163324356\n",
      "Loss:  0.05302152782678604\n",
      "Loss:  0.08352380245923996\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.05011894181370735\n",
      "Loss:  0.04646093025803566\n",
      "Loss:  0.04893408715724945\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.051687415689229965\n",
      "Loss:  0.06454380601644516\n",
      "Loss:  0.056862108409404755\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.09782993793487549\n",
      "Eval Loss:  0.039230942726135254\n",
      "Eval Loss:  0.08471906185150146\n",
      "[[18439   905]\n",
      " [ 2334 10383]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     19344\n",
      "           1       0.92      0.82      0.87     12717\n",
      "\n",
      "    accuracy                           0.90     32061\n",
      "   macro avg       0.90      0.88      0.89     32061\n",
      "weighted avg       0.90      0.90      0.90     32061\n",
      "\n",
      "acc:  0.8989738311344001\n",
      "pre:  0.919826364280652\n",
      "rec:  0.8164661476763387\n",
      "ma F1:  0.8921654691167681\n",
      "mi F1:  0.8989738311344001\n",
      "we F1:  0.8977661412978564\n",
      "[[801  19]\n",
      " [  9   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       820\n",
      "           1       0.05      0.10      0.07        10\n",
      "\n",
      "    accuracy                           0.97       830\n",
      "   macro avg       0.52      0.54      0.52       830\n",
      "weighted avg       0.98      0.97      0.97       830\n",
      "\n",
      "acc:  0.9662650602409638\n",
      "pre:  0.05\n",
      "rec:  0.1\n",
      "ma F1:  0.5247443762781187\n",
      "mi F1:  0.9662650602409638\n",
      "we F1:  0.9717840687904994\n",
      "Loss:  0.06533772498369217\n",
      "Loss:  0.10413198173046112\n",
      "Loss:  0.044916167855262756\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.07058010995388031\n",
      "Loss:  0.0702483281493187\n",
      "Loss:  0.0629165917634964\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.04350441321730614\n",
      "Loss:  0.07654798775911331\n",
      "Loss:  0.042901553213596344\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.3262510299682617\n",
      "Eval Loss:  0.12133181095123291\n",
      "Eval Loss:  0.06312859058380127\n",
      "[[17952  1392]\n",
      " [ 1523 11194]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92     19344\n",
      "           1       0.89      0.88      0.88     12717\n",
      "\n",
      "    accuracy                           0.91     32061\n",
      "   macro avg       0.91      0.90      0.90     32061\n",
      "weighted avg       0.91      0.91      0.91     32061\n",
      "\n",
      "acc:  0.9090795670752628\n",
      "pre:  0.8894009216589862\n",
      "rec:  0.8802390500904301\n",
      "ma F1:  0.9048520875697232\n",
      "mi F1:  0.9090795670752628\n",
      "we F1:  0.9089976197809115\n",
      "[[778  42]\n",
      " [  8   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97       820\n",
      "           1       0.05      0.20      0.07        10\n",
      "\n",
      "    accuracy                           0.94       830\n",
      "   macro avg       0.52      0.57      0.52       830\n",
      "weighted avg       0.98      0.94      0.96       830\n",
      "\n",
      "acc:  0.9397590361445783\n",
      "pre:  0.045454545454545456\n",
      "rec:  0.2\n",
      "ma F1:  0.5214704118813707\n",
      "mi F1:  0.9397590361445783\n",
      "we F1:  0.9580861150427084\n",
      "Loss:  0.10463398694992065\n",
      "Loss:  0.06116771697998047\n",
      "Loss:  0.04836028441786766\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.03629288822412491\n",
      "Loss:  0.018967466428875923\n",
      "Loss:  0.05317750573158264\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.06826191395521164\n",
      "Loss:  0.10909869521856308\n",
      "Loss:  0.0603116974234581\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.3432333469390869\n",
      "Eval Loss:  0.2681712508201599\n",
      "Eval Loss:  0.05939328670501709\n",
      "[[17704  1640]\n",
      " [ 1354 11363]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92     19344\n",
      "           1       0.87      0.89      0.88     12717\n",
      "\n",
      "    accuracy                           0.91     32061\n",
      "   macro avg       0.90      0.90      0.90     32061\n",
      "weighted avg       0.91      0.91      0.91     32061\n",
      "\n",
      "acc:  0.9066155141761018\n",
      "pre:  0.8738752595554872\n",
      "rec:  0.8935283478807895\n",
      "ma F1:  0.9028139228265636\n",
      "mi F1:  0.9066155141761018\n",
      "we F1:  0.9067869784760494\n",
      "[[772  48]\n",
      " [  8   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97       820\n",
      "           1       0.04      0.20      0.07        10\n",
      "\n",
      "    accuracy                           0.93       830\n",
      "   macro avg       0.51      0.57      0.52       830\n",
      "weighted avg       0.98      0.93      0.95       830\n",
      "\n",
      "acc:  0.9325301204819277\n",
      "pre:  0.04\n",
      "rec:  0.2\n",
      "ma F1:  0.5158333333333334\n",
      "mi F1:  0.9325301204819277\n",
      "we F1:  0.9541767068273093\n",
      "Loss:  0.03970487043261528\n",
      "Loss:  0.08374526351690292\n",
      "Loss:  0.07525748759508133\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.055205248296260834\n",
      "Loss:  0.0702834501862526\n",
      "Loss:  0.08610684424638748\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.06169143319129944\n",
      "Loss:  0.04678511247038841\n",
      "Loss:  0.05164078623056412\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.17020463943481445\n",
      "Eval Loss:  0.10680747032165527\n",
      "Eval Loss:  0.09900045394897461\n",
      "[[18222  1122]\n",
      " [ 1741 10976]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93     19344\n",
      "           1       0.91      0.86      0.88     12717\n",
      "\n",
      "    accuracy                           0.91     32061\n",
      "   macro avg       0.91      0.90      0.91     32061\n",
      "weighted avg       0.91      0.91      0.91     32061\n",
      "\n",
      "acc:  0.9107014753126852\n",
      "pre:  0.9072573979170111\n",
      "rec:  0.8630966422898483\n",
      "ma F1:  0.9058946674262773\n",
      "mi F1:  0.9107014753126852\n",
      "we F1:  0.9102908468167307\n",
      "[[796  24]\n",
      " [  8   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       820\n",
      "           1       0.08      0.20      0.11        10\n",
      "\n",
      "    accuracy                           0.96       830\n",
      "   macro avg       0.53      0.59      0.55       830\n",
      "weighted avg       0.98      0.96      0.97       830\n",
      "\n",
      "acc:  0.9614457831325302\n",
      "pre:  0.07692307692307693\n",
      "rec:  0.2\n",
      "ma F1:  0.5457033388067871\n",
      "mi F1:  0.9614457831325302\n",
      "we F1:  0.969823464630278\n",
      "Loss:  0.05972016230225563\n",
      "Loss:  0.034525368362665176\n",
      "Loss:  0.05819986015558243\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.07418077439069748\n",
      "Loss:  0.05295591056346893\n",
      "Loss:  0.07286789268255234\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.0644834041595459\n",
      "Loss:  0.07904534786939621\n",
      "Loss:  0.0392574667930603\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.45891350507736206\n",
      "Eval Loss:  0.23484772443771362\n",
      "Eval Loss:  0.04169154167175293\n",
      "[[17425  1919]\n",
      " [ 1030 11687]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.90      0.92     19344\n",
      "           1       0.86      0.92      0.89     12717\n",
      "\n",
      "    accuracy                           0.91     32061\n",
      "   macro avg       0.90      0.91      0.90     32061\n",
      "weighted avg       0.91      0.91      0.91     32061\n",
      "\n",
      "acc:  0.9080190886123327\n",
      "pre:  0.8589592826694106\n",
      "rec:  0.9190060548871589\n",
      "ma F1:  0.904975379797339\n",
      "mi F1:  0.9080190886123327\n",
      "we F1:  0.9084906566040596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[773  47]\n",
      " [  8   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97       820\n",
      "           1       0.04      0.20      0.07        10\n",
      "\n",
      "    accuracy                           0.93       830\n",
      "   macro avg       0.52      0.57      0.52       830\n",
      "weighted avg       0.98      0.93      0.95       830\n",
      "\n",
      "acc:  0.9337349397590361\n",
      "pre:  0.04081632653061224\n",
      "rec:  0.2\n",
      "ma F1:  0.5167215405625721\n",
      "mi F1:  0.9337349397590361\n",
      "we F1:  0.9548290027534098\n",
      "Loss:  0.06667620688676834\n",
      "Loss:  0.07243861258029938\n",
      "Loss:  0.05110885947942734\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.05225975811481476\n",
      "Loss:  0.07578878104686737\n",
      "Loss:  0.09607861936092377\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.0833008736371994\n",
      "Loss:  0.03768476843833923\n",
      "Loss:  0.08993582427501678\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.34942418336868286\n",
      "Eval Loss:  0.12974131107330322\n",
      "Eval Loss:  0.05985891819000244\n",
      "[[17946  1398]\n",
      " [ 1318 11399]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19344\n",
      "           1       0.89      0.90      0.89     12717\n",
      "\n",
      "    accuracy                           0.92     32061\n",
      "   macro avg       0.91      0.91      0.91     32061\n",
      "weighted avg       0.92      0.92      0.92     32061\n",
      "\n",
      "acc:  0.9152864851377063\n",
      "pre:  0.8907556458544972\n",
      "rec:  0.8963592042148305\n",
      "ma F1:  0.9116002627909672\n",
      "mi F1:  0.9152864851377063\n",
      "we F1:  0.9153315283311901\n",
      "[[796  24]\n",
      " [  8   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       820\n",
      "           1       0.08      0.20      0.11        10\n",
      "\n",
      "    accuracy                           0.96       830\n",
      "   macro avg       0.53      0.59      0.55       830\n",
      "weighted avg       0.98      0.96      0.97       830\n",
      "\n",
      "acc:  0.9614457831325302\n",
      "pre:  0.07692307692307693\n",
      "rec:  0.2\n",
      "ma F1:  0.5457033388067871\n",
      "mi F1:  0.9614457831325302\n",
      "we F1:  0.969823464630278\n",
      "Loss:  0.07913549244403839\n",
      "Loss:  0.08514323085546494\n",
      "Loss:  0.06217275559902191\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.04171965271234512\n",
      "Loss:  0.06709636002779007\n",
      "Loss:  0.08208583295345306\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.03813299909234047\n",
      "Loss:  0.07800287008285522\n",
      "Loss:  0.04585128277540207\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.290401816368103\n",
      "Eval Loss:  0.07889378070831299\n",
      "Eval Loss:  0.06098902225494385\n",
      "[[17972  1372]\n",
      " [ 1337 11380]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19344\n",
      "           1       0.89      0.89      0.89     12717\n",
      "\n",
      "    accuracy                           0.92     32061\n",
      "   macro avg       0.91      0.91      0.91     32061\n",
      "weighted avg       0.92      0.92      0.92     32061\n",
      "\n",
      "acc:  0.9155048189388977\n",
      "pre:  0.8924090338770388\n",
      "rec:  0.8948651411496422\n",
      "ma F1:  0.9117751418034061\n",
      "mi F1:  0.9155048189388977\n",
      "we F1:  0.915524621532912\n",
      "[[785  35]\n",
      " [  8   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97       820\n",
      "           1       0.05      0.20      0.09        10\n",
      "\n",
      "    accuracy                           0.95       830\n",
      "   macro avg       0.52      0.58      0.53       830\n",
      "weighted avg       0.98      0.95      0.96       830\n",
      "\n",
      "acc:  0.9481927710843373\n",
      "pre:  0.05405405405405406\n",
      "rec:  0.2\n",
      "ma F1:  0.5292239912413766\n",
      "mi F1:  0.9481927710843373\n",
      "we F1:  0.9626399703892671\n",
      "Loss:  0.05454810708761215\n",
      "Loss:  0.06736808270215988\n",
      "Loss:  0.05680391564965248\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.04619153216481209\n",
      "Loss:  0.10531836003065109\n",
      "Loss:  0.056452177464962006\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.06591984629631042\n",
      "Loss:  0.06336149573326111\n",
      "Loss:  0.08864080905914307\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  16 Training Time 5866.856218338013 Best Test Acc:  0.9746987951807229\n",
      "test subjects:  ['./seg\\\\b05', './seg\\\\x11']\n",
      "*********\n",
      "33423 890\n",
      "32001 890\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.6336763501167297\n",
      "Eval Loss:  0.7320131659507751\n",
      "Eval Loss:  0.6230421662330627\n",
      "[[19322    22]\n",
      " [12644    13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75     19344\n",
      "           1       0.37      0.00      0.00     12657\n",
      "\n",
      "    accuracy                           0.60     32001\n",
      "   macro avg       0.49      0.50      0.38     32001\n",
      "weighted avg       0.51      0.60      0.46     32001\n",
      "\n",
      "acc:  0.6041998687541015\n",
      "pre:  0.37142857142857144\n",
      "rec:  0.0010270996286639803\n",
      "ma F1:  0.3775980345517869\n",
      "mi F1:  0.6041998687541015\n",
      "we F1:  0.45607369177437107\n",
      "[[816   4]\n",
      " [ 70   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      1.00      0.96       820\n",
      "           1       0.00      0.00      0.00        70\n",
      "\n",
      "    accuracy                           0.92       890\n",
      "   macro avg       0.46      0.50      0.48       890\n",
      "weighted avg       0.85      0.92      0.88       890\n",
      "\n",
      "acc:  0.9168539325842696\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4783118405627198\n",
      "mi F1:  0.9168539325842696\n",
      "we F1:  0.8813836163178207\n",
      "Subject 17 Current Train Acc:  0.6041998687541015 Current Test Acc:  0.9168539325842696\n",
      "Loss:  0.17066724598407745\n",
      "Loss:  0.16441674530506134\n",
      "Loss:  0.15932779014110565\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.15033410489559174\n",
      "Loss:  0.1230393722653389\n",
      "Loss:  0.10665742307901382\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.12036322802305222\n",
      "Loss:  0.07967977225780487\n",
      "Loss:  0.15188831090927124\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.03753948211669922\n",
      "Eval Loss:  0.5288028717041016\n",
      "Eval Loss:  1.2721271514892578\n",
      "[[17542  1802]\n",
      " [ 4050  8607]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.91      0.86     19344\n",
      "           1       0.83      0.68      0.75     12657\n",
      "\n",
      "    accuracy                           0.82     32001\n",
      "   macro avg       0.82      0.79      0.80     32001\n",
      "weighted avg       0.82      0.82      0.81     32001\n",
      "\n",
      "acc:  0.8171307146651667\n",
      "pre:  0.8268805841099048\n",
      "rec:  0.6800189618392984\n",
      "ma F1:  0.8016691945541865\n",
      "mi F1:  0.8171307146651668\n",
      "we F1:  0.8132406758056835\n",
      "[[454 366]\n",
      " [ 31  39]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.55      0.70       820\n",
      "           1       0.10      0.56      0.16        70\n",
      "\n",
      "    accuracy                           0.55       890\n",
      "   macro avg       0.52      0.56      0.43       890\n",
      "weighted avg       0.87      0.55      0.65       890\n",
      "\n",
      "acc:  0.553932584269663\n",
      "pre:  0.0962962962962963\n",
      "rec:  0.5571428571428572\n",
      "ma F1:  0.42999798346440815\n",
      "mi F1:  0.553932584269663\n",
      "we F1:  0.6539761776907722\n",
      "Loss:  0.09414166212081909\n",
      "Loss:  0.10328271985054016\n",
      "Loss:  0.12218060344457626\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.1122593954205513\n",
      "Loss:  0.12349127233028412\n",
      "Loss:  0.07980372756719589\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.08649197220802307\n",
      "Loss:  0.08837195485830307\n",
      "Loss:  0.06482935696840286\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.02184772491455078\n",
      "Eval Loss:  1.2302932739257812\n",
      "Eval Loss:  1.6563701629638672\n",
      "[[18368   976]\n",
      " [ 4216  8441]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.95      0.88     19344\n",
      "           1       0.90      0.67      0.76     12657\n",
      "\n",
      "    accuracy                           0.84     32001\n",
      "   macro avg       0.85      0.81      0.82     32001\n",
      "weighted avg       0.85      0.84      0.83     32001\n",
      "\n",
      "acc:  0.8377550701540577\n",
      "pre:  0.8963576510565998\n",
      "rec:  0.6669036896578968\n",
      "ma F1:  0.8204799135592156\n",
      "mi F1:  0.8377550701540577\n",
      "we F1:  0.8321167597513893\n",
      "[[704 116]\n",
      " [ 58  12]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.86      0.89       820\n",
      "           1       0.09      0.17      0.12        70\n",
      "\n",
      "    accuracy                           0.80       890\n",
      "   macro avg       0.51      0.51      0.51       890\n",
      "weighted avg       0.86      0.80      0.83       890\n",
      "\n",
      "acc:  0.8044943820224719\n",
      "pre:  0.09375\n",
      "rec:  0.17142857142857143\n",
      "ma F1:  0.5056123817185764\n",
      "mi F1:  0.8044943820224719\n",
      "we F1:  0.8295451855161511\n",
      "Loss:  0.10356980562210083\n",
      "Loss:  0.07166330516338348\n",
      "Loss:  0.0785006657242775\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.09291426837444305\n",
      "Loss:  0.09410055726766586\n",
      "Loss:  0.08198267221450806\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.12888555228710175\n",
      "Loss:  0.07108937948942184\n",
      "Loss:  0.08953281491994858\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.02913844585418701\n",
      "Eval Loss:  1.2949810028076172\n",
      "Eval Loss:  1.5281317234039307\n",
      "[[18344  1000]\n",
      " [ 3648  9009]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.89     19344\n",
      "           1       0.90      0.71      0.79     12657\n",
      "\n",
      "    accuracy                           0.85     32001\n",
      "   macro avg       0.87      0.83      0.84     32001\n",
      "weighted avg       0.86      0.85      0.85     32001\n",
      "\n",
      "acc:  0.8547545389206588\n",
      "pre:  0.9000899190728344\n",
      "rec:  0.7117800426641384\n",
      "ma F1:  0.8412453933614198\n",
      "mi F1:  0.8547545389206588\n",
      "we F1:  0.8509224856329389\n",
      "[[794  26]\n",
      " [ 66   4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.95       820\n",
      "           1       0.13      0.06      0.08        70\n",
      "\n",
      "    accuracy                           0.90       890\n",
      "   macro avg       0.53      0.51      0.51       890\n",
      "weighted avg       0.86      0.90      0.88       890\n",
      "\n",
      "acc:  0.8966292134831461\n",
      "pre:  0.13333333333333333\n",
      "rec:  0.05714285714285714\n",
      "ma F1:  0.5126190476190476\n",
      "mi F1:  0.8966292134831461\n",
      "we F1:  0.8771856607811664\n",
      "Loss:  0.0708344504237175\n",
      "Loss:  0.11036256700754166\n",
      "Loss:  0.07620880007743835\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.04084692895412445\n",
      "Loss:  0.08974962681531906\n",
      "Loss:  0.07607383280992508\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.07274316996335983\n",
      "Loss:  0.08640795201063156\n",
      "Loss:  0.06138451769948006\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.018819570541381836\n",
      "Eval Loss:  1.3899357318878174\n",
      "Eval Loss:  0.8728534579277039\n",
      "[[18852   492]\n",
      " [ 4834  7823]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.88     19344\n",
      "           1       0.94      0.62      0.75     12657\n",
      "\n",
      "    accuracy                           0.83     32001\n",
      "   macro avg       0.87      0.80      0.81     32001\n",
      "weighted avg       0.85      0.83      0.82     32001\n",
      "\n",
      "acc:  0.8335677010093434\n",
      "pre:  0.940829825616356\n",
      "rec:  0.6180769534644861\n",
      "ma F1:  0.8111341155426119\n",
      "mi F1:  0.8335677010093434\n",
      "we F1:  0.8247358370056669\n",
      "[[813   7]\n",
      " [ 67   3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.99      0.96       820\n",
      "           1       0.30      0.04      0.07        70\n",
      "\n",
      "    accuracy                           0.92       890\n",
      "   macro avg       0.61      0.52      0.52       890\n",
      "weighted avg       0.87      0.92      0.89       890\n",
      "\n",
      "acc:  0.9168539325842696\n",
      "pre:  0.3\n",
      "rec:  0.04285714285714286\n",
      "ma F1:  0.5157352941176471\n",
      "mi F1:  0.9168539325842696\n",
      "we F1:  0.8871414408460012\n",
      "Loss:  0.0656539648771286\n",
      "Loss:  0.061530664563179016\n",
      "Loss:  0.0620407909154892\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.06791925430297852\n",
      "Loss:  0.09648100286722183\n",
      "Loss:  0.06104506552219391\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.0577738881111145\n",
      "Loss:  0.0739469900727272\n",
      "Loss:  0.09270383417606354\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.03251361846923828\n",
      "Eval Loss:  1.2521142959594727\n",
      "Eval Loss:  1.2435798645019531\n",
      "[[18498   846]\n",
      " [ 3275  9382]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90     19344\n",
      "           1       0.92      0.74      0.82     12657\n",
      "\n",
      "    accuracy                           0.87     32001\n",
      "   macro avg       0.88      0.85      0.86     32001\n",
      "weighted avg       0.88      0.87      0.87     32001\n",
      "\n",
      "acc:  0.8712227742883035\n",
      "pre:  0.9172858818928432\n",
      "rec:  0.7412499012404203\n",
      "ma F1:  0.8598497658586399\n",
      "mi F1:  0.8712227742883035\n",
      "we F1:  0.8681923840430585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[798  22]\n",
      " [ 63   7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95       820\n",
      "           1       0.24      0.10      0.14        70\n",
      "\n",
      "    accuracy                           0.90       890\n",
      "   macro avg       0.58      0.54      0.55       890\n",
      "weighted avg       0.87      0.90      0.89       890\n",
      "\n",
      "acc:  0.9044943820224719\n",
      "pre:  0.2413793103448276\n",
      "rec:  0.1\n",
      "ma F1:  0.545424500808201\n",
      "mi F1:  0.9044943820224719\n",
      "we F1:  0.8858826688369029\n",
      "Loss:  0.06781947612762451\n",
      "Loss:  0.06758543848991394\n",
      "Loss:  0.046368975192308426\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.08285465091466904\n",
      "Loss:  0.07213427126407623\n",
      "Loss:  0.07136312127113342\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.06942738592624664\n",
      "Loss:  0.07002362608909607\n",
      "Loss:  0.10682383924722672\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.08161675930023193\n",
      "Eval Loss:  0.6647317409515381\n",
      "Eval Loss:  1.3273371458053589\n",
      "[[18372   972]\n",
      " [ 2753  9904]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91     19344\n",
      "           1       0.91      0.78      0.84     12657\n",
      "\n",
      "    accuracy                           0.88     32001\n",
      "   macro avg       0.89      0.87      0.87     32001\n",
      "weighted avg       0.89      0.88      0.88     32001\n",
      "\n",
      "acc:  0.8835973875816381\n",
      "pre:  0.9106289076866495\n",
      "rec:  0.7824919017144663\n",
      "ma F1:  0.8748329377756004\n",
      "mi F1:  0.8835973875816381\n",
      "we F1:  0.8817540378999478\n",
      "[[732  88]\n",
      " [ 60  10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.91       820\n",
      "           1       0.10      0.14      0.12        70\n",
      "\n",
      "    accuracy                           0.83       890\n",
      "   macro avg       0.51      0.52      0.51       890\n",
      "weighted avg       0.86      0.83      0.85       890\n",
      "\n",
      "acc:  0.8337078651685393\n",
      "pre:  0.10204081632653061\n",
      "rec:  0.14285714285714285\n",
      "ma F1:  0.5136181023277797\n",
      "mi F1:  0.8337078651685393\n",
      "we F1:  0.8461213185751063\n",
      "Loss:  0.08347207307815552\n",
      "Loss:  0.07585214823484421\n",
      "Loss:  0.10282959043979645\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.08491002768278122\n",
      "Loss:  0.0646534413099289\n",
      "Loss:  0.0977817177772522\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.06568610668182373\n",
      "Loss:  0.07980454713106155\n",
      "Loss:  0.07009748369455338\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.056523680686950684\n",
      "Eval Loss:  0.8202753663063049\n",
      "Eval Loss:  1.003143072128296\n",
      "[[18110  1234]\n",
      " [ 2176 10481]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91     19344\n",
      "           1       0.89      0.83      0.86     12657\n",
      "\n",
      "    accuracy                           0.89     32001\n",
      "   macro avg       0.89      0.88      0.89     32001\n",
      "weighted avg       0.89      0.89      0.89     32001\n",
      "\n",
      "acc:  0.8934408299740633\n",
      "pre:  0.8946649594536918\n",
      "rec:  0.8280793236943984\n",
      "ma F1:  0.8870197095163745\n",
      "mi F1:  0.8934408299740633\n",
      "we F1:  0.8926479743611203\n",
      "[[625 195]\n",
      " [ 42  28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.76      0.84       820\n",
      "           1       0.13      0.40      0.19        70\n",
      "\n",
      "    accuracy                           0.73       890\n",
      "   macro avg       0.53      0.58      0.52       890\n",
      "weighted avg       0.87      0.73      0.79       890\n",
      "\n",
      "acc:  0.7337078651685394\n",
      "pre:  0.12556053811659193\n",
      "rec:  0.4\n",
      "ma F1:  0.5158724876116331\n",
      "mi F1:  0.7337078651685394\n",
      "we F1:  0.7895350222308621\n",
      "Loss:  0.05823785066604614\n",
      "Loss:  0.07881501317024231\n",
      "Loss:  0.0767582580447197\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.07865072041749954\n",
      "Loss:  0.053778842091560364\n",
      "Loss:  0.07557661831378937\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.07098624110221863\n",
      "Loss:  0.05413757264614105\n",
      "Loss:  0.0742407888174057\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.08742046356201172\n",
      "Eval Loss:  0.8043628931045532\n",
      "Eval Loss:  1.4983800649642944\n",
      "[[18215  1129]\n",
      " [ 2096 10561]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     19344\n",
      "           1       0.90      0.83      0.87     12657\n",
      "\n",
      "    accuracy                           0.90     32001\n",
      "   macro avg       0.90      0.89      0.89     32001\n",
      "weighted avg       0.90      0.90      0.90     32001\n",
      "\n",
      "acc:  0.8992218993156463\n",
      "pre:  0.9034217279726262\n",
      "rec:  0.834399936793869\n",
      "ma F1:  0.8931068540669374\n",
      "mi F1:  0.8992218993156463\n",
      "we F1:  0.8984493295801483\n",
      "[[662 158]\n",
      " [ 48  22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.81      0.87       820\n",
      "           1       0.12      0.31      0.18        70\n",
      "\n",
      "    accuracy                           0.77       890\n",
      "   macro avg       0.53      0.56      0.52       890\n",
      "weighted avg       0.87      0.77      0.81       890\n",
      "\n",
      "acc:  0.7685393258426966\n",
      "pre:  0.12222222222222222\n",
      "rec:  0.3142857142857143\n",
      "ma F1:  0.5206797385620915\n",
      "mi F1:  0.7685393258426967\n",
      "we F1:  0.8111401924065508\n",
      "Loss:  0.06487134099006653\n",
      "Loss:  0.06983331590890884\n",
      "Loss:  0.08943545073270798\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.037503376603126526\n",
      "Loss:  0.051018353551626205\n",
      "Loss:  0.08073004335165024\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.05411510914564133\n",
      "Loss:  0.05372815951704979\n",
      "Loss:  0.07188528776168823\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.07370638847351074\n",
      "Eval Loss:  0.772532045841217\n",
      "Eval Loss:  1.0664660930633545\n",
      "[[17991  1353]\n",
      " [ 1679 10978]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92     19344\n",
      "           1       0.89      0.87      0.88     12657\n",
      "\n",
      "    accuracy                           0.91     32001\n",
      "   macro avg       0.90      0.90      0.90     32001\n",
      "weighted avg       0.91      0.91      0.91     32001\n",
      "\n",
      "acc:  0.9052529608449736\n",
      "pre:  0.8902765388046388\n",
      "rec:  0.8673461325748597\n",
      "ma F1:  0.9004730329203632\n",
      "mi F1:  0.9052529608449736\n",
      "we F1:  0.9050307654216994\n",
      "[[481 339]\n",
      " [ 29  41]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.59      0.72       820\n",
      "           1       0.11      0.59      0.18        70\n",
      "\n",
      "    accuracy                           0.59       890\n",
      "   macro avg       0.53      0.59      0.45       890\n",
      "weighted avg       0.88      0.59      0.68       890\n",
      "\n",
      "acc:  0.5865168539325842\n",
      "pre:  0.10789473684210527\n",
      "rec:  0.5857142857142857\n",
      "ma F1:  0.4527652464494569\n",
      "mi F1:  0.5865168539325842\n",
      "we F1:  0.6807509410229694\n",
      "Loss:  0.07900207489728928\n",
      "Loss:  0.0998590961098671\n",
      "Loss:  0.08425639569759369\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.07192467153072357\n",
      "Loss:  0.05088244006037712\n",
      "Loss:  0.045679014176130295\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.06210321933031082\n",
      "Loss:  0.06604567170143127\n",
      "Loss:  0.06672823429107666\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.06197082996368408\n",
      "Eval Loss:  0.5941144824028015\n",
      "Eval Loss:  1.302132487297058\n",
      "[[17989  1355]\n",
      " [ 1593 11064]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92     19344\n",
      "           1       0.89      0.87      0.88     12657\n",
      "\n",
      "    accuracy                           0.91     32001\n",
      "   macro avg       0.90      0.90      0.90     32001\n",
      "weighted avg       0.91      0.91      0.91     32001\n",
      "\n",
      "acc:  0.907877878816287\n",
      "pre:  0.8908929865528625\n",
      "rec:  0.8741407916567907\n",
      "ma F1:  0.9033519736951832\n",
      "mi F1:  0.907877878816287\n",
      "we F1:  0.9077223314633883\n",
      "[[528 292]\n",
      " [ 32  38]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.64      0.77       820\n",
      "           1       0.12      0.54      0.19        70\n",
      "\n",
      "    accuracy                           0.64       890\n",
      "   macro avg       0.53      0.59      0.48       890\n",
      "weighted avg       0.88      0.64      0.72       890\n",
      "\n",
      "acc:  0.6359550561797753\n",
      "pre:  0.11515151515151516\n",
      "rec:  0.5428571428571428\n",
      "ma F1:  0.4776086956521739\n",
      "mi F1:  0.6359550561797753\n",
      "we F1:  0.7199755740107474\n",
      "Loss:  0.05586649850010872\n",
      "Loss:  0.0633755475282669\n",
      "Loss:  0.07580836117267609\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.057583052664995193\n",
      "Loss:  0.0563625693321228\n",
      "Loss:  0.07822068780660629\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.08762437105178833\n",
      "Loss:  0.05502348020672798\n",
      "Loss:  0.08376283198595047\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.015991926193237305\n",
      "Eval Loss:  0.36648571491241455\n",
      "Eval Loss:  1.2865064144134521\n",
      "[[18400   944]\n",
      " [ 2277 10380]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     19344\n",
      "           1       0.92      0.82      0.87     12657\n",
      "\n",
      "    accuracy                           0.90     32001\n",
      "   macro avg       0.90      0.89      0.89     32001\n",
      "weighted avg       0.90      0.90      0.90     32001\n",
      "\n",
      "acc:  0.8993468954095184\n",
      "pre:  0.9166372306605439\n",
      "rec:  0.8200995496563167\n",
      "ma F1:  0.8926012938324348\n",
      "mi F1:  0.8993468954095184\n",
      "we F1:  0.8982257125040007\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[534 286]\n",
      " [ 32  38]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.65      0.77       820\n",
      "           1       0.12      0.54      0.19        70\n",
      "\n",
      "    accuracy                           0.64       890\n",
      "   macro avg       0.53      0.60      0.48       890\n",
      "weighted avg       0.88      0.64      0.73       890\n",
      "\n",
      "acc:  0.6426966292134831\n",
      "pre:  0.11728395061728394\n",
      "rec:  0.5428571428571428\n",
      "ma F1:  0.48172808578899945\n",
      "mi F1:  0.6426966292134831\n",
      "we F1:  0.7251281010477951\n",
      "Loss:  0.054587554186582565\n",
      "Loss:  0.03413230553269386\n",
      "Loss:  0.07342574000358582\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.06161493808031082\n",
      "Loss:  0.049201127141714096\n",
      "Loss:  0.03399854525923729\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.045396141707897186\n",
      "Loss:  0.058031804859638214\n",
      "Loss:  0.05253809317946434\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.11248040199279785\n",
      "Eval Loss:  0.7880911231040955\n",
      "Eval Loss:  1.8067102432250977\n",
      "[[17652  1692]\n",
      " [ 1254 11403]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92     19344\n",
      "           1       0.87      0.90      0.89     12657\n",
      "\n",
      "    accuracy                           0.91     32001\n",
      "   macro avg       0.90      0.91      0.90     32001\n",
      "weighted avg       0.91      0.91      0.91     32001\n",
      "\n",
      "acc:  0.907940376863223\n",
      "pre:  0.8707903780068729\n",
      "rec:  0.9009243896657976\n",
      "ma F1:  0.9042907552583008\n",
      "mi F1:  0.907940376863223\n",
      "we F1:  0.9081961832743217\n",
      "[[675 145]\n",
      " [ 50  20]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.82      0.87       820\n",
      "           1       0.12      0.29      0.17        70\n",
      "\n",
      "    accuracy                           0.78       890\n",
      "   macro avg       0.53      0.55      0.52       890\n",
      "weighted avg       0.87      0.78      0.82       890\n",
      "\n",
      "acc:  0.7808988764044944\n",
      "pre:  0.12121212121212122\n",
      "rec:  0.2857142857142857\n",
      "ma F1:  0.5219995868622185\n",
      "mi F1:  0.7808988764044944\n",
      "we F1:  0.8184491550403968\n",
      "Loss:  0.05666127800941467\n",
      "Loss:  0.04787282645702362\n",
      "Loss:  0.056618597358465195\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.0633336529135704\n",
      "Loss:  0.04871784895658493\n",
      "Loss:  0.04804075136780739\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.03763742372393608\n",
      "Loss:  0.06560230255126953\n",
      "Loss:  0.06695462763309479\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.05455493927001953\n",
      "Eval Loss:  0.4624042212963104\n",
      "Eval Loss:  2.119050979614258\n",
      "[[17855  1489]\n",
      " [ 1385 11272]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93     19344\n",
      "           1       0.88      0.89      0.89     12657\n",
      "\n",
      "    accuracy                           0.91     32001\n",
      "   macro avg       0.91      0.91      0.91     32001\n",
      "weighted avg       0.91      0.91      0.91     32001\n",
      "\n",
      "acc:  0.9101903065529202\n",
      "pre:  0.883316354517671\n",
      "rec:  0.8905743857154144\n",
      "ma F1:  0.9062218438783776\n",
      "mi F1:  0.9101903065529202\n",
      "we F1:  0.9102530013908593\n",
      "[[537 283]\n",
      " [ 28  42]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.65      0.78       820\n",
      "           1       0.13      0.60      0.21        70\n",
      "\n",
      "    accuracy                           0.65       890\n",
      "   macro avg       0.54      0.63      0.49       890\n",
      "weighted avg       0.89      0.65      0.73       890\n",
      "\n",
      "acc:  0.650561797752809\n",
      "pre:  0.12923076923076923\n",
      "rec:  0.6\n",
      "ma F1:  0.4940547456930037\n",
      "mi F1:  0.650561797752809\n",
      "we F1:  0.7311866427533148\n",
      "Loss:  0.04071328043937683\n",
      "Loss:  0.025620751082897186\n",
      "Loss:  0.06649675220251083\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.07812365889549255\n",
      "Loss:  0.050704456865787506\n",
      "Loss:  0.06840451061725616\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.05304349213838577\n",
      "Loss:  0.06350844353437424\n",
      "Loss:  0.05631900206208229\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.04605424404144287\n",
      "Eval Loss:  0.23935645818710327\n",
      "Eval Loss:  1.921177625656128\n",
      "[[17764  1580]\n",
      " [ 1214 11443]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     19344\n",
      "           1       0.88      0.90      0.89     12657\n",
      "\n",
      "    accuracy                           0.91     32001\n",
      "   macro avg       0.91      0.91      0.91     32001\n",
      "weighted avg       0.91      0.91      0.91     32001\n",
      "\n",
      "acc:  0.9126902284303615\n",
      "pre:  0.8786761882822698\n",
      "rec:  0.9040846962155329\n",
      "ma F1:  0.9091454324326038\n",
      "mi F1:  0.9126902284303615\n",
      "we F1:  0.9128954800258654\n",
      "[[526 294]\n",
      " [ 32  38]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.64      0.76       820\n",
      "           1       0.11      0.54      0.19        70\n",
      "\n",
      "    accuracy                           0.63       890\n",
      "   macro avg       0.53      0.59      0.48       890\n",
      "weighted avg       0.88      0.63      0.72       890\n",
      "\n",
      "acc:  0.6337078651685393\n",
      "pre:  0.1144578313253012\n",
      "rec:  0.5428571428571428\n",
      "ma F1:  0.47623999017972546\n",
      "mi F1:  0.6337078651685393\n",
      "we F1:  0.7182500439535172\n",
      "Loss:  0.04691263660788536\n",
      "Loss:  0.07627414166927338\n",
      "Loss:  0.0861685499548912\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.028768176212906837\n",
      "Loss:  0.05914430692791939\n",
      "Loss:  0.08189950883388519\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.044980015605688095\n",
      "Loss:  0.0790044292807579\n",
      "Loss:  0.04129956290125847\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.1473432183265686\n",
      "Eval Loss:  0.39071565866470337\n",
      "Eval Loss:  1.9572532176971436\n",
      "[[17628  1716]\n",
      " [ 1147 11510]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92     19344\n",
      "           1       0.87      0.91      0.89     12657\n",
      "\n",
      "    accuracy                           0.91     32001\n",
      "   macro avg       0.90      0.91      0.91     32001\n",
      "weighted avg       0.91      0.91      0.91     32001\n",
      "\n",
      "acc:  0.9105340458110684\n",
      "pre:  0.8702555572357478\n",
      "rec:  0.9093782096863395\n",
      "ma F1:  0.9071399770839256\n",
      "mi F1:  0.9105340458110684\n",
      "we F1:  0.9108497086266526\n",
      "[[578 242]\n",
      " [ 35  35]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.70      0.81       820\n",
      "           1       0.13      0.50      0.20        70\n",
      "\n",
      "    accuracy                           0.69       890\n",
      "   macro avg       0.53      0.60      0.50       890\n",
      "weighted avg       0.88      0.69      0.76       890\n",
      "\n",
      "acc:  0.6887640449438203\n",
      "pre:  0.1263537906137184\n",
      "rec:  0.5\n",
      "ma F1:  0.5042141695039326\n",
      "mi F1:  0.6887640449438203\n",
      "we F1:  0.7591173123767055\n",
      "Loss:  0.04607680067420006\n",
      "Loss:  0.030992072075605392\n",
      "Loss:  0.07664311677217484\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.08382844179868698\n",
      "Loss:  0.0534653440117836\n",
      "Loss:  0.05214198678731918\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.07083071768283844\n",
      "Loss:  0.0750335305929184\n",
      "Loss:  0.07050134986639023\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.042326927185058594\n",
      "Eval Loss:  0.29737603664398193\n",
      "Eval Loss:  1.6311379671096802\n",
      "[[18220  1124]\n",
      " [ 1575 11082]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19344\n",
      "           1       0.91      0.88      0.89     12657\n",
      "\n",
      "    accuracy                           0.92     32001\n",
      "   macro avg       0.91      0.91      0.91     32001\n",
      "weighted avg       0.92      0.92      0.92     32001\n",
      "\n",
      "acc:  0.9156588856598231\n",
      "pre:  0.9079141405865968\n",
      "rec:  0.8755629296041716\n",
      "ma F1:  0.9112428846222835\n",
      "mi F1:  0.9156588856598231\n",
      "we F1:  0.9153798696234081\n",
      "[[562 258]\n",
      " [ 33  37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.69      0.79       820\n",
      "           1       0.13      0.53      0.20        70\n",
      "\n",
      "    accuracy                           0.67       890\n",
      "   macro avg       0.53      0.61      0.50       890\n",
      "weighted avg       0.88      0.67      0.75       890\n",
      "\n",
      "acc:  0.6730337078651686\n",
      "pre:  0.12542372881355932\n",
      "rec:  0.5285714285714286\n",
      "ma F1:  0.4985430078900238\n",
      "mi F1:  0.6730337078651686\n",
      "we F1:  0.7478154364259451\n",
      "Loss:  0.062016088515520096\n",
      "Loss:  0.08432213217020035\n",
      "Loss:  0.07850860059261322\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.0675589069724083\n",
      "Loss:  0.05922134220600128\n",
      "Loss:  0.0441436767578125\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.07064102590084076\n",
      "Loss:  0.06762173026800156\n",
      "Loss:  0.03969990462064743\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.13581520318984985\n",
      "Eval Loss:  0.3814623951911926\n",
      "Eval Loss:  2.0616979598999023\n",
      "[[17708  1636]\n",
      " [ 1279 11378]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92     19344\n",
      "           1       0.87      0.90      0.89     12657\n",
      "\n",
      "    accuracy                           0.91     32001\n",
      "   macro avg       0.90      0.91      0.91     32001\n",
      "weighted avg       0.91      0.91      0.91     32001\n",
      "\n",
      "acc:  0.9089090965907315\n",
      "pre:  0.8742892269863224\n",
      "rec:  0.898949198072213\n",
      "ma F1:  0.9051998176564093\n",
      "mi F1:  0.9089090965907314\n",
      "we F1:  0.909118292890819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[597 223]\n",
      " [ 38  32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.73      0.82       820\n",
      "           1       0.13      0.46      0.20        70\n",
      "\n",
      "    accuracy                           0.71       890\n",
      "   macro avg       0.53      0.59      0.51       890\n",
      "weighted avg       0.88      0.71      0.77       890\n",
      "\n",
      "acc:  0.7067415730337079\n",
      "pre:  0.12549019607843137\n",
      "rec:  0.45714285714285713\n",
      "ma F1:  0.5087708168120539\n",
      "mi F1:  0.7067415730337079\n",
      "we F1:  0.7715638560443379\n",
      "Loss:  0.0516105517745018\n",
      "Loss:  0.038525957614183426\n",
      "Loss:  0.07686153799295425\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.08573339134454727\n",
      "Loss:  0.08285203576087952\n",
      "Loss:  0.06468618661165237\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.07402446120977402\n",
      "Loss:  0.05825052782893181\n",
      "Loss:  0.08802828192710876\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.053490519523620605\n",
      "Eval Loss:  0.33677345514297485\n",
      "Eval Loss:  1.683439016342163\n",
      "[[17808  1536]\n",
      " [ 1210 11447]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     19344\n",
      "           1       0.88      0.90      0.89     12657\n",
      "\n",
      "    accuracy                           0.91     32001\n",
      "   macro avg       0.91      0.91      0.91     32001\n",
      "weighted avg       0.91      0.91      0.91     32001\n",
      "\n",
      "acc:  0.9141901815568263\n",
      "pre:  0.8816914426557806\n",
      "rec:  0.9044007268705064\n",
      "ma F1:  0.9106602318938699\n",
      "mi F1:  0.9141901815568263\n",
      "we F1:  0.9143710907833825\n",
      "[[545 275]\n",
      " [ 37  33]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.66      0.78       820\n",
      "           1       0.11      0.47      0.17        70\n",
      "\n",
      "    accuracy                           0.65       890\n",
      "   macro avg       0.52      0.57      0.48       890\n",
      "weighted avg       0.87      0.65      0.73       890\n",
      "\n",
      "acc:  0.6494382022471911\n",
      "pre:  0.10714285714285714\n",
      "rec:  0.4714285714285714\n",
      "ma F1:  0.47603197246563866\n",
      "mi F1:  0.6494382022471911\n",
      "we F1:  0.7300450043722094\n",
      "Loss:  0.046920668333768845\n",
      "Loss:  0.05399807542562485\n",
      "Loss:  0.07413361966609955\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.06770817935466766\n",
      "Loss:  0.04501050338149071\n",
      "Loss:  0.023404452949762344\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.035367123782634735\n",
      "Loss:  0.04610828310251236\n",
      "Loss:  0.04912731051445007\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.04870152473449707\n",
      "Eval Loss:  0.33671966195106506\n",
      "Eval Loss:  1.776139497756958\n",
      "[[17824  1520]\n",
      " [ 1187 11470]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     19344\n",
      "           1       0.88      0.91      0.89     12657\n",
      "\n",
      "    accuracy                           0.92     32001\n",
      "   macro avg       0.91      0.91      0.91     32001\n",
      "weighted avg       0.92      0.92      0.92     32001\n",
      "\n",
      "acc:  0.915408893472079\n",
      "pre:  0.8829869130100076\n",
      "rec:  0.9062179031366042\n",
      "ma F1:  0.9119370465523926\n",
      "mi F1:  0.915408893472079\n",
      "we F1:  0.9155908457893998\n",
      "[[396 424]\n",
      " [ 23  47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.48      0.64       820\n",
      "           1       0.10      0.67      0.17        70\n",
      "\n",
      "    accuracy                           0.50       890\n",
      "   macro avg       0.52      0.58      0.41       890\n",
      "weighted avg       0.88      0.50      0.60       890\n",
      "\n",
      "acc:  0.49775280898876406\n",
      "pre:  0.09978768577494693\n",
      "rec:  0.6714285714285714\n",
      "ma F1:  0.4064887460670536\n",
      "mi F1:  0.49775280898876406\n",
      "we F1:  0.6026149557841962\n",
      "Loss:  0.05500517785549164\n",
      "Loss:  0.060258954763412476\n",
      "Loss:  0.05446234345436096\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.08075869828462601\n",
      "Loss:  0.05300204083323479\n",
      "Loss:  0.054988253861665726\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.03550083190202713\n",
      "Loss:  0.05058993399143219\n",
      "Loss:  0.08235138654708862\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  17 Training Time 5844.705446004868 Best Test Acc:  0.9168539325842696\n",
      "test subjects:  ['./seg\\\\c01', './seg\\\\x35']\n",
      "*********\n",
      "33346 967\n",
      "31939 952\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.7355660200119019\n",
      "Eval Loss:  0.634566068649292\n",
      "Eval Loss:  0.6638168096542358\n",
      "[[19202    10]\n",
      " [12723     4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75     19212\n",
      "           1       0.29      0.00      0.00     12727\n",
      "\n",
      "    accuracy                           0.60     31939\n",
      "   macro avg       0.44      0.50      0.38     31939\n",
      "weighted avg       0.48      0.60      0.45     31939\n",
      "\n",
      "acc:  0.6013337925420332\n",
      "pre:  0.2857142857142857\n",
      "rec:  0.0003142924491239098\n",
      "ma F1:  0.37581505197505294\n",
      "mi F1:  0.6013337925420332\n",
      "we F1:  0.45199429109250616\n",
      "[[952]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       952\n",
      "\n",
      "    accuracy                           1.00       952\n",
      "   macro avg       1.00      1.00      1.00       952\n",
      "weighted avg       1.00      1.00      1.00       952\n",
      "\n",
      "acc:  1.0\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  1.0\n",
      "mi F1:  1.0\n",
      "we F1:  1.0\n",
      "Subject 18 Current Train Acc:  0.6013337925420332 Current Test Acc:  1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\install\\envs\\pytorch-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.16677722334861755\n",
      "Loss:  0.17237958312034607\n",
      "Loss:  0.16759829223155975\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.13935616612434387\n",
      "Loss:  0.14670611917972565\n",
      "Loss:  0.13222847878932953\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.11412359029054642\n",
      "Loss:  0.09980547428131104\n",
      "Loss:  0.08653584122657776\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.33769792318344116\n",
      "Eval Loss:  0.08522462844848633\n",
      "Eval Loss:  0.03223252296447754\n",
      "[[17600  1612]\n",
      " [ 3995  8732]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.92      0.86     19212\n",
      "           1       0.84      0.69      0.76     12727\n",
      "\n",
      "    accuracy                           0.82     31939\n",
      "   macro avg       0.83      0.80      0.81     31939\n",
      "weighted avg       0.83      0.82      0.82     31939\n",
      "\n",
      "acc:  0.8244466013337926\n",
      "pre:  0.8441608662026295\n",
      "rec:  0.6861004164374951\n",
      "ma F1:  0.8097823625638514\n",
      "mi F1:  0.8244466013337927\n",
      "we F1:  0.8205060419079052\n",
      "[[911  41]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.96      0.98       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.96       952\n",
      "   macro avg       0.50      0.48      0.49       952\n",
      "weighted avg       1.00      0.96      0.98       952\n",
      "\n",
      "acc:  0.9569327731092437\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48899624261943103\n",
      "mi F1:  0.9569327731092437\n",
      "we F1:  0.9779924852388621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\install\\envs\\pytorch-gpu\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.12253939360380173\n",
      "Loss:  0.1003619134426117\n",
      "Loss:  0.07905679941177368\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.09082452207803726\n",
      "Loss:  0.10514496266841888\n",
      "Loss:  0.07900265604257584\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.11785859614610672\n",
      "Loss:  0.09176720678806305\n",
      "Loss:  0.08964744955301285\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.14091801643371582\n",
      "Eval Loss:  0.062279582023620605\n",
      "Eval Loss:  0.01519918441772461\n",
      "[[18422   790]\n",
      " [ 4563  8164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.96      0.87     19212\n",
      "           1       0.91      0.64      0.75     12727\n",
      "\n",
      "    accuracy                           0.83     31939\n",
      "   macro avg       0.86      0.80      0.81     31939\n",
      "weighted avg       0.85      0.83      0.83     31939\n",
      "\n",
      "acc:  0.8323992610914556\n",
      "pre:  0.911771275407639\n",
      "rec:  0.6414708886618999\n",
      "ma F1:  0.813122217334878\n",
      "mi F1:  0.8323992610914556\n",
      "we F1:  0.825308962193662\n",
      "[[932  20]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98       952\n",
      "   macro avg       0.50      0.49      0.49       952\n",
      "weighted avg       1.00      0.98      0.99       952\n",
      "\n",
      "acc:  0.9789915966386554\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.494692144373673\n",
      "mi F1:  0.9789915966386554\n",
      "we F1:  0.989384288747346\n",
      "Loss:  0.1136537566781044\n",
      "Loss:  0.08169331401586533\n",
      "Loss:  0.07423222810029984\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.07033797353506088\n",
      "Loss:  0.07988474518060684\n",
      "Loss:  0.08780092746019363\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.10501173883676529\n",
      "Loss:  0.08994410187005997\n",
      "Loss:  0.05213657394051552\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.13421940803527832\n",
      "Eval Loss:  0.05395686626434326\n",
      "Eval Loss:  0.014436721801757812\n",
      "[[17940  1272]\n",
      " [ 3175  9552]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.93      0.89     19212\n",
      "           1       0.88      0.75      0.81     12727\n",
      "\n",
      "    accuracy                           0.86     31939\n",
      "   macro avg       0.87      0.84      0.85     31939\n",
      "weighted avg       0.86      0.86      0.86     31939\n",
      "\n",
      "acc:  0.8607658348727262\n",
      "pre:  0.8824833702882483\n",
      "rec:  0.7505303685078966\n",
      "ma F1:  0.8504511161149749\n",
      "mi F1:  0.8607658348727262\n",
      "we F1:  0.8584257168712954\n",
      "[[921  31]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.97      0.98       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97       952\n",
      "   macro avg       0.50      0.48      0.49       952\n",
      "weighted avg       1.00      0.97      0.98       952\n",
      "\n",
      "acc:  0.967436974789916\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4917245061398825\n",
      "mi F1:  0.967436974789916\n",
      "we F1:  0.983449012279765\n",
      "Loss:  0.12228171527385712\n",
      "Loss:  0.07451843470335007\n",
      "Loss:  0.10003573447465897\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.09901022166013718\n",
      "Loss:  0.11699890345335007\n",
      "Loss:  0.06983132660388947\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.06739487498998642\n",
      "Loss:  0.067357636988163\n",
      "Loss:  0.06405024230480194\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.1288362741470337\n",
      "Eval Loss:  0.05124104022979736\n",
      "Eval Loss:  0.010313272476196289\n",
      "[[18490   722]\n",
      " [ 4244  8483]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.96      0.88     19212\n",
      "           1       0.92      0.67      0.77     12727\n",
      "\n",
      "    accuracy                           0.84     31939\n",
      "   macro avg       0.87      0.81      0.83     31939\n",
      "weighted avg       0.86      0.84      0.84     31939\n",
      "\n",
      "acc:  0.8445161088324619\n",
      "pre:  0.9215643671917436\n",
      "rec:  0.6665357114795317\n",
      "ma F1:  0.8275912751097203\n",
      "mi F1:  0.8445161088324619\n",
      "we F1:  0.8385593521250077\n",
      "[[931  21]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98       952\n",
      "   macro avg       0.50      0.49      0.49       952\n",
      "weighted avg       1.00      0.98      0.99       952\n",
      "\n",
      "acc:  0.9779411764705882\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4944237918215613\n",
      "mi F1:  0.9779411764705882\n",
      "we F1:  0.9888475836431226\n",
      "Loss:  0.08148454129695892\n",
      "Loss:  0.08059417456388474\n",
      "Loss:  0.07802602648735046\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.07249696552753448\n",
      "Loss:  0.06243172287940979\n",
      "Loss:  0.08402253687381744\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.057354886084795\n",
      "Loss:  0.0666637122631073\n",
      "Loss:  0.12362445145845413\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.07694482803344727\n",
      "Eval Loss:  0.0672837495803833\n",
      "Eval Loss:  0.007123470306396484\n",
      "[[18237   975]\n",
      " [ 3177  9550]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90     19212\n",
      "           1       0.91      0.75      0.82     12727\n",
      "\n",
      "    accuracy                           0.87     31939\n",
      "   macro avg       0.88      0.85      0.86     31939\n",
      "weighted avg       0.87      0.87      0.87     31939\n",
      "\n",
      "acc:  0.870002191677886\n",
      "pre:  0.9073634204275535\n",
      "rec:  0.7503732222833347\n",
      "ma F1:  0.8596170770381768\n",
      "mi F1:  0.870002191677886\n",
      "we F1:  0.867369749818022\n",
      "[[920  32]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.97      0.98       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97       952\n",
      "   macro avg       0.50      0.48      0.49       952\n",
      "weighted avg       1.00      0.97      0.98       952\n",
      "\n",
      "acc:  0.9663865546218487\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49145299145299143\n",
      "mi F1:  0.9663865546218487\n",
      "we F1:  0.9829059829059829\n",
      "Loss:  0.07327853888273239\n",
      "Loss:  0.09672772139310837\n",
      "Loss:  0.08774689584970474\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.09203954041004181\n",
      "Loss:  0.0906643494963646\n",
      "Loss:  0.09449204802513123\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.0703931376338005\n",
      "Loss:  0.11176997423171997\n",
      "Loss:  0.10855748504400253\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.06459712982177734\n",
      "Eval Loss:  0.06326150894165039\n",
      "Eval Loss:  0.008392572402954102\n",
      "[[18338   874]\n",
      " [ 3239  9488]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90     19212\n",
      "           1       0.92      0.75      0.82     12727\n",
      "\n",
      "    accuracy                           0.87     31939\n",
      "   macro avg       0.88      0.85      0.86     31939\n",
      "weighted avg       0.88      0.87      0.87     31939\n",
      "\n",
      "acc:  0.8712232693572122\n",
      "pre:  0.9156533487743679\n",
      "rec:  0.7455016893219141\n",
      "ma F1:  0.860513607601898\n",
      "mi F1:  0.8712232693572122\n",
      "we F1:  0.8683613088994361\n",
      "[[909  43]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.98       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.95       952\n",
      "   macro avg       0.50      0.48      0.49       952\n",
      "weighted avg       1.00      0.95      0.98       952\n",
      "\n",
      "acc:  0.9548319327731093\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4884470714669532\n",
      "mi F1:  0.9548319327731093\n",
      "we F1:  0.9768941429339063\n",
      "Loss:  0.05070851370692253\n",
      "Loss:  0.07443314045667648\n",
      "Loss:  0.06645120680332184\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.06553325802087784\n",
      "Loss:  0.06891433894634247\n",
      "Loss:  0.1307789832353592\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.07746744155883789\n",
      "Loss:  0.10585171729326248\n",
      "Loss:  0.08790350705385208\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.0669325590133667\n",
      "Eval Loss:  0.060463547706604004\n",
      "Eval Loss:  0.006989717483520508\n",
      "[[18474   738]\n",
      " [ 3371  9356]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90     19212\n",
      "           1       0.93      0.74      0.82     12727\n",
      "\n",
      "    accuracy                           0.87     31939\n",
      "   macro avg       0.89      0.85      0.86     31939\n",
      "weighted avg       0.88      0.87      0.87     31939\n",
      "\n",
      "acc:  0.8713485080935534\n",
      "pre:  0.9268872597582722\n",
      "rec:  0.735130038500825\n",
      "ma F1:  0.8599330822022831\n",
      "mi F1:  0.8713485080935534\n",
      "we F1:  0.8680520816434859\n",
      "[[915  37]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.96      0.98       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.96       952\n",
      "   macro avg       0.50      0.48      0.49       952\n",
      "weighted avg       1.00      0.96      0.98       952\n",
      "\n",
      "acc:  0.9611344537815126\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49009105516871987\n",
      "mi F1:  0.9611344537815126\n",
      "we F1:  0.9801821103374397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.08045824617147446\n",
      "Loss:  0.08225901424884796\n",
      "Loss:  0.07967841625213623\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.07084834575653076\n",
      "Loss:  0.07488203793764114\n",
      "Loss:  0.09528769552707672\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.032845936715602875\n",
      "Loss:  0.09701402485370636\n",
      "Loss:  0.05349888652563095\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.0525357723236084\n",
      "Eval Loss:  0.07801187038421631\n",
      "Eval Loss:  0.0058879852294921875\n",
      "[[18307   905]\n",
      " [ 2823  9904]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91     19212\n",
      "           1       0.92      0.78      0.84     12727\n",
      "\n",
      "    accuracy                           0.88     31939\n",
      "   macro avg       0.89      0.87      0.87     31939\n",
      "weighted avg       0.89      0.88      0.88     31939\n",
      "\n",
      "acc:  0.8832774977300479\n",
      "pre:  0.9162734758071978\n",
      "rec:  0.7781881040308006\n",
      "ma F1:  0.8745972276937015\n",
      "mi F1:  0.8832774977300479\n",
      "we F1:  0.8812962103410543\n",
      "[[907  45]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.98       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.95       952\n",
      "   macro avg       0.50      0.48      0.49       952\n",
      "weighted avg       1.00      0.95      0.98       952\n",
      "\n",
      "acc:  0.9527310924369747\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4878967186659494\n",
      "mi F1:  0.9527310924369747\n",
      "we F1:  0.9757934373318988\n",
      "Loss:  0.07262174785137177\n",
      "Loss:  0.06935501843690872\n",
      "Loss:  0.06921211630105972\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.09985306113958359\n",
      "Loss:  0.09643377363681793\n",
      "Loss:  0.06348618119955063\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.07601050287485123\n",
      "Loss:  0.08597162365913391\n",
      "Loss:  0.08060973882675171\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.03728163242340088\n",
      "Eval Loss:  0.06123244762420654\n",
      "Eval Loss:  0.004330158233642578\n",
      "[[18108  1104]\n",
      " [ 2172 10555]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92     19212\n",
      "           1       0.91      0.83      0.87     12727\n",
      "\n",
      "    accuracy                           0.90     31939\n",
      "   macro avg       0.90      0.89      0.89     31939\n",
      "weighted avg       0.90      0.90      0.90     31939\n",
      "\n",
      "acc:  0.8974294749365979\n",
      "pre:  0.9053092031906681\n",
      "rec:  0.8293392001257169\n",
      "ma F1:  0.8913535576885911\n",
      "mi F1:  0.8974294749365979\n",
      "we F1:  0.8965703355719916\n",
      "[[909  43]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.98       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.95       952\n",
      "   macro avg       0.50      0.48      0.49       952\n",
      "weighted avg       1.00      0.95      0.98       952\n",
      "\n",
      "acc:  0.9548319327731093\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4884470714669532\n",
      "mi F1:  0.9548319327731093\n",
      "we F1:  0.9768941429339063\n",
      "Loss:  0.07367575913667679\n",
      "Loss:  0.05245399847626686\n",
      "Loss:  0.042740754783153534\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.07587051391601562\n",
      "Loss:  0.08314479887485504\n",
      "Loss:  0.08065825700759888\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.087442547082901\n",
      "Loss:  0.1183980256319046\n",
      "Loss:  0.04209217056632042\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.03536999225616455\n",
      "Eval Loss:  0.054786086082458496\n",
      "Eval Loss:  0.005598545074462891\n",
      "[[17553  1659]\n",
      " [ 1595 11132]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.92     19212\n",
      "           1       0.87      0.87      0.87     12727\n",
      "\n",
      "    accuracy                           0.90     31939\n",
      "   macro avg       0.89      0.89      0.89     31939\n",
      "weighted avg       0.90      0.90      0.90     31939\n",
      "\n",
      "acc:  0.8981182879864742\n",
      "pre:  0.8702994292862168\n",
      "rec:  0.874675885911841\n",
      "ma F1:  0.8938271118360828\n",
      "mi F1:  0.8981182879864742\n",
      "we F1:  0.8981610594042635\n",
      "[[819 133]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.86      0.92       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.86       952\n",
      "   macro avg       0.50      0.43      0.46       952\n",
      "weighted avg       1.00      0.86      0.92       952\n",
      "\n",
      "acc:  0.8602941176470589\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4624505928853755\n",
      "mi F1:  0.8602941176470589\n",
      "we F1:  0.924901185770751\n",
      "Loss:  0.08233612030744553\n",
      "Loss:  0.078484907746315\n",
      "Loss:  0.05154115706682205\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.08083440363407135\n",
      "Loss:  0.08812850713729858\n",
      "Loss:  0.05900648981332779\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.04851877689361572\n",
      "Loss:  0.042966119945049286\n",
      "Loss:  0.05625594034790993\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.031561970710754395\n",
      "Eval Loss:  0.04739940166473389\n",
      "Eval Loss:  0.004441499710083008\n",
      "[[17618  1594]\n",
      " [ 1602 11125]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92     19212\n",
      "           1       0.87      0.87      0.87     12727\n",
      "\n",
      "    accuracy                           0.90     31939\n",
      "   macro avg       0.90      0.90      0.90     31939\n",
      "weighted avg       0.90      0.90      0.90     31939\n",
      "\n",
      "acc:  0.899934249663421\n",
      "pre:  0.8746756820504756\n",
      "rec:  0.8741258741258742\n",
      "ma F1:  0.8956204124415432\n",
      "mi F1:  0.899934249663421\n",
      "we F1:  0.8999289346013888\n",
      "[[767 185]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.81      0.89       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.81       952\n",
      "   macro avg       0.50      0.40      0.45       952\n",
      "weighted avg       1.00      0.81      0.89       952\n",
      "\n",
      "acc:  0.805672268907563\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.44618964514252474\n",
      "mi F1:  0.8056722689075629\n",
      "we F1:  0.8923792902850495\n",
      "Loss:  0.06762668490409851\n",
      "Loss:  0.07349155843257904\n",
      "Loss:  0.043378669768571854\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.06451334059238434\n",
      "Loss:  0.0637587383389473\n",
      "Loss:  0.06775273382663727\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.06034090742468834\n",
      "Loss:  0.09378626197576523\n",
      "Loss:  0.04186926409602165\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.03758740425109863\n",
      "Eval Loss:  0.10279560089111328\n",
      "Eval Loss:  0.0037469863891601562\n",
      "[[17674  1538]\n",
      " [ 1483 11244]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92     19212\n",
      "           1       0.88      0.88      0.88     12727\n",
      "\n",
      "    accuracy                           0.91     31939\n",
      "   macro avg       0.90      0.90      0.90     31939\n",
      "weighted avg       0.91      0.91      0.91     31939\n",
      "\n",
      "acc:  0.9054134443783463\n",
      "pre:  0.8796745423251447\n",
      "rec:  0.8834760744873105\n",
      "ma F1:  0.9014178863492923\n",
      "mi F1:  0.9054134443783463\n",
      "we F1:  0.905447621002234\n",
      "[[844 108]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.89      0.94       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.89       952\n",
      "   macro avg       0.50      0.44      0.47       952\n",
      "weighted avg       1.00      0.89      0.94       952\n",
      "\n",
      "acc:  0.8865546218487395\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.46993318485523383\n",
      "mi F1:  0.8865546218487395\n",
      "we F1:  0.9398663697104677\n",
      "Loss:  0.044707369059324265\n",
      "Loss:  0.055015914142131805\n",
      "Loss:  0.06341005116701126\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.04949697107076645\n",
      "Loss:  0.07000631839036942\n",
      "Loss:  0.05666531249880791\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.0418224111199379\n",
      "Loss:  0.08136214315891266\n",
      "Loss:  0.05327177792787552\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.02836465835571289\n",
      "Eval Loss:  0.04353046417236328\n",
      "Eval Loss:  0.003885984420776367\n",
      "[[18263   949]\n",
      " [ 2075 10652]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     19212\n",
      "           1       0.92      0.84      0.88     12727\n",
      "\n",
      "    accuracy                           0.91     31939\n",
      "   macro avg       0.91      0.89      0.90     31939\n",
      "weighted avg       0.91      0.91      0.90     31939\n",
      "\n",
      "acc:  0.9053195153260903\n",
      "pre:  0.9181967071804155\n",
      "rec:  0.8369607920169718\n",
      "ma F1:  0.8996193031519091\n",
      "mi F1:  0.9053195153260903\n",
      "we F1:  0.9044762044723093\n",
      "[[871  81]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.91      0.96       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.91       952\n",
      "   macro avg       0.50      0.46      0.48       952\n",
      "weighted avg       1.00      0.91      0.96       952\n",
      "\n",
      "acc:  0.9149159663865546\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4777838727372463\n",
      "mi F1:  0.9149159663865546\n",
      "we F1:  0.9555677454744927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.04629984498023987\n",
      "Loss:  0.05657699331641197\n",
      "Loss:  0.07011173665523529\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.07196319103240967\n",
      "Loss:  0.08762875199317932\n",
      "Loss:  0.08157500624656677\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.049443379044532776\n",
      "Loss:  0.07055402547121048\n",
      "Loss:  0.07967237383127213\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.023072242736816406\n",
      "Eval Loss:  0.05282783508300781\n",
      "Eval Loss:  0.004251718521118164\n",
      "[[17391  1821]\n",
      " [ 1186 11541]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92     19212\n",
      "           1       0.86      0.91      0.88     12727\n",
      "\n",
      "    accuracy                           0.91     31939\n",
      "   macro avg       0.90      0.91      0.90     31939\n",
      "weighted avg       0.91      0.91      0.91     31939\n",
      "\n",
      "acc:  0.9058517799555402\n",
      "pre:  0.8637180062864841\n",
      "rec:  0.9068122888347607\n",
      "ma F1:  0.9025836372377247\n",
      "mi F1:  0.9058517799555402\n",
      "we F1:  0.9062065270710641\n",
      "[[854  98]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.90      0.95       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.90       952\n",
      "   macro avg       0.50      0.45      0.47       952\n",
      "weighted avg       1.00      0.90      0.95       952\n",
      "\n",
      "acc:  0.8970588235294118\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.47286821705426363\n",
      "mi F1:  0.8970588235294118\n",
      "we F1:  0.9457364341085273\n",
      "Loss:  0.06051476672291756\n",
      "Loss:  0.06595952063798904\n",
      "Loss:  0.05517301708459854\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.05988587439060211\n",
      "Loss:  0.061284519731998444\n",
      "Loss:  0.0556563138961792\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.06640579551458359\n",
      "Loss:  0.056971967220306396\n",
      "Loss:  0.03315562382340431\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.01994776725769043\n",
      "Eval Loss:  0.058350324630737305\n",
      "Eval Loss:  0.004271268844604492\n",
      "[[17630  1582]\n",
      " [ 1257 11470]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.93     19212\n",
      "           1       0.88      0.90      0.89     12727\n",
      "\n",
      "    accuracy                           0.91     31939\n",
      "   macro avg       0.91      0.91      0.91     31939\n",
      "weighted avg       0.91      0.91      0.91     31939\n",
      "\n",
      "acc:  0.9111118068818685\n",
      "pre:  0.878792522218817\n",
      "rec:  0.9012335978628113\n",
      "ma F1:  0.9076776047045675\n",
      "mi F1:  0.9111118068818685\n",
      "we F1:  0.9112929944967424\n",
      "[[834 118]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.88      0.93       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.88       952\n",
      "   macro avg       0.50      0.44      0.47       952\n",
      "weighted avg       1.00      0.88      0.93       952\n",
      "\n",
      "acc:  0.8760504201680672\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4669652855543113\n",
      "mi F1:  0.8760504201680672\n",
      "we F1:  0.9339305711086227\n",
      "Loss:  0.06133495271205902\n",
      "Loss:  0.052068427205085754\n",
      "Loss:  0.06622907519340515\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.05079185217618942\n",
      "Loss:  0.04726998507976532\n",
      "Loss:  0.05392885208129883\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.05712847411632538\n",
      "Loss:  0.05976789444684982\n",
      "Loss:  0.07095351815223694\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.021906137466430664\n",
      "Eval Loss:  0.04162168502807617\n",
      "Eval Loss:  0.0033216476440429688\n",
      "[[18031  1181]\n",
      " [ 1639 11088]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19212\n",
      "           1       0.90      0.87      0.89     12727\n",
      "\n",
      "    accuracy                           0.91     31939\n",
      "   macro avg       0.91      0.90      0.91     31939\n",
      "weighted avg       0.91      0.91      0.91     31939\n",
      "\n",
      "acc:  0.9117066908794891\n",
      "pre:  0.9037411361969191\n",
      "rec:  0.8712186689714779\n",
      "ma F1:  0.9073274078669726\n",
      "mi F1:  0.9117066908794891\n",
      "we F1:  0.911417808318675\n",
      "[[885  67]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.93      0.96       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.93       952\n",
      "   macro avg       0.50      0.46      0.48       952\n",
      "weighted avg       1.00      0.93      0.96       952\n",
      "\n",
      "acc:  0.9296218487394958\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48176374523679916\n",
      "mi F1:  0.9296218487394958\n",
      "we F1:  0.9635274904735983\n",
      "Loss:  0.047048669308423996\n",
      "Loss:  0.07105233520269394\n",
      "Loss:  0.09806933999061584\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.04478507861495018\n",
      "Loss:  0.05847395956516266\n",
      "Loss:  0.05224987491965294\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.07750392705202103\n",
      "Loss:  0.08259317278862\n",
      "Loss:  0.06607656180858612\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.014740824699401855\n",
      "Eval Loss:  0.1425817608833313\n",
      "Eval Loss:  0.003019094467163086\n",
      "[[17260  1952]\n",
      " [  946 11781]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92     19212\n",
      "           1       0.86      0.93      0.89     12727\n",
      "\n",
      "    accuracy                           0.91     31939\n",
      "   macro avg       0.90      0.91      0.91     31939\n",
      "weighted avg       0.91      0.91      0.91     31939\n",
      "\n",
      "acc:  0.9092645355208366\n",
      "pre:  0.857860627685138\n",
      "rec:  0.9256698357821953\n",
      "ma F1:  0.9065134172756173\n",
      "mi F1:  0.9092645355208366\n",
      "we F1:  0.909769668748559\n",
      "[[864  88]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.91      0.95       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.91       952\n",
      "   macro avg       0.50      0.45      0.48       952\n",
      "weighted avg       1.00      0.91      0.95       952\n",
      "\n",
      "acc:  0.907563025210084\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4757709251101322\n",
      "mi F1:  0.907563025210084\n",
      "we F1:  0.9515418502202644\n",
      "Loss:  0.045609455555677414\n",
      "Loss:  0.055205222219228745\n",
      "Loss:  0.06959085166454315\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.051708780229091644\n",
      "Loss:  0.04311303794384003\n",
      "Loss:  0.0365842767059803\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.0541134849190712\n",
      "Loss:  0.09265530854463577\n",
      "Loss:  0.04984849691390991\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.015272021293640137\n",
      "Eval Loss:  0.03417682647705078\n",
      "Eval Loss:  0.0024979114532470703\n",
      "[[17393  1819]\n",
      " [ 1094 11633]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.91      0.92     19212\n",
      "           1       0.86      0.91      0.89     12727\n",
      "\n",
      "    accuracy                           0.91     31939\n",
      "   macro avg       0.90      0.91      0.91     31939\n",
      "weighted avg       0.91      0.91      0.91     31939\n",
      "\n",
      "acc:  0.9087948902595573\n",
      "pre:  0.8647784716027357\n",
      "rec:  0.9140410151646107\n",
      "ma F1:  0.9057288259931504\n",
      "mi F1:  0.9087948902595573\n",
      "we F1:  0.9091808098069784\n",
      "[[895  57]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.94      0.97       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.94       952\n",
      "   macro avg       0.50      0.47      0.48       952\n",
      "weighted avg       1.00      0.94      0.97       952\n",
      "\n",
      "acc:  0.9401260504201681\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.484569572279372\n",
      "mi F1:  0.9401260504201681\n",
      "we F1:  0.969139144558744\n",
      "Loss:  0.050710778683423996\n",
      "Loss:  0.040324967354536057\n",
      "Loss:  0.05271026864647865\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.07152102887630463\n",
      "Loss:  0.06656758487224579\n",
      "Loss:  0.06700154393911362\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.056903399527072906\n",
      "Loss:  0.04673028737306595\n",
      "Loss:  0.04909732937812805\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.0175248384475708\n",
      "Eval Loss:  0.05003809928894043\n",
      "Eval Loss:  0.0027527809143066406\n",
      "[[17973  1239]\n",
      " [ 1368 11359]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     19212\n",
      "           1       0.90      0.89      0.90     12727\n",
      "\n",
      "    accuracy                           0.92     31939\n",
      "   macro avg       0.92      0.91      0.91     31939\n",
      "weighted avg       0.92      0.92      0.92     31939\n",
      "\n",
      "acc:  0.9183756535896552\n",
      "pre:  0.9016510557231306\n",
      "rec:  0.8925119823996228\n",
      "ma F1:  0.9147185230245083\n",
      "mi F1:  0.9183756535896552\n",
      "we F1:  0.9183043246143145\n",
      "[[884  68]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.93      0.96       952\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.93       952\n",
      "   macro avg       0.50      0.46      0.48       952\n",
      "weighted avg       1.00      0.93      0.96       952\n",
      "\n",
      "acc:  0.9285714285714286\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4814814814814815\n",
      "mi F1:  0.9285714285714286\n",
      "we F1:  0.962962962962963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.06991968303918839\n",
      "Loss:  0.07975080609321594\n",
      "Loss:  0.0653209537267685\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.06997038424015045\n",
      "Loss:  0.061637990176677704\n",
      "Loss:  0.05038584768772125\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.05981709063053131\n",
      "Loss:  0.057947032153606415\n",
      "Loss:  0.07501766830682755\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  18 Training Time 5841.481066226959 Best Test Acc:  1.0\n",
      "test subjects:  ['./seg\\\\c02', './seg\\\\c09']\n",
      "*********\n",
      "33343 970\n",
      "31940 951\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.7838558554649353\n",
      "Eval Loss:  0.8263049721717834\n",
      "Eval Loss:  0.614149272441864\n",
      "[[    0 19216]\n",
      " [    0 12724]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     19216\n",
      "           1       0.40      1.00      0.57     12724\n",
      "\n",
      "    accuracy                           0.40     31940\n",
      "   macro avg       0.20      0.50      0.28     31940\n",
      "weighted avg       0.16      0.40      0.23     31940\n",
      "\n",
      "acc:  0.3983719474013776\n",
      "pre:  0.3983719474013776\n",
      "rec:  1.0\n",
      "ma F1:  0.284882679562959\n",
      "mi F1:  0.3983719474013776\n",
      "we F1:  0.2269785356768372\n",
      "[[  0 948]\n",
      " [  0   3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       948\n",
      "           1       0.00      1.00      0.01         3\n",
      "\n",
      "    accuracy                           0.00       951\n",
      "   macro avg       0.00      0.50      0.00       951\n",
      "weighted avg       0.00      0.00      0.00       951\n",
      "\n",
      "acc:  0.0031545741324921135\n",
      "pre:  0.0031545741324921135\n",
      "rec:  1.0\n",
      "ma F1:  0.0031446540880503146\n",
      "mi F1:  0.0031545741324921135\n",
      "we F1:  1.98400888835982e-05\n",
      "Subject 19 Current Train Acc:  0.3983719474013776 Current Test Acc:  0.0031545741324921135\n",
      "Loss:  0.1728430688381195\n",
      "Loss:  0.1692664921283722\n",
      "Loss:  0.15190434455871582\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.1447191834449768\n",
      "Loss:  0.13461756706237793\n",
      "Loss:  0.1458868682384491\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.11237474530935287\n",
      "Loss:  0.11261491477489471\n",
      "Loss:  0.11878474801778793\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.018067359924316406\n",
      "Eval Loss:  0.019959688186645508\n",
      "Eval Loss:  0.24164479970932007\n",
      "[[17535  1681]\n",
      " [ 4104  8620]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.91      0.86     19216\n",
      "           1       0.84      0.68      0.75     12724\n",
      "\n",
      "    accuracy                           0.82     31940\n",
      "   macro avg       0.82      0.79      0.80     31940\n",
      "weighted avg       0.82      0.82      0.81     31940\n",
      "\n",
      "acc:  0.8188791484032562\n",
      "pre:  0.8368119600038831\n",
      "rec:  0.6774599182646966\n",
      "ma F1:  0.8035765108216859\n",
      "mi F1:  0.8188791484032562\n",
      "we F1:  0.8147200580095215\n",
      "[[908  40]\n",
      " [  2   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       948\n",
      "           1       0.02      0.33      0.05         3\n",
      "\n",
      "    accuracy                           0.96       951\n",
      "   macro avg       0.51      0.65      0.51       951\n",
      "weighted avg       0.99      0.96      0.97       951\n",
      "\n",
      "acc:  0.9558359621451105\n",
      "pre:  0.024390243902439025\n",
      "rec:  0.3333333333333333\n",
      "ma F1:  0.5114247969468637\n",
      "mi F1:  0.9558359621451105\n",
      "we F1:  0.9744551730354448\n",
      "Subject 19 Current Train Acc:  0.8188791484032562 Current Test Acc:  0.9558359621451105\n",
      "Loss:  0.0871734768152237\n",
      "Loss:  0.08683371543884277\n",
      "Loss:  0.10012343525886536\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.06882157176733017\n",
      "Loss:  0.099042609333992\n",
      "Loss:  0.08610973507165909\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.07995247095823288\n",
      "Loss:  0.10440555214881897\n",
      "Loss:  0.09184638410806656\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.012255668640136719\n",
      "Eval Loss:  0.013910531997680664\n",
      "Eval Loss:  0.08701729774475098\n",
      "[[18093  1123]\n",
      " [ 3607  9117]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.94      0.88     19216\n",
      "           1       0.89      0.72      0.79     12724\n",
      "\n",
      "    accuracy                           0.85     31940\n",
      "   macro avg       0.86      0.83      0.84     31940\n",
      "weighted avg       0.86      0.85      0.85     31940\n",
      "\n",
      "acc:  0.8519098309329993\n",
      "pre:  0.89033203125\n",
      "rec:  0.7165199622760138\n",
      "ma F1:  0.8392113664492971\n",
      "mi F1:  0.8519098309329993\n",
      "we F1:  0.848395683676146\n",
      "[[917  31]\n",
      " [  2   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       948\n",
      "           1       0.03      0.33      0.06         3\n",
      "\n",
      "    accuracy                           0.97       951\n",
      "   macro avg       0.51      0.65      0.52       951\n",
      "weighted avg       0.99      0.97      0.98       951\n",
      "\n",
      "acc:  0.9652996845425867\n",
      "pre:  0.03125\n",
      "rec:  0.3333333333333333\n",
      "ma F1:  0.5197337210192057\n",
      "mi F1:  0.9652996845425867\n",
      "we F1:  0.9794060305493313\n",
      "Subject 19 Current Train Acc:  0.8519098309329993 Current Test Acc:  0.9652996845425867\n",
      "Loss:  0.11282192170619965\n",
      "Loss:  0.06391433626413345\n",
      "Loss:  0.086830735206604\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.07092240452766418\n",
      "Loss:  0.09973368048667908\n",
      "Loss:  0.07938334345817566\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.06859888136386871\n",
      "Loss:  0.055396564304828644\n",
      "Loss:  0.08394841849803925\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.010572195053100586\n",
      "Eval Loss:  0.012263059616088867\n",
      "Eval Loss:  0.0940936803817749\n",
      "[[18663   553]\n",
      " [ 4793  7931]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.87     19216\n",
      "           1       0.93      0.62      0.75     12724\n",
      "\n",
      "    accuracy                           0.83     31940\n",
      "   macro avg       0.87      0.80      0.81     31940\n",
      "weighted avg       0.85      0.83      0.82     31940\n",
      "\n",
      "acc:  0.8326236693800877\n",
      "pre:  0.9348184818481848\n",
      "rec:  0.6233102797862308\n",
      "ma F1:  0.8113220481775878\n",
      "mi F1:  0.8326236693800877\n",
      "we F1:  0.8242078220171918\n",
      "[[935  13]\n",
      " [  2   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       948\n",
      "           1       0.07      0.33      0.12         3\n",
      "\n",
      "    accuracy                           0.98       951\n",
      "   macro avg       0.53      0.66      0.55       951\n",
      "weighted avg       0.99      0.98      0.99       951\n",
      "\n",
      "acc:  0.9842271293375394\n",
      "pre:  0.07142857142857142\n",
      "rec:  0.3333333333333333\n",
      "ma F1:  0.5548447495709159\n",
      "mi F1:  0.9842271293375394\n",
      "we F1:  0.9892840952662685\n",
      "Subject 19 Current Train Acc:  0.8326236693800877 Current Test Acc:  0.9842271293375394\n",
      "Loss:  0.08431430906057358\n",
      "Loss:  0.07296548038721085\n",
      "Loss:  0.10945896059274673\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.08875472098588943\n",
      "Loss:  0.1024647206068039\n",
      "Loss:  0.09192780405282974\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.07659225910902023\n",
      "Loss:  0.07276060432195663\n",
      "Loss:  0.061195943504571915\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.011682987213134766\n",
      "Eval Loss:  0.01096343994140625\n",
      "Eval Loss:  0.08477592468261719\n",
      "[[18463   753]\n",
      " [ 3837  8887]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89     19216\n",
      "           1       0.92      0.70      0.79     12724\n",
      "\n",
      "    accuracy                           0.86     31940\n",
      "   macro avg       0.87      0.83      0.84     31940\n",
      "weighted avg       0.87      0.86      0.85     31940\n",
      "\n",
      "acc:  0.8562930494677521\n",
      "pre:  0.9218879668049793\n",
      "rec:  0.6984438855705752\n",
      "ma F1:  0.8420998253131724\n",
      "mi F1:  0.8562930494677522\n",
      "we F1:  0.8517220487061894\n",
      "[[927  21]\n",
      " [  2   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       948\n",
      "           1       0.05      0.33      0.08         3\n",
      "\n",
      "    accuracy                           0.98       951\n",
      "   macro avg       0.52      0.66      0.53       951\n",
      "weighted avg       0.99      0.98      0.98       951\n",
      "\n",
      "acc:  0.9758149316508938\n",
      "pre:  0.045454545454545456\n",
      "rec:  0.3333333333333333\n",
      "ma F1:  0.5338732019179542\n",
      "mi F1:  0.9758149316508938\n",
      "we F1:  0.9848828505115049\n",
      "Loss:  0.09103447943925858\n",
      "Loss:  0.12198495864868164\n",
      "Loss:  0.071937195956707\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.09616892039775848\n",
      "Loss:  0.07396408170461655\n",
      "Loss:  0.07480020076036453\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.058234311640262604\n",
      "Loss:  0.10056531429290771\n",
      "Loss:  0.06534440070390701\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.010670661926269531\n",
      "Eval Loss:  0.009486198425292969\n",
      "Eval Loss:  0.1285514235496521\n",
      "[[18710   506]\n",
      " [ 4504  8220]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.97      0.88     19216\n",
      "           1       0.94      0.65      0.77     12724\n",
      "\n",
      "    accuracy                           0.84     31940\n",
      "   macro avg       0.87      0.81      0.82     31940\n",
      "weighted avg       0.86      0.84      0.84     31940\n",
      "\n",
      "acc:  0.843143393863494\n",
      "pre:  0.9420123768049508\n",
      "rec:  0.6460232631248035\n",
      "ma F1:  0.824178367001841\n",
      "mi F1:  0.843143393863494\n",
      "we F1:  0.83591535026074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[938  10]\n",
      " [  2   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       948\n",
      "           1       0.09      0.33      0.14         3\n",
      "\n",
      "    accuracy                           0.99       951\n",
      "   macro avg       0.54      0.66      0.57       951\n",
      "weighted avg       1.00      0.99      0.99       951\n",
      "\n",
      "acc:  0.9873817034700315\n",
      "pre:  0.09090909090909091\n",
      "rec:  0.3333333333333333\n",
      "ma F1:  0.5682506053268765\n",
      "mi F1:  0.9873817034700315\n",
      "we F1:  0.9909601973709337\n",
      "Subject 19 Current Train Acc:  0.843143393863494 Current Test Acc:  0.9873817034700315\n",
      "Loss:  0.07040530443191528\n",
      "Loss:  0.057943571358919144\n",
      "Loss:  0.09253980219364166\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.08071311563253403\n",
      "Loss:  0.09595206379890442\n",
      "Loss:  0.0767446905374527\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.06030265614390373\n",
      "Loss:  0.09086012095212936\n",
      "Loss:  0.06168016791343689\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.01474905014038086\n",
      "Eval Loss:  0.008323907852172852\n",
      "Eval Loss:  0.02871990203857422\n",
      "[[18129  1087]\n",
      " [ 2604 10120]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.91     19216\n",
      "           1       0.90      0.80      0.85     12724\n",
      "\n",
      "    accuracy                           0.88     31940\n",
      "   macro avg       0.89      0.87      0.88     31940\n",
      "weighted avg       0.89      0.88      0.88     31940\n",
      "\n",
      "acc:  0.884439574201628\n",
      "pre:  0.9030070491657\n",
      "rec:  0.7953473750392959\n",
      "ma F1:  0.8766860533105915\n",
      "mi F1:  0.884439574201628\n",
      "we F1:  0.8829709649880306\n",
      "[[908  40]\n",
      " [  2   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       948\n",
      "           1       0.02      0.33      0.05         3\n",
      "\n",
      "    accuracy                           0.96       951\n",
      "   macro avg       0.51      0.65      0.51       951\n",
      "weighted avg       0.99      0.96      0.97       951\n",
      "\n",
      "acc:  0.9558359621451105\n",
      "pre:  0.024390243902439025\n",
      "rec:  0.3333333333333333\n",
      "ma F1:  0.5114247969468637\n",
      "mi F1:  0.9558359621451105\n",
      "we F1:  0.9744551730354448\n",
      "Loss:  0.09629759192466736\n",
      "Loss:  0.07715123891830444\n",
      "Loss:  0.054976463317871094\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.0629376471042633\n",
      "Loss:  0.06238964945077896\n",
      "Loss:  0.0800301656126976\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.0653131902217865\n",
      "Loss:  0.08804705739021301\n",
      "Loss:  0.049253836274147034\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.014672040939331055\n",
      "Eval Loss:  0.008453607559204102\n",
      "Eval Loss:  0.027025699615478516\n",
      "[[18204  1012]\n",
      " [ 2713 10011]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91     19216\n",
      "           1       0.91      0.79      0.84     12724\n",
      "\n",
      "    accuracy                           0.88     31940\n",
      "   macro avg       0.89      0.87      0.88     31940\n",
      "weighted avg       0.89      0.88      0.88     31940\n",
      "\n",
      "acc:  0.8833750782717595\n",
      "pre:  0.9081919622607275\n",
      "rec:  0.786780886513675\n",
      "ma F1:  0.8751608475407513\n",
      "mi F1:  0.8833750782717595\n",
      "we F1:  0.8816696704268377\n",
      "[[923  25]\n",
      " [  2   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99       948\n",
      "           1       0.04      0.33      0.07         3\n",
      "\n",
      "    accuracy                           0.97       951\n",
      "   macro avg       0.52      0.65      0.53       951\n",
      "weighted avg       0.99      0.97      0.98       951\n",
      "\n",
      "acc:  0.9716088328075709\n",
      "pre:  0.038461538461538464\n",
      "rec:  0.3333333333333333\n",
      "ma F1:  0.5272750704199421\n",
      "mi F1:  0.9716088328075709\n",
      "we F1:  0.982693080676243\n",
      "Loss:  0.07505295425653458\n",
      "Loss:  0.05140630528330803\n",
      "Loss:  0.06294211745262146\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.06882239878177643\n",
      "Loss:  0.06309749186038971\n",
      "Loss:  0.07431525737047195\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.07545390725135803\n",
      "Loss:  0.0549987331032753\n",
      "Loss:  0.0694500282406807\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.01604914665222168\n",
      "Eval Loss:  0.017787694931030273\n",
      "Eval Loss:  0.036301493644714355\n",
      "[[17968  1248]\n",
      " [ 2081 10643]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     19216\n",
      "           1       0.90      0.84      0.86     12724\n",
      "\n",
      "    accuracy                           0.90     31940\n",
      "   macro avg       0.90      0.89      0.89     31940\n",
      "weighted avg       0.90      0.90      0.90     31940\n",
      "\n",
      "acc:  0.8957733249843457\n",
      "pre:  0.8950466739550921\n",
      "rec:  0.8364508016347061\n",
      "ma F1:  0.8899871881553796\n",
      "mi F1:  0.8957733249843458\n",
      "we F1:  0.8951153247142394\n",
      "[[705 243]\n",
      " [  2   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.74      0.85       948\n",
      "           1       0.00      0.33      0.01         3\n",
      "\n",
      "    accuracy                           0.74       951\n",
      "   macro avg       0.50      0.54      0.43       951\n",
      "weighted avg       0.99      0.74      0.85       951\n",
      "\n",
      "acc:  0.7423764458464774\n",
      "pre:  0.004098360655737705\n",
      "rec:  0.3333333333333333\n",
      "ma F1:  0.4300304561077339\n",
      "mi F1:  0.7423764458464774\n",
      "we F1:  0.8493017065382915\n",
      "Loss:  0.0654803216457367\n",
      "Loss:  0.0750403106212616\n",
      "Loss:  0.05379209667444229\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.06959175318479538\n",
      "Loss:  0.053724244236946106\n",
      "Loss:  0.07194484770298004\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.06389756500720978\n",
      "Loss:  0.04692495986819267\n",
      "Loss:  0.07935439795255661\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.013256311416625977\n",
      "Eval Loss:  0.010317802429199219\n",
      "Eval Loss:  0.030209064483642578\n",
      "[[18224   992]\n",
      " [ 2313 10411]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     19216\n",
      "           1       0.91      0.82      0.86     12724\n",
      "\n",
      "    accuracy                           0.90     31940\n",
      "   macro avg       0.90      0.88      0.89     31940\n",
      "weighted avg       0.90      0.90      0.90     31940\n",
      "\n",
      "acc:  0.8965247338760175\n",
      "pre:  0.9130053494694379\n",
      "rec:  0.8182175416535681\n",
      "ma F1:  0.8899390789978743\n",
      "mi F1:  0.8965247338760175\n",
      "we F1:  0.8954112499269548\n",
      "[[853  95]\n",
      " [  2   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95       948\n",
      "           1       0.01      0.33      0.02         3\n",
      "\n",
      "    accuracy                           0.90       951\n",
      "   macro avg       0.50      0.62      0.48       951\n",
      "weighted avg       0.99      0.90      0.94       951\n",
      "\n",
      "acc:  0.8980021030494216\n",
      "pre:  0.010416666666666666\n",
      "rec:  0.3333333333333333\n",
      "ma F1:  0.48320139834282927\n",
      "mi F1:  0.8980021030494216\n",
      "we F1:  0.9432796447603523\n",
      "Loss:  0.03686904534697533\n",
      "Loss:  0.060222793370485306\n",
      "Loss:  0.08265393227338791\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.09693033248186111\n",
      "Loss:  0.06283044815063477\n",
      "Loss:  0.04461987316608429\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.06254836916923523\n",
      "Loss:  0.06630839407444\n",
      "Loss:  0.09000343084335327\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.008117914199829102\n",
      "Eval Loss:  0.008430957794189453\n",
      "Eval Loss:  0.026613473892211914\n",
      "[[18318   898]\n",
      " [ 2395 10329]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.92     19216\n",
      "           1       0.92      0.81      0.86     12724\n",
      "\n",
      "    accuracy                           0.90     31940\n",
      "   macro avg       0.90      0.88      0.89     31940\n",
      "weighted avg       0.90      0.90      0.90     31940\n",
      "\n",
      "acc:  0.8969004383218535\n",
      "pre:  0.9200142513583326\n",
      "rec:  0.81177302734989\n",
      "ma F1:  0.8900197865825008\n",
      "mi F1:  0.8969004383218535\n",
      "we F1:  0.895611123557326\n",
      "[[741 207]\n",
      " [  2   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.88       948\n",
      "           1       0.00      0.33      0.01         3\n",
      "\n",
      "    accuracy                           0.78       951\n",
      "   macro avg       0.50      0.56      0.44       951\n",
      "weighted avg       0.99      0.78      0.87       951\n",
      "\n",
      "acc:  0.7802313354363828\n",
      "pre:  0.004807692307692308\n",
      "rec:  0.3333333333333333\n",
      "ma F1:  0.44294158368390224\n",
      "mi F1:  0.7802313354363828\n",
      "we F1:  0.8736697126110565\n",
      "Loss:  0.03765534609556198\n",
      "Loss:  0.0429382398724556\n",
      "Loss:  0.07970871031284332\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.09116129577159882\n",
      "Loss:  0.02966737002134323\n",
      "Loss:  0.06819888949394226\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.09793814271688461\n",
      "Loss:  0.09518453478813171\n",
      "Loss:  0.07451803237199783\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.008445978164672852\n",
      "Eval Loss:  0.008121728897094727\n",
      "Eval Loss:  0.043505191802978516\n",
      "[[18578   638]\n",
      " [ 2748  9976]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.92     19216\n",
      "           1       0.94      0.78      0.85     12724\n",
      "\n",
      "    accuracy                           0.89     31940\n",
      "   macro avg       0.91      0.88      0.89     31940\n",
      "weighted avg       0.90      0.89      0.89     31940\n",
      "\n",
      "acc:  0.893988728866625\n",
      "pre:  0.9398907103825137\n",
      "rec:  0.7840301791889343\n",
      "ma F1:  0.8856982023329367\n",
      "mi F1:  0.893988728866625\n",
      "we F1:  0.8919551307515258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[873  75]\n",
      " [  1   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96       948\n",
      "           1       0.03      0.67      0.05         3\n",
      "\n",
      "    accuracy                           0.92       951\n",
      "   macro avg       0.51      0.79      0.50       951\n",
      "weighted avg       1.00      0.92      0.96       951\n",
      "\n",
      "acc:  0.9200841219768665\n",
      "pre:  0.025974025974025976\n",
      "rec:  0.6666666666666666\n",
      "ma F1:  0.5041437980241493\n",
      "mi F1:  0.9200841219768665\n",
      "we F1:  0.9554223354929412\n",
      "Loss:  0.05691468343138695\n",
      "Loss:  0.07200554013252258\n",
      "Loss:  0.08232395350933075\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.07206002622842789\n",
      "Loss:  0.0658772811293602\n",
      "Loss:  0.08741755783557892\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.08031268417835236\n",
      "Loss:  0.06321095675230026\n",
      "Loss:  0.08030157536268234\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.008317232131958008\n",
      "Eval Loss:  0.0065653324127197266\n",
      "Eval Loss:  0.04749870300292969\n",
      "[[18491   725]\n",
      " [ 2540 10184]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     19216\n",
      "           1       0.93      0.80      0.86     12724\n",
      "\n",
      "    accuracy                           0.90     31940\n",
      "   macro avg       0.91      0.88      0.89     31940\n",
      "weighted avg       0.90      0.90      0.90     31940\n",
      "\n",
      "acc:  0.897777082028804\n",
      "pre:  0.933541112842607\n",
      "rec:  0.8003772398616787\n",
      "ma F1:  0.8903608326315715\n",
      "mi F1:  0.897777082028804\n",
      "we F1:  0.8961567025108098\n",
      "[[821 127]\n",
      " [  1   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.87      0.93       948\n",
      "           1       0.02      0.67      0.03         3\n",
      "\n",
      "    accuracy                           0.87       951\n",
      "   macro avg       0.51      0.77      0.48       951\n",
      "weighted avg       1.00      0.87      0.92       951\n",
      "\n",
      "acc:  0.8654048370136698\n",
      "pre:  0.015503875968992248\n",
      "rec:  0.6666666666666666\n",
      "ma F1:  0.4789933230611197\n",
      "mi F1:  0.8654048370136698\n",
      "we F1:  0.9248527622371392\n",
      "Loss:  0.06166231632232666\n",
      "Loss:  0.04238172248005867\n",
      "Loss:  0.09114428609609604\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.07455411553382874\n",
      "Loss:  0.05629189312458038\n",
      "Loss:  0.06985701620578766\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.07759474217891693\n",
      "Loss:  0.08963677287101746\n",
      "Loss:  0.06269378215074539\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.008215904235839844\n",
      "Eval Loss:  0.007343292236328125\n",
      "Eval Loss:  0.028443336486816406\n",
      "[[18085  1131]\n",
      " [ 1718 11006]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93     19216\n",
      "           1       0.91      0.86      0.89     12724\n",
      "\n",
      "    accuracy                           0.91     31940\n",
      "   macro avg       0.91      0.90      0.91     31940\n",
      "weighted avg       0.91      0.91      0.91     31940\n",
      "\n",
      "acc:  0.9108015028177834\n",
      "pre:  0.9068138749279064\n",
      "rec:  0.864979566174159\n",
      "ma F1:  0.9061935647471219\n",
      "mi F1:  0.9108015028177834\n",
      "we F1:  0.9104194065262905\n",
      "[[752 196]\n",
      " [  1   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.79      0.88       948\n",
      "           1       0.01      0.67      0.02         3\n",
      "\n",
      "    accuracy                           0.79       951\n",
      "   macro avg       0.50      0.73      0.45       951\n",
      "weighted avg       1.00      0.79      0.88       951\n",
      "\n",
      "acc:  0.7928496319663512\n",
      "pre:  0.010101010101010102\n",
      "rec:  0.6666666666666666\n",
      "ma F1:  0.45204313529354984\n",
      "mi F1:  0.7928496319663512\n",
      "we F1:  0.8814593211012794\n",
      "Loss:  0.06728582829236984\n",
      "Loss:  0.07213243842124939\n",
      "Loss:  0.027220821008086205\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.08208416402339935\n",
      "Loss:  0.0534914992749691\n",
      "Loss:  0.08405839651823044\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.06215439736843109\n",
      "Loss:  0.05376999452710152\n",
      "Loss:  0.04691191390156746\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.0090179443359375\n",
      "Eval Loss:  0.010811090469360352\n",
      "Eval Loss:  0.019362330436706543\n",
      "[[18182  1034]\n",
      " [ 1730 10994]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19216\n",
      "           1       0.91      0.86      0.89     12724\n",
      "\n",
      "    accuracy                           0.91     31940\n",
      "   macro avg       0.91      0.91      0.91     31940\n",
      "weighted avg       0.91      0.91      0.91     31940\n",
      "\n",
      "acc:  0.9134627426424546\n",
      "pre:  0.9140339208513468\n",
      "rec:  0.8640364665199622\n",
      "ma F1:  0.9088461525245173\n",
      "mi F1:  0.9134627426424545\n",
      "we F1:  0.9130157272387144\n",
      "[[739 209]\n",
      " [  1   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.88       948\n",
      "           1       0.01      0.67      0.02         3\n",
      "\n",
      "    accuracy                           0.78       951\n",
      "   macro avg       0.50      0.72      0.45       951\n",
      "weighted avg       1.00      0.78      0.87       951\n",
      "\n",
      "acc:  0.7791798107255521\n",
      "pre:  0.009478672985781991\n",
      "rec:  0.6666666666666666\n",
      "ma F1:  0.447142002923329\n",
      "mi F1:  0.7791798107255521\n",
      "we F1:  0.8728892598746189\n",
      "Loss:  0.07830113917589188\n",
      "Loss:  0.0574914813041687\n",
      "Loss:  0.05291321501135826\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.04661291092634201\n",
      "Loss:  0.053604524582624435\n",
      "Loss:  0.03567805141210556\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.06740487366914749\n",
      "Loss:  0.04462553188204765\n",
      "Loss:  0.0630495697259903\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.005098819732666016\n",
      "Eval Loss:  0.006554126739501953\n",
      "Eval Loss:  0.06318128108978271\n",
      "[[18553   663]\n",
      " [ 2548 10176]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92     19216\n",
      "           1       0.94      0.80      0.86     12724\n",
      "\n",
      "    accuracy                           0.90     31940\n",
      "   macro avg       0.91      0.88      0.89     31940\n",
      "weighted avg       0.90      0.90      0.90     31940\n",
      "\n",
      "acc:  0.8994677520350658\n",
      "pre:  0.9388319955715472\n",
      "rec:  0.7997485067588809\n",
      "ma F1:  0.8920416034800454\n",
      "mi F1:  0.8994677520350658\n",
      "we F1:  0.8977967134739802\n",
      "[[890  58]\n",
      " [  2   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97       948\n",
      "           1       0.02      0.33      0.03         3\n",
      "\n",
      "    accuracy                           0.94       951\n",
      "   macro avg       0.51      0.64      0.50       951\n",
      "weighted avg       0.99      0.94      0.96       951\n",
      "\n",
      "acc:  0.9369085173501577\n",
      "pre:  0.01694915254237288\n",
      "rec:  0.3333333333333333\n",
      "ma F1:  0.4998246844319776\n",
      "mi F1:  0.9369085173501577\n",
      "we F1:  0.9644413572190196\n",
      "Loss:  0.05913667380809784\n",
      "Loss:  0.06545127183198929\n",
      "Loss:  0.05506465211510658\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.057416487485170364\n",
      "Loss:  0.03883625939488411\n",
      "Loss:  0.06829727441072464\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.06649471074342728\n",
      "Loss:  0.044439807534217834\n",
      "Loss:  0.05869143083691597\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.015555381774902344\n",
      "Eval Loss:  0.015078306198120117\n",
      "Eval Loss:  0.012534618377685547\n",
      "[[17993  1223]\n",
      " [ 1524 11200]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19216\n",
      "           1       0.90      0.88      0.89     12724\n",
      "\n",
      "    accuracy                           0.91     31940\n",
      "   macro avg       0.91      0.91      0.91     31940\n",
      "weighted avg       0.91      0.91      0.91     31940\n",
      "\n",
      "acc:  0.9139949906073889\n",
      "pre:  0.9015535699911454\n",
      "rec:  0.8802263439170073\n",
      "ma F1:  0.909920440535749\n",
      "mi F1:  0.9139949906073889\n",
      "we F1:  0.9138144459921138\n",
      "[[645 303]\n",
      " [  1   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.68      0.81       948\n",
      "           1       0.01      0.67      0.01         3\n",
      "\n",
      "    accuracy                           0.68       951\n",
      "   macro avg       0.50      0.67      0.41       951\n",
      "weighted avg       1.00      0.68      0.81       951\n",
      "\n",
      "acc:  0.6803364879074658\n",
      "pre:  0.006557377049180328\n",
      "rec:  0.6666666666666666\n",
      "ma F1:  0.4111359155273835\n",
      "mi F1:  0.6803364879074658\n",
      "we F1:  0.806772837610086\n",
      "Loss:  0.035659585148096085\n",
      "Loss:  0.0651792660355568\n",
      "Loss:  0.05633965879678726\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.05948062241077423\n",
      "Loss:  0.06628687679767609\n",
      "Loss:  0.03763722628355026\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.06322266161441803\n",
      "Loss:  0.06981125473976135\n",
      "Loss:  0.04287954792380333\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.006107807159423828\n",
      "Eval Loss:  0.012125015258789062\n",
      "Eval Loss:  0.034783124923706055\n",
      "[[18022  1194]\n",
      " [ 1408 11316]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     19216\n",
      "           1       0.90      0.89      0.90     12724\n",
      "\n",
      "    accuracy                           0.92     31940\n",
      "   macro avg       0.92      0.91      0.91     31940\n",
      "weighted avg       0.92      0.92      0.92     31940\n",
      "\n",
      "acc:  0.9185347526612399\n",
      "pre:  0.9045563549160671\n",
      "rec:  0.8893429739075762\n",
      "ma F1:  0.9147780326319022\n",
      "mi F1:  0.9185347526612399\n",
      "we F1:  0.9184148692603633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[858  90]\n",
      " [  1   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95       948\n",
      "           1       0.02      0.67      0.04         3\n",
      "\n",
      "    accuracy                           0.90       951\n",
      "   macro avg       0.51      0.79      0.50       951\n",
      "weighted avg       1.00      0.90      0.95       951\n",
      "\n",
      "acc:  0.9043112513144059\n",
      "pre:  0.021739130434782608\n",
      "rec:  0.6666666666666666\n",
      "ma F1:  0.49587277546383945\n",
      "mi F1:  0.9043112513144059\n",
      "we F1:  0.9467774012568129\n",
      "Loss:  0.06855130940675735\n",
      "Loss:  0.08035430312156677\n",
      "Loss:  0.04991302639245987\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.059546951204538345\n",
      "Loss:  0.04954046756029129\n",
      "Loss:  0.041586391627788544\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.08206973224878311\n",
      "Loss:  0.08527719974517822\n",
      "Loss:  0.054473958909511566\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.005179166793823242\n",
      "Eval Loss:  0.018643617630004883\n",
      "Eval Loss:  0.026093006134033203\n",
      "[[17812  1404]\n",
      " [ 1179 11545]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93     19216\n",
      "           1       0.89      0.91      0.90     12724\n",
      "\n",
      "    accuracy                           0.92     31940\n",
      "   macro avg       0.91      0.92      0.92     31940\n",
      "weighted avg       0.92      0.92      0.92     31940\n",
      "\n",
      "acc:  0.9191296180338134\n",
      "pre:  0.8915746389682601\n",
      "rec:  0.907340458975165\n",
      "ma F1:  0.9158915249832149\n",
      "mi F1:  0.9191296180338134\n",
      "we F1:  0.9192458731696654\n",
      "[[809 139]\n",
      " [  1   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92       948\n",
      "           1       0.01      0.67      0.03         3\n",
      "\n",
      "    accuracy                           0.85       951\n",
      "   macro avg       0.51      0.76      0.47       951\n",
      "weighted avg       1.00      0.85      0.92       951\n",
      "\n",
      "acc:  0.8527865404837014\n",
      "pre:  0.014184397163120567\n",
      "rec:  0.6666666666666666\n",
      "ma F1:  0.4740709139173303\n",
      "mi F1:  0.8527865404837014\n",
      "we F1:  0.9175483204913336\n",
      "Loss:  0.03790266439318657\n",
      "Loss:  0.04128272458910942\n",
      "Loss:  0.0497661791741848\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.08653173595666885\n",
      "Loss:  0.06199220195412636\n",
      "Loss:  0.06019168347120285\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.062436915934085846\n",
      "Loss:  0.033299002796411514\n",
      "Loss:  0.06477166712284088\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.006688117980957031\n",
      "Eval Loss:  0.029860258102416992\n",
      "Eval Loss:  0.041513681411743164\n",
      "[[18075  1141]\n",
      " [ 1508 11216]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19216\n",
      "           1       0.91      0.88      0.89     12724\n",
      "\n",
      "    accuracy                           0.92     31940\n",
      "   macro avg       0.92      0.91      0.91     31940\n",
      "weighted avg       0.92      0.92      0.92     31940\n",
      "\n",
      "acc:  0.9170632435817158\n",
      "pre:  0.9076636724123979\n",
      "rec:  0.8814838101226029\n",
      "ma F1:  0.9130536230632544\n",
      "mi F1:  0.9170632435817158\n",
      "we F1:  0.9168487034548349\n",
      "[[813 135]\n",
      " [  1   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.86      0.92       948\n",
      "           1       0.01      0.67      0.03         3\n",
      "\n",
      "    accuracy                           0.86       951\n",
      "   macro avg       0.51      0.76      0.48       951\n",
      "weighted avg       1.00      0.86      0.92       951\n",
      "\n",
      "acc:  0.8569926393270242\n",
      "pre:  0.014598540145985401\n",
      "rec:  0.6666666666666666\n",
      "ma F1:  0.47569320577266094\n",
      "mi F1:  0.8569926393270242\n",
      "we F1:  0.9199940253890275\n",
      "Loss:  0.03691214695572853\n",
      "Loss:  0.05709785223007202\n",
      "Loss:  0.031540293246507645\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.05004147067666054\n",
      "Loss:  0.07384444773197174\n",
      "Loss:  0.059624552726745605\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.06584794074296951\n",
      "Loss:  0.07849506288766861\n",
      "Loss:  0.08453363180160522\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAD4CAYAAAANbUbJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAvMklEQVR4nO3deXwU5f0H8M83ISHc9yVXOJRDBIQQQRFEETm0aL2r1XoUUPH2p8GraqtStbZVEaRorVZFqqhoOEUUAYWE+4YQAoQACfcRINfz+2Nnk9ndmd3Zc7K7n/frxYvd2Wdmnskm33nmOUUpBSIiim0JdmeAiIjCj8GeiCgOMNgTEcUBBnsiojjAYE9EFAdq2J0BI02bNlWpqal2Z4OIKGqsXLnyoFKqmdnn1TLYp6amIjs72+5sEBFFDRHZ5e1zVuMQEcUBBnsiojjAYE9EFAcY7ImI4gCDPRFRHGCwJyKKAwz2RERxgMGeyCaFx89g/sb9dmeD4gSDPZFNbpn6K8Z8vBJl5RV2Z4XiAIM9kU12HS62OwsURxjsiYjiAIM9EVEcYLAnIooDDPZERHGAwZ6IKA4w2BMRxQEGeyKiOMBgT0QUBxjsiYjiAIM9EVEcYLAnIooDDPZERHGAwZ6IKA4w2BMRxQEGeyKbKKXszgLFEQZ7IpuJiN1ZoDjAYE9EFAcsBXsRGS4iW0UkR0QyDD6/TUTWaf+WiUgv3Wd5IrJeRNaISHYoM09ERNbU8JVARBIBTAJwJYB8AFkiMksptUmXbCeAwUqpIyIyAsBUABfpPh+ilDoYwnwTEZEfrJTs0wHkKKVylVIlAKYDGK1PoJRappQ6or39FUCb0GaTiIiCYSXYtwawR/c+X9tm5h4Ac3TvFYD5IrJSRMaY7SQiY0QkW0Syi4qKLGSLiIis8lmNA8Coq4BhnzERGQJHsB+o23yJUqpARJoDWCAiW5RSiz0OqNRUOKp/kJaWxj5pREQhZKVknw+gre59GwAF7olEpCeAaQBGK6UOObcrpQq0/wsBfAVHtRAREUWQlWCfBeBcEekgIskAbgEwS59ARNoBmAng90qpbbrtdUSknvM1gGEANoQq80REZI3PahylVJmIjAcwD0AigA+UUhtFZJz2+RQAzwNoAuBdbYBImVIqDUALAF9p22oA+FQpNTcsV0JERKas1NlDKTUbwGy3bVN0r+8FcK/BfrkAerlvJyKiyOIIWiKiOMBgT0QUBxjsiYjiAIM9xawl2w/i5Nkyu7NBVC0w2FNM2n/sDG5/fzkemb7a7qyY4shBiiQGe4pJp0vLAQA5hSdtzolvnM2eIoHBnsgmXKiKIonBnshmXKiKIoHBPgTyDp7CN2v22p0NMsDCM5GDpRG05N1V/1iMs2UVGN3b28zPFEksLBO5Ysk+BM6WVdidBSIirxjsiYjiAIM9EVEcYLCnmMbujUQODPYUk9idkcgVgz0RURxgsCciigMM9kREcYDBnmKa4hhaIgAM9hSjhGNoiVww2FNAHpm+GoNfX2R3NojIIs6NQwH5ek2B3VkgIj+wZE8xjYOqiBwY7CkmcVAVkSsGeyKiOMBgT0QUBywFexEZLiJbRSRHRDIMPr9NRNZp/5aJSC+r+xIRUfj5DPYikghgEoARALoDuFVEursl2wlgsFKqJ4A/A5jqx75EYcMGWiIHKyX7dAA5SqlcpVQJgOkARusTKKWWKaWOaG9/BdDG6r5ERBR+VoJ9awB7dO/ztW1m7gEwx999RWSMiGSLSHZRUZGFbBERkVVWgr1RJzbDh2MRGQJHsH/K332VUlOVUmlKqbRmzZpZyBYREVllZQRtPoC2uvdtAHgMnxSRngCmARihlDrkz75ERBReVkr2WQDOFZEOIpIM4BYAs/QJRKQdgJkAfq+U2ubPvkREFH4+S/ZKqTIRGQ9gHoBEAB8opTaKyDjt8ykAngfQBMC74hi6WKZVyRjuG6ZrIarEEbRErixNhKaUmg1gttu2KbrX9wK41+q+REQUWRxBS0QUBxjsKaYpjqoiAhBjwf62ab/izQXbfCekmCestCdyEVPBfmnOIby1cLvd2SAiG23edxypGZnYsv+43VmpVmIq2Eer9fnHMGHmOlY5EIXAnA37AQBztf/JgcG+Grjz3yvw2Yo9OHyqxO6sEFGMYrCnmBYNz0r69oXdh4rx6pzNfMqjkGOwp5gUrc2zY/+7Eu/9lIvthSftzgrFmJgM9ll5h+3OAlFAysor7M4CxaiYDPY3TvnF7iwQEVUrMRnsoxVraYkoXBjsq4ForV+OBmznJHKI2WC/69Apu7NANuIAWiJXMRvsB7/+o91ZICKqNmIq2P/zlt4u77cdOGFPRoiIqpmYCvaje7uuZT7s74uxeFv0LF7O+mUiCpeYCvZG3pi/1e4s+MT6ZSIKt5gL9r9MuNzl/br8Y6Zp1+w5ivkbOVkSEcW+mAv2jWone2xbs+eoYe+cayctxZiPV0YiW0REtrK0Bm00SUlK9Nh27aSlAIC8iaMinR0iomoh5kr2RHqK45KJAMRZsP88a7fdWfCKgSl0hOOSiVzEZLB/cngXw+1PfbkeAFBaXoF9x05HMks+VK/AdP8nKzFzVb7d2SCiEIq5OnsAqJeS5PXzCTPX44uVDGZmZq/fj9nr9+O3fdrYnRUiCpGYLNl3aFLH6+c/bCmMUE6IiKqHmAz2A89t6vVz9yXfUjMysakg+JXofS0ld6a0HEu2Hwz6PGQdRyXHL373rmIy2Adi5Fs/I7eoaim4s2XlKCkL7apBf/pmI25/f7n5nD385QwZjkqOX/zqjVkK9iIyXES2ikiOiGQYfN5VRH4RkbMi8oTbZ3kisl5E1ohIdqgyHqi1e47iSHGp4WfXvL2k8nWXZ+fikr/+ENJz52g3k+OnXc/PwFTlzg9WYOKcLXZngyjm+Az2IpIIYBKAEQC6A7hVRLq7JTsM4CEAb5gcZohSqrdSKi2YzIbCaG2AlZFTJeUu74tOnK18vWhrIc6Wlbvv4hdf1TyR9M2avchct8/ubHj4aVsRpvy0w+5sEMUcKyX7dAA5SqlcpVQJgOkARusTKKUKlVJZAIyLzFFuzZ6juOvfWXg5c3NIjlcdSvIPT1+DBz5dZXc2wq763F6J7GUl2LcGsEf3Pl/bZpUCMF9EVorIGLNEIjJGRLJFJLuoKPhpiX984rKgj+F0pLgEALDrUHHIjknhVQ3upwEJ5OY0a20BDp8qCXleKLZYCfZGfzf+/E5eopTqA0c10AMiMsgokVJqqlIqTSmV1qxZMz8Obyy1qfful+Hgq5YmWkuZh0+V4ODJs74ThsGZ0nKs3n3ElnPbyerNat+x03jos9UY919O6EfeWQn2+QDa6t63AVBg9QRKqQLt/0IAX8FRLRRdAozSFRUK5RVGOxv/KVs5zW/fXYq/L9gWWIYC1OfPC5D2l+8jek6n577egOveXYY9h/lUZcTZY2z/sTM254SqOyvBPgvAuSLSQUSSAdwCYJaVg4tIHRGp53wNYBiADYFm1m7+1rX/8aNsdHp6NpZsP4jF24pcSv76xlp/Drtq91H8c+F2AMDMVfn4Zs1e/zIVZdbvdaxHcOJMmc05IYpuPqdLUEqVich4APMAJAL4QCm1UUTGaZ9PEZGWALIB1AdQISKPwNFzpymAr8QRJWsA+FQpNTcsVxImqRmZuHNAe4/tX63Ox+VdW6BBraqpGe7+TxY+vKvqwWWhNlL39veXu+x7/eRlaF6vJlY8MzSovD02Yy0Az+UYnU6cKUWNhATUSvac9jleVKMOUGHFSfTIF0tz4yilZgOY7bZtiu71fjiqd9wdB9ArmAxG2ifLd2H6ij0u275Z66i1cgaObQdO4NHP12Jot+aYdme/ynQ/brXesFx4wnsdeFl5BZ79egMeGNIZbRvX1s7v3x/0BS/MR6PaSVj9/DC/9osJ0dpC6yfO7klWxeREaMF45ivftUxnSh397fcfD009qT6EL889hO2FJ9G5eV1Mz9qD3IOnMGPsAADApyv8n6LZbAAZxQaW6MkqTpfgBxFg96FibD9w0jRNaXkF3vlhe+UNwQr94K2bp/6KZ7+uuuHkHTxVWaJftCX4LqkUm1jC919xSRn6v7IQy3LiY74qBns/KAUMen0RHv+fo678xJkybNjruqD59Kw9eGP+Nry7KMfn8fYcLsY8HwueF544iw+W5gWc51hxtJj9yL2J1RL+rkOnfP6NBGr7gZPYf/wMJs6Nj+k5GOwtcFaV/7TNtWS961AxrtbNpwMAZ7QpF4pLfJfsL31tEcaaLHi+YufhyterorCf+emScpy28DOw6nfTlvtOZCg2g6BTrJfoh775k+nfCPknpoP9U8O7RvycL892TKlg2L3eh890dfJvGvalDzxwHS0uQWl5aGfx9Kbb83NxwQvzInY+d/4EwbNl5fjr3C0oLgmse+fq3Uewctdh3wmDtPPgKY+puGO1RO9UWh7b1xdJMR3sxw7qaNu5l+3wvx7wH99vD0NOHHq/tADjDebCOX4mfA24ZYHc8YK0YudhdJiQ6df0AZ/8uhuTf9yBdxcFNgHbde8uw/WTfwloX38MeeNHjHzrZ8PPYr2E78136wqQmpGJk2c5FsObmA72CQn2/QFEshTty+Z9jtLgvI0HPD5bvftoUMf+bMVu5B08BQAoOGr/ur5TF++AUv5VfZVo31V1+s78paBQWl6BV2dvxrHT8dUD6+2Fjvax/COBjbKOl7EYMR3sQ6W6//Ho+98XnjiDvW5Bd8Q/jUuDAILuiTBh5npc++5SLN5WhIsn/oA566vXtMnV7Q/5+00HQjrXj75E/+3aAry3OBd/jZMGx2BVh9lnIynmg/29AzvYnYWg+fqd/M+yPGzYewwT52xB+ssLcclE64uuvLc4N7jMAThaXIqNWl3ymvyjLp8FOmdLakZmQFVhzuDuz99xqG4If/wo2+eEcfd+lI3r3l0WmhO6KdPqt0tDvMJadTF3w37cHnBDPcV8sB9/eWdbzhuO+mqzoPT95kJc/fYSl0U/UjMyDdOGYq1df5RVeAaeL1bmIzUj0+dYhE+Wuw4iM55UztgzX/s/BdPxM6WoCOJ7W7DpAKb8GPqFV8orFDYWHDP8LFYaaI8Wl+DLlfle04z770osiZM+8eEQ88G+Ye1kW84b6rnvV+8+UjnXTjB2FJkPCIuE79YV4AltnMLBk2eRmpFp2HBspNPTs3HM4ohgf24MTp+t2FM5yVy4/bLjkOW0kxblYNRbS3wnDKPUjEw8+cXasB3/4elr8Pj/1iKn0L/fz9LyCmw1WdM5Nm6DoRPzwT5WfL3adXbLP30T2slDy8orUHj8jEsQeueH7UGVdI386+edHtu+82N5xKKT3quF3HN76FSJz+mR9aXjcA3gcXfrv361nHZdvnGpHohsL5wZ2d5L3sE4oE09UuJnFdQ/DXqw+VsXH+zTUeHxMyg84fp7+fP2Iny3zvJM8BHBYB8Ffs097NFv/z+/7ArpOfq9/D3SX1noEoTemL8Ny9xKoI9MX43JBlUVZeHoyRKi+8ymfaGruiopq8BNU37Byl2hHej28/YipGZkGpZs3YPXR7/kGXY1DFWVTml5BSYtyrE85cfeo6cx7O8/oTAEc0X5ew17dD1w/G17CdWNMv2VhUh/eaHLtt+/vwLjP10dkuOHCoN9FDh48iw+/jU0wf3Bz1bj11zXAF5WXmE6YZq+zv0P/16Br9cUGPb2+FsAC6q8MGuj3/v4YmVm0Nyik5WBbOWuI9i637gawHDfgyexIu8wnp65PuA8GvnkV+uT3L2/xPF0dFA3p5L+sn2VbI8Wl+Dm937BvmPGXWVnZO/B6/O24p0ffE/5AThuPtsOnMSXqwJfW0GioGtMWXkF1u45anc2AsZgH4cyvlzn8v61eVst7efPFM6mdFHp+81VbRCrdx/BD1s8xwFs8RKID5486zJ4SimF3SZVNvlHipGakYnPVuzG5X/7CY9rawFcP3kZvllToDuGtctwf2wPVrmXE7uHwUB7Dx0rLsUrszdjRvYeLN95GO/9ZNwT67QfU35UR75+PoGOlH5zwTaMnrTUYz6saMEpjuNQnlvj8dQQdL+s5OMPbYNJbyCj7oiZBn329X/IzqUS8yaOAuAo8e4oOuWxj8CxBgHgKLUCCLhXh/PRPxJTR89Zvw/3fbIKae0bGedFjF+7O3a6FL1enF/5vmebBqHKoqFTZ8uwJOcgrjq/pd/7hnJchNGP5KvV+Xj087VY8KjhUtheObsXF9m0HnOwWLKPAYGWVKx43WKp38wny3e7lL4D6SVj5JSuvjqn0BHIvzDpuqevInC+qggwqsxaW1VVUVGh8NiMNW7nCuiwhv7zSx6AqhuVk7/12kVuC+V4a/ANhYyZ6zH245Ue+S4pq6j8/vccLsY63ZiMQH9s/u7nfJrUPzH6+lX4cOlOpGZkWvo7e2zGGrwR5N9MuDDYx4BVQU554M1GP/vlu1ejTP5xBx6e7mioCkWgf+TzNXjws9Uuc8QMfXMxAO9VPs4/6ARnNPYjKw98sgqfZznq1Gevr+qts/foacx0q6d2DxyHT5XgxW83+rW+gZNZA2LVwDHx2AZY7zXjfmNatLUQqRmZOO7ver9u1+z8HTjl1oB83rNzKgdFXfraIvzmnaWVnzlLy5Ecre7txvzCrI2VPdOmaW0kh076nm9p5qq9eMfC9OZ2YLCnkJqzYT+WulWRFJ04i8LjZ/DkF+tM9rJuY8FxfLu2wGMcw83vmU9Epv+bztZ60RSbBN+tB054zDiauX4fnvpyvcexfFmy/SD6/HkB/r00D58u994A636T8BaIKoO9uKYz68JqNkXUv5fmuTx1TdIaZLf50WCt557nE2fKKp+6nH7JNR5f4Hz6+F/2HsPPs/KMZxXV/9iUcozYdr/JmPGW7sNleR7dY53nemHWRp8jpaujuAj28x7xv36OAufeV33L/hNIf2UhvlwVvn7ay3d6n2LYPZh6e8p4SzewyltJ86yPPuH6doEDJ86YjoI1Y6VKyHld+UdO4+ftVeerqFD4y3ebsOdwsdeeLvrvyugnUlxSht+8Y21Al/vP+I4PVlQ+denpB/b9vL0Iz+lGO5vl9cYp1mYV7f/qwsqSuJEduq6tE7QeVe7VTXrfrSvACbcnnV2HivHSt5ss5cebo8Ullm9MoRAXwb5Ly3p2ZyGufBTiMQDB+vv3xt1CF2727P2jtyznoEvDpru5GzxL0mZx9b2fcv0eBWt2LOdEd95uBhsKjmHakp148LPVftdrO4+7vfAEVu464ruO388T6GdH/f37K4LqVuzvtemr+nK12Vq9zZk//tPVlTd8fbdeZ5tPMFWTvV9agIv9mMcqWHER7Cm+bSw4bjjXea5Bzx09X9MkGzXsBd6bxHXHoW8uxtKcQwafWDyatlOFUpYbjd3HKOifFJwKj5/BA5+ucm2s1O1WUaE8+qLr52wKBX231z1Hqm4cZg30oXLgeFXVjfMJ5DkLI9lfnb0ZmVoVW2l5BT5bsbvyJhHJNoq4CfY/PnGZ3VkgG71lMOfN/E3mUyM88MkqTM9yrT9e5DY3kdlAstSMTKRmZCLbpJ7ZX+7VCE4igg+WmldZVKbzUv7Vf+KM2fqbg/vN67V5W5G5bp9h+4AIsMDgaWniHP+nXJ6Rtcd03WH9aFX9SGYrP4tgnNa185SWVWBH0UmXG4zZdOHvLc7FA9r8T9N+3okJM9dXdgGOpLjpZ9+yQYrdWSAbOR/Z9bLyzEvuRn38v1nje4ToTt15skM8pYI7gaMh0Yg+Rv8l03f98umS8sqFbIKZRiCQXkfuth04gSe/XIfZG5qZpnnx243o1qp+0OcysmCT9+o9AJi7cT/murVNTfjK96jqI9oN7LgNa2TETbAnCtbXawpQw8fqZ3mHvFcNhZKV6hkBMN9C8HIZT6Ev2bulC7SqJNPiZHcC4Gypo+Hb24jtfy/N83mssooKHC0usTzz7Y9bC/Hit5tcbtj+OGKwFObr86rPQjJxU40TBVNvUBSwY11dM0Z16oEye2q584MVxjvofgzO0qq39gr98SM1U+ekRTvQ+6UFhp8dMug6+cjnawIO9AAMxydMMlnX2I7fIkvBXkSGi8hWEckRkQyDz7uKyC8iclZEnvBnX6JYctDCwBt3T/xvrcs8QVb9uNV8HysTwgFVhaBDulKplVD8/eYDWLSlELsPFVcO4tq07zh+MFlzwWzO+UgwWpvW/Z6dW3QSR8M8BcaMrD2m38uB42fw9sLtlr+3QPisxhGRRACTAFwJIB9AlojMUkrpKwIPA3gIwLUB7EsUMwLpXRFo1Yh+NK8752CptT66Tb79Qw6Gdmvhss3KDJTzNx3wqB76dq35/O2WF/MJQ6F/4F8XVc6fZOb/QjDgz5cnv1yHBrWSPLYXnjiDi15xNDoP6docPVqHZ+4iKyX7dAA5SqlcpVQJgOkARusTKKUKlVJZANx/033uS0Shl2FxCub8I6fx1JfhD3RWRaqKx65q3ao++1XbnvmqqvtmqOaOMmKlgbY1AH0/oXwAF1k8fjD7ElGA3Cc/8+a0Ww+asyHoUePN7e+bLxputGZxKAx6bRFqJyeG5diBWL6zatqIUt3CP+G8CVkJ9kant3r7sbyviIwBMAYA2rVrZ/Hw1iWyhZbIUNZO1y6idjZCV6jwBDyzdQ6cvE2ZEA76nkb61+F8srFSjZMPoK3ufRsAVhdXtLyvUmqqUipNKZXWrJl5/9pA1UhMwKTf9Qn5cYmiXYnbkpJ2Fou+XVsQkT7oG/a6zuZqNnAt0sJZJrUS7LMAnCsiHUQkGcAtAGZZPH4w+4bcqJ6t7Do1UdSwe4nA300zr+YJlXBOyldd+Qz2SqkyAOMBzAOwGcAMpdRGERknIuMAQERaikg+gMcAPCsi+SJS32zfcF2MFbf0a+s7EVEcY41nbJJw9usMVFpamsrOzg7b8VMzMsN2bKJol5yY4FG1Q5Hx+g09cWNaYAVSEVmplEoz+zxuRtASkTUM9PYJZ3//uAz2fU0WcCYiilVxGew/H9MfizjlMRHFkbgM9jUSE9ChaR27s0FEFDFxGeyJiOINgz0RURyI62D/8T3pdmeBiCgi4jrYd25e1+4sEBFFRFwH+1YNatmdBSKiiIjrYE9EFC8Y7ImI4kDcB/uFjw/Gs6O62Z0NIqKwivtg36lZXVx7YWu7s0FEFFZxH+wBoEmdZLuzQEQUVgz2sH+xBiKicGOwJyKKAwz2RERxgMFeM6JHS7uzQEQUNgz2msm397U7C0REYcNgT0QUBxjsdW7o28buLBARhQWDvc5r1/fEiqevQNO67HdPRLGFwV4nIUHQvH4KAPa7J6LYwmDvB/bYIaJoxWBPRBQHGOyJiOIAg72BQKfKade4dmgzQkQUIpaCvYgMF5GtIpIjIhkGn4uIvKV9vk5E+ug+yxOR9SKyRkSyQ5n5cLlvcCfD7f1SG3vdLyXJ8eN85boLQp4nIqJg+Az2IpIIYBKAEQC6A7hVRLq7JRsB4Fzt3xgAk90+H6KU6q2USgs+y+F398AOyJs4qvJ9qwYpAIDhugbat2+90HT/Pu0bhi1vRESBsFKyTweQo5TKVUqVAJgOYLRbmtEAPlIOvwJoKCKtQpxX23x4VzruGNC+MugDwDW9zvFIJwZdNv/vqi6Gx0xr3yh0GSQi8sFKsG8NYI/ufb62zWoaBWC+iKwUkTFmJxGRMSKSLSLZRUVFFrIVOV1a1sNLo3v4nPf+oSvOBQC0aVRVd39jmuuo3G/HD8TaPw3DS6N7VG5rUb+mX/n51x1R8YBERNVIDQtpjCKc8iPNJUqpAhFpDmCBiGxRSi32SKzUVABTASAtLc39+NXShe0aomGtJPym9zk4U1qBUT1bYVTPUabpp9zeFxe0aQAAaFArCQ1rJ+FocSnGDOqEP3+3yfJ5L+7UJOi8E1F8sRLs8wG01b1vA6DAahqllPP/QhH5Co5qIY9gH42+uv8S089qJSXidGm5y7bhboOyMh+6FJsKjmNot+a47aJ26PrcXJ/nHNKlGerUtPK1ERFVsVKNkwXgXBHpICLJAG4BMMstzSwAd2i9cvoDOKaU2icidUSkHgCISB0AwwBsCGH+q606NRMBOOrxc18ZiR2vjPRI07phLVzZvQVEBClJiabHeuGa7kiu4fiqvKUjIjLjM9grpcoAjAcwD8BmADOUUhtFZJyIjNOSzQaQCyAHwL8A3K9tbwFgiYisBbACQKZSynfxNcYkJAgSEwKfb6dWciJmjB0AAOjRuoHl/fImjkKfdg0DPi8RxQ5L9QFKqdlwBHT9tim61wrAAwb75QLoFWQeo5IKYauDQNC7bUN89+BAdG9V3zDNGzf2wovfbsTNaW0xbcnOyu2BVvkkCFARFS0nRGQFR9CGWaCjcbu1qo8OTeu4bOvRugESTJ4QbujbButfuAqPDTvPZfubN/X2+9wt66cg91XzhmYiij4M9tVIxoiula/nPHwp+jr74hvE95o1jL+6mjUcdfrOGTqb1auJ8UM6Y4Lu2M+M7Ga473/vucjlfbRO/3CObjwEETkw2IdJIDUg40ymaTDSv6Nx98vEBEHexFEua+o+cVUXjB3cCTPvvxgPXd4ZV/cyHu/WsHYSgKqnkbmPXIqVzw61nKdQqhVEQ/S0O/uFMCdEsYHBPsyCWQbl6p6OoNzXYLTt5Nv7YGi35n4dr0+7RnhsWBe0alDLUvrayTXQpG5NzLz/4sptHd2qlgCgd9uGLu/NxgEkJgiu7e058tjItRe6j9uzrvs5ru0aZu0cRPGEwb6aGTu4I969zTGP3GVdmiNv4ih0albXI13t5BoYqz0JJJtU6VihL0Gf09BxE+jasp5Lmj7tqm42l3XxvMF8MW4Anh3VDW0aOfb/5N6L8OMTl+Gju9Nd0u14ZSQ66q4lb+IovPpbx6RxDWsnIa19I/xWC/J92zdC3sRReGn0+X5dzwCDJ57ayVXXWJdjFChOMdiHyeTb+uCKrs3RqLZ/69lOGNENIy+wNq1QnWRH4DIqbVu14pkrKl83rpOMGWMH4O3f9TFNrwwqqGokJuDeSztizsOXYslTQyAiSG1aB4POa+aR1vmk47yhON9f1b0lvrjvYjTR1v+tq41TaFrXfCqJj+9J99j2kdu2t2+9EJ2bV91gaiV7Vg9d2b2F6TmIYgWDfZhc1LEJ3v9DP9PeM3a76vwW6NOuIeqlJGHJU0Pw9QOO0cDpHRp7Lf0meOleVC8lyWVeIG+uMKmCenxYF7x8XQ9cdb7vJSB7tm7o8n71c1ciKdH1V9p9wjqjxul/3ZGGD/7gmG/oks5N8Nkf+3s979xHLkWzev7NZ+SL/unj6ZFdvaQkCgyDfRQzKmVb9d7v0zBTm+6hTaPaHvXuZpzVJI8OPc9HSlcvXOOYFdvsXuG8lpSkRNx2UXuPSec6NK2DD+9ya3gVIFc3MrlRnaqnqFvT22KK1kjtWETe8bRl1hbgnLE0KTEBA3zMPdS1ZX2PdoAv7xvgkW7eI4O8HgcA+ndsjLyJo9CiflUPotG9A2+vIDLDYB8DfM3GGUpDu7dA5kMD8dAVnf3a786LUwFU5dX5hOAr65d3bY6RF7TEx/ek47IuzfHU8KpSr4hjdPKCRwdh7iOXuuz36m97Vs5F9ODlnfHPW3pXvn/Ly1oEVgfDuee7Qa0kjzRdWtbz2jg89fd98cm9/bXzOk784V39XAJ/KLVuaNwwr1+7wR/678LdDX3bmH4WTlNuN6+CjHcM9n66Z2AHl3nt49H55zTw+wbjTH/XJam4vX+7ysZlX1KSEvHubX0rq4fuu6xqP+cN49wW9dC1pXlQTUpMwOjerSvz8BuDtQi8dZsabdCDqEsL10bszs3r4aO70/Fntwbl6WP747Xrexoet1Pzuh7TaLRv4mh/+fnJIfjHzb0xdnBHj/3cxxG8dn1PjNJ6bukX1enbvhGev7pqnaGlGZe7jLcI1rjBHdGkjnGb1HAL1XDhMKy7PeeNBgz2fnru6u74ZcIVvhNGQCinZPDl6ZFdcWt6W98JfaidXAN/ufaCynaB+imOEnETLw2xnsdwTjIXOqlakL2si2ejsnOdAr0nDBalGXReM9zcr53LtvopSbipn++f2+PDHMdrqZXq2zaujWsvbI0JIzzbGNwb0G/q1xaTftcHeRNHubRRfHnfxRjd+xw0rJ2E7x4cCAC4sJ1nN95AiQgevdK4Os9sLihnPvyRN3GU5acPf9vIHrrc8YSqb8S3y99v7lXZdhYO7IcWAyJRiTNmkPUBX/4Y3qMlXru+J0ZfaK3/PQC8NLoH/vzdppDMAOosoXdoWgern7uycmCZXnJiAr4dPxBZeYcre0q5NwRXpvWjG6z+Zn1Nr3MMVz8DHO0BM1ftRf1aSR5VJ2bjFuqnOP60m9StiTXPD6vcnt7B+zrKZv5wcSo+XJbnsd1sJLfZL2Vbtwb8Qec1w+Jt5osVDe3m2VPqgz+k4e4PXZeznnn/xSgpqwAA/OXaHnj2a8fkuvdf1gnlFQpjB3eCUgordh7GfZ+sqtyvtdZdeEDHJvjuwYE+pxlPSUrAmdIKr2n81aJ+TYzu3RrXXRjeqi+W7KOYs4Tr/IW1Q9vGwZ1bRHBTv7aV0zxYcUPfNlj7p2FBzSTq9IWuYbVRnWSX6qmmdZPxj5t7o23j2rigTQPcPbADWuqqUPTdVq0KpOTWt31jvHzdBS6B3lkVZTQL6qrnrsQyL0+fv7uoncc2s3aMod1a4B839zZte7jOpMG7s248RY/WVfs2cLuZPuyj7efvN3vOo3iBWy8swDEWxDmq/Pb+7XH3JR0AADUSBBNGdkPjOsloUrcmOjRz7aZ83YVtcPclHfDEVV2QkpSIfqmuTz7uTxR/vNSzWg0wHvgIAM+Ocn0y++OlHZD1TNWo9JSkBCx8/DI8bTKFSSgx2Hvxtxt7ma4hWx10bFYXk2/rg7/dZM/EogsfH4zvxl/qO2E19OjQ8/DU8K6ol+JZkgeATS9dhaUZl3sdydu8nv9tN73bNkTHyoATeD2cczyCkcZ1kr12nx0/pCrAOudDMmzHADDtzjSvP4MaiQmV7QX369pT9E063z14KZ4e2RXntXCtKlmacTn6tm+M6WPMu7oaPb01rZuMJ4d3wY9PXIZNL12FJU8N8UhTT3uycW9Jd2+kTq6RgOev6V7ZwH7PwA4ex/pKN4LcrOrUbPCf82fjVKEc81VNdg6cPK95xAb6sRrHi+tt6lHgjxEWB2CFg9HI3mjx8FDPeni92smh/dNo17g2dh8uBuAYUJZbdAq1gjiH0eL2/mpZPwUDz20a9HG6tKiHTOzDkK7N8c2aAuw9etojzZhBnTyqAp2Bt3/HJrjtonbo0boBhnRpjsXbi3BTmvd2jvsvq7phGX1Xzpjs/lOql5KEvImjkJqRaXjcizt7/jz07RxmU4Y7A7b7CnXuT5/OJ4cBnZqgWb2aeGCIf73agsGSPYWV83E63s0afwkWPOrod//Gjb3w6R8vMu0KGW6+OlItzbjcY5vRyGOnB4Z0xpf3DUC/1MYYrzV4Nqlj3uD+0ujzMf9R1zEIL193AW5Nb4eWDVJ8BnpLtCK4v72SfXV6MCr5/+fudLRvUgfT7khDlm7iwKdHdnV5+lv57FAM7+EonDWsnYysZ4ZWrkkdCSzZU1g9f013PH9Nd98JY1zD2sloqE2dUTu5Bi7uFHyJOlxaN6yFewZ2wPu6RXBGXtAKe4+exrSfd+LgybMu6RMTBH3bOxp+b01vh1vTPdsE9O4YkBpU/qx0+60q2RunfXpkV79WfXNKrpEAEdebwmBtWpCh2rQbM++/GM3q1kRbtynC/elxFg4M9kTk4bmru+M5XR/9xATBuMGdMKBjE4yetNSwl0y43ZTWxrSNxd2t6e0wZ8N+3GzS7TWY3mW+Sv993Lq3rnruyoDPFUoM9kRRKBSDpgOZbqNX24YBj7gN1ms3WO+IcE7DWvj+scH+n8TCjyS1SW3kHXK0v1hZd6GxycCzSGOwJ4pikRxYRw4zxg7AuvxjaN+kdmXVXDRgsCeKQqEYSBeKHj2xqn5KDax74SrDz5rXT8HQ7tE3ZQqDPVGccq/GubFvG9NRvPGiZpKjg+Konq4/h+ev7h7w6OPqgsGeKIoFUu9uVqJ//UZ7BudVJylJiVj93JVVg7I0dxt0uYw2DPZEUSiYBtr6tRx/9jeHoj97DGpUTRpUQ43BnijEWjVIwb5jZyJyrkAaaGsn18D2l0egRjVdRY3Cg8GeKMQWPDYYZ3RD5sMh2AVrzGbtpNjFYE8UYnVr1gj75Fbn6aZmrq5mjB2AY6dL7c4GaSzd3kVkuIhsFZEcEckw+FxE5C3t83Ui0sfqvkTkv+v7tEbmQwMxzKYVoaxI79AYV3aP/EhbMuaz+CEiiQAmAbgSQD6ALBGZpZTapEs2AsC52r+LAEwGcJHFfYmi1od39cOps+GtsjEiIjj/nMhNokXRz8qzZjqAHKVULgCIyHQAowHoA/ZoAB8px6rJv4pIQxFpBSDVwr5EUeuyLs3tzgKRJVaqcVoD2KN7n69ts5LGyr4AABEZIyLZIpJdVGS+TBkREfnPSrA3avZ37/BllsbKvo6NSk1VSqUppdKaNfNc9JmIiAJnpRonH4B+9EUbAAUW0yRb2JeIiMLMSsk+C8C5ItJBRJIB3AJglluaWQDu0Hrl9AdwTCm1z+K+REQUZj5L9kqpMhEZD2AegEQAHyilNorIOO3zKQBmAxgJIAdAMYC7vO0blishIiJToqrhhNhpaWkqOzvb7mwQEUUNEVmplEoz+5xjpomI4gCDPRFRHKiW1TgiUgRgV4C7NwVwMITZsVOsXEusXAfAa6mOYuU6gOCupb1SyrTferUM9sEQkWxv9VbRJFauJVauA+C1VEexch1AeK+F1ThERHGAwZ6IKA7EYrCfancGQihWriVWrgPgtVRHsXIdQBivJebq7ImIyFMsluyJiMgNgz0RURyImWAfLcsfikieiKwXkTUikq1taywiC0Rku/Z/I136Cdo1bRWRq3Tb+2rHydGWhAxuBWrf+f5ARApFZINuW8jyLSI1ReRzbftyEUmN8LW8ICJ7te9ljYiMjJJraSsii0Rks4hsFJGHte1R9d14uY6o+15EJEVEVojIWu1aXtS22/udKKWi/h8ck6ztANARjmmV1wLobne+TPKaB6Cp27bXAGRorzMA/FV73V27lpoAOmjXmKh9tgLAADjWDJgDYESY8z0IQB8AG8KRbwD3A5iivb4FwOcRvpYXADxhkLa6X0srAH201/UAbNPyHFXfjZfriLrvRTtvXe11EoDlAPrb/Z2ELThE8p/2w5inez8BwAS782WS1zx4BvutAFppr1sB2Gp0HXDMHjpAS7NFt/1WAO9FIO+pcA2QIcu3M432ugYcowglgtdiFlSq/bW45fcbONZ8jtrvxu06ovp7AVAbwCo41ua29TuJlWocy8sfVgMKwHwRWSkiY7RtLZRj/n9o/zsXNvW23GO+wfZIC2W+K/dRSpUBOAagSdhybmy8iKzTqnmcj9hRcy3ao/yFcJQko/a7cbsOIAq/FxFJFJE1AAoBLFBK2f6dxEqwt7z8YTVwiVKqD4ARAB4QkUFe0ga93KNNAsm33dc0GUAnAL0B7APwN217VFyLiNQF8CWAR5RSx70lNdhWba7H4Dqi8ntRSpUrpXrDsTpfuoj08JI8ItcSK8HeytKJ1YJSqkD7vxDAVwDSARwQkVYAoP1fqCU3u6587bX79kgLZb4r9xGRGgAaADgctpy7UUod0P5AKwD8C47vxSVfmmp3LSKSBEeA/EQpNVPbHHXfjdF1RPP3AgBKqaMAfgQwHDZ/J7ES7KNi+UMRqSMi9ZyvAQwDsAGOvN6pJbsTjvpKaNtv0VreOwA4F8AK7RHwhIj011rn79DtE0mhzLf+WDcA+EFpFZKR4Pwj1FwHx/fizFe1vRbt3O8D2KyUelP3UVR9N2bXEY3fi4g0E5GG2utaAIYC2AK7v5NwNk5E8h8cyyJug6Ml+xm782OSx45wtLqvBbDRmU846toWAtiu/d9Yt88z2jVtha7HDYA0OH7xdwB4B+FvaPoMjsfoUjhKFfeEMt8AUgD8D46lLVcA6Bjha/kYwHoA67Q/pFZRci0D4Xh8XwdgjfZvZLR9N16uI+q+FwA9AazW8rwBwPPadlu/E06XQEQUB2KlGoeIiLxgsCciigMM9kREcYDBnogoDjDYExHFAQZ7IqI4wGBPRBQH/h8rLzXCdIbLhwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  19 Training Time 5860.26583981514 Best Test Acc:  0.9873817034700315\n",
      "test subjects:  ['./seg\\\\c03', './seg\\\\x04']\n",
      "*********\n",
      "33377 936\n",
      "31961 930\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.595427393913269\n",
      "Eval Loss:  0.7862551808357239\n",
      "Eval Loss:  0.7953088283538818\n",
      "[[19234     0]\n",
      " [12727     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75     19234\n",
      "           1       0.00      0.00      0.00     12727\n",
      "\n",
      "    accuracy                           0.60     31961\n",
      "   macro avg       0.30      0.50      0.38     31961\n",
      "weighted avg       0.36      0.60      0.45     31961\n",
      "\n",
      "acc:  0.601795938800413\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.3757007520265651\n",
      "mi F1:  0.601795938800413\n",
      "we F1:  0.4521903735476958\n",
      "[[930]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       930\n",
      "\n",
      "    accuracy                           1.00       930\n",
      "   macro avg       1.00      1.00      1.00       930\n",
      "weighted avg       1.00      1.00      1.00       930\n",
      "\n",
      "acc:  1.0\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  1.0\n",
      "mi F1:  1.0\n",
      "we F1:  1.0\n",
      "Subject 20 Current Train Acc:  0.601795938800413 Current Test Acc:  1.0\n",
      "Loss:  0.17669209837913513\n",
      "Loss:  0.16260989010334015\n",
      "Loss:  0.1620483547449112\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.1486516296863556\n",
      "Loss:  0.14080066978931427\n",
      "Loss:  0.12961901724338531\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.13623209297657013\n",
      "Loss:  0.1259065717458725\n",
      "Loss:  0.07435782253742218\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.5978696346282959\n",
      "Eval Loss:  0.3140226900577545\n",
      "Eval Loss:  0.43368709087371826\n",
      "[[16389  2845]\n",
      " [ 3077  9650]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85     19234\n",
      "           1       0.77      0.76      0.77     12727\n",
      "\n",
      "    accuracy                           0.81     31961\n",
      "   macro avg       0.81      0.81      0.81     31961\n",
      "weighted avg       0.81      0.81      0.81     31961\n",
      "\n",
      "acc:  0.8147116798598292\n",
      "pre:  0.7723089235694278\n",
      "rec:  0.7582305335114324\n",
      "ma F1:  0.806090861982802\n",
      "mi F1:  0.8147116798598292\n",
      "we F1:  0.8144148955079268\n",
      "[[832  98]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.89      0.94       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.89       930\n",
      "   macro avg       0.50      0.45      0.47       930\n",
      "weighted avg       1.00      0.89      0.94       930\n",
      "\n",
      "acc:  0.8946236559139785\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4721906923950057\n",
      "mi F1:  0.8946236559139785\n",
      "we F1:  0.9443813847900115\n",
      "Loss:  0.0979142114520073\n",
      "Loss:  0.10266116261482239\n",
      "Loss:  0.10358747094869614\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.11133050173521042\n",
      "Loss:  0.08738566935062408\n",
      "Loss:  0.10654044896364212\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.07147010415792465\n",
      "Loss:  0.08190000802278519\n",
      "Loss:  0.09796997159719467\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.06726539134979248\n",
      "Eval Loss:  0.35223424434661865\n",
      "Eval Loss:  0.8753730654716492\n",
      "[[18477   757]\n",
      " [ 4760  7967]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.96      0.87     19234\n",
      "           1       0.91      0.63      0.74     12727\n",
      "\n",
      "    accuracy                           0.83     31961\n",
      "   macro avg       0.85      0.79      0.81     31961\n",
      "weighted avg       0.84      0.83      0.82     31961\n",
      "\n",
      "acc:  0.8273833734864366\n",
      "pre:  0.9132278771205868\n",
      "rec:  0.6259919855425473\n",
      "ma F1:  0.8064543952084691\n",
      "mi F1:  0.8273833734864366\n",
      "we F1:  0.8194120414172926\n",
      "[[901  29]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.97      0.98       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97       930\n",
      "   macro avg       0.50      0.48      0.49       930\n",
      "weighted avg       1.00      0.97      0.98       930\n",
      "\n",
      "acc:  0.9688172043010753\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49208083014746046\n",
      "mi F1:  0.9688172043010753\n",
      "we F1:  0.9841616602949209\n",
      "Loss:  0.08887077867984772\n",
      "Loss:  0.10217485576868057\n",
      "Loss:  0.0894237607717514\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.09048420935869217\n",
      "Loss:  0.06583722680807114\n",
      "Loss:  0.08233045041561127\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.05963259935379028\n",
      "Loss:  0.07216775417327881\n",
      "Loss:  0.11510360985994339\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.05632281303405762\n",
      "Eval Loss:  0.19485044479370117\n",
      "Eval Loss:  1.3354573249816895\n",
      "[[18006  1228]\n",
      " [ 3034  9693]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.89     19234\n",
      "           1       0.89      0.76      0.82     12727\n",
      "\n",
      "    accuracy                           0.87     31961\n",
      "   macro avg       0.87      0.85      0.86     31961\n",
      "weighted avg       0.87      0.87      0.86     31961\n",
      "\n",
      "acc:  0.866649979662714\n",
      "pre:  0.8875560846076367\n",
      "rec:  0.7616091773395144\n",
      "ma F1:  0.8569741221381844\n",
      "mi F1:  0.866649979662714\n",
      "we F1:  0.8645478987425527\n",
      "[[902  28]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.97      0.98       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97       930\n",
      "   macro avg       0.50      0.48      0.49       930\n",
      "weighted avg       1.00      0.97      0.98       930\n",
      "\n",
      "acc:  0.9698924731182795\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4923580786026201\n",
      "mi F1:  0.9698924731182795\n",
      "we F1:  0.9847161572052402\n",
      "Loss:  0.07869965583086014\n",
      "Loss:  0.08335267007350922\n",
      "Loss:  0.06071111932396889\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.08162656426429749\n",
      "Loss:  0.0998934805393219\n",
      "Loss:  0.07479947805404663\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.053068988025188446\n",
      "Loss:  0.1022612676024437\n",
      "Loss:  0.05432606860995293\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.04360628128051758\n",
      "Eval Loss:  0.38317564129829407\n",
      "Eval Loss:  1.8341233730316162\n",
      "[[18537   697]\n",
      " [ 3802  8925]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89     19234\n",
      "           1       0.93      0.70      0.80     12727\n",
      "\n",
      "    accuracy                           0.86     31961\n",
      "   macro avg       0.88      0.83      0.85     31961\n",
      "weighted avg       0.87      0.86      0.85     31961\n",
      "\n",
      "acc:  0.8592346922812177\n",
      "pre:  0.9275618374558304\n",
      "rec:  0.7012650271077238\n",
      "ma F1:  0.8452370884556304\n",
      "mi F1:  0.8592346922812177\n",
      "we F1:  0.8547129944162106\n",
      "[[907  23]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98       930\n",
      "   macro avg       0.50      0.49      0.49       930\n",
      "weighted avg       1.00      0.98      0.99       930\n",
      "\n",
      "acc:  0.975268817204301\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49373979314099076\n",
      "mi F1:  0.9752688172043009\n",
      "we F1:  0.9874795862819815\n",
      "Loss:  0.06267087906599045\n",
      "Loss:  0.06962351500988007\n",
      "Loss:  0.10894738882780075\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.11169841140508652\n",
      "Loss:  0.12173613905906677\n",
      "Loss:  0.04926023259758949\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.07359260320663452\n",
      "Loss:  0.07287374138832092\n",
      "Loss:  0.0944722518324852\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.04262042045593262\n",
      "Eval Loss:  0.27443575859069824\n",
      "Eval Loss:  1.727891206741333\n",
      "[[18722   512]\n",
      " [ 4171  8556]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89     19234\n",
      "           1       0.94      0.67      0.79     12727\n",
      "\n",
      "    accuracy                           0.85     31961\n",
      "   macro avg       0.88      0.82      0.84     31961\n",
      "weighted avg       0.87      0.85      0.85     31961\n",
      "\n",
      "acc:  0.8534776759175244\n",
      "pre:  0.9435377150419056\n",
      "rec:  0.6722715486760431\n",
      "ma F1:  0.8369851717182457\n",
      "mi F1:  0.8534776759175244\n",
      "we F1:  0.8475416073689154\n",
      "[[913  17]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98       930\n",
      "   macro avg       0.50      0.49      0.50       930\n",
      "weighted avg       1.00      0.98      0.99       930\n",
      "\n",
      "acc:  0.9817204301075269\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4953879544221378\n",
      "mi F1:  0.9817204301075269\n",
      "we F1:  0.9907759088442756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.06863687187433243\n",
      "Loss:  0.07755646109580994\n",
      "Loss:  0.08363353461027145\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.07214989513158798\n",
      "Loss:  0.07226290553808212\n",
      "Loss:  0.051597025245428085\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.05673450976610184\n",
      "Loss:  0.08856700360774994\n",
      "Loss:  0.07227206230163574\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.03103458881378174\n",
      "Eval Loss:  0.3493017256259918\n",
      "Eval Loss:  1.363337516784668\n",
      "[[18555   679]\n",
      " [ 3270  9457]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90     19234\n",
      "           1       0.93      0.74      0.83     12727\n",
      "\n",
      "    accuracy                           0.88     31961\n",
      "   macro avg       0.89      0.85      0.87     31961\n",
      "weighted avg       0.88      0.88      0.87     31961\n",
      "\n",
      "acc:  0.8764431651074748\n",
      "pre:  0.9330110497237569\n",
      "rec:  0.7430659228412038\n",
      "ma F1:  0.8655484204656057\n",
      "mi F1:  0.8764431651074748\n",
      "we F1:  0.8733404740361314\n",
      "[[910  20]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98       930\n",
      "   macro avg       0.50      0.49      0.49       930\n",
      "weighted avg       1.00      0.98      0.99       930\n",
      "\n",
      "acc:  0.978494623655914\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4945652173913044\n",
      "mi F1:  0.978494623655914\n",
      "we F1:  0.9891304347826088\n",
      "Loss:  0.06497751921415329\n",
      "Loss:  0.08375924080610275\n",
      "Loss:  0.06019032746553421\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.03797812759876251\n",
      "Loss:  0.061770133674144745\n",
      "Loss:  0.086030513048172\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.07617215067148209\n",
      "Loss:  0.05815599858760834\n",
      "Loss:  0.062147993594408035\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.027820110321044922\n",
      "Eval Loss:  0.4957386255264282\n",
      "Eval Loss:  1.5349712371826172\n",
      "[[18484   750]\n",
      " [ 3038  9689]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91     19234\n",
      "           1       0.93      0.76      0.84     12727\n",
      "\n",
      "    accuracy                           0.88     31961\n",
      "   macro avg       0.89      0.86      0.87     31961\n",
      "weighted avg       0.89      0.88      0.88     31961\n",
      "\n",
      "acc:  0.8814805544257063\n",
      "pre:  0.9281540377430788\n",
      "rec:  0.7612948848903905\n",
      "ma F1:  0.8717705664249322\n",
      "mi F1:  0.8814805544257063\n",
      "we F1:  0.8789545222999792\n",
      "[[890  40]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.96      0.98       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.96       930\n",
      "   macro avg       0.50      0.48      0.49       930\n",
      "weighted avg       1.00      0.96      0.98       930\n",
      "\n",
      "acc:  0.956989247311828\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.489010989010989\n",
      "mi F1:  0.956989247311828\n",
      "we F1:  0.9780219780219779\n",
      "Loss:  0.10231335461139679\n",
      "Loss:  0.05923570320010185\n",
      "Loss:  0.09995155036449432\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.07608627527952194\n",
      "Loss:  0.08590483665466309\n",
      "Loss:  0.029762156307697296\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.05324330925941467\n",
      "Loss:  0.07705944031476974\n",
      "Loss:  0.048809703439474106\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.01720738410949707\n",
      "Eval Loss:  0.44164469838142395\n",
      "Eval Loss:  1.4918620586395264\n",
      "[[18443   791]\n",
      " [ 2784  9943]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91     19234\n",
      "           1       0.93      0.78      0.85     12727\n",
      "\n",
      "    accuracy                           0.89     31961\n",
      "   macro avg       0.90      0.87      0.88     31961\n",
      "weighted avg       0.89      0.89      0.89     31961\n",
      "\n",
      "acc:  0.8881449266293295\n",
      "pre:  0.9263089249114962\n",
      "rec:  0.7812524554097587\n",
      "ma F1:  0.8796313822018844\n",
      "mi F1:  0.8881449266293295\n",
      "we F1:  0.8861487508594591\n",
      "[[897  33]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.96      0.98       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.96       930\n",
      "   macro avg       0.50      0.48      0.49       930\n",
      "weighted avg       1.00      0.96      0.98       930\n",
      "\n",
      "acc:  0.964516129032258\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4909688013136289\n",
      "mi F1:  0.964516129032258\n",
      "we F1:  0.9819376026272578\n",
      "Loss:  0.046870842576026917\n",
      "Loss:  0.07224110513925552\n",
      "Loss:  0.06861410290002823\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.06275846064090729\n",
      "Loss:  0.09404567629098892\n",
      "Loss:  0.09397720545530319\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.09604182094335556\n",
      "Loss:  0.06004192680120468\n",
      "Loss:  0.055079616606235504\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.022129416465759277\n",
      "Eval Loss:  0.3481658697128296\n",
      "Eval Loss:  1.4763758182525635\n",
      "[[18403   831]\n",
      " [ 2519 10208]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     19234\n",
      "           1       0.92      0.80      0.86     12727\n",
      "\n",
      "    accuracy                           0.90     31961\n",
      "   macro avg       0.90      0.88      0.89     31961\n",
      "weighted avg       0.90      0.90      0.89     31961\n",
      "\n",
      "acc:  0.8951847564218892\n",
      "pre:  0.9247214421596159\n",
      "rec:  0.8020743301642178\n",
      "ma F1:  0.8878088427446348\n",
      "mi F1:  0.8951847564218892\n",
      "we F1:  0.8936654712129563\n",
      "[[887  43]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.98       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.95       930\n",
      "   macro avg       0.50      0.48      0.49       930\n",
      "weighted avg       1.00      0.95      0.98       930\n",
      "\n",
      "acc:  0.953763440860215\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.488167308750688\n",
      "mi F1:  0.953763440860215\n",
      "we F1:  0.976334617501376\n",
      "Loss:  0.0584096685051918\n",
      "Loss:  0.0518440343439579\n",
      "Loss:  0.061622023582458496\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.07584034651517868\n",
      "Loss:  0.06053214147686958\n",
      "Loss:  0.039593905210494995\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.052752863615751266\n",
      "Loss:  0.051454540342092514\n",
      "Loss:  0.03903976082801819\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.05786752700805664\n",
      "Eval Loss:  0.2877792716026306\n",
      "Eval Loss:  1.3764808177947998\n",
      "[[18336   898]\n",
      " [ 2190 10537]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     19234\n",
      "           1       0.92      0.83      0.87     12727\n",
      "\n",
      "    accuracy                           0.90     31961\n",
      "   macro avg       0.91      0.89      0.90     31961\n",
      "weighted avg       0.90      0.90      0.90     31961\n",
      "\n",
      "acc:  0.9033822471136698\n",
      "pre:  0.9214691735898557\n",
      "rec:  0.8279248841046594\n",
      "ma F1:  0.897265007144098\n",
      "mi F1:  0.9033822471136698\n",
      "we F1:  0.902368851288476\n",
      "[[877  53]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.94      0.97       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.94       930\n",
      "   macro avg       0.50      0.47      0.49       930\n",
      "weighted avg       1.00      0.94      0.97       930\n",
      "\n",
      "acc:  0.943010752688172\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4853348090758163\n",
      "mi F1:  0.943010752688172\n",
      "we F1:  0.9706696181516326\n",
      "Loss:  0.08197251707315445\n",
      "Loss:  0.0953783243894577\n",
      "Loss:  0.03598592430353165\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.058810748159885406\n",
      "Loss:  0.04544579237699509\n",
      "Loss:  0.04972120001912117\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.035762444138526917\n",
      "Loss:  0.05952151119709015\n",
      "Loss:  0.07060016691684723\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.024996042251586914\n",
      "Eval Loss:  0.2830193042755127\n",
      "Eval Loss:  0.9430626034736633\n",
      "[[18223  1011]\n",
      " [ 1936 10791]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93     19234\n",
      "           1       0.91      0.85      0.88     12727\n",
      "\n",
      "    accuracy                           0.91     31961\n",
      "   macro avg       0.91      0.90      0.90     31961\n",
      "weighted avg       0.91      0.91      0.91     31961\n",
      "\n",
      "acc:  0.9077938737836738\n",
      "pre:  0.9143365531265887\n",
      "rec:  0.8478824546240277\n",
      "ma F1:  0.9025231254584707\n",
      "mi F1:  0.9077938737836738\n",
      "we F1:  0.9071378670289897\n",
      "[[880  50]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.95      0.97       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.95       930\n",
      "   macro avg       0.50      0.47      0.49       930\n",
      "weighted avg       1.00      0.95      0.97       930\n",
      "\n",
      "acc:  0.946236559139785\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48618784530386744\n",
      "mi F1:  0.946236559139785\n",
      "we F1:  0.9723756906077349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.05625632032752037\n",
      "Loss:  0.0611887127161026\n",
      "Loss:  0.0525619201362133\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.06387629359960556\n",
      "Loss:  0.04987413436174393\n",
      "Loss:  0.06635968387126923\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.06522276997566223\n",
      "Loss:  0.08752801269292831\n",
      "Loss:  0.05280778184533119\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.04646444320678711\n",
      "Eval Loss:  0.2745774984359741\n",
      "Eval Loss:  1.152712106704712\n",
      "[[18429   805]\n",
      " [ 2283 10444]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92     19234\n",
      "           1       0.93      0.82      0.87     12727\n",
      "\n",
      "    accuracy                           0.90     31961\n",
      "   macro avg       0.91      0.89      0.90     31961\n",
      "weighted avg       0.91      0.90      0.90     31961\n",
      "\n",
      "acc:  0.9033822471136698\n",
      "pre:  0.9284380833851898\n",
      "rec:  0.8206175846625284\n",
      "ma F1:  0.8969500884920034\n",
      "mi F1:  0.9033822471136698\n",
      "we F1:  0.902191673482759\n",
      "[[891  39]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.96      0.98       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.96       930\n",
      "   macro avg       0.50      0.48      0.49       930\n",
      "weighted avg       1.00      0.96      0.98       930\n",
      "\n",
      "acc:  0.9580645161290322\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4892915980230642\n",
      "mi F1:  0.9580645161290322\n",
      "we F1:  0.9785831960461284\n",
      "Loss:  0.05679234489798546\n",
      "Loss:  0.07264633476734161\n",
      "Loss:  0.07973550260066986\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.03933846578001976\n",
      "Loss:  0.05999372527003288\n",
      "Loss:  0.07314027845859528\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.04619751498103142\n",
      "Loss:  0.060717228800058365\n",
      "Loss:  0.08522037416696548\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.10444343090057373\n",
      "Eval Loss:  0.08381533622741699\n",
      "Eval Loss:  0.8937407732009888\n",
      "[[17998  1236]\n",
      " [ 1558 11169]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19234\n",
      "           1       0.90      0.88      0.89     12727\n",
      "\n",
      "    accuracy                           0.91     31961\n",
      "   macro avg       0.91      0.91      0.91     31961\n",
      "weighted avg       0.91      0.91      0.91     31961\n",
      "\n",
      "acc:  0.9125809580426144\n",
      "pre:  0.9003627569528416\n",
      "rec:  0.8775830910662371\n",
      "ma F1:  0.9083990600267351\n",
      "mi F1:  0.9125809580426144\n",
      "we F1:  0.9123837738046421\n",
      "[[853  77]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.92      0.96       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.92       930\n",
      "   macro avg       0.50      0.46      0.48       930\n",
      "weighted avg       1.00      0.92      0.96       930\n",
      "\n",
      "acc:  0.9172043010752688\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4784071789119462\n",
      "mi F1:  0.9172043010752688\n",
      "we F1:  0.9568143578238923\n",
      "Loss:  0.06964153796434402\n",
      "Loss:  0.06550075113773346\n",
      "Loss:  0.07085084170103073\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.05962448567152023\n",
      "Loss:  0.044403016567230225\n",
      "Loss:  0.0430486761033535\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.046038705855607986\n",
      "Loss:  0.049540117383003235\n",
      "Loss:  0.06220558285713196\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.07456684112548828\n",
      "Eval Loss:  0.2176523208618164\n",
      "Eval Loss:  0.38339513540267944\n",
      "[[18162  1072]\n",
      " [ 1705 11022]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93     19234\n",
      "           1       0.91      0.87      0.89     12727\n",
      "\n",
      "    accuracy                           0.91     31961\n",
      "   macro avg       0.91      0.91      0.91     31961\n",
      "weighted avg       0.91      0.91      0.91     31961\n",
      "\n",
      "acc:  0.9131128562936078\n",
      "pre:  0.9113610054572515\n",
      "rec:  0.8660328435609335\n",
      "ma F1:  0.9085488650230495\n",
      "mi F1:  0.9131128562936078\n",
      "we F1:  0.9127082335381086\n",
      "[[859  71]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.92      0.96       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.92       930\n",
      "   macro avg       0.50      0.46      0.48       930\n",
      "weighted avg       1.00      0.92      0.96       930\n",
      "\n",
      "acc:  0.9236559139784947\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48015651201788706\n",
      "mi F1:  0.9236559139784947\n",
      "we F1:  0.9603130240357741\n",
      "Loss:  0.0606815330684185\n",
      "Loss:  0.058239877223968506\n",
      "Loss:  0.086652472615242\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.05195016413927078\n",
      "Loss:  0.06414984166622162\n",
      "Loss:  0.06631076335906982\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.07684545964002609\n",
      "Loss:  0.06523571908473969\n",
      "Loss:  0.043906912207603455\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.045397162437438965\n",
      "Eval Loss:  0.20236992835998535\n",
      "Eval Loss:  0.7452072501182556\n",
      "[[18142  1092]\n",
      " [ 1634 11093]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19234\n",
      "           1       0.91      0.87      0.89     12727\n",
      "\n",
      "    accuracy                           0.91     31961\n",
      "   macro avg       0.91      0.91      0.91     31961\n",
      "weighted avg       0.91      0.91      0.91     31961\n",
      "\n",
      "acc:  0.914708551046588\n",
      "pre:  0.9103816167418958\n",
      "rec:  0.8716115345328829\n",
      "ma F1:  0.9103476526530012\n",
      "mi F1:  0.914708551046588\n",
      "we F1:  0.9143732400905199\n",
      "[[800 130]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.86      0.92       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.86       930\n",
      "   macro avg       0.50      0.43      0.46       930\n",
      "weighted avg       1.00      0.86      0.92       930\n",
      "\n",
      "acc:  0.8602150537634409\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4624277456647399\n",
      "mi F1:  0.8602150537634409\n",
      "we F1:  0.9248554913294798\n",
      "Loss:  0.06997725367546082\n",
      "Loss:  0.05983271077275276\n",
      "Loss:  0.054703712463378906\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.07195790112018585\n",
      "Loss:  0.06753361225128174\n",
      "Loss:  0.09147871285676956\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.07229834794998169\n",
      "Loss:  0.040186136960983276\n",
      "Loss:  0.04954875633120537\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.06093788146972656\n",
      "Eval Loss:  0.3297087848186493\n",
      "Eval Loss:  0.4894578456878662\n",
      "[[18064  1170]\n",
      " [ 1427 11300]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     19234\n",
      "           1       0.91      0.89      0.90     12727\n",
      "\n",
      "    accuracy                           0.92     31961\n",
      "   macro avg       0.92      0.91      0.91     31961\n",
      "weighted avg       0.92      0.92      0.92     31961\n",
      "\n",
      "acc:  0.9187447201276556\n",
      "pre:  0.9061748195669607\n",
      "rec:  0.8878761687750452\n",
      "ma F1:  0.914934776709689\n",
      "mi F1:  0.9187447201276556\n",
      "we F1:  0.9185999603023426\n",
      "[[860  70]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.92      0.96       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.92       930\n",
      "   macro avg       0.50      0.46      0.48       930\n",
      "weighted avg       1.00      0.92      0.96       930\n",
      "\n",
      "acc:  0.9247311827956989\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4804469273743016\n",
      "mi F1:  0.9247311827956989\n",
      "we F1:  0.9608938547486032\n",
      "Loss:  0.059058643877506256\n",
      "Loss:  0.07513479143381119\n",
      "Loss:  0.03268633782863617\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.06181398779153824\n",
      "Loss:  0.03625347092747688\n",
      "Loss:  0.06780512630939484\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.11279032379388809\n",
      "Loss:  0.0337313711643219\n",
      "Loss:  0.06621517241001129\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.0494309663772583\n",
      "Eval Loss:  0.5231213569641113\n",
      "Eval Loss:  1.1023187637329102\n",
      "[[18282   952]\n",
      " [ 1726 11001]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19234\n",
      "           1       0.92      0.86      0.89     12727\n",
      "\n",
      "    accuracy                           0.92     31961\n",
      "   macro avg       0.92      0.91      0.91     31961\n",
      "weighted avg       0.92      0.92      0.92     31961\n",
      "\n",
      "acc:  0.9162103814023341\n",
      "pre:  0.9203547226637664\n",
      "rec:  0.8643828082030329\n",
      "ma F1:  0.9116239385463116\n",
      "mi F1:  0.9162103814023341\n",
      "we F1:  0.9157228238181339\n",
      "[[848  82]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.91      0.95       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.91       930\n",
      "   macro avg       0.50      0.46      0.48       930\n",
      "weighted avg       1.00      0.91      0.95       930\n",
      "\n",
      "acc:  0.9118279569892473\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4769403824521935\n",
      "mi F1:  0.9118279569892473\n",
      "we F1:  0.953880764904387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.09582047164440155\n",
      "Loss:  0.05974183976650238\n",
      "Loss:  0.056277744472026825\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.08183619379997253\n",
      "Loss:  0.07865113019943237\n",
      "Loss:  0.06682921946048737\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.04234650731086731\n",
      "Loss:  0.05061442032456398\n",
      "Loss:  0.053077373653650284\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.019926071166992188\n",
      "Eval Loss:  0.2860338091850281\n",
      "Eval Loss:  1.0662022829055786\n",
      "[[18193  1041]\n",
      " [ 1662 11065]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     19234\n",
      "           1       0.91      0.87      0.89     12727\n",
      "\n",
      "    accuracy                           0.92     31961\n",
      "   macro avg       0.92      0.91      0.91     31961\n",
      "weighted avg       0.92      0.92      0.92     31961\n",
      "\n",
      "acc:  0.9154281780920497\n",
      "pre:  0.9140095820254419\n",
      "rec:  0.8694114873890155\n",
      "ma F1:  0.9110015063328656\n",
      "mi F1:  0.9154281780920497\n",
      "we F1:  0.9150425210827269\n",
      "[[863  67]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.93      0.96       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.93       930\n",
      "   macro avg       0.50      0.46      0.48       930\n",
      "weighted avg       1.00      0.93      0.96       930\n",
      "\n",
      "acc:  0.9279569892473118\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48131622978248745\n",
      "mi F1:  0.9279569892473118\n",
      "we F1:  0.9626324595649749\n",
      "Loss:  0.028492795303463936\n",
      "Loss:  0.03142436966300011\n",
      "Loss:  0.0745006650686264\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.04183756932616234\n",
      "Loss:  0.04020507261157036\n",
      "Loss:  0.04943966120481491\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.060850854963064194\n",
      "Loss:  0.0529973991215229\n",
      "Loss:  0.04016687348484993\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.04453253746032715\n",
      "Eval Loss:  0.47832661867141724\n",
      "Eval Loss:  0.7237750291824341\n",
      "[[17934  1300]\n",
      " [ 1271 11456]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19234\n",
      "           1       0.90      0.90      0.90     12727\n",
      "\n",
      "    accuracy                           0.92     31961\n",
      "   macro avg       0.92      0.92      0.92     31961\n",
      "weighted avg       0.92      0.92      0.92     31961\n",
      "\n",
      "acc:  0.9195582115703513\n",
      "pre:  0.8980871746629038\n",
      "rec:  0.9001335742908777\n",
      "ma F1:  0.9161120076687506\n",
      "mi F1:  0.9195582115703513\n",
      "we F1:  0.9195736391580552\n",
      "[[827 103]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.89      0.94       930\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.89       930\n",
      "   macro avg       0.50      0.44      0.47       930\n",
      "weighted avg       1.00      0.89      0.94       930\n",
      "\n",
      "acc:  0.889247311827957\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4706886738759249\n",
      "mi F1:  0.889247311827957\n",
      "we F1:  0.9413773477518498\n",
      "Loss:  0.039018187671899796\n",
      "Loss:  0.03265945613384247\n",
      "Loss:  0.0661843791604042\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.06428384780883789\n",
      "Loss:  0.06862261891365051\n",
      "Loss:  0.05762550234794617\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.0672474354505539\n",
      "Loss:  0.04297296330332756\n",
      "Loss:  0.029979180544614792\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  20 Training Time 5810.995578527451 Best Test Acc:  1.0\n",
      "test subjects:  ['./seg\\\\c04', './seg\\\\x29']\n",
      "*********\n",
      "33361 952\n",
      "31959 932\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.6928166747093201\n",
      "Eval Loss:  0.7127252221107483\n",
      "Eval Loss:  0.6976088881492615\n",
      "[[ 7704 11528]\n",
      " [ 6084  6643]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.40      0.47     19232\n",
      "           1       0.37      0.52      0.43     12727\n",
      "\n",
      "    accuracy                           0.45     31959\n",
      "   macro avg       0.46      0.46      0.45     31959\n",
      "weighted avg       0.48      0.45      0.45     31959\n",
      "\n",
      "acc:  0.4489189273757001\n",
      "pre:  0.3655825216003522\n",
      "rec:  0.5219611848825332\n",
      "ma F1:  0.44831087803056024\n",
      "mi F1:  0.4489189273757001\n",
      "we F1:  0.4520388337234302\n",
      "[[450 482]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.48      0.65       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.48       932\n",
      "   macro avg       0.50      0.24      0.33       932\n",
      "weighted avg       1.00      0.48      0.65       932\n",
      "\n",
      "acc:  0.48283261802575106\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.3256150506512301\n",
      "mi F1:  0.48283261802575106\n",
      "we F1:  0.6512301013024602\n",
      "Subject 21 Current Train Acc:  0.4489189273757001 Current Test Acc:  0.48283261802575106\n",
      "Loss:  0.16835397481918335\n",
      "Loss:  0.16491812467575073\n",
      "Loss:  0.16723698377609253\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.14362521469593048\n",
      "Loss:  0.12531733512878418\n",
      "Loss:  0.14174500107765198\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.09466628730297089\n",
      "Loss:  0.12301213294267654\n",
      "Loss:  0.13662971556186676\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.06171214580535889\n",
      "Eval Loss:  0.030749201774597168\n",
      "Eval Loss:  2.962113857269287\n",
      "[[16638  2594]\n",
      " [ 3409  9318]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.87      0.85     19232\n",
      "           1       0.78      0.73      0.76     12727\n",
      "\n",
      "    accuracy                           0.81     31959\n",
      "   macro avg       0.81      0.80      0.80     31959\n",
      "weighted avg       0.81      0.81      0.81     31959\n",
      "\n",
      "acc:  0.8121655871585469\n",
      "pre:  0.7822364002686367\n",
      "rec:  0.7321442602341479\n",
      "ma F1:  0.8017660544885422\n",
      "mi F1:  0.8121655871585468\n",
      "we F1:  0.8110077156932389\n",
      "[[821 111]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.88      0.94       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.88       932\n",
      "   macro avg       0.50      0.44      0.47       932\n",
      "weighted avg       1.00      0.88      0.94       932\n",
      "\n",
      "acc:  0.880901287553648\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.46833998859098686\n",
      "mi F1:  0.8809012875536482\n",
      "we F1:  0.9366799771819737\n",
      "Subject 21 Current Train Acc:  0.8121655871585469 Current Test Acc:  0.880901287553648\n",
      "Loss:  0.126931294798851\n",
      "Loss:  0.11385788023471832\n",
      "Loss:  0.07206113636493683\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.1033131405711174\n",
      "Loss:  0.09662823379039764\n",
      "Loss:  0.0912708267569542\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.0832885354757309\n",
      "Loss:  0.12039941549301147\n",
      "Loss:  0.11478476971387863\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.023585796356201172\n",
      "Eval Loss:  0.017645597457885742\n",
      "Eval Loss:  3.8995749950408936\n",
      "[[18066  1166]\n",
      " [ 3932  8795]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.94      0.88     19232\n",
      "           1       0.88      0.69      0.78     12727\n",
      "\n",
      "    accuracy                           0.84     31959\n",
      "   macro avg       0.85      0.82      0.83     31959\n",
      "weighted avg       0.85      0.84      0.84     31959\n",
      "\n",
      "acc:  0.8404831189962139\n",
      "pre:  0.8829434795703243\n",
      "rec:  0.6910505225111967\n",
      "ma F1:  0.8258259443310036\n",
      "mi F1:  0.8404831189962139\n",
      "we F1:  0.8361101554406135\n",
      "[[906  26]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.97      0.99       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97       932\n",
      "   macro avg       0.50      0.49      0.49       932\n",
      "weighted avg       1.00      0.97      0.99       932\n",
      "\n",
      "acc:  0.9721030042918455\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4929270946681175\n",
      "mi F1:  0.9721030042918455\n",
      "we F1:  0.985854189336235\n",
      "Subject 21 Current Train Acc:  0.8404831189962139 Current Test Acc:  0.9721030042918455\n",
      "Loss:  0.1249331533908844\n",
      "Loss:  0.08951057493686676\n",
      "Loss:  0.11146026849746704\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.09906681627035141\n",
      "Loss:  0.08451654762029648\n",
      "Loss:  0.10002528876066208\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.05231982842087746\n",
      "Loss:  0.10536571592092514\n",
      "Loss:  0.07327430695295334\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.020119905471801758\n",
      "Eval Loss:  0.0176999568939209\n",
      "Eval Loss:  4.092514991760254\n",
      "[[17899  1333]\n",
      " [ 2949  9778]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.93      0.89     19232\n",
      "           1       0.88      0.77      0.82     12727\n",
      "\n",
      "    accuracy                           0.87     31959\n",
      "   macro avg       0.87      0.85      0.86     31959\n",
      "weighted avg       0.87      0.87      0.86     31959\n",
      "\n",
      "acc:  0.8660158327857568\n",
      "pre:  0.8800288002880029\n",
      "rec:  0.7682878918833975\n",
      "ma F1:  0.8567672545671305\n",
      "mi F1:  0.8660158327857568\n",
      "we F1:  0.86417545568918\n",
      "[[913  19]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98       932\n",
      "   macro avg       0.50      0.49      0.49       932\n",
      "weighted avg       1.00      0.98      0.99       932\n",
      "\n",
      "acc:  0.9796137339055794\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4948509485094851\n",
      "mi F1:  0.9796137339055794\n",
      "we F1:  0.9897018970189702\n",
      "Subject 21 Current Train Acc:  0.8660158327857568 Current Test Acc:  0.9796137339055794\n",
      "Loss:  0.09782672673463821\n",
      "Loss:  0.09760506451129913\n",
      "Loss:  0.0924968421459198\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.08164934068918228\n",
      "Loss:  0.09863588213920593\n",
      "Loss:  0.06141055002808571\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.07543085515499115\n",
      "Loss:  0.09499581903219223\n",
      "Loss:  0.06179027631878853\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.018440604209899902\n",
      "Eval Loss:  0.016036272048950195\n",
      "Eval Loss:  4.203558921813965\n",
      "[[18130  1102]\n",
      " [ 2820  9907]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.90     19232\n",
      "           1       0.90      0.78      0.83     12727\n",
      "\n",
      "    accuracy                           0.88     31959\n",
      "   macro avg       0.88      0.86      0.87     31959\n",
      "weighted avg       0.88      0.88      0.88     31959\n",
      "\n",
      "acc:  0.8772802653399668\n",
      "pre:  0.8999000817512944\n",
      "rec:  0.7784238233676436\n",
      "ma F1:  0.8685799317352758\n",
      "mi F1:  0.8772802653399668\n",
      "we F1:  0.8754625378520843\n",
      "[[925   7]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      1.00       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99       932\n",
      "   macro avg       0.50      0.50      0.50       932\n",
      "weighted avg       1.00      0.99      1.00       932\n",
      "\n",
      "acc:  0.9924892703862661\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.498115239633818\n",
      "mi F1:  0.9924892703862661\n",
      "we F1:  0.996230479267636\n",
      "Subject 21 Current Train Acc:  0.8772802653399668 Current Test Acc:  0.9924892703862661\n",
      "Loss:  0.07312487810850143\n",
      "Loss:  0.05986463651061058\n",
      "Loss:  0.06348688900470734\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.07732692360877991\n",
      "Loss:  0.04884196072816849\n",
      "Loss:  0.08157448470592499\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.082275390625\n",
      "Loss:  0.08551768213510513\n",
      "Loss:  0.0754493921995163\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.024288535118103027\n",
      "Eval Loss:  0.010789871215820312\n",
      "Eval Loss:  2.3745429515838623\n",
      "[[18168  1064]\n",
      " [ 2665 10062]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.91     19232\n",
      "           1       0.90      0.79      0.84     12727\n",
      "\n",
      "    accuracy                           0.88     31959\n",
      "   macro avg       0.89      0.87      0.88     31959\n",
      "weighted avg       0.88      0.88      0.88     31959\n",
      "\n",
      "acc:  0.8833192527926406\n",
      "pre:  0.9043681466834442\n",
      "rec:  0.7906026557711952\n",
      "ma F1:  0.8752968540321449\n",
      "mi F1:  0.8833192527926405\n",
      "we F1:  0.8817347647078202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[921  11]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      0.99       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99       932\n",
      "   macro avg       0.50      0.49      0.50       932\n",
      "weighted avg       1.00      0.99      0.99       932\n",
      "\n",
      "acc:  0.9881974248927039\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4970318402590394\n",
      "mi F1:  0.9881974248927039\n",
      "we F1:  0.9940636805180788\n",
      "Loss:  0.07059888541698456\n",
      "Loss:  0.07333937287330627\n",
      "Loss:  0.05939383804798126\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.0637911856174469\n",
      "Loss:  0.07166949659585953\n",
      "Loss:  0.07231830060482025\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.0654979720711708\n",
      "Loss:  0.08974166214466095\n",
      "Loss:  0.06741761416196823\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.039354920387268066\n",
      "Eval Loss:  0.011485576629638672\n",
      "Eval Loss:  2.0334367752075195\n",
      "[[17880  1352]\n",
      " [ 2106 10621]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.93      0.91     19232\n",
      "           1       0.89      0.83      0.86     12727\n",
      "\n",
      "    accuracy                           0.89     31959\n",
      "   macro avg       0.89      0.88      0.89     31959\n",
      "weighted avg       0.89      0.89      0.89     31959\n",
      "\n",
      "acc:  0.8917988672987265\n",
      "pre:  0.8870792616720955\n",
      "rec:  0.8345250255362615\n",
      "ma F1:  0.8859131011270335\n",
      "mi F1:  0.8917988672987265\n",
      "we F1:  0.8911875065474582\n",
      "[[912  20]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98       932\n",
      "   macro avg       0.50      0.49      0.49       932\n",
      "weighted avg       1.00      0.98      0.99       932\n",
      "\n",
      "acc:  0.9785407725321889\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4945770065075922\n",
      "mi F1:  0.9785407725321889\n",
      "we F1:  0.9891540130151844\n",
      "Loss:  0.09443961828947067\n",
      "Loss:  0.053836580365896225\n",
      "Loss:  0.07365645468235016\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.09426794201135635\n",
      "Loss:  0.08136527985334396\n",
      "Loss:  0.06552038341760635\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.0621025376021862\n",
      "Loss:  0.0671289935708046\n",
      "Loss:  0.0762660950422287\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.042132019996643066\n",
      "Eval Loss:  0.009497880935668945\n",
      "Eval Loss:  0.860446572303772\n",
      "[[18060  1172]\n",
      " [ 2043 10684]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     19232\n",
      "           1       0.90      0.84      0.87     12727\n",
      "\n",
      "    accuracy                           0.90     31959\n",
      "   macro avg       0.90      0.89      0.89     31959\n",
      "weighted avg       0.90      0.90      0.90     31959\n",
      "\n",
      "acc:  0.8994023592728183\n",
      "pre:  0.9011470985155196\n",
      "rec:  0.8394751316099631\n",
      "ma F1:  0.893742370418768\n",
      "mi F1:  0.8994023592728184\n",
      "we F1:  0.8987339956215333\n",
      "[[918  14]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98       932\n",
      "   macro avg       0.50      0.49      0.50       932\n",
      "weighted avg       1.00      0.98      0.99       932\n",
      "\n",
      "acc:  0.9849785407725322\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4962162162162162\n",
      "mi F1:  0.9849785407725322\n",
      "we F1:  0.9924324324324325\n",
      "Loss:  0.061408672481775284\n",
      "Loss:  0.11752817779779434\n",
      "Loss:  0.06800439953804016\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.08189301937818527\n",
      "Loss:  0.08088196814060211\n",
      "Loss:  0.07470430433750153\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.08185392618179321\n",
      "Loss:  0.10366076231002808\n",
      "Loss:  0.0689125582575798\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.0430525541305542\n",
      "Eval Loss:  0.009443521499633789\n",
      "Eval Loss:  0.44967737793922424\n",
      "[[17718  1514]\n",
      " [ 1746 10981]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92     19232\n",
      "           1       0.88      0.86      0.87     12727\n",
      "\n",
      "    accuracy                           0.90     31959\n",
      "   macro avg       0.89      0.89      0.89     31959\n",
      "weighted avg       0.90      0.90      0.90     31959\n",
      "\n",
      "acc:  0.8979943052035421\n",
      "pre:  0.8788315326130453\n",
      "rec:  0.8628113459574134\n",
      "ma F1:  0.8932506630761181\n",
      "mi F1:  0.8979943052035421\n",
      "we F1:  0.8978309498564199\n",
      "[[906  26]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.97      0.99       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.97       932\n",
      "   macro avg       0.50      0.49      0.49       932\n",
      "weighted avg       1.00      0.97      0.99       932\n",
      "\n",
      "acc:  0.9721030042918455\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4929270946681175\n",
      "mi F1:  0.9721030042918455\n",
      "we F1:  0.985854189336235\n",
      "Loss:  0.06065124645829201\n",
      "Loss:  0.08012764900922775\n",
      "Loss:  0.051976483315229416\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.08600115776062012\n",
      "Loss:  0.046679895371198654\n",
      "Loss:  0.061151016503572464\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.062043994665145874\n",
      "Loss:  0.04894575476646423\n",
      "Loss:  0.05387056991457939\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.02302992343902588\n",
      "Eval Loss:  0.008221626281738281\n",
      "Eval Loss:  1.273723840713501\n",
      "[[18036  1196]\n",
      " [ 1837 10890]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92     19232\n",
      "           1       0.90      0.86      0.88     12727\n",
      "\n",
      "    accuracy                           0.91     31959\n",
      "   macro avg       0.90      0.90      0.90     31959\n",
      "weighted avg       0.90      0.91      0.90     31959\n",
      "\n",
      "acc:  0.9050971557307801\n",
      "pre:  0.9010425285454244\n",
      "rec:  0.8556611927398444\n",
      "ma F1:  0.900102636536019\n",
      "mi F1:  0.9050971557307801\n",
      "we F1:  0.9046491447030943\n",
      "[[922  10]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      0.99       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99       932\n",
      "   macro avg       0.50      0.49      0.50       932\n",
      "weighted avg       1.00      0.99      0.99       932\n",
      "\n",
      "acc:  0.9892703862660944\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49730312837108953\n",
      "mi F1:  0.9892703862660944\n",
      "we F1:  0.9946062567421791\n",
      "Loss:  0.06888343393802643\n",
      "Loss:  0.06356128305196762\n",
      "Loss:  0.04275629296898842\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.07513062655925751\n",
      "Loss:  0.10326835513114929\n",
      "Loss:  0.04106706380844116\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.050016600638628006\n",
      "Loss:  0.05435921996831894\n",
      "Loss:  0.056144047528505325\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.021022796630859375\n",
      "Eval Loss:  0.0063152313232421875\n",
      "Eval Loss:  0.9079322218894958\n",
      "[[18419   813]\n",
      " [ 2433 10294]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     19232\n",
      "           1       0.93      0.81      0.86     12727\n",
      "\n",
      "    accuracy                           0.90     31959\n",
      "   macro avg       0.91      0.88      0.89     31959\n",
      "weighted avg       0.90      0.90      0.90     31959\n",
      "\n",
      "acc:  0.8984323664695391\n",
      "pre:  0.9268029170793194\n",
      "rec:  0.8088316178203818\n",
      "ma F1:  0.8914140316244672\n",
      "mi F1:  0.8984323664695391\n",
      "we F1:  0.8970330184758141\n",
      "[[929   3]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           1.00       932\n",
      "   macro avg       0.50      0.50      0.50       932\n",
      "weighted avg       1.00      1.00      1.00       932\n",
      "\n",
      "acc:  0.9967811158798283\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49919398173025253\n",
      "mi F1:  0.9967811158798283\n",
      "we F1:  0.9983879634605051\n",
      "Subject 21 Current Train Acc:  0.8984323664695391 Current Test Acc:  0.9967811158798283\n",
      "Loss:  0.07940054684877396\n",
      "Loss:  0.05057776719331741\n",
      "Loss:  0.05424681678414345\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.05542929470539093\n",
      "Loss:  0.07979457825422287\n",
      "Loss:  0.06217927113175392\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.052718862891197205\n",
      "Loss:  0.0623873770236969\n",
      "Loss:  0.0675874799489975\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.04605364799499512\n",
      "Eval Loss:  0.00435638427734375\n",
      "Eval Loss:  0.4907976984977722\n",
      "[[18204  1028]\n",
      " [ 2021 10706]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     19232\n",
      "           1       0.91      0.84      0.88     12727\n",
      "\n",
      "    accuracy                           0.90     31959\n",
      "   macro avg       0.91      0.89      0.90     31959\n",
      "weighted avg       0.90      0.90      0.90     31959\n",
      "\n",
      "acc:  0.9045965142839263\n",
      "pre:  0.9123913414010567\n",
      "rec:  0.8412037400801445\n",
      "ma F1:  0.899039303813153\n",
      "mi F1:  0.9045965142839263\n",
      "we F1:  0.9038605433586825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[918  14]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98       932\n",
      "   macro avg       0.50      0.49      0.50       932\n",
      "weighted avg       1.00      0.98      0.99       932\n",
      "\n",
      "acc:  0.9849785407725322\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4962162162162162\n",
      "mi F1:  0.9849785407725322\n",
      "we F1:  0.9924324324324325\n",
      "Loss:  0.07454178482294083\n",
      "Loss:  0.06124488264322281\n",
      "Loss:  0.06642250716686249\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.058939605951309204\n",
      "Loss:  0.06724361330270767\n",
      "Loss:  0.05028268322348595\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.05632457882165909\n",
      "Loss:  0.048179034143686295\n",
      "Loss:  0.08201959729194641\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.05855357646942139\n",
      "Eval Loss:  0.004940509796142578\n",
      "Eval Loss:  1.8599965572357178\n",
      "[[18431   801]\n",
      " [ 2267 10460]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92     19232\n",
      "           1       0.93      0.82      0.87     12727\n",
      "\n",
      "    accuracy                           0.90     31959\n",
      "   macro avg       0.91      0.89      0.90     31959\n",
      "weighted avg       0.91      0.90      0.90     31959\n",
      "\n",
      "acc:  0.9040020025657874\n",
      "pre:  0.9288695497735547\n",
      "rec:  0.8218747544590241\n",
      "ma F1:  0.8976341288600724\n",
      "mi F1:  0.9040020025657874\n",
      "we F1:  0.902830844260358\n",
      "[[922  10]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      0.99       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99       932\n",
      "   macro avg       0.50      0.49      0.50       932\n",
      "weighted avg       1.00      0.99      0.99       932\n",
      "\n",
      "acc:  0.9892703862660944\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49730312837108953\n",
      "mi F1:  0.9892703862660944\n",
      "we F1:  0.9946062567421791\n",
      "Loss:  0.07453219592571259\n",
      "Loss:  0.09401941299438477\n",
      "Loss:  0.08668245375156403\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.08606547117233276\n",
      "Loss:  0.06201663985848427\n",
      "Loss:  0.04826951399445534\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.07714422047138214\n",
      "Loss:  0.11055141687393188\n",
      "Loss:  0.06526987999677658\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.06684362888336182\n",
      "Eval Loss:  0.005490303039550781\n",
      "Eval Loss:  1.6553272008895874\n",
      "[[18269   963]\n",
      " [ 1870 10857]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19232\n",
      "           1       0.92      0.85      0.88     12727\n",
      "\n",
      "    accuracy                           0.91     31959\n",
      "   macro avg       0.91      0.90      0.91     31959\n",
      "weighted avg       0.91      0.91      0.91     31959\n",
      "\n",
      "acc:  0.9113551738164524\n",
      "pre:  0.9185279187817259\n",
      "rec:  0.8530682800345721\n",
      "ma F1:  0.9063161159491695\n",
      "mi F1:  0.9113551738164524\n",
      "we F1:  0.9107385486834754\n",
      "[[918  14]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98       932\n",
      "   macro avg       0.50      0.49      0.50       932\n",
      "weighted avg       1.00      0.98      0.99       932\n",
      "\n",
      "acc:  0.9849785407725322\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4962162162162162\n",
      "mi F1:  0.9849785407725322\n",
      "we F1:  0.9924324324324325\n",
      "Loss:  0.06558656692504883\n",
      "Loss:  0.055717796087265015\n",
      "Loss:  0.06640676409006119\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.05764337256550789\n",
      "Loss:  0.07330134510993958\n",
      "Loss:  0.04594002291560173\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.07351231575012207\n",
      "Loss:  0.040225379168987274\n",
      "Loss:  0.050400055944919586\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.03410601615905762\n",
      "Eval Loss:  0.004423379898071289\n",
      "Eval Loss:  0.3906208872795105\n",
      "[[18243   989]\n",
      " [ 1770 10957]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19232\n",
      "           1       0.92      0.86      0.89     12727\n",
      "\n",
      "    accuracy                           0.91     31959\n",
      "   macro avg       0.91      0.90      0.91     31959\n",
      "weighted avg       0.91      0.91      0.91     31959\n",
      "\n",
      "acc:  0.913670640508151\n",
      "pre:  0.9172107818516658\n",
      "rec:  0.8609255912626699\n",
      "ma F1:  0.9089377052870788\n",
      "mi F1:  0.913670640508151\n",
      "we F1:  0.9131633083083628\n",
      "[[930   2]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           1.00       932\n",
      "   macro avg       0.50      0.50      0.50       932\n",
      "weighted avg       1.00      1.00      1.00       932\n",
      "\n",
      "acc:  0.9978540772532188\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49946294307196565\n",
      "mi F1:  0.9978540772532188\n",
      "we F1:  0.9989258861439313\n",
      "Subject 21 Current Train Acc:  0.913670640508151 Current Test Acc:  0.9978540772532188\n",
      "Loss:  0.06716084480285645\n",
      "Loss:  0.044529989361763\n",
      "Loss:  0.06608758866786957\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.07442130893468857\n",
      "Loss:  0.06649918854236603\n",
      "Loss:  0.06688069552183151\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.04599814489483833\n",
      "Loss:  0.0455164797604084\n",
      "Loss:  0.047534793615341187\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.04791605472564697\n",
      "Eval Loss:  0.0038259029388427734\n",
      "Eval Loss:  0.6261483430862427\n",
      "[[18144  1088]\n",
      " [ 1506 11221]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19232\n",
      "           1       0.91      0.88      0.90     12727\n",
      "\n",
      "    accuracy                           0.92     31959\n",
      "   macro avg       0.92      0.91      0.91     31959\n",
      "weighted avg       0.92      0.92      0.92     31959\n",
      "\n",
      "acc:  0.9188335054288307\n",
      "pre:  0.9116093915021529\n",
      "rec:  0.881668892904848\n",
      "ma F1:  0.914837262190807\n",
      "mi F1:  0.9188335054288307\n",
      "we F1:  0.918592218461693\n",
      "[[924   8]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      1.00       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99       932\n",
      "   macro avg       0.50      0.50      0.50       932\n",
      "weighted avg       1.00      0.99      1.00       932\n",
      "\n",
      "acc:  0.9914163090128756\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4978448275862069\n",
      "mi F1:  0.9914163090128756\n",
      "we F1:  0.9956896551724138\n",
      "Loss:  0.07868705689907074\n",
      "Loss:  0.033988889306783676\n",
      "Loss:  0.05017168074846268\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.07651636749505997\n",
      "Loss:  0.05410931259393692\n",
      "Loss:  0.054576922208070755\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.07276139408349991\n",
      "Loss:  0.055972084403038025\n",
      "Loss:  0.0609462633728981\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.04242110252380371\n",
      "Eval Loss:  0.003289937973022461\n",
      "Eval Loss:  0.9163002967834473\n",
      "[[17927  1305]\n",
      " [ 1343 11384]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19232\n",
      "           1       0.90      0.89      0.90     12727\n",
      "\n",
      "    accuracy                           0.92     31959\n",
      "   macro avg       0.91      0.91      0.91     31959\n",
      "weighted avg       0.92      0.92      0.92     31959\n",
      "\n",
      "acc:  0.9171438405456992\n",
      "pre:  0.8971550161557255\n",
      "rec:  0.8944763102066473\n",
      "ma F1:  0.9135190063339486\n",
      "mi F1:  0.9171438405456992\n",
      "we F1:  0.9171227884747766\n",
      "[[913  19]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98       932\n",
      "   macro avg       0.50      0.49      0.49       932\n",
      "weighted avg       1.00      0.98      0.99       932\n",
      "\n",
      "acc:  0.9796137339055794\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4948509485094851\n",
      "mi F1:  0.9796137339055794\n",
      "we F1:  0.9897018970189702\n",
      "Loss:  0.044403865933418274\n",
      "Loss:  0.05959055945277214\n",
      "Loss:  0.048467330634593964\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.05266651511192322\n",
      "Loss:  0.07299412786960602\n",
      "Loss:  0.05596315860748291\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.042361196130514145\n",
      "Loss:  0.05792452394962311\n",
      "Loss:  0.05615617334842682\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.03966248035430908\n",
      "Eval Loss:  0.004291057586669922\n",
      "Eval Loss:  0.6318714618682861\n",
      "[[18133  1099]\n",
      " [ 1457 11270]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     19232\n",
      "           1       0.91      0.89      0.90     12727\n",
      "\n",
      "    accuracy                           0.92     31959\n",
      "   macro avg       0.92      0.91      0.92     31959\n",
      "weighted avg       0.92      0.92      0.92     31959\n",
      "\n",
      "acc:  0.9200225288651084\n",
      "pre:  0.9111488398415394\n",
      "rec:  0.8855189754066158\n",
      "ma F1:  0.9161560712422999\n",
      "mi F1:  0.9200225288651084\n",
      "we F1:  0.9198208398327661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[927   5]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.99      1.00       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.99       932\n",
      "   macro avg       0.50      0.50      0.50       932\n",
      "weighted avg       1.00      0.99      1.00       932\n",
      "\n",
      "acc:  0.9946351931330472\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4986551909628833\n",
      "mi F1:  0.9946351931330472\n",
      "we F1:  0.9973103819257667\n",
      "Loss:  0.08108130842447281\n",
      "Loss:  0.05163799971342087\n",
      "Loss:  0.07457894086837769\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.05094803497195244\n",
      "Loss:  0.060640521347522736\n",
      "Loss:  0.07581336051225662\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.063232421875\n",
      "Loss:  0.03907988592982292\n",
      "Loss:  0.07563742995262146\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.043320536613464355\n",
      "Eval Loss:  0.002948760986328125\n",
      "Eval Loss:  0.6540194153785706\n",
      "[[18249   983]\n",
      " [ 1570 11157]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     19232\n",
      "           1       0.92      0.88      0.90     12727\n",
      "\n",
      "    accuracy                           0.92     31959\n",
      "   macro avg       0.92      0.91      0.92     31959\n",
      "weighted avg       0.92      0.92      0.92     31959\n",
      "\n",
      "acc:  0.9201163991363935\n",
      "pre:  0.9190280065897858\n",
      "rec:  0.8766402137188654\n",
      "ma F1:  0.91597888459622\n",
      "mi F1:  0.9201163991363935\n",
      "we F1:  0.9197739398815876\n",
      "[[928   4]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           1.00       932\n",
      "   macro avg       0.50      0.50      0.50       932\n",
      "weighted avg       1.00      1.00      1.00       932\n",
      "\n",
      "acc:  0.9957081545064378\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4989247311827957\n",
      "mi F1:  0.9957081545064378\n",
      "we F1:  0.9978494623655914\n",
      "Loss:  0.061013661324977875\n",
      "Loss:  0.0798788070678711\n",
      "Loss:  0.08058563619852066\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.0441533699631691\n",
      "Loss:  0.05102887749671936\n",
      "Loss:  0.04481520131230354\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.051162656396627426\n",
      "Loss:  0.05310654267668724\n",
      "Loss:  0.032593607902526855\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.023595571517944336\n",
      "Eval Loss:  0.004216909408569336\n",
      "Eval Loss:  0.23593389987945557\n",
      "[[18001  1231]\n",
      " [ 1231 11496]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.94      0.94     19232\n",
      "           1       0.90      0.90      0.90     12727\n",
      "\n",
      "    accuracy                           0.92     31959\n",
      "   macro avg       0.92      0.92      0.92     31959\n",
      "weighted avg       0.92      0.92      0.92     31959\n",
      "\n",
      "acc:  0.9229637973653744\n",
      "pre:  0.9032764987821168\n",
      "rec:  0.9032764987821168\n",
      "ma F1:  0.9196342976439702\n",
      "mi F1:  0.9229637973653744\n",
      "we F1:  0.9229637973653744\n",
      "[[914  18]\n",
      " [  0   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99       932\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.98       932\n",
      "   macro avg       0.50      0.49      0.50       932\n",
      "weighted avg       1.00      0.98      0.99       932\n",
      "\n",
      "acc:  0.98068669527897\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49512459371614304\n",
      "mi F1:  0.98068669527897\n",
      "we F1:  0.9902491874322861\n",
      "Loss:  0.039257340133190155\n",
      "Loss:  0.0435999371111393\n",
      "Loss:  0.06254395842552185\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.047884829342365265\n",
      "Loss:  0.0647815689444542\n",
      "Loss:  0.0605643056333065\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.04795768857002258\n",
      "Loss:  0.03416425362229347\n",
      "Loss:  0.03515118360519409\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  21 Training Time 5824.757781505585 Best Test Acc:  0.9978540772532188\n",
      "test subjects:  ['./seg\\\\c05', './seg\\\\x33']\n",
      "*********\n",
      "33374 939\n",
      "32152 739\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.6179713010787964\n",
      "Eval Loss:  0.6338748335838318\n",
      "Eval Loss:  0.7218332290649414\n",
      "[[19425     4]\n",
      " [12722     1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75     19429\n",
      "           1       0.20      0.00      0.00     12723\n",
      "\n",
      "    accuracy                           0.60     32152\n",
      "   macro avg       0.40      0.50      0.38     32152\n",
      "weighted avg       0.44      0.60      0.46     32152\n",
      "\n",
      "acc:  0.604192585220204\n",
      "pre:  0.2\n",
      "rec:  7.859781498074354e-05\n",
      "ma F1:  0.3767072314341461\n",
      "mi F1:  0.604192585220204\n",
      "we F1:  0.45524495705653617\n",
      "[[735   0]\n",
      " [  4   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       735\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.99       739\n",
      "   macro avg       0.50      0.50      0.50       739\n",
      "weighted avg       0.99      0.99      0.99       739\n",
      "\n",
      "acc:  0.9945872801082544\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4986431478968792\n",
      "mi F1:  0.9945872801082544\n",
      "we F1:  0.9918882644227502\n",
      "Subject 22 Current Train Acc:  0.604192585220204 Current Test Acc:  0.9945872801082544\n",
      "Loss:  0.1652737706899643\n",
      "Loss:  0.16108614206314087\n",
      "Loss:  0.15493886172771454\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.15054728090763092\n",
      "Loss:  0.13697242736816406\n",
      "Loss:  0.14305368065834045\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.10906735062599182\n",
      "Loss:  0.11682607978582382\n",
      "Loss:  0.11027456820011139\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.053221940994262695\n",
      "Eval Loss:  0.08679282665252686\n",
      "Eval Loss:  0.2579304575920105\n",
      "[[17236  2193]\n",
      " [ 3387  9336]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.89      0.86     19429\n",
      "           1       0.81      0.73      0.77     12723\n",
      "\n",
      "    accuracy                           0.83     32152\n",
      "   macro avg       0.82      0.81      0.82     32152\n",
      "weighted avg       0.83      0.83      0.82     32152\n",
      "\n",
      "acc:  0.8264493655138094\n",
      "pre:  0.809784022898777\n",
      "rec:  0.7337892006602217\n",
      "ma F1:  0.8152984988886046\n",
      "mi F1:  0.8264493655138094\n",
      "we F1:  0.8247640320010886\n",
      "[[688  47]\n",
      " [  2   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97       735\n",
      "           1       0.04      0.50      0.08         4\n",
      "\n",
      "    accuracy                           0.93       739\n",
      "   macro avg       0.52      0.72      0.52       739\n",
      "weighted avg       0.99      0.93      0.96       739\n",
      "\n",
      "acc:  0.9336941813261164\n",
      "pre:  0.04081632653061224\n",
      "rec:  0.5\n",
      "ma F1:  0.5205428666004634\n",
      "mi F1:  0.9336941813261164\n",
      "we F1:  0.9607959439538925\n",
      "Loss:  0.1503663957118988\n",
      "Loss:  0.11261140555143356\n",
      "Loss:  0.10321873426437378\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.0998091772198677\n",
      "Loss:  0.12032640725374222\n",
      "Loss:  0.11084048449993134\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.08734116703271866\n",
      "Loss:  0.12731453776359558\n",
      "Loss:  0.08132241666316986\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.01714158058166504\n",
      "Eval Loss:  0.02632296085357666\n",
      "Eval Loss:  0.11526608467102051\n",
      "[[18196  1233]\n",
      " [ 3714  9009]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.94      0.88     19429\n",
      "           1       0.88      0.71      0.78     12723\n",
      "\n",
      "    accuracy                           0.85     32152\n",
      "   macro avg       0.86      0.82      0.83     32152\n",
      "weighted avg       0.85      0.85      0.84     32152\n",
      "\n",
      "acc:  0.8461370987807912\n",
      "pre:  0.8796133567662566\n",
      "rec:  0.7080877151615185\n",
      "ma F1:  0.832458080389908\n",
      "mi F1:  0.8461370987807912\n",
      "we F1:  0.8424430044488241\n",
      "[[709  26]\n",
      " [  3   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       735\n",
      "           1       0.04      0.25      0.06         4\n",
      "\n",
      "    accuracy                           0.96       739\n",
      "   macro avg       0.52      0.61      0.52       739\n",
      "weighted avg       0.99      0.96      0.98       739\n",
      "\n",
      "acc:  0.9607577807848444\n",
      "pre:  0.037037037037037035\n",
      "rec:  0.25\n",
      "ma F1:  0.5222373319660254\n",
      "mi F1:  0.9607577807848444\n",
      "we F1:  0.975003501579806\n",
      "Loss:  0.07779204100370407\n",
      "Loss:  0.08075696229934692\n",
      "Loss:  0.09185139089822769\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.09096144139766693\n",
      "Loss:  0.08164596557617188\n",
      "Loss:  0.07868542522192001\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.05780754238367081\n",
      "Loss:  0.09127283096313477\n",
      "Loss:  0.08743171393871307\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.011240243911743164\n",
      "Eval Loss:  0.014494657516479492\n",
      "Eval Loss:  0.06509983539581299\n",
      "[[18467   962]\n",
      " [ 3522  9201]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89     19429\n",
      "           1       0.91      0.72      0.80     12723\n",
      "\n",
      "    accuracy                           0.86     32152\n",
      "   macro avg       0.87      0.84      0.85     32152\n",
      "weighted avg       0.87      0.86      0.86     32152\n",
      "\n",
      "acc:  0.8605374471261508\n",
      "pre:  0.9053429105579062\n",
      "rec:  0.7231784956378212\n",
      "ma F1:  0.8479051251925498\n",
      "mi F1:  0.8605374471261508\n",
      "we F1:  0.8570474035097017\n",
      "[[714  21]\n",
      " [  4   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       735\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.97       739\n",
      "   macro avg       0.50      0.49      0.49       739\n",
      "weighted avg       0.99      0.97      0.98       739\n",
      "\n",
      "acc:  0.9661705006765899\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.491397109428768\n",
      "mi F1:  0.9661705006765899\n",
      "we F1:  0.9774746290396332\n",
      "Loss:  0.09993746876716614\n",
      "Loss:  0.11432744562625885\n",
      "Loss:  0.07826758176088333\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.08441265672445297\n",
      "Loss:  0.08570148050785065\n",
      "Loss:  0.0665263682603836\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.07618781179189682\n",
      "Loss:  0.06779281049966812\n",
      "Loss:  0.07531556487083435\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.009421348571777344\n",
      "Eval Loss:  0.01250004768371582\n",
      "Eval Loss:  0.05353844165802002\n",
      "[[18549   880]\n",
      " [ 3271  9452]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90     19429\n",
      "           1       0.91      0.74      0.82     12723\n",
      "\n",
      "    accuracy                           0.87     32152\n",
      "   macro avg       0.88      0.85      0.86     32152\n",
      "weighted avg       0.88      0.87      0.87     32152\n",
      "\n",
      "acc:  0.8708945011196815\n",
      "pre:  0.9148277197057685\n",
      "rec:  0.7429065471979879\n",
      "ma F1:  0.8596597726974988\n",
      "mi F1:  0.8708945011196815\n",
      "we F1:  0.867941633618589\n",
      "[[718  17]\n",
      " [  4   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       735\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.97       739\n",
      "   macro avg       0.50      0.49      0.49       739\n",
      "weighted avg       0.99      0.97      0.98       739\n",
      "\n",
      "acc:  0.9715832205683356\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49279341111873715\n",
      "mi F1:  0.9715832205683356\n",
      "we F1:  0.9802521168397071\n",
      "Loss:  0.08346603810787201\n",
      "Loss:  0.1075984314084053\n",
      "Loss:  0.08216820657253265\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.10667302459478378\n",
      "Loss:  0.08133625984191895\n",
      "Loss:  0.09554678201675415\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.08354443311691284\n",
      "Loss:  0.07756754755973816\n",
      "Loss:  0.08672420680522919\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.006738424301147461\n",
      "Eval Loss:  0.011205673217773438\n",
      "Eval Loss:  0.031016945838928223\n",
      "[[18789   640]\n",
      " [ 3657  9066]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.97      0.90     19429\n",
      "           1       0.93      0.71      0.81     12723\n",
      "\n",
      "    accuracy                           0.87     32152\n",
      "   macro avg       0.89      0.84      0.85     32152\n",
      "weighted avg       0.88      0.87      0.86     32152\n",
      "\n",
      "acc:  0.8663535705399353\n",
      "pre:  0.9340614053162992\n",
      "rec:  0.7125677906154209\n",
      "ma F1:  0.8529013740872524\n",
      "mi F1:  0.8663535705399353\n",
      "we F1:  0.8621794188688725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[724  11]\n",
      " [  4   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       735\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.98       739\n",
      "   macro avg       0.50      0.49      0.49       739\n",
      "weighted avg       0.99      0.98      0.98       739\n",
      "\n",
      "acc:  0.979702300405954\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4948735475051264\n",
      "mi F1:  0.979702300405954\n",
      "we F1:  0.9843898712212935\n",
      "Loss:  0.06391201168298721\n",
      "Loss:  0.06682774424552917\n",
      "Loss:  0.0705312043428421\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.04712696000933647\n",
      "Loss:  0.08598832786083221\n",
      "Loss:  0.08180921524763107\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.09033991396427155\n",
      "Loss:  0.07654152065515518\n",
      "Loss:  0.07239016890525818\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.0047948360443115234\n",
      "Eval Loss:  0.012161016464233398\n",
      "Eval Loss:  0.020040154457092285\n",
      "[[18861   568]\n",
      " [ 3695  9028]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.97      0.90     19429\n",
      "           1       0.94      0.71      0.81     12723\n",
      "\n",
      "    accuracy                           0.87     32152\n",
      "   macro avg       0.89      0.84      0.85     32152\n",
      "weighted avg       0.88      0.87      0.86     32152\n",
      "\n",
      "acc:  0.8674110475242598\n",
      "pre:  0.940808670279283\n",
      "rec:  0.7095810736461526\n",
      "ma F1:  0.8537302779514188\n",
      "mi F1:  0.8674110475242598\n",
      "we F1:  0.8630604153210387\n",
      "[[729   6]\n",
      " [  4   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       735\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.99       739\n",
      "   macro avg       0.50      0.50      0.50       739\n",
      "weighted avg       0.99      0.99      0.99       739\n",
      "\n",
      "acc:  0.986468200270636\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4965940054495913\n",
      "mi F1:  0.986468200270636\n",
      "we F1:  0.9878121623963453\n",
      "Loss:  0.1030360758304596\n",
      "Loss:  0.0712316632270813\n",
      "Loss:  0.05115872621536255\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.0706922635436058\n",
      "Loss:  0.07719094306230545\n",
      "Loss:  0.07119310647249222\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.1175166666507721\n",
      "Loss:  0.05548728257417679\n",
      "Loss:  0.07704678177833557\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.005709648132324219\n",
      "Eval Loss:  0.014057397842407227\n",
      "Eval Loss:  0.017388463020324707\n",
      "[[18619   810]\n",
      " [ 2715 10008]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91     19429\n",
      "           1       0.93      0.79      0.85     12723\n",
      "\n",
      "    accuracy                           0.89     32152\n",
      "   macro avg       0.90      0.87      0.88     32152\n",
      "weighted avg       0.89      0.89      0.89     32152\n",
      "\n",
      "acc:  0.8903645185369495\n",
      "pre:  0.9251247920133111\n",
      "rec:  0.7866069323272813\n",
      "ma F1:  0.8818928830606394\n",
      "mi F1:  0.8903645185369495\n",
      "we F1:  0.8884903499639185\n",
      "[[723  12]\n",
      " [  4   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       735\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.98       739\n",
      "   macro avg       0.50      0.49      0.49       739\n",
      "weighted avg       0.99      0.98      0.98       739\n",
      "\n",
      "acc:  0.9783491204330176\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49452804377564985\n",
      "mi F1:  0.9783491204330176\n",
      "we F1:  0.9837026039921587\n",
      "Loss:  0.046828195452690125\n",
      "Loss:  0.05003688484430313\n",
      "Loss:  0.0542449913918972\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.0761876180768013\n",
      "Loss:  0.0752127468585968\n",
      "Loss:  0.041622456163167953\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.04737976938486099\n",
      "Loss:  0.03453223407268524\n",
      "Loss:  0.037824295461177826\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.004091024398803711\n",
      "Eval Loss:  0.015337467193603516\n",
      "Eval Loss:  0.014541387557983398\n",
      "[[18635   794]\n",
      " [ 2598 10125]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     19429\n",
      "           1       0.93      0.80      0.86     12723\n",
      "\n",
      "    accuracy                           0.89     32152\n",
      "   macro avg       0.90      0.88      0.89     32152\n",
      "weighted avg       0.90      0.89      0.89     32152\n",
      "\n",
      "acc:  0.8945011196815128\n",
      "pre:  0.9272827181976372\n",
      "rec:  0.7958028766800282\n",
      "ma F1:  0.8865535559071598\n",
      "mi F1:  0.8945011196815128\n",
      "we F1:  0.8928163482304045\n",
      "[[726   9]\n",
      " [  4   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       735\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.98       739\n",
      "   macro avg       0.50      0.49      0.50       739\n",
      "weighted avg       0.99      0.98      0.99       739\n",
      "\n",
      "acc:  0.9824086603518268\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4955631399317406\n",
      "mi F1:  0.9824086603518268\n",
      "we F1:  0.9857615909332323\n",
      "Loss:  0.0711669772863388\n",
      "Loss:  0.08280813694000244\n",
      "Loss:  0.07900723814964294\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.07825051248073578\n",
      "Loss:  0.06741763651371002\n",
      "Loss:  0.1079988181591034\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.06967516988515854\n",
      "Loss:  0.08263681828975677\n",
      "Loss:  0.05207391828298569\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.006166219711303711\n",
      "Eval Loss:  0.030104637145996094\n",
      "Eval Loss:  0.013089179992675781\n",
      "[[18406  1023]\n",
      " [ 2042 10681]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     19429\n",
      "           1       0.91      0.84      0.87     12723\n",
      "\n",
      "    accuracy                           0.90     32152\n",
      "   macro avg       0.91      0.89      0.90     32152\n",
      "weighted avg       0.91      0.90      0.90     32152\n",
      "\n",
      "acc:  0.9046715600895745\n",
      "pre:  0.9125939849624061\n",
      "rec:  0.8395032618093217\n",
      "ma F1:  0.8988313717729877\n",
      "mi F1:  0.9046715600895745\n",
      "we F1:  0.9039011844397878\n",
      "[[721  14]\n",
      " [  4   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       735\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.98       739\n",
      "   macro avg       0.50      0.49      0.49       739\n",
      "weighted avg       0.99      0.98      0.98       739\n",
      "\n",
      "acc:  0.9756427604871448\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4938356164383562\n",
      "mi F1:  0.9756427604871448\n",
      "we F1:  0.9823252451480158\n",
      "Loss:  0.051782913506031036\n",
      "Loss:  0.07597551494836807\n",
      "Loss:  0.1095927357673645\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.06676698476076126\n",
      "Loss:  0.055328886955976486\n",
      "Loss:  0.05977096036076546\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.07353414595127106\n",
      "Loss:  0.0703386589884758\n",
      "Loss:  0.08559778332710266\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.003839731216430664\n",
      "Eval Loss:  0.021190762519836426\n",
      "Eval Loss:  0.010286331176757812\n",
      "[[18501   928]\n",
      " [ 2061 10662]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93     19429\n",
      "           1       0.92      0.84      0.88     12723\n",
      "\n",
      "    accuracy                           0.91     32152\n",
      "   macro avg       0.91      0.90      0.90     32152\n",
      "weighted avg       0.91      0.91      0.91     32152\n",
      "\n",
      "acc:  0.9070353321721821\n",
      "pre:  0.9199309749784297\n",
      "rec:  0.8380099033246876\n",
      "ma F1:  0.9011599186751169\n",
      "mi F1:  0.9070353321721821\n",
      "we F1:  0.9061861366763057\n",
      "[[723  12]\n",
      " [  4   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       735\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.98       739\n",
      "   macro avg       0.50      0.49      0.49       739\n",
      "weighted avg       0.99      0.98      0.98       739\n",
      "\n",
      "acc:  0.9783491204330176\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49452804377564985\n",
      "mi F1:  0.9783491204330176\n",
      "we F1:  0.9837026039921587\n",
      "Loss:  0.05344119295477867\n",
      "Loss:  0.10021569579839706\n",
      "Loss:  0.07954069972038269\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.0500892773270607\n",
      "Loss:  0.03562700375914574\n",
      "Loss:  0.06420477479696274\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.055158715695142746\n",
      "Loss:  0.04113708436489105\n",
      "Loss:  0.07570189237594604\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.005100250244140625\n",
      "Eval Loss:  0.02823770046234131\n",
      "Eval Loss:  0.01031947135925293\n",
      "[[18655   774]\n",
      " [ 2262 10461]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92     19429\n",
      "           1       0.93      0.82      0.87     12723\n",
      "\n",
      "    accuracy                           0.91     32152\n",
      "   macro avg       0.91      0.89      0.90     32152\n",
      "weighted avg       0.91      0.91      0.90     32152\n",
      "\n",
      "acc:  0.9055735257526748\n",
      "pre:  0.9311081441922563\n",
      "rec:  0.8222117425135581\n",
      "ma F1:  0.8990145707945827\n",
      "mi F1:  0.9055735257526748\n",
      "we F1:  0.9043824438662164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[727   8]\n",
      " [  4   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       735\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.98       739\n",
      "   macro avg       0.50      0.49      0.50       739\n",
      "weighted avg       0.99      0.98      0.99       739\n",
      "\n",
      "acc:  0.9837618403247632\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49590723055934516\n",
      "mi F1:  0.9837618403247631\n",
      "we F1:  0.9864460472560722\n",
      "Loss:  0.06601641327142715\n",
      "Loss:  0.049969080835580826\n",
      "Loss:  0.0670880675315857\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.04340530186891556\n",
      "Loss:  0.05389165133237839\n",
      "Loss:  0.03652547672390938\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.04215048998594284\n",
      "Loss:  0.04228179529309273\n",
      "Loss:  0.06745704263448715\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.004069805145263672\n",
      "Eval Loss:  0.031154751777648926\n",
      "Eval Loss:  0.00931692123413086\n",
      "[[17871  1558]\n",
      " [ 1385 11338]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92     19429\n",
      "           1       0.88      0.89      0.89     12723\n",
      "\n",
      "    accuracy                           0.91     32152\n",
      "   macro avg       0.90      0.91      0.90     32152\n",
      "weighted avg       0.91      0.91      0.91     32152\n",
      "\n",
      "acc:  0.9084660363274446\n",
      "pre:  0.8791873449131513\n",
      "rec:  0.8911420262516702\n",
      "ma F1:  0.9045241616717042\n",
      "mi F1:  0.9084660363274446\n",
      "we F1:  0.9085704208851431\n",
      "[[714  21]\n",
      " [  4   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       735\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.97       739\n",
      "   macro avg       0.50      0.49      0.49       739\n",
      "weighted avg       0.99      0.97      0.98       739\n",
      "\n",
      "acc:  0.9661705006765899\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.491397109428768\n",
      "mi F1:  0.9661705006765899\n",
      "we F1:  0.9774746290396332\n",
      "Loss:  0.05638530105352402\n",
      "Loss:  0.07931214570999146\n",
      "Loss:  0.055227890610694885\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.07777601480484009\n",
      "Loss:  0.06036609038710594\n",
      "Loss:  0.030631475150585175\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.03874802961945534\n",
      "Loss:  0.05563966929912567\n",
      "Loss:  0.08961424976587296\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.003791332244873047\n",
      "Eval Loss:  0.02016139030456543\n",
      "Eval Loss:  0.00817108154296875\n",
      "[[18583   846]\n",
      " [ 2218 10505]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92     19429\n",
      "           1       0.93      0.83      0.87     12723\n",
      "\n",
      "    accuracy                           0.90     32152\n",
      "   macro avg       0.91      0.89      0.90     32152\n",
      "weighted avg       0.91      0.90      0.90     32152\n",
      "\n",
      "acc:  0.9047026623538194\n",
      "pre:  0.9254691216632895\n",
      "rec:  0.8256700463727108\n",
      "ma F1:  0.8982818470623688\n",
      "mi F1:  0.9047026623538194\n",
      "we F1:  0.903612125267923\n",
      "[[733   2]\n",
      " [  4   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       735\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.99       739\n",
      "   macro avg       0.50      0.50      0.50       739\n",
      "weighted avg       0.99      0.99      0.99       739\n",
      "\n",
      "acc:  0.9918809201623816\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4979619565217392\n",
      "mi F1:  0.9918809201623816\n",
      "we F1:  0.9905332558686828\n",
      "Loss:  0.05721953511238098\n",
      "Loss:  0.05728501081466675\n",
      "Loss:  0.04889068007469177\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.06320042908191681\n",
      "Loss:  0.1012004166841507\n",
      "Loss:  0.05779656395316124\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.07888089865446091\n",
      "Loss:  0.06845813989639282\n",
      "Loss:  0.04097319766879082\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.004860401153564453\n",
      "Eval Loss:  0.07354485988616943\n",
      "Eval Loss:  0.008081674575805664\n",
      "[[18252  1177]\n",
      " [ 1575 11148]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19429\n",
      "           1       0.90      0.88      0.89     12723\n",
      "\n",
      "    accuracy                           0.91     32152\n",
      "   macro avg       0.91      0.91      0.91     32152\n",
      "weighted avg       0.91      0.91      0.91     32152\n",
      "\n",
      "acc:  0.9144065687982085\n",
      "pre:  0.9045030425963488\n",
      "rec:  0.8762084414053289\n",
      "ma F1:  0.910013507711007\n",
      "mi F1:  0.9144065687982085\n",
      "we F1:  0.9141604485402263\n",
      "[[723  12]\n",
      " [  4   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       735\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.98       739\n",
      "   macro avg       0.50      0.49      0.49       739\n",
      "weighted avg       0.99      0.98      0.98       739\n",
      "\n",
      "acc:  0.9783491204330176\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49452804377564985\n",
      "mi F1:  0.9783491204330176\n",
      "we F1:  0.9837026039921587\n",
      "Loss:  0.052162401378154755\n",
      "Loss:  0.04602035880088806\n",
      "Loss:  0.04899023845791817\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.08211486786603928\n",
      "Loss:  0.048750054091215134\n",
      "Loss:  0.039188988506793976\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.03819763660430908\n",
      "Loss:  0.06486713141202927\n",
      "Loss:  0.04189595580101013\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.003905773162841797\n",
      "Eval Loss:  0.016412019729614258\n",
      "Eval Loss:  0.008305072784423828\n",
      "[[18350  1079]\n",
      " [ 1544 11179]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19429\n",
      "           1       0.91      0.88      0.90     12723\n",
      "\n",
      "    accuracy                           0.92     32152\n",
      "   macro avg       0.92      0.91      0.91     32152\n",
      "weighted avg       0.92      0.92      0.92     32152\n",
      "\n",
      "acc:  0.9184187608857924\n",
      "pre:  0.9119758525044869\n",
      "rec:  0.8786449736697319\n",
      "ma F1:  0.914148117775623\n",
      "mi F1:  0.9184187608857924\n",
      "we F1:  0.9181418331147383\n",
      "[[724  11]\n",
      " [  4   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       735\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.98       739\n",
      "   macro avg       0.50      0.49      0.49       739\n",
      "weighted avg       0.99      0.98      0.98       739\n",
      "\n",
      "acc:  0.979702300405954\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4948735475051264\n",
      "mi F1:  0.979702300405954\n",
      "we F1:  0.9843898712212935\n",
      "Loss:  0.07174970209598541\n",
      "Loss:  0.07998248189687729\n",
      "Loss:  0.06910380721092224\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.08614566922187805\n",
      "Loss:  0.031334273517131805\n",
      "Loss:  0.05505776032805443\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.06161941960453987\n",
      "Loss:  0.05586298182606697\n",
      "Loss:  0.05675400421023369\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.0038542747497558594\n",
      "Eval Loss:  0.054435133934020996\n",
      "Eval Loss:  0.008380413055419922\n",
      "[[17827  1602]\n",
      " [ 1173 11550]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     19429\n",
      "           1       0.88      0.91      0.89     12723\n",
      "\n",
      "    accuracy                           0.91     32152\n",
      "   macro avg       0.91      0.91      0.91     32152\n",
      "weighted avg       0.91      0.91      0.91     32152\n",
      "\n",
      "acc:  0.9136912167205773\n",
      "pre:  0.8781934306569343\n",
      "rec:  0.9078047630275878\n",
      "ma F1:  0.9102712663028865\n",
      "mi F1:  0.9136912167205773\n",
      "we F1:  0.9139249523792023\n",
      "[[723  12]\n",
      " [  3   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       735\n",
      "           1       0.08      0.25      0.12         4\n",
      "\n",
      "    accuracy                           0.98       739\n",
      "   macro avg       0.54      0.62      0.55       739\n",
      "weighted avg       0.99      0.98      0.99       739\n",
      "\n",
      "acc:  0.979702300405954\n",
      "pre:  0.07692307692307693\n",
      "rec:  0.25\n",
      "ma F1:  0.5536900591858921\n",
      "mi F1:  0.979702300405954\n",
      "we F1:  0.9850127023048192\n",
      "Loss:  0.08134238421916962\n",
      "Loss:  0.03613430634140968\n",
      "Loss:  0.0393851138651371\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.04509812593460083\n",
      "Loss:  0.07018531113862991\n",
      "Loss:  0.02681119740009308\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.06914740800857544\n",
      "Loss:  0.07353129982948303\n",
      "Loss:  0.07538460195064545\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.004065990447998047\n",
      "Eval Loss:  0.02427828311920166\n",
      "Eval Loss:  0.0077362060546875\n",
      "[[18408  1021]\n",
      " [ 1630 11093]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     19429\n",
      "           1       0.92      0.87      0.89     12723\n",
      "\n",
      "    accuracy                           0.92     32152\n",
      "   macro avg       0.92      0.91      0.91     32152\n",
      "weighted avg       0.92      0.92      0.92     32152\n",
      "\n",
      "acc:  0.9175478974869371\n",
      "pre:  0.9157173518243354\n",
      "rec:  0.8718855615813881\n",
      "ma F1:  0.9130470205099245\n",
      "mi F1:  0.9175478974869371\n",
      "we F1:  0.9171731833271282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[729   6]\n",
      " [  4   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       735\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.99       739\n",
      "   macro avg       0.50      0.50      0.50       739\n",
      "weighted avg       0.99      0.99      0.99       739\n",
      "\n",
      "acc:  0.986468200270636\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4965940054495913\n",
      "mi F1:  0.986468200270636\n",
      "we F1:  0.9878121623963453\n",
      "Loss:  0.08668491989374161\n",
      "Loss:  0.03711199015378952\n",
      "Loss:  0.0707501620054245\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.07542390376329422\n",
      "Loss:  0.09300819784402847\n",
      "Loss:  0.04317593574523926\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.05161444470286369\n",
      "Loss:  0.05826534330844879\n",
      "Loss:  0.04730645939707756\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.004547119140625\n",
      "Eval Loss:  0.0366663932800293\n",
      "Eval Loss:  0.006323099136352539\n",
      "[[18218  1211]\n",
      " [ 1379 11344]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     19429\n",
      "           1       0.90      0.89      0.90     12723\n",
      "\n",
      "    accuracy                           0.92     32152\n",
      "   macro avg       0.92      0.91      0.92     32152\n",
      "weighted avg       0.92      0.92      0.92     32152\n",
      "\n",
      "acc:  0.9194451356058722\n",
      "pre:  0.9035444046196734\n",
      "rec:  0.8916136131415546\n",
      "ma F1:  0.9155866749446661\n",
      "mi F1:  0.9194451356058722\n",
      "we F1:  0.9193508351416472\n",
      "[[726   9]\n",
      " [  3   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       735\n",
      "           1       0.10      0.25      0.14         4\n",
      "\n",
      "    accuracy                           0.98       739\n",
      "   macro avg       0.55      0.62      0.57       739\n",
      "weighted avg       0.99      0.98      0.99       739\n",
      "\n",
      "acc:  0.9837618403247632\n",
      "pre:  0.1\n",
      "rec:  0.25\n",
      "ma F1:  0.5673302107728337\n",
      "mi F1:  0.9837618403247631\n",
      "we F1:  0.9872081710520895\n",
      "Loss:  0.04541958495974541\n",
      "Loss:  0.06586987525224686\n",
      "Loss:  0.05155787989497185\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.05589737370610237\n",
      "Loss:  0.06110811233520508\n",
      "Loss:  0.08863366395235062\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.05539500340819359\n",
      "Loss:  0.05056536942720413\n",
      "Loss:  0.041860807687044144\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.005346059799194336\n",
      "Eval Loss:  0.047239065170288086\n",
      "Eval Loss:  0.005783796310424805\n",
      "[[17936  1493]\n",
      " [ 1100 11623]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     19429\n",
      "           1       0.89      0.91      0.90     12723\n",
      "\n",
      "    accuracy                           0.92     32152\n",
      "   macro avg       0.91      0.92      0.92     32152\n",
      "weighted avg       0.92      0.92      0.92     32152\n",
      "\n",
      "acc:  0.9193518288131376\n",
      "pre:  0.8861695638914303\n",
      "rec:  0.9135424035211821\n",
      "ma F1:  0.916117943131006\n",
      "mi F1:  0.9193518288131376\n",
      "we F1:  0.9195531462649159\n",
      "[[721  14]\n",
      " [  3   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       735\n",
      "           1       0.07      0.25      0.11         4\n",
      "\n",
      "    accuracy                           0.98       739\n",
      "   macro avg       0.53      0.62      0.55       739\n",
      "weighted avg       0.99      0.98      0.98       739\n",
      "\n",
      "acc:  0.9769959404600812\n",
      "pre:  0.06666666666666667\n",
      "rec:  0.25\n",
      "ma F1:  0.546805670791097\n",
      "mi F1:  0.9769959404600812\n",
      "we F1:  0.9835682918022461\n",
      "Loss:  0.05418747290968895\n",
      "Loss:  0.06908358633518219\n",
      "Loss:  0.08827980607748032\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.07500576972961426\n",
      "Loss:  0.07320237159729004\n",
      "Loss:  0.05223851278424263\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.03756515681743622\n",
      "Loss:  0.05818983539938927\n",
      "Loss:  0.05043132230639458\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  22 Training Time 5849.645237207413 Best Test Acc:  0.9945872801082544\n",
      "test subjects:  ['./seg\\\\c06']\n",
      "*********\n",
      "33845 468\n",
      "32423 468\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.7468750476837158\n",
      "Eval Loss:  0.630771815776825\n",
      "Eval Loss:  0.6436210870742798\n",
      "[[19694     3]\n",
      " [12725     1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      1.00      0.76     19697\n",
      "           1       0.25      0.00      0.00     12726\n",
      "\n",
      "    accuracy                           0.61     32423\n",
      "   macro avg       0.43      0.50      0.38     32423\n",
      "weighted avg       0.47      0.61      0.46     32423\n",
      "\n",
      "acc:  0.6074391635567344\n",
      "pre:  0.25\n",
      "rec:  7.857928650007858e-05\n",
      "ma F1:  0.3779663433743217\n",
      "mi F1:  0.6074391635567344\n",
      "we F1:  0.4591959696116446\n",
      "[[467   0]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      1.00      1.00       468\n",
      "\n",
      "acc:  0.9978632478632479\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4994652406417112\n",
      "mi F1:  0.9978632478632479\n",
      "we F1:  0.9967960144430733\n",
      "Subject 23 Current Train Acc:  0.6074391635567344 Current Test Acc:  0.9978632478632479\n",
      "Loss:  0.16708435118198395\n",
      "Loss:  0.16197936236858368\n",
      "Loss:  0.15013998746871948\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.13593201339244843\n",
      "Loss:  0.1094968244433403\n",
      "Loss:  0.12428484112024307\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.10626845061779022\n",
      "Loss:  0.1344108134508133\n",
      "Loss:  0.10123568028211594\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.5021771192550659\n",
      "Eval Loss:  0.02855074405670166\n",
      "Eval Loss:  0.018160581588745117\n",
      "[[18010  1687]\n",
      " [ 4742  7984]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.91      0.85     19697\n",
      "           1       0.83      0.63      0.71     12726\n",
      "\n",
      "    accuracy                           0.80     32423\n",
      "   macro avg       0.81      0.77      0.78     32423\n",
      "weighted avg       0.80      0.80      0.80     32423\n",
      "\n",
      "acc:  0.8017148320636586\n",
      "pre:  0.8255609554337711\n",
      "rec:  0.6273770234166274\n",
      "ma F1:  0.7807501482757591\n",
      "mi F1:  0.8017148320636586\n",
      "we F1:  0.7953267302312195\n",
      "[[452  15]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.97       468\n",
      "   macro avg       0.50      0.48      0.49       468\n",
      "weighted avg       1.00      0.97      0.98       468\n",
      "\n",
      "acc:  0.9658119658119658\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49130434782608695\n",
      "mi F1:  0.9658119658119658\n",
      "we F1:  0.9805091044221479\n",
      "Loss:  0.11401210725307465\n",
      "Loss:  0.09917441010475159\n",
      "Loss:  0.128618985414505\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.08896495401859283\n",
      "Loss:  0.09257311373949051\n",
      "Loss:  0.08081776648759842\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.10072778165340424\n",
      "Loss:  0.1040111631155014\n",
      "Loss:  0.08065535128116608\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  1.247435212135315\n",
      "Eval Loss:  0.014313220977783203\n",
      "Eval Loss:  0.011128902435302734\n",
      "[[19090   607]\n",
      " [ 4928  7798]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.97      0.87     19697\n",
      "           1       0.93      0.61      0.74     12726\n",
      "\n",
      "    accuracy                           0.83     32423\n",
      "   macro avg       0.86      0.79      0.81     32423\n",
      "weighted avg       0.85      0.83      0.82     32423\n",
      "\n",
      "acc:  0.8292878512167289\n",
      "pre:  0.9277810826888757\n",
      "rec:  0.6127612761276128\n",
      "ma F1:  0.8057234919678545\n",
      "mi F1:  0.8292878512167289\n",
      "we F1:  0.8202707066617887\n",
      "[[463   4]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.99       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      0.99      0.99       468\n",
      "\n",
      "acc:  0.9893162393162394\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49731471535982813\n",
      "mi F1:  0.9893162393162394\n",
      "we F1:  0.9925041541582894\n",
      "Loss:  0.1157616376876831\n",
      "Loss:  0.1497626155614853\n",
      "Loss:  0.08672401309013367\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.0912514179944992\n",
      "Loss:  0.06911998242139816\n",
      "Loss:  0.10316671431064606\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.08823136985301971\n",
      "Loss:  0.06945725530385971\n",
      "Loss:  0.09779923409223557\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  1.3147004842758179\n",
      "Eval Loss:  0.012023448944091797\n",
      "Eval Loss:  0.010416269302368164\n",
      "[[19398   299]\n",
      " [ 5998  6728]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.98      0.86     19697\n",
      "           1       0.96      0.53      0.68     12726\n",
      "\n",
      "    accuracy                           0.81     32423\n",
      "   macro avg       0.86      0.76      0.77     32423\n",
      "weighted avg       0.84      0.81      0.79     32423\n",
      "\n",
      "acc:  0.8057860160996824\n",
      "pre:  0.9574498363455244\n",
      "rec:  0.5286814395725287\n",
      "ma F1:  0.7707841230454175\n",
      "mi F1:  0.8057860160996824\n",
      "we F1:  0.7900420706761421\n",
      "[[466   1]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      1.00      1.00       468\n",
      "\n",
      "acc:  0.9957264957264957\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4989293361884368\n",
      "mi F1:  0.9957264957264957\n",
      "we F1:  0.9957264957264957\n",
      "Loss:  0.053897563368082047\n",
      "Loss:  0.06454719603061676\n",
      "Loss:  0.14936542510986328\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.08368637412786484\n",
      "Loss:  0.0859178751707077\n",
      "Loss:  0.11645001918077469\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.08330502361059189\n",
      "Loss:  0.08320990204811096\n",
      "Loss:  0.06596114486455917\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  1.0990207195281982\n",
      "Eval Loss:  0.021698951721191406\n",
      "Eval Loss:  0.008629560470581055\n",
      "[[18661  1036]\n",
      " [ 3110  9616]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90     19697\n",
      "           1       0.90      0.76      0.82     12726\n",
      "\n",
      "    accuracy                           0.87     32423\n",
      "   macro avg       0.88      0.85      0.86     32423\n",
      "weighted avg       0.88      0.87      0.87     32423\n",
      "\n",
      "acc:  0.8721278105048885\n",
      "pre:  0.9027412692452121\n",
      "rec:  0.7556184189847556\n",
      "ma F1:  0.8613365345197659\n",
      "mi F1:  0.8721278105048885\n",
      "we F1:  0.8696533929932087\n",
      "[[464   3]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.99       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      0.99      0.99       468\n",
      "\n",
      "acc:  0.9914529914529915\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4978540772532189\n",
      "mi F1:  0.9914529914529915\n",
      "we F1:  0.9935805729797147\n",
      "Loss:  0.04831140860915184\n",
      "Loss:  0.101299948990345\n",
      "Loss:  0.0860712006688118\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.06449837237596512\n",
      "Loss:  0.05922403559088707\n",
      "Loss:  0.04732116684317589\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.08144677430391312\n",
      "Loss:  0.09857243299484253\n",
      "Loss:  0.07429159432649612\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.764790952205658\n",
      "Eval Loss:  0.009888410568237305\n",
      "Eval Loss:  0.00700831413269043\n",
      "[[19320   377]\n",
      " [ 5160  7566]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.98      0.87     19697\n",
      "           1       0.95      0.59      0.73     12726\n",
      "\n",
      "    accuracy                           0.83     32423\n",
      "   macro avg       0.87      0.79      0.80     32423\n",
      "weighted avg       0.85      0.83      0.82     32423\n",
      "\n",
      "acc:  0.8292261666101225\n",
      "pre:  0.9525368248772504\n",
      "rec:  0.5945308816595946\n",
      "ma F1:  0.8033870885156758\n",
      "mi F1:  0.8292261666101225\n",
      "we F1:  0.8187115919524962\n",
      "[[466   1]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      1.00      1.00       468\n",
      "\n",
      "acc:  0.9957264957264957\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4989293361884368\n",
      "mi F1:  0.9957264957264957\n",
      "we F1:  0.9957264957264957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.07913488149642944\n",
      "Loss:  0.06977681070566177\n",
      "Loss:  0.07395441830158234\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.08688519150018692\n",
      "Loss:  0.07464957237243652\n",
      "Loss:  0.08427150547504425\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.06708956509828568\n",
      "Loss:  0.07986661046743393\n",
      "Loss:  0.08787669241428375\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.6586238741874695\n",
      "Eval Loss:  0.015418767929077148\n",
      "Eval Loss:  0.005075931549072266\n",
      "[[18973   724]\n",
      " [ 3437  9289]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90     19697\n",
      "           1       0.93      0.73      0.82     12726\n",
      "\n",
      "    accuracy                           0.87     32423\n",
      "   macro avg       0.89      0.85      0.86     32423\n",
      "weighted avg       0.88      0.87      0.87     32423\n",
      "\n",
      "acc:  0.8716651759553403\n",
      "pre:  0.9276939978028563\n",
      "rec:  0.72992299229923\n",
      "ma F1:  0.8590953744667333\n",
      "mi F1:  0.8716651759553403\n",
      "we F1:  0.8681437105031935\n",
      "[[465   2]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.99       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      0.99      0.99       468\n",
      "\n",
      "acc:  0.9935897435897436\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4983922829581994\n",
      "mi F1:  0.9935897435897436\n",
      "we F1:  0.9946546843652954\n",
      "Loss:  0.06881137937307358\n",
      "Loss:  0.05830996483564377\n",
      "Loss:  0.0739101693034172\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.10163163393735886\n",
      "Loss:  0.05322137475013733\n",
      "Loss:  0.0864911824464798\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.10227742791175842\n",
      "Loss:  0.06985556334257126\n",
      "Loss:  0.10439687967300415\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.6181389689445496\n",
      "Eval Loss:  0.010126829147338867\n",
      "Eval Loss:  0.002920866012573242\n",
      "[[19224   473]\n",
      " [ 4036  8690]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.98      0.90     19697\n",
      "           1       0.95      0.68      0.79     12726\n",
      "\n",
      "    accuracy                           0.86     32423\n",
      "   macro avg       0.89      0.83      0.84     32423\n",
      "weighted avg       0.87      0.86      0.86     32423\n",
      "\n",
      "acc:  0.860932054405823\n",
      "pre:  0.9483793517406963\n",
      "rec:  0.6828539996856828\n",
      "ma F1:  0.8445203456247551\n",
      "mi F1:  0.860932054405823\n",
      "we F1:  0.8553809894364909\n",
      "[[466   1]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      1.00      1.00       468\n",
      "\n",
      "acc:  0.9957264957264957\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4989293361884368\n",
      "mi F1:  0.9957264957264957\n",
      "we F1:  0.9957264957264957\n",
      "Loss:  0.06312625110149384\n",
      "Loss:  0.0890883132815361\n",
      "Loss:  0.07587672770023346\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.053664155304431915\n",
      "Loss:  0.060674142092466354\n",
      "Loss:  0.06436018645763397\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.11598572880029678\n",
      "Loss:  0.09150021523237228\n",
      "Loss:  0.09150053560733795\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.5934140682220459\n",
      "Eval Loss:  0.02232980728149414\n",
      "Eval Loss:  0.0027818679809570312\n",
      "[[18765   932]\n",
      " [ 2628 10098]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91     19697\n",
      "           1       0.92      0.79      0.85     12726\n",
      "\n",
      "    accuracy                           0.89     32423\n",
      "   macro avg       0.90      0.87      0.88     32423\n",
      "weighted avg       0.89      0.89      0.89     32423\n",
      "\n",
      "acc:  0.8902014002405699\n",
      "pre:  0.9155031731640979\n",
      "rec:  0.7934936350777935\n",
      "ma F1:  0.8817520184010794\n",
      "mi F1:  0.8902014002405699\n",
      "we F1:  0.888547984802728\n",
      "[[466   1]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      1.00      1.00       468\n",
      "\n",
      "acc:  0.9957264957264957\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4989293361884368\n",
      "mi F1:  0.9957264957264957\n",
      "we F1:  0.9957264957264957\n",
      "Loss:  0.07475395500659943\n",
      "Loss:  0.05481821671128273\n",
      "Loss:  0.07639220356941223\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.05759894847869873\n",
      "Loss:  0.026530887931585312\n",
      "Loss:  0.08416160196065903\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.10300463438034058\n",
      "Loss:  0.04852094128727913\n",
      "Loss:  0.054104916751384735\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.8894747495651245\n",
      "Eval Loss:  0.028034210205078125\n",
      "Eval Loss:  0.0021865367889404297\n",
      "[[18693  1004]\n",
      " [ 2280 10446]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     19697\n",
      "           1       0.91      0.82      0.86     12726\n",
      "\n",
      "    accuracy                           0.90     32423\n",
      "   macro avg       0.90      0.88      0.89     32423\n",
      "weighted avg       0.90      0.90      0.90     32423\n",
      "\n",
      "acc:  0.8987138759522562\n",
      "pre:  0.9123144104803493\n",
      "rec:  0.8208392267798208\n",
      "ma F1:  0.8917076631869527\n",
      "mi F1:  0.8987138759522562\n",
      "we F1:  0.8976298541881568\n",
      "[[467   0]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      1.00      1.00       468\n",
      "\n",
      "acc:  0.9978632478632479\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4994652406417112\n",
      "mi F1:  0.9978632478632479\n",
      "we F1:  0.9967960144430733\n",
      "Loss:  0.0611344538629055\n",
      "Loss:  0.05447927117347717\n",
      "Loss:  0.048895180225372314\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.0669701099395752\n",
      "Loss:  0.04655631631612778\n",
      "Loss:  0.07046887278556824\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.03809824585914612\n",
      "Loss:  0.08582952618598938\n",
      "Loss:  0.03288742154836655\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.6573202013969421\n",
      "Eval Loss:  0.027219533920288086\n",
      "Eval Loss:  0.002337217330932617\n",
      "[[18864   833]\n",
      " [ 2614 10112]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     19697\n",
      "           1       0.92      0.79      0.85     12726\n",
      "\n",
      "    accuracy                           0.89     32423\n",
      "   macro avg       0.90      0.88      0.89     32423\n",
      "weighted avg       0.90      0.89      0.89     32423\n",
      "\n",
      "acc:  0.8936865805138328\n",
      "pre:  0.9238921882137963\n",
      "rec:  0.7945937450887945\n",
      "ma F1:  0.8853314643612933\n",
      "mi F1:  0.8936865805138328\n",
      "we F1:  0.8919863449256618\n",
      "[[467   0]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      1.00      1.00       468\n",
      "\n",
      "acc:  0.9978632478632479\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4994652406417112\n",
      "mi F1:  0.9978632478632479\n",
      "we F1:  0.9967960144430733\n",
      "Loss:  0.08615893125534058\n",
      "Loss:  0.0715431198477745\n",
      "Loss:  0.09301629662513733\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.07475283741950989\n",
      "Loss:  0.09938158094882965\n",
      "Loss:  0.075994111597538\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.05070806294679642\n",
      "Loss:  0.0947638526558876\n",
      "Loss:  0.06903565675020218\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.6666949987411499\n",
      "Eval Loss:  0.0306779146194458\n",
      "Eval Loss:  0.0019865036010742188\n",
      "[[18950   747]\n",
      " [ 2669 10057]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     19697\n",
      "           1       0.93      0.79      0.85     12726\n",
      "\n",
      "    accuracy                           0.89     32423\n",
      "   macro avg       0.90      0.88      0.89     32423\n",
      "weighted avg       0.90      0.89      0.89     32423\n",
      "\n",
      "acc:  0.8946426919162324\n",
      "pre:  0.9308589411329138\n",
      "rec:  0.7902718843312903\n",
      "ma F1:  0.8860718979653547\n",
      "mi F1:  0.8946426919162324\n",
      "we F1:  0.8927903287121858\n",
      "[[467   0]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      1.00      1.00       468\n",
      "\n",
      "acc:  0.9978632478632479\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4994652406417112\n",
      "mi F1:  0.9978632478632479\n",
      "we F1:  0.9967960144430733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.041967637836933136\n",
      "Loss:  0.056215256452560425\n",
      "Loss:  0.08471104502677917\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.06952866911888123\n",
      "Loss:  0.04727686941623688\n",
      "Loss:  0.06495624035596848\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.08378928899765015\n",
      "Loss:  0.037209536880254745\n",
      "Loss:  0.06974825263023376\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.8957897424697876\n",
      "Eval Loss:  0.038980841636657715\n",
      "Eval Loss:  0.0019562244415283203\n",
      "[[18530  1167]\n",
      " [ 1905 10821]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92     19697\n",
      "           1       0.90      0.85      0.88     12726\n",
      "\n",
      "    accuracy                           0.91     32423\n",
      "   macro avg       0.90      0.90      0.90     32423\n",
      "weighted avg       0.91      0.91      0.90     32423\n",
      "\n",
      "acc:  0.9052524442525368\n",
      "pre:  0.9026526526526526\n",
      "rec:  0.8503064592173503\n",
      "ma F1:  0.8995752956733433\n",
      "mi F1:  0.9052524442525369\n",
      "we F1:  0.9047089579830537\n",
      "[[466   1]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      1.00      1.00       468\n",
      "\n",
      "acc:  0.9957264957264957\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4989293361884368\n",
      "mi F1:  0.9957264957264957\n",
      "we F1:  0.9957264957264957\n",
      "Loss:  0.05245315656065941\n",
      "Loss:  0.05067621171474457\n",
      "Loss:  0.0696200579404831\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.05946100875735283\n",
      "Loss:  0.06929831951856613\n",
      "Loss:  0.04906407743692398\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.043953265994787216\n",
      "Loss:  0.04147247597575188\n",
      "Loss:  0.08426348865032196\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.5832158327102661\n",
      "Eval Loss:  0.03149676322937012\n",
      "Eval Loss:  0.0023345947265625\n",
      "[[18725   972]\n",
      " [ 1952 10774]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19697\n",
      "           1       0.92      0.85      0.88     12726\n",
      "\n",
      "    accuracy                           0.91     32423\n",
      "   macro avg       0.91      0.90      0.90     32423\n",
      "weighted avg       0.91      0.91      0.91     32423\n",
      "\n",
      "acc:  0.909817105141412\n",
      "pre:  0.9172484249957432\n",
      "rec:  0.8466132327518466\n",
      "ma F1:  0.9040468311383367\n",
      "mi F1:  0.909817105141412\n",
      "we F1:  0.9091058903856561\n",
      "[[467   0]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      1.00      1.00       468\n",
      "\n",
      "acc:  0.9978632478632479\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4994652406417112\n",
      "mi F1:  0.9978632478632479\n",
      "we F1:  0.9967960144430733\n",
      "Loss:  0.0597282238304615\n",
      "Loss:  0.07149362564086914\n",
      "Loss:  0.04866854101419449\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.08032291382551193\n",
      "Loss:  0.07634701579809189\n",
      "Loss:  0.05030268430709839\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.07863083481788635\n",
      "Loss:  0.04921051114797592\n",
      "Loss:  0.0595308318734169\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.5151033401489258\n",
      "Eval Loss:  0.03940248489379883\n",
      "Eval Loss:  0.0022487640380859375\n",
      "[[18569  1128]\n",
      " [ 1778 10948]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93     19697\n",
      "           1       0.91      0.86      0.88     12726\n",
      "\n",
      "    accuracy                           0.91     32423\n",
      "   macro avg       0.91      0.90      0.91     32423\n",
      "weighted avg       0.91      0.91      0.91     32423\n",
      "\n",
      "acc:  0.9103722666008698\n",
      "pre:  0.9065915866180855\n",
      "rec:  0.8602860286028603\n",
      "ma F1:  0.9051309284325586\n",
      "mi F1:  0.9103722666008698\n",
      "we F1:  0.909925229491645\n",
      "[[467   0]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      1.00      1.00       468\n",
      "\n",
      "acc:  0.9978632478632479\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4994652406417112\n",
      "mi F1:  0.9978632478632479\n",
      "we F1:  0.9967960144430733\n",
      "Loss:  0.07276606559753418\n",
      "Loss:  0.05468492582440376\n",
      "Loss:  0.07147523760795593\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.07236992567777634\n",
      "Loss:  0.06789924949407578\n",
      "Loss:  0.03370502218604088\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.06272696703672409\n",
      "Loss:  0.04246020317077637\n",
      "Loss:  0.05826341733336449\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.5141981244087219\n",
      "Eval Loss:  0.03646659851074219\n",
      "Eval Loss:  0.0018246173858642578\n",
      "[[18407  1290]\n",
      " [ 1620 11106]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93     19697\n",
      "           1       0.90      0.87      0.88     12726\n",
      "\n",
      "    accuracy                           0.91     32423\n",
      "   macro avg       0.91      0.90      0.91     32423\n",
      "weighted avg       0.91      0.91      0.91     32423\n",
      "\n",
      "acc:  0.9102488973876569\n",
      "pre:  0.8959341723136496\n",
      "rec:  0.8727015558698727\n",
      "ma F1:  0.9054549053864548\n",
      "mi F1:  0.9102488973876569\n",
      "we F1:  0.9100322123636332\n",
      "[[467   0]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      1.00      1.00       468\n",
      "\n",
      "acc:  0.9978632478632479\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4994652406417112\n",
      "mi F1:  0.9978632478632479\n",
      "we F1:  0.9967960144430733\n",
      "Loss:  0.07669060677289963\n",
      "Loss:  0.0585658922791481\n",
      "Loss:  0.07320591807365417\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.08929147571325302\n",
      "Loss:  0.04667603224515915\n",
      "Loss:  0.053517743945121765\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.04183097928762436\n",
      "Loss:  0.05894184857606888\n",
      "Loss:  0.06361525505781174\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.6025546193122864\n",
      "Eval Loss:  0.028325319290161133\n",
      "Eval Loss:  0.001558542251586914\n",
      "[[18912   785]\n",
      " [ 2104 10622]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     19697\n",
      "           1       0.93      0.83      0.88     12726\n",
      "\n",
      "    accuracy                           0.91     32423\n",
      "   macro avg       0.92      0.90      0.90     32423\n",
      "weighted avg       0.91      0.91      0.91     32423\n",
      "\n",
      "acc:  0.9108965857570244\n",
      "pre:  0.9311826071710353\n",
      "rec:  0.8346691812038347\n",
      "ma F1:  0.9046641330951382\n",
      "mi F1:  0.9108965857570244\n",
      "we F1:  0.9099049566784926\n",
      "[[467   0]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      1.00      1.00       468\n",
      "\n",
      "acc:  0.9978632478632479\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4994652406417112\n",
      "mi F1:  0.9978632478632479\n",
      "we F1:  0.9967960144430733\n",
      "Loss:  0.044640347361564636\n",
      "Loss:  0.05237223953008652\n",
      "Loss:  0.06639684736728668\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.06723248213529587\n",
      "Loss:  0.06669683754444122\n",
      "Loss:  0.05814659595489502\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.0827598124742508\n",
      "Loss:  0.05036325752735138\n",
      "Loss:  0.08022582530975342\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.3933921456336975\n",
      "Eval Loss:  0.034108757972717285\n",
      "Eval Loss:  0.0018248558044433594\n",
      "[[18686  1011]\n",
      " [ 1807 10919]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19697\n",
      "           1       0.92      0.86      0.89     12726\n",
      "\n",
      "    accuracy                           0.91     32423\n",
      "   macro avg       0.91      0.90      0.91     32423\n",
      "weighted avg       0.91      0.91      0.91     32423\n",
      "\n",
      "acc:  0.9130863892915523\n",
      "pre:  0.9152556580050294\n",
      "rec:  0.858007229294358\n",
      "ma F1:  0.9077951941935767\n",
      "mi F1:  0.9130863892915523\n",
      "we F1:  0.9125441218397705\n",
      "[[466   1]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      1.00      1.00       468\n",
      "\n",
      "acc:  0.9957264957264957\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4989293361884368\n",
      "mi F1:  0.9957264957264957\n",
      "we F1:  0.9957264957264957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.06113559380173683\n",
      "Loss:  0.07548842579126358\n",
      "Loss:  0.08613681048154831\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.04323119670152664\n",
      "Loss:  0.050237759947776794\n",
      "Loss:  0.04733185097575188\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.03403382748365402\n",
      "Loss:  0.07205779105424881\n",
      "Loss:  0.05832820385694504\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.5209461450576782\n",
      "Eval Loss:  0.04479658603668213\n",
      "Eval Loss:  0.0012745857238769531\n",
      "[[18667  1030]\n",
      " [ 1653 11073]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     19697\n",
      "           1       0.91      0.87      0.89     12726\n",
      "\n",
      "    accuracy                           0.92     32423\n",
      "   macro avg       0.92      0.91      0.91     32423\n",
      "weighted avg       0.92      0.92      0.92     32423\n",
      "\n",
      "acc:  0.9172501002374858\n",
      "pre:  0.9148971329422457\n",
      "rec:  0.8701084394153701\n",
      "ma F1:  0.912447185176877\n",
      "mi F1:  0.9172501002374858\n",
      "we F1:  0.9168560765236643\n",
      "[[466   1]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      1.00      1.00       468\n",
      "\n",
      "acc:  0.9957264957264957\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4989293361884368\n",
      "mi F1:  0.9957264957264957\n",
      "we F1:  0.9957264957264957\n",
      "Loss:  0.07525744289159775\n",
      "Loss:  0.04175541549921036\n",
      "Loss:  0.06660804897546768\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.06065310165286064\n",
      "Loss:  0.0459427535533905\n",
      "Loss:  0.09189433604478836\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.052312131971120834\n",
      "Loss:  0.05378670617938042\n",
      "Loss:  0.06432630866765976\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.4180639684200287\n",
      "Eval Loss:  0.04371321201324463\n",
      "Eval Loss:  0.0015935897827148438\n",
      "[[18344  1353]\n",
      " [ 1275 11451]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93     19697\n",
      "           1       0.89      0.90      0.90     12726\n",
      "\n",
      "    accuracy                           0.92     32423\n",
      "   macro avg       0.91      0.92      0.92     32423\n",
      "weighted avg       0.92      0.92      0.92     32423\n",
      "\n",
      "acc:  0.9189464269191623\n",
      "pre:  0.8943298969072165\n",
      "rec:  0.8998114097123998\n",
      "ma F1:  0.9151096320523968\n",
      "mi F1:  0.9189464269191623\n",
      "we F1:  0.9189898434286079\n",
      "[[467   0]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       467\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       468\n",
      "   macro avg       0.50      0.50      0.50       468\n",
      "weighted avg       1.00      1.00      1.00       468\n",
      "\n",
      "acc:  0.9978632478632479\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4994652406417112\n",
      "mi F1:  0.9978632478632479\n",
      "we F1:  0.9967960144430733\n",
      "Loss:  0.050093501806259155\n",
      "Loss:  0.0779934972524643\n",
      "Loss:  0.03144458308815956\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.07069165259599686\n",
      "Loss:  0.06610353291034698\n",
      "Loss:  0.04550712928175926\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.10610811412334442\n",
      "Loss:  0.08300606161355972\n",
      "Loss:  0.0513516329228878\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  23 Training Time 5875.03933930397 Best Test Acc:  0.9978632478632479\n",
      "test subjects:  ['./seg\\\\c07', './seg\\\\x34']\n",
      "*********\n",
      "33409 904\n",
      "32303 588\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.8737848401069641\n",
      "Eval Loss:  0.5588791966438293\n",
      "Eval Loss:  0.5713838338851929\n",
      "[[19578     0]\n",
      " [12725     0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      1.00      0.75     19578\n",
      "           1       0.00      0.00      0.00     12725\n",
      "\n",
      "    accuracy                           0.61     32303\n",
      "   macro avg       0.30      0.50      0.38     32303\n",
      "weighted avg       0.37      0.61      0.46     32303\n",
      "\n",
      "acc:  0.6060737392811814\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.37736358204352266\n",
      "mi F1:  0.6060737392811814\n",
      "we F1:  0.4574203144753173\n",
      "[[586   0]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       586\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           1.00       588\n",
      "   macro avg       0.50      0.50      0.50       588\n",
      "weighted avg       0.99      1.00      0.99       588\n",
      "\n",
      "acc:  0.9965986394557823\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4991482112436116\n",
      "mi F1:  0.9965986394557823\n",
      "we F1:  0.9949008564243415\n",
      "Subject 24 Current Train Acc:  0.6060737392811814 Current Test Acc:  0.9965986394557823\n",
      "Loss:  0.17131482064723969\n",
      "Loss:  0.16927440464496613\n",
      "Loss:  0.1685566008090973\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.1473357379436493\n",
      "Loss:  0.13885557651519775\n",
      "Loss:  0.12101893872022629\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.11053013801574707\n",
      "Loss:  0.11222455650568008\n",
      "Loss:  0.10054837167263031\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.2935119867324829\n",
      "Eval Loss:  0.04468369483947754\n",
      "Eval Loss:  0.2410488724708557\n",
      "[[17264  2314]\n",
      " [ 3220  9505]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86     19578\n",
      "           1       0.80      0.75      0.77     12725\n",
      "\n",
      "    accuracy                           0.83     32303\n",
      "   macro avg       0.82      0.81      0.82     32303\n",
      "weighted avg       0.83      0.83      0.83     32303\n",
      "\n",
      "acc:  0.8286846422932854\n",
      "pre:  0.8042135544462307\n",
      "rec:  0.7469548133595285\n",
      "ma F1:  0.8181957450143932\n",
      "mi F1:  0.8286846422932854\n",
      "we F1:  0.8274598786723707\n",
      "[[522  64]\n",
      " [  1   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94       586\n",
      "           1       0.02      0.50      0.03         2\n",
      "\n",
      "    accuracy                           0.89       588\n",
      "   macro avg       0.51      0.70      0.49       588\n",
      "weighted avg       0.99      0.89      0.94       588\n",
      "\n",
      "acc:  0.8894557823129252\n",
      "pre:  0.015384615384615385\n",
      "rec:  0.5\n",
      "ma F1:  0.48561969234082064\n",
      "mi F1:  0.8894557823129253\n",
      "we F1:  0.9382881693920855\n",
      "Loss:  0.09858532249927521\n",
      "Loss:  0.12992973625659943\n",
      "Loss:  0.08788375556468964\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.10663394629955292\n",
      "Loss:  0.08427561074495316\n",
      "Loss:  0.11608103662729263\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.09229827672243118\n",
      "Loss:  0.0907028466463089\n",
      "Loss:  0.0840686783194542\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.12288987636566162\n",
      "Eval Loss:  0.017309188842773438\n",
      "Eval Loss:  0.08264613151550293\n",
      "[[18922   656]\n",
      " [ 4998  7727]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.97      0.87     19578\n",
      "           1       0.92      0.61      0.73     12725\n",
      "\n",
      "    accuracy                           0.82     32303\n",
      "   macro avg       0.86      0.79      0.80     32303\n",
      "weighted avg       0.84      0.82      0.82     32303\n",
      "\n",
      "acc:  0.8249698170448565\n",
      "pre:  0.9217463915066205\n",
      "rec:  0.6072298624754421\n",
      "ma F1:  0.8010782427309744\n",
      "mi F1:  0.8249698170448566\n",
      "we F1:  0.8157034288652338\n",
      "[[564  22]\n",
      " [  1   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       586\n",
      "           1       0.04      0.50      0.08         2\n",
      "\n",
      "    accuracy                           0.96       588\n",
      "   macro avg       0.52      0.73      0.53       588\n",
      "weighted avg       0.99      0.96      0.98       588\n",
      "\n",
      "acc:  0.9608843537414966\n",
      "pre:  0.043478260869565216\n",
      "rec:  0.5\n",
      "ma F1:  0.5300086880973067\n",
      "mi F1:  0.9608843537414966\n",
      "we F1:  0.9769560926021146\n",
      "Loss:  0.10137799382209778\n",
      "Loss:  0.05542351305484772\n",
      "Loss:  0.102171391248703\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.08178296685218811\n",
      "Loss:  0.09653282165527344\n",
      "Loss:  0.061779484152793884\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.11590442061424255\n",
      "Loss:  0.09017401188611984\n",
      "Loss:  0.10234086215496063\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.0672522783279419\n",
      "Eval Loss:  0.01623666286468506\n",
      "Eval Loss:  0.06975531578063965\n",
      "[[18719   859]\n",
      " [ 3847  8878]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89     19578\n",
      "           1       0.91      0.70      0.79     12725\n",
      "\n",
      "    accuracy                           0.85     32303\n",
      "   macro avg       0.87      0.83      0.84     32303\n",
      "weighted avg       0.86      0.85      0.85     32303\n",
      "\n",
      "acc:  0.8543169365074451\n",
      "pre:  0.9117798089760707\n",
      "rec:  0.6976817288801572\n",
      "ma F1:  0.8394129189721719\n",
      "mi F1:  0.8543169365074451\n",
      "we F1:  0.8497916642388346\n",
      "[[563  23]\n",
      " [  1   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       586\n",
      "           1       0.04      0.50      0.08         2\n",
      "\n",
      "    accuracy                           0.96       588\n",
      "   macro avg       0.52      0.73      0.53       588\n",
      "weighted avg       0.99      0.96      0.98       588\n",
      "\n",
      "acc:  0.9591836734693877\n",
      "pre:  0.041666666666666664\n",
      "rec:  0.5\n",
      "ma F1:  0.5280267558528428\n",
      "mi F1:  0.9591836734693877\n",
      "we F1:  0.9760617022728824\n",
      "Loss:  0.07604188472032547\n",
      "Loss:  0.09795119613409042\n",
      "Loss:  0.07156684994697571\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.07340023666620255\n",
      "Loss:  0.06110435724258423\n",
      "Loss:  0.08342831581830978\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.07439785450696945\n",
      "Loss:  0.11430338770151138\n",
      "Loss:  0.08828282356262207\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.044946908950805664\n",
      "Eval Loss:  0.010437965393066406\n",
      "Eval Loss:  0.061473846435546875\n",
      "[[18763   815]\n",
      " [ 3615  9110]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.89     19578\n",
      "           1       0.92      0.72      0.80     12725\n",
      "\n",
      "    accuracy                           0.86     32303\n",
      "   macro avg       0.88      0.84      0.85     32303\n",
      "weighted avg       0.87      0.86      0.86     32303\n",
      "\n",
      "acc:  0.8628610345788317\n",
      "pre:  0.9178841309823678\n",
      "rec:  0.7159135559921415\n",
      "ma F1:  0.8494141029065032\n",
      "mi F1:  0.8628610345788317\n",
      "we F1:  0.8589605467841026\n",
      "[[565  21]\n",
      " [  1   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       586\n",
      "           1       0.05      0.50      0.08         2\n",
      "\n",
      "    accuracy                           0.96       588\n",
      "   macro avg       0.52      0.73      0.53       588\n",
      "weighted avg       0.99      0.96      0.98       588\n",
      "\n",
      "acc:  0.9625850340136054\n",
      "pre:  0.045454545454545456\n",
      "rec:  0.5\n",
      "ma F1:  0.5321180555555556\n",
      "mi F1:  0.9625850340136054\n",
      "we F1:  0.977849820483749\n",
      "Loss:  0.12412092834711075\n",
      "Loss:  0.09627986699342728\n",
      "Loss:  0.07002593576908112\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.09412837773561478\n",
      "Loss:  0.07801353186368942\n",
      "Loss:  0.07417546212673187\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.06497646123170853\n",
      "Loss:  0.07100676745176315\n",
      "Loss:  0.09730309247970581\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.0390239953994751\n",
      "Eval Loss:  0.007688999176025391\n",
      "Eval Loss:  0.050429701805114746\n",
      "[[18850   728]\n",
      " [ 3623  9102]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90     19578\n",
      "           1       0.93      0.72      0.81     12725\n",
      "\n",
      "    accuracy                           0.87     32303\n",
      "   macro avg       0.88      0.84      0.85     32303\n",
      "weighted avg       0.87      0.87      0.86     32303\n",
      "\n",
      "acc:  0.8653066278673808\n",
      "pre:  0.925940996948118\n",
      "rec:  0.7152848722986248\n",
      "ma F1:  0.8518120871700582\n",
      "mi F1:  0.8653066278673808\n",
      "we F1:  0.8612989652372259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[572  14]\n",
      " [  1   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       586\n",
      "           1       0.07      0.50      0.12         2\n",
      "\n",
      "    accuracy                           0.97       588\n",
      "   macro avg       0.53      0.74      0.55       588\n",
      "weighted avg       1.00      0.97      0.98       588\n",
      "\n",
      "acc:  0.9744897959183674\n",
      "pre:  0.06666666666666667\n",
      "rec:  0.5\n",
      "ma F1:  0.5523524336395472\n",
      "mi F1:  0.9744897959183674\n",
      "we F1:  0.9841006290350479\n",
      "Loss:  0.0755578801035881\n",
      "Loss:  0.07162223011255264\n",
      "Loss:  0.07034604996442795\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.07450710237026215\n",
      "Loss:  0.0983864814043045\n",
      "Loss:  0.07681591063737869\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.05798587575554848\n",
      "Loss:  0.06320494413375854\n",
      "Loss:  0.0736866220831871\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.04550802707672119\n",
      "Eval Loss:  0.006439208984375\n",
      "Eval Loss:  0.046032071113586426\n",
      "[[19070   508]\n",
      " [ 4100  8625]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89     19578\n",
      "           1       0.94      0.68      0.79     12725\n",
      "\n",
      "    accuracy                           0.86     32303\n",
      "   macro avg       0.88      0.83      0.84     32303\n",
      "weighted avg       0.87      0.86      0.85     32303\n",
      "\n",
      "acc:  0.8573507104603287\n",
      "pre:  0.9443775320267163\n",
      "rec:  0.6777996070726916\n",
      "ma F1:  0.8406951105754408\n",
      "mi F1:  0.8573507104603287\n",
      "we F1:  0.8516229062682257\n",
      "[[571  15]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99       586\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.97       588\n",
      "   macro avg       0.50      0.49      0.49       588\n",
      "weighted avg       0.99      0.97      0.98       588\n",
      "\n",
      "acc:  0.9710884353741497\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49266609145815354\n",
      "mi F1:  0.9710884353741497\n",
      "we F1:  0.9819807129063877\n",
      "Loss:  0.0824638158082962\n",
      "Loss:  0.07874590158462524\n",
      "Loss:  0.05072111636400223\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.0851636454463005\n",
      "Loss:  0.081796795129776\n",
      "Loss:  0.06486620008945465\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.07860621064901352\n",
      "Loss:  0.06688840687274933\n",
      "Loss:  0.06079148128628731\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.029291749000549316\n",
      "Eval Loss:  0.0043871402740478516\n",
      "Eval Loss:  0.06224393844604492\n",
      "[[18853   725]\n",
      " [ 3183  9542]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91     19578\n",
      "           1       0.93      0.75      0.83     12725\n",
      "\n",
      "    accuracy                           0.88     32303\n",
      "   macro avg       0.89      0.86      0.87     32303\n",
      "weighted avg       0.88      0.88      0.88     32303\n",
      "\n",
      "acc:  0.8790205244094975\n",
      "pre:  0.9293854095646246\n",
      "rec:  0.7498624754420432\n",
      "ma F1:  0.8680585663201043\n",
      "mi F1:  0.8790205244094975\n",
      "we F1:  0.8761266899144134\n",
      "[[572  14]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       586\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.97       588\n",
      "   macro avg       0.50      0.49      0.49       588\n",
      "weighted avg       0.99      0.97      0.98       588\n",
      "\n",
      "acc:  0.9727891156462585\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.493103448275862\n",
      "mi F1:  0.9727891156462585\n",
      "we F1:  0.9828524513253575\n",
      "Loss:  0.0770467221736908\n",
      "Loss:  0.07569028437137604\n",
      "Loss:  0.059458136558532715\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.05866604298353195\n",
      "Loss:  0.07558377832174301\n",
      "Loss:  0.06060712784528732\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.043913353234529495\n",
      "Loss:  0.06857046484947205\n",
      "Loss:  0.0670020654797554\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.023916244506835938\n",
      "Eval Loss:  0.0040132999420166016\n",
      "Eval Loss:  0.045355916023254395\n",
      "[[18848   730]\n",
      " [ 3210  9515]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.91     19578\n",
      "           1       0.93      0.75      0.83     12725\n",
      "\n",
      "    accuracy                           0.88     32303\n",
      "   macro avg       0.89      0.86      0.87     32303\n",
      "weighted avg       0.88      0.88      0.88     32303\n",
      "\n",
      "acc:  0.8780299043432499\n",
      "pre:  0.928745729624207\n",
      "rec:  0.7477406679764244\n",
      "ma F1:  0.8669211362375072\n",
      "mi F1:  0.8780299043432499\n",
      "we F1:  0.8750780405371595\n",
      "[[571  15]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99       586\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.97       588\n",
      "   macro avg       0.50      0.49      0.49       588\n",
      "weighted avg       0.99      0.97      0.98       588\n",
      "\n",
      "acc:  0.9710884353741497\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49266609145815354\n",
      "mi F1:  0.9710884353741497\n",
      "we F1:  0.9819807129063877\n",
      "Loss:  0.09153352677822113\n",
      "Loss:  0.07795345038175583\n",
      "Loss:  0.07441294193267822\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.04028008133172989\n",
      "Loss:  0.06492669880390167\n",
      "Loss:  0.04268493875861168\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.05236344039440155\n",
      "Loss:  0.09311307221651077\n",
      "Loss:  0.051473114639520645\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.018710851669311523\n",
      "Eval Loss:  0.004085540771484375\n",
      "Eval Loss:  0.04074430465698242\n",
      "[[18336  1242]\n",
      " [ 1934 10791]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     19578\n",
      "           1       0.90      0.85      0.87     12725\n",
      "\n",
      "    accuracy                           0.90     32303\n",
      "   macro avg       0.90      0.89      0.90     32303\n",
      "weighted avg       0.90      0.90      0.90     32303\n",
      "\n",
      "acc:  0.9016809584249141\n",
      "pre:  0.8967838444278234\n",
      "rec:  0.8480157170923379\n",
      "ma F1:  0.8960076807905261\n",
      "mi F1:  0.9016809584249141\n",
      "we F1:  0.9011606260030458\n",
      "[[565  21]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       586\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.96       588\n",
      "   macro avg       0.50      0.48      0.49       588\n",
      "weighted avg       0.99      0.96      0.98       588\n",
      "\n",
      "acc:  0.9608843537414966\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4900260190806592\n",
      "mi F1:  0.9608843537414966\n",
      "we F1:  0.9767185278274364\n",
      "Loss:  0.0996236503124237\n",
      "Loss:  0.08080743253231049\n",
      "Loss:  0.06948959082365036\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.04468991607427597\n",
      "Loss:  0.07009643316268921\n",
      "Loss:  0.06378510594367981\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.06829829514026642\n",
      "Loss:  0.048265572637319565\n",
      "Loss:  0.07295728474855423\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.023395061492919922\n",
      "Eval Loss:  0.0031125545501708984\n",
      "Eval Loss:  0.034305453300476074\n",
      "[[18461  1117]\n",
      " [ 2007 10718]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     19578\n",
      "           1       0.91      0.84      0.87     12725\n",
      "\n",
      "    accuracy                           0.90     32303\n",
      "   macro avg       0.90      0.89      0.90     32303\n",
      "weighted avg       0.90      0.90      0.90     32303\n",
      "\n",
      "acc:  0.9032907160325666\n",
      "pre:  0.9056189269117025\n",
      "rec:  0.842278978388998\n",
      "ma F1:  0.897395507381495\n",
      "mi F1:  0.9032907160325666\n",
      "we F1:  0.9026131058427882\n",
      "[[560  26]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       586\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.95       588\n",
      "   macro avg       0.50      0.48      0.49       588\n",
      "weighted avg       0.99      0.95      0.97       588\n",
      "\n",
      "acc:  0.9523809523809523\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48780487804878053\n",
      "mi F1:  0.9523809523809523\n",
      "we F1:  0.9722913555666169\n",
      "Loss:  0.07296479493379593\n",
      "Loss:  0.08734280616044998\n",
      "Loss:  0.07753987610340118\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.04921966418623924\n",
      "Loss:  0.11723677068948746\n",
      "Loss:  0.05042922869324684\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.077008917927742\n",
      "Loss:  0.07953984290361404\n",
      "Loss:  0.04212021455168724\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.018948674201965332\n",
      "Eval Loss:  0.003597736358642578\n",
      "Eval Loss:  0.04293930530548096\n",
      "[[18546  1032]\n",
      " [ 2015 10710]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     19578\n",
      "           1       0.91      0.84      0.88     12725\n",
      "\n",
      "    accuracy                           0.91     32303\n",
      "   macro avg       0.91      0.89      0.90     32303\n",
      "weighted avg       0.91      0.91      0.90     32303\n",
      "\n",
      "acc:  0.9056743955669752\n",
      "pre:  0.9121103730199285\n",
      "rec:  0.8416502946954814\n",
      "ma F1:  0.8997768516859467\n",
      "mi F1:  0.9056743955669752\n",
      "we F1:  0.9049345684057897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[561  25]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       586\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.95       588\n",
      "   macro avg       0.50      0.48      0.49       588\n",
      "weighted avg       0.99      0.95      0.97       588\n",
      "\n",
      "acc:  0.9540816326530612\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48825065274151436\n",
      "mi F1:  0.9540816326530612\n",
      "we F1:  0.9731798724711817\n",
      "Loss:  0.0509776696562767\n",
      "Loss:  0.055700816214084625\n",
      "Loss:  0.06801546365022659\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.03182016313076019\n",
      "Loss:  0.09394167363643646\n",
      "Loss:  0.060670845210552216\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.05108572542667389\n",
      "Loss:  0.06112341955304146\n",
      "Loss:  0.036058396100997925\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.016246318817138672\n",
      "Eval Loss:  0.003046751022338867\n",
      "Eval Loss:  0.07839381694793701\n",
      "[[18448  1130]\n",
      " [ 1867 10858]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92     19578\n",
      "           1       0.91      0.85      0.88     12725\n",
      "\n",
      "    accuracy                           0.91     32303\n",
      "   macro avg       0.91      0.90      0.90     32303\n",
      "weighted avg       0.91      0.91      0.91     32303\n",
      "\n",
      "acc:  0.9072222394204873\n",
      "pre:  0.905739072405739\n",
      "rec:  0.8532809430255403\n",
      "ma F1:  0.901800916569697\n",
      "mi F1:  0.9072222394204873\n",
      "we F1:  0.9066958211146857\n",
      "[[559  27]\n",
      " [  1   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.98       586\n",
      "           1       0.04      0.50      0.07         2\n",
      "\n",
      "    accuracy                           0.95       588\n",
      "   macro avg       0.52      0.73      0.52       588\n",
      "weighted avg       0.99      0.95      0.97       588\n",
      "\n",
      "acc:  0.9523809523809523\n",
      "pre:  0.03571428571428571\n",
      "rec:  0.5\n",
      "ma F1:  0.5211169284467714\n",
      "mi F1:  0.9523809523809523\n",
      "we F1:  0.9724756918474194\n",
      "Loss:  0.04313094913959503\n",
      "Loss:  0.08737742900848389\n",
      "Loss:  0.03792048245668411\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.08137384057044983\n",
      "Loss:  0.05367092043161392\n",
      "Loss:  0.08997700363397598\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.05517314746975899\n",
      "Loss:  0.07072915881872177\n",
      "Loss:  0.04282304644584656\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.013896465301513672\n",
      "Eval Loss:  0.0027265548706054688\n",
      "Eval Loss:  0.01936817169189453\n",
      "[[18456  1122]\n",
      " [ 1784 10941]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93     19578\n",
      "           1       0.91      0.86      0.88     12725\n",
      "\n",
      "    accuracy                           0.91     32303\n",
      "   macro avg       0.91      0.90      0.90     32303\n",
      "weighted avg       0.91      0.91      0.91     32303\n",
      "\n",
      "acc:  0.9100393152338792\n",
      "pre:  0.906988311365332\n",
      "rec:  0.859803536345776\n",
      "ma F1:  0.9048918930172141\n",
      "mi F1:  0.9100393152338792\n",
      "we F1:  0.9095858763107344\n",
      "[[564  22]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       586\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.96       588\n",
      "   macro avg       0.50      0.48      0.49       588\n",
      "weighted avg       0.99      0.96      0.98       588\n",
      "\n",
      "acc:  0.9591836734693877\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48958333333333326\n",
      "mi F1:  0.9591836734693877\n",
      "we F1:  0.9758361678004535\n",
      "Loss:  0.047571927309036255\n",
      "Loss:  0.052195388823747635\n",
      "Loss:  0.059722885489463806\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.08571472018957138\n",
      "Loss:  0.07463403046131134\n",
      "Loss:  0.09115126729011536\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.07346212863922119\n",
      "Loss:  0.0652056410908699\n",
      "Loss:  0.0677686259150505\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.014449834823608398\n",
      "Eval Loss:  0.0030274391174316406\n",
      "Eval Loss:  0.022551655769348145\n",
      "[[18461  1117]\n",
      " [ 1624 11101]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19578\n",
      "           1       0.91      0.87      0.89     12725\n",
      "\n",
      "    accuracy                           0.92     32303\n",
      "   macro avg       0.91      0.91      0.91     32303\n",
      "weighted avg       0.91      0.92      0.91     32303\n",
      "\n",
      "acc:  0.915147199950469\n",
      "pre:  0.9085775085938779\n",
      "rec:  0.87237721021611\n",
      "ma F1:  0.9105011105728493\n",
      "mi F1:  0.915147199950469\n",
      "we F1:  0.914827150043614\n",
      "[[565  21]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       586\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.96       588\n",
      "   macro avg       0.50      0.48      0.49       588\n",
      "weighted avg       0.99      0.96      0.98       588\n",
      "\n",
      "acc:  0.9608843537414966\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4900260190806592\n",
      "mi F1:  0.9608843537414966\n",
      "we F1:  0.9767185278274364\n",
      "Loss:  0.03889098018407822\n",
      "Loss:  0.06176011264324188\n",
      "Loss:  0.05610819160938263\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.032220009714365005\n",
      "Loss:  0.04149380326271057\n",
      "Loss:  0.04754769057035446\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.03711623325943947\n",
      "Loss:  0.035856347531080246\n",
      "Loss:  0.07092221081256866\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.011733293533325195\n",
      "Eval Loss:  0.002317190170288086\n",
      "Eval Loss:  0.013410568237304688\n",
      "[[18563  1015]\n",
      " [ 1649 11076]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     19578\n",
      "           1       0.92      0.87      0.89     12725\n",
      "\n",
      "    accuracy                           0.92     32303\n",
      "   macro avg       0.92      0.91      0.91     32303\n",
      "weighted avg       0.92      0.92      0.92     32303\n",
      "\n",
      "acc:  0.9175308794848775\n",
      "pre:  0.9160532627574228\n",
      "rec:  0.8704125736738704\n",
      "ma F1:  0.9128492039688053\n",
      "mi F1:  0.9175308794848775\n",
      "we F1:  0.9171344346769185\n",
      "[[567  19]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       586\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.96       588\n",
      "   macro avg       0.50      0.48      0.49       588\n",
      "weighted avg       0.99      0.96      0.98       588\n",
      "\n",
      "acc:  0.9642857142857143\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4909090909090909\n",
      "mi F1:  0.9642857142857143\n",
      "we F1:  0.9784786641929499\n",
      "Loss:  0.078821562230587\n",
      "Loss:  0.051657404750585556\n",
      "Loss:  0.06316421926021576\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.04575549066066742\n",
      "Loss:  0.05373740196228027\n",
      "Loss:  0.06772658228874207\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.038176536560058594\n",
      "Loss:  0.04196368530392647\n",
      "Loss:  0.08742320537567139\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.014703512191772461\n",
      "Eval Loss:  0.003087759017944336\n",
      "Eval Loss:  0.04820060729980469\n",
      "[[18039  1539]\n",
      " [ 1236 11489]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     19578\n",
      "           1       0.88      0.90      0.89     12725\n",
      "\n",
      "    accuracy                           0.91     32303\n",
      "   macro avg       0.91      0.91      0.91     32303\n",
      "weighted avg       0.91      0.91      0.91     32303\n",
      "\n",
      "acc:  0.9140946661300808\n",
      "pre:  0.881869818851704\n",
      "rec:  0.9028683693516699\n",
      "ma F1:  0.910411253744497\n",
      "mi F1:  0.914094666130081\n",
      "we F1:  0.9142650590999789\n",
      "[[550  36]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97       586\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.94       588\n",
      "   macro avg       0.50      0.47      0.48       588\n",
      "weighted avg       0.99      0.94      0.96       588\n",
      "\n",
      "acc:  0.935374149659864\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4833040421792618\n",
      "mi F1:  0.935374149659864\n",
      "we F1:  0.9633203017586647\n",
      "Loss:  0.06668157130479813\n",
      "Loss:  0.040804628282785416\n",
      "Loss:  0.04695460945367813\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.054716404527425766\n",
      "Loss:  0.07690822333097458\n",
      "Loss:  0.027830597013235092\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.0605170913040638\n",
      "Loss:  0.038605283945798874\n",
      "Loss:  0.06085596978664398\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.012714624404907227\n",
      "Eval Loss:  0.0029458999633789062\n",
      "Eval Loss:  0.01400899887084961\n",
      "[[18131  1447]\n",
      " [ 1280 11445]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19578\n",
      "           1       0.89      0.90      0.89     12725\n",
      "\n",
      "    accuracy                           0.92     32303\n",
      "   macro avg       0.91      0.91      0.91     32303\n",
      "weighted avg       0.92      0.92      0.92     32303\n",
      "\n",
      "acc:  0.9155805962294524\n",
      "pre:  0.8877598510704313\n",
      "rec:  0.8994106090373281\n",
      "ma F1:  0.9118022246980331\n",
      "mi F1:  0.9155805962294524\n",
      "we F1:  0.9156749707502042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[557  29]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       586\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.95       588\n",
      "   macro avg       0.50      0.48      0.49       588\n",
      "weighted avg       0.99      0.95      0.97       588\n",
      "\n",
      "acc:  0.9472789115646258\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48646288209606986\n",
      "mi F1:  0.9472789115646257\n",
      "we F1:  0.9696164928853638\n",
      "Loss:  0.05590822547674179\n",
      "Loss:  0.08574281632900238\n",
      "Loss:  0.055508192628622055\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.05268396437168121\n",
      "Loss:  0.04257363826036453\n",
      "Loss:  0.039674270898103714\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.06796037405729294\n",
      "Loss:  0.028266912326216698\n",
      "Loss:  0.054259758442640305\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.009287357330322266\n",
      "Eval Loss:  0.0024933815002441406\n",
      "Eval Loss:  0.03462481498718262\n",
      "[[17808  1770]\n",
      " [ 1016 11709]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93     19578\n",
      "           1       0.87      0.92      0.89     12725\n",
      "\n",
      "    accuracy                           0.91     32303\n",
      "   macro avg       0.91      0.91      0.91     32303\n",
      "weighted avg       0.92      0.91      0.91     32303\n",
      "\n",
      "acc:  0.9137541404823082\n",
      "pre:  0.8686846205208102\n",
      "rec:  0.9201571709233791\n",
      "ma F1:  0.9105660246843063\n",
      "mi F1:  0.9137541404823082\n",
      "we F1:  0.9141482771131811\n",
      "[[544  42]\n",
      " [  1   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96       586\n",
      "           1       0.02      0.50      0.04         2\n",
      "\n",
      "    accuracy                           0.93       588\n",
      "   macro avg       0.51      0.71      0.50       588\n",
      "weighted avg       0.99      0.93      0.96       588\n",
      "\n",
      "acc:  0.9268707482993197\n",
      "pre:  0.023255813953488372\n",
      "rec:  0.5\n",
      "ma F1:  0.5032124963159447\n",
      "mi F1:  0.9268707482993197\n",
      "we F1:  0.9588596770862781\n",
      "Loss:  0.07144419103860855\n",
      "Loss:  0.0739857405424118\n",
      "Loss:  0.07778850942850113\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.031991422176361084\n",
      "Loss:  0.08628251403570175\n",
      "Loss:  0.03835130110383034\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.10328805446624756\n",
      "Loss:  0.04472028836607933\n",
      "Loss:  0.05317134037613869\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.010507822036743164\n",
      "Eval Loss:  0.002307891845703125\n",
      "Eval Loss:  0.012176275253295898\n",
      "[[18219  1359]\n",
      " [ 1289 11436]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19578\n",
      "           1       0.89      0.90      0.90     12725\n",
      "\n",
      "    accuracy                           0.92     32303\n",
      "   macro avg       0.91      0.91      0.91     32303\n",
      "weighted avg       0.92      0.92      0.92     32303\n",
      "\n",
      "acc:  0.9180261895180014\n",
      "pre:  0.8937866354044549\n",
      "rec:  0.8987033398821218\n",
      "ma F1:  0.9142451008683208\n",
      "mi F1:  0.9180261895180014\n",
      "we F1:  0.918065210040702\n",
      "[[558  28]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       586\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.95       588\n",
      "   macro avg       0.50      0.48      0.49       588\n",
      "weighted avg       0.99      0.95      0.97       588\n",
      "\n",
      "acc:  0.9489795918367347\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4869109947643979\n",
      "mi F1:  0.9489795918367347\n",
      "we F1:  0.9705096698365211\n",
      "Loss:  0.05163590610027313\n",
      "Loss:  0.03916488215327263\n",
      "Loss:  0.043490342795848846\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.04077360779047012\n",
      "Loss:  0.0293602142482996\n",
      "Loss:  0.05466115474700928\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.0830208957195282\n",
      "Loss:  0.06489323079586029\n",
      "Loss:  0.06879796087741852\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  24 Training Time 6148.614852666855 Best Test Acc:  0.9965986394557823\n",
      "test subjects:  ['./seg\\\\c10', './seg\\\\x18']\n",
      "*********\n",
      "33423 890\n",
      "32016 875\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.6519367694854736\n",
      "Eval Loss:  0.7211053967475891\n",
      "Eval Loss:  0.6742326021194458\n",
      "[[    3 19289]\n",
      " [   45 12679]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.06      0.00      0.00     19292\n",
      "           1       0.40      1.00      0.57     12724\n",
      "\n",
      "    accuracy                           0.40     32016\n",
      "   macro avg       0.23      0.50      0.28     32016\n",
      "weighted avg       0.20      0.40      0.23     32016\n",
      "\n",
      "acc:  0.3961144427786107\n",
      "pre:  0.39661536536536535\n",
      "rec:  0.9964633762967621\n",
      "ma F1:  0.2838524249300579\n",
      "mi F1:  0.3961144427786107\n",
      "we F1:  0.22568447500663605\n",
      "[[  0 872]\n",
      " [  0   3]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       872\n",
      "           1       0.00      1.00      0.01         3\n",
      "\n",
      "    accuracy                           0.00       875\n",
      "   macro avg       0.00      0.50      0.00       875\n",
      "weighted avg       0.00      0.00      0.00       875\n",
      "\n",
      "acc:  0.0034285714285714284\n",
      "pre:  0.0034285714285714284\n",
      "rec:  1.0\n",
      "ma F1:  0.003416856492027335\n",
      "mi F1:  0.0034285714285714284\n",
      "we F1:  2.342987308818744e-05\n",
      "Subject 25 Current Train Acc:  0.3961144427786107 Current Test Acc:  0.0034285714285714284\n",
      "Loss:  0.17019115388393402\n",
      "Loss:  0.1663973033428192\n",
      "Loss:  0.16403494775295258\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.1492263525724411\n",
      "Loss:  0.12646211683750153\n",
      "Loss:  0.12676988542079926\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.13519558310508728\n",
      "Loss:  0.11936212331056595\n",
      "Loss:  0.11890809237957001\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.34759238362312317\n",
      "Eval Loss:  0.04220426082611084\n",
      "Eval Loss:  0.2724252939224243\n",
      "[[15946  3346]\n",
      " [ 2703 10021]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.83      0.84     19292\n",
      "           1       0.75      0.79      0.77     12724\n",
      "\n",
      "    accuracy                           0.81     32016\n",
      "   macro avg       0.80      0.81      0.80     32016\n",
      "weighted avg       0.81      0.81      0.81     32016\n",
      "\n",
      "acc:  0.8110632183908046\n",
      "pre:  0.749682052816638\n",
      "rec:  0.7875668028921723\n",
      "ma F1:  0.8043629264716808\n",
      "mi F1:  0.8110632183908046\n",
      "we F1:  0.8117903555560361\n",
      "[[639 233]\n",
      " [  2   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.73      0.84       872\n",
      "           1       0.00      0.33      0.01         3\n",
      "\n",
      "    accuracy                           0.73       875\n",
      "   macro avg       0.50      0.53      0.43       875\n",
      "weighted avg       0.99      0.73      0.84       875\n",
      "\n",
      "acc:  0.7314285714285714\n",
      "pre:  0.004273504273504274\n",
      "rec:  0.3333333333333333\n",
      "ma F1:  0.42655913168851667\n",
      "mi F1:  0.7314285714285713\n",
      "we F1:  0.841812334093074\n",
      "Subject 25 Current Train Acc:  0.8110632183908046 Current Test Acc:  0.7314285714285714\n",
      "Loss:  0.09510724246501923\n",
      "Loss:  0.09427819401025772\n",
      "Loss:  0.11762738227844238\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.1106196716427803\n",
      "Loss:  0.09117656946182251\n",
      "Loss:  0.09365612268447876\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.09289874881505966\n",
      "Loss:  0.09936195611953735\n",
      "Loss:  0.10720128566026688\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.15370434522628784\n",
      "Eval Loss:  0.016123533248901367\n",
      "Eval Loss:  0.2667137384414673\n",
      "[[18262  1030]\n",
      " [ 4384  8340]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.95      0.87     19292\n",
      "           1       0.89      0.66      0.75     12724\n",
      "\n",
      "    accuracy                           0.83     32016\n",
      "   macro avg       0.85      0.80      0.81     32016\n",
      "weighted avg       0.84      0.83      0.82     32016\n",
      "\n",
      "acc:  0.8308970514742628\n",
      "pre:  0.8900747065101388\n",
      "rec:  0.6554542596667715\n",
      "ma F1:  0.8129303827373131\n",
      "mi F1:  0.8308970514742628\n",
      "we F1:  0.8248236583132339\n",
      "[[836  36]\n",
      " [  3   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       872\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.96       875\n",
      "   macro avg       0.50      0.48      0.49       875\n",
      "weighted avg       0.99      0.96      0.97       875\n",
      "\n",
      "acc:  0.9554285714285714\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4886031560490941\n",
      "mi F1:  0.9554285714285714\n",
      "we F1:  0.9738558904567087\n",
      "Subject 25 Current Train Acc:  0.8308970514742628 Current Test Acc:  0.9554285714285714\n",
      "Loss:  0.1067224070429802\n",
      "Loss:  0.05788501352071762\n",
      "Loss:  0.08918435871601105\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.08478011190891266\n",
      "Loss:  0.08388976752758026\n",
      "Loss:  0.08751390874385834\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.11215265840291977\n",
      "Loss:  0.10174216330051422\n",
      "Loss:  0.09365244954824448\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.1935790777206421\n",
      "Eval Loss:  0.01216268539428711\n",
      "Eval Loss:  0.07561767101287842\n",
      "[[18419   873]\n",
      " [ 3782  8942]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.89     19292\n",
      "           1       0.91      0.70      0.79     12724\n",
      "\n",
      "    accuracy                           0.85     32016\n",
      "   macro avg       0.87      0.83      0.84     32016\n",
      "weighted avg       0.86      0.85      0.85     32016\n",
      "\n",
      "acc:  0.854603948025987\n",
      "pre:  0.9110545084055017\n",
      "rec:  0.7027664256523106\n",
      "ma F1:  0.8406407500499082\n",
      "mi F1:  0.854603948025987\n",
      "we F1:  0.8503178930600259\n",
      "[[857  15]\n",
      " [  3   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       872\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.98       875\n",
      "   macro avg       0.50      0.49      0.49       875\n",
      "weighted avg       0.99      0.98      0.99       875\n",
      "\n",
      "acc:  0.9794285714285714\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4948036951501154\n",
      "mi F1:  0.9794285714285714\n",
      "we F1:  0.9862144506763443\n",
      "Subject 25 Current Train Acc:  0.854603948025987 Current Test Acc:  0.9794285714285714\n",
      "Loss:  0.07634355127811432\n",
      "Loss:  0.07654732465744019\n",
      "Loss:  0.10116007924079895\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.07614530622959137\n",
      "Loss:  0.08435949683189392\n",
      "Loss:  0.1134781688451767\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.08724918961524963\n",
      "Loss:  0.10132109373807907\n",
      "Loss:  0.0787104070186615\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.08717560768127441\n",
      "Eval Loss:  0.008055448532104492\n",
      "Eval Loss:  0.07805407047271729\n",
      "[[18481   811]\n",
      " [ 3616  9108]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.89     19292\n",
      "           1       0.92      0.72      0.80     12724\n",
      "\n",
      "    accuracy                           0.86     32016\n",
      "   macro avg       0.88      0.84      0.85     32016\n",
      "weighted avg       0.87      0.86      0.86     32016\n",
      "\n",
      "acc:  0.8617253873063468\n",
      "pre:  0.9182377255771751\n",
      "rec:  0.7158126375353663\n",
      "ma F1:  0.8487631256271086\n",
      "mi F1:  0.8617253873063468\n",
      "we F1:  0.8578462510628535\n",
      "[[859  13]\n",
      " [  3   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       872\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.98       875\n",
      "   macro avg       0.50      0.49      0.50       875\n",
      "weighted avg       0.99      0.98      0.99       875\n",
      "\n",
      "acc:  0.9817142857142858\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4953863898500577\n",
      "mi F1:  0.9817142857142858\n",
      "we F1:  0.9873758444554293\n",
      "Subject 25 Current Train Acc:  0.8617253873063468 Current Test Acc:  0.9817142857142858\n",
      "Loss:  0.09365630894899368\n",
      "Loss:  0.05285859480500221\n",
      "Loss:  0.0682273656129837\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.06889571994543076\n",
      "Loss:  0.07398973405361176\n",
      "Loss:  0.10501424968242645\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.08155424147844315\n",
      "Loss:  0.07332349568605423\n",
      "Loss:  0.05201615393161774\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.06311392784118652\n",
      "Eval Loss:  0.005941152572631836\n",
      "Eval Loss:  0.05450105667114258\n",
      "[[18586   706]\n",
      " [ 3447  9277]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90     19292\n",
      "           1       0.93      0.73      0.82     12724\n",
      "\n",
      "    accuracy                           0.87     32016\n",
      "   macro avg       0.89      0.85      0.86     32016\n",
      "weighted avg       0.88      0.87      0.87     32016\n",
      "\n",
      "acc:  0.870283608195902\n",
      "pre:  0.9292797756185516\n",
      "rec:  0.729094624331971\n",
      "ma F1:  0.8583043948886617\n",
      "mi F1:  0.8702836081959019\n",
      "we F1:  0.8667563739414014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[857  15]\n",
      " [  3   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       872\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.98       875\n",
      "   macro avg       0.50      0.49      0.49       875\n",
      "weighted avg       0.99      0.98      0.99       875\n",
      "\n",
      "acc:  0.9794285714285714\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4948036951501154\n",
      "mi F1:  0.9794285714285714\n",
      "we F1:  0.9862144506763443\n",
      "Loss:  0.08718904852867126\n",
      "Loss:  0.062120284885168076\n",
      "Loss:  0.07503891736268997\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.06209542602300644\n",
      "Loss:  0.0525490827858448\n",
      "Loss:  0.08192617446184158\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.02968018129467964\n",
      "Loss:  0.05152994766831398\n",
      "Loss:  0.08444886654615402\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.0344548225402832\n",
      "Eval Loss:  0.0045049190521240234\n",
      "Eval Loss:  0.043807387351989746\n",
      "[[18602   690]\n",
      " [ 3315  9409]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90     19292\n",
      "           1       0.93      0.74      0.82     12724\n",
      "\n",
      "    accuracy                           0.87     32016\n",
      "   macro avg       0.89      0.85      0.86     32016\n",
      "weighted avg       0.88      0.87      0.87     32016\n",
      "\n",
      "acc:  0.8749062968515742\n",
      "pre:  0.9316764036043172\n",
      "rec:  0.7394687205281358\n",
      "ma F1:  0.8636658089301683\n",
      "mi F1:  0.8749062968515742\n",
      "we F1:  0.8716966502950976\n",
      "[[853  19]\n",
      " [  3   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       872\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.97       875\n",
      "   macro avg       0.50      0.49      0.49       875\n",
      "weighted avg       0.99      0.97      0.98       875\n",
      "\n",
      "acc:  0.9748571428571429\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4936342592592592\n",
      "mi F1:  0.9748571428571429\n",
      "we F1:  0.9838835978835978\n",
      "Loss:  0.053428612649440765\n",
      "Loss:  0.06919525563716888\n",
      "Loss:  0.08902886509895325\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.05348991975188255\n",
      "Loss:  0.09056434035301208\n",
      "Loss:  0.06133619323372841\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.05374681204557419\n",
      "Loss:  0.053526606410741806\n",
      "Loss:  0.11604323983192444\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.02097296714782715\n",
      "Eval Loss:  0.0036385059356689453\n",
      "Eval Loss:  0.03357875347137451\n",
      "[[18454   838]\n",
      " [ 2657 10067]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91     19292\n",
      "           1       0.92      0.79      0.85     12724\n",
      "\n",
      "    accuracy                           0.89     32016\n",
      "   macro avg       0.90      0.87      0.88     32016\n",
      "weighted avg       0.89      0.89      0.89     32016\n",
      "\n",
      "acc:  0.890835832083958\n",
      "pre:  0.9231545162769372\n",
      "rec:  0.79118201823326\n",
      "ma F1:  0.882792528904953\n",
      "mi F1:  0.890835832083958\n",
      "we F1:  0.8890913741749786\n",
      "[[823  49]\n",
      " [  3   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97       872\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.94       875\n",
      "   macro avg       0.50      0.47      0.48       875\n",
      "weighted avg       0.99      0.94      0.97       875\n",
      "\n",
      "acc:  0.9405714285714286\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48468786808009423\n",
      "mi F1:  0.9405714285714286\n",
      "we F1:  0.9660521622076392\n",
      "Loss:  0.07674207538366318\n",
      "Loss:  0.05061238631606102\n",
      "Loss:  0.08787531405687332\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.08521892130374908\n",
      "Loss:  0.07316494733095169\n",
      "Loss:  0.09190510958433151\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.049379799515008926\n",
      "Loss:  0.04934355616569519\n",
      "Loss:  0.057787828147411346\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.016292333602905273\n",
      "Eval Loss:  0.0042877197265625\n",
      "Eval Loss:  0.024471521377563477\n",
      "[[18275  1017]\n",
      " [ 2393 10331]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91     19292\n",
      "           1       0.91      0.81      0.86     12724\n",
      "\n",
      "    accuracy                           0.89     32016\n",
      "   macro avg       0.90      0.88      0.89     32016\n",
      "weighted avg       0.89      0.89      0.89     32016\n",
      "\n",
      "acc:  0.8934907546226887\n",
      "pre:  0.9103806838209376\n",
      "rec:  0.8119302106255895\n",
      "ma F1:  0.8865031532030535\n",
      "mi F1:  0.8934907546226887\n",
      "we F1:  0.8922804147997508\n",
      "[[829  43]\n",
      " [  3   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       872\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.95       875\n",
      "   macro avg       0.50      0.48      0.49       875\n",
      "weighted avg       0.99      0.95      0.97       875\n",
      "\n",
      "acc:  0.9474285714285714\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4865023474178404\n",
      "mi F1:  0.9474285714285714\n",
      "we F1:  0.9696686787391012\n",
      "Loss:  0.07436519861221313\n",
      "Loss:  0.06862442195415497\n",
      "Loss:  0.11836136132478714\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.05405104160308838\n",
      "Loss:  0.07725846022367477\n",
      "Loss:  0.08829636871814728\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.029227733612060547\n",
      "Loss:  0.045678842812776566\n",
      "Loss:  0.07804395258426666\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.01746082305908203\n",
      "Eval Loss:  0.002913236618041992\n",
      "Eval Loss:  0.02456808090209961\n",
      "[[18586   706]\n",
      " [ 2819  9905]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91     19292\n",
      "           1       0.93      0.78      0.85     12724\n",
      "\n",
      "    accuracy                           0.89     32016\n",
      "   macro avg       0.90      0.87      0.88     32016\n",
      "weighted avg       0.89      0.89      0.89     32016\n",
      "\n",
      "acc:  0.8898988005997002\n",
      "pre:  0.9334652718876637\n",
      "rec:  0.7784501729016032\n",
      "ma F1:  0.8811618202068299\n",
      "mi F1:  0.8898988005997002\n",
      "we F1:  0.8877721746844675\n",
      "[[850  22]\n",
      " [  3   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99       872\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.97       875\n",
      "   macro avg       0.50      0.49      0.49       875\n",
      "weighted avg       0.99      0.97      0.98       875\n",
      "\n",
      "acc:  0.9714285714285714\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49275362318840576\n",
      "mi F1:  0.9714285714285714\n",
      "we F1:  0.9821283643892339\n",
      "Loss:  0.045596249401569366\n",
      "Loss:  0.06005780026316643\n",
      "Loss:  0.054312460124492645\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.07992089539766312\n",
      "Loss:  0.06424897909164429\n",
      "Loss:  0.061763301491737366\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.08735840767621994\n",
      "Loss:  0.07316688448190689\n",
      "Loss:  0.08129452168941498\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.01441812515258789\n",
      "Eval Loss:  0.0036134719848632812\n",
      "Eval Loss:  0.016751527786254883\n",
      "[[18109  1183]\n",
      " [ 1995 10729]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     19292\n",
      "           1       0.90      0.84      0.87     12724\n",
      "\n",
      "    accuracy                           0.90     32016\n",
      "   macro avg       0.90      0.89      0.90     32016\n",
      "weighted avg       0.90      0.90      0.90     32016\n",
      "\n",
      "acc:  0.9007371314342829\n",
      "pre:  0.9006883814640698\n",
      "rec:  0.8432096824897831\n",
      "ma F1:  0.8951668489367234\n",
      "mi F1:  0.9007371314342829\n",
      "we F1:  0.900124249403386\n",
      "[[678 194]\n",
      " [  2   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.87       872\n",
      "           1       0.01      0.33      0.01         3\n",
      "\n",
      "    accuracy                           0.78       875\n",
      "   macro avg       0.50      0.56      0.44       875\n",
      "weighted avg       0.99      0.78      0.87       875\n",
      "\n",
      "acc:  0.776\n",
      "pre:  0.005128205128205128\n",
      "rec:  0.3333333333333333\n",
      "ma F1:  0.4419061751535978\n",
      "mi F1:  0.776\n",
      "we F1:  0.8707503905029678\n",
      "Loss:  0.048943519592285156\n",
      "Loss:  0.08012715727090836\n",
      "Loss:  0.04772749915719032\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.06977177411317825\n",
      "Loss:  0.08827409893274307\n",
      "Loss:  0.059807971119880676\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.05667838454246521\n",
      "Loss:  0.08161305636167526\n",
      "Loss:  0.056192249059677124\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.015839815139770508\n",
      "Eval Loss:  0.0029268264770507812\n",
      "Eval Loss:  0.016900300979614258\n",
      "[[18450   842]\n",
      " [ 2577 10147]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     19292\n",
      "           1       0.92      0.80      0.86     12724\n",
      "\n",
      "    accuracy                           0.89     32016\n",
      "   macro avg       0.90      0.88      0.89     32016\n",
      "weighted avg       0.90      0.89      0.89     32016\n",
      "\n",
      "acc:  0.8932096451774113\n",
      "pre:  0.9233779233779233\n",
      "rec:  0.7974693492612386\n",
      "ma F1:  0.8855093769766134\n",
      "mi F1:  0.8932096451774113\n",
      "we F1:  0.891600592385844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[772 100]\n",
      " [  3   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94       872\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.88       875\n",
      "   macro avg       0.50      0.44      0.47       875\n",
      "weighted avg       0.99      0.88      0.93       875\n",
      "\n",
      "acc:  0.8822857142857143\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4687310261080753\n",
      "mi F1:  0.8822857142857143\n",
      "we F1:  0.9342478966085523\n",
      "Loss:  0.08481373637914658\n",
      "Loss:  0.051121942698955536\n",
      "Loss:  0.04553160071372986\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.0456770583987236\n",
      "Loss:  0.04724469035863876\n",
      "Loss:  0.07106267660856247\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.09492853283882141\n",
      "Loss:  0.03734583035111427\n",
      "Loss:  0.04511239752173424\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.013438940048217773\n",
      "Eval Loss:  0.003007173538208008\n",
      "Eval Loss:  0.01992511749267578\n",
      "[[18373   919]\n",
      " [ 2286 10438]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     19292\n",
      "           1       0.92      0.82      0.87     12724\n",
      "\n",
      "    accuracy                           0.90     32016\n",
      "   macro avg       0.90      0.89      0.89     32016\n",
      "weighted avg       0.90      0.90      0.90     32016\n",
      "\n",
      "acc:  0.8998938030984508\n",
      "pre:  0.9190807431540019\n",
      "rec:  0.8203395158755108\n",
      "ma F1:  0.8933421234708794\n",
      "mi F1:  0.8998938030984508\n",
      "we F1:  0.8987651142451564\n",
      "[[800  72]\n",
      " [  2   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96       872\n",
      "           1       0.01      0.33      0.03         3\n",
      "\n",
      "    accuracy                           0.92       875\n",
      "   macro avg       0.51      0.63      0.49       875\n",
      "weighted avg       0.99      0.92      0.95       875\n",
      "\n",
      "acc:  0.9154285714285715\n",
      "pre:  0.0136986301369863\n",
      "rec:  0.3333333333333333\n",
      "ma F1:  0.4910551468276426\n",
      "mi F1:  0.9154285714285715\n",
      "we F1:  0.9526077200168882\n",
      "Loss:  0.06319208443164825\n",
      "Loss:  0.031884219497442245\n",
      "Loss:  0.0871574655175209\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.050612568855285645\n",
      "Loss:  0.07807207852602005\n",
      "Loss:  0.05894961580634117\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.06907963007688522\n",
      "Loss:  0.04834155738353729\n",
      "Loss:  0.08057849109172821\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.01055002212524414\n",
      "Eval Loss:  0.0021572113037109375\n",
      "Eval Loss:  0.01187443733215332\n",
      "[[18288  1004]\n",
      " [ 2057 10667]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     19292\n",
      "           1       0.91      0.84      0.87     12724\n",
      "\n",
      "    accuracy                           0.90     32016\n",
      "   macro avg       0.91      0.89      0.90     32016\n",
      "weighted avg       0.90      0.90      0.90     32016\n",
      "\n",
      "acc:  0.9043915542228885\n",
      "pre:  0.9139748093565248\n",
      "rec:  0.8383370009430997\n",
      "ma F1:  0.898648821783862\n",
      "mi F1:  0.9043915542228885\n",
      "we F1:  0.9035980760365226\n",
      "[[830  42]\n",
      " [  3   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       872\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.95       875\n",
      "   macro avg       0.50      0.48      0.49       875\n",
      "weighted avg       0.99      0.95      0.97       875\n",
      "\n",
      "acc:  0.9485714285714286\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48680351906158353\n",
      "mi F1:  0.9485714285714286\n",
      "we F1:  0.9702689568496019\n",
      "Loss:  0.10747628659009933\n",
      "Loss:  0.07047420740127563\n",
      "Loss:  0.05638854205608368\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.05431704968214035\n",
      "Loss:  0.06198674440383911\n",
      "Loss:  0.06829340010881424\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.05467822030186653\n",
      "Loss:  0.035017840564250946\n",
      "Loss:  0.04834684729576111\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.011446714401245117\n",
      "Eval Loss:  0.0018198490142822266\n",
      "Eval Loss:  0.01363682746887207\n",
      "[[18392   900]\n",
      " [ 2220 10504]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     19292\n",
      "           1       0.92      0.83      0.87     12724\n",
      "\n",
      "    accuracy                           0.90     32016\n",
      "   macro avg       0.91      0.89      0.90     32016\n",
      "weighted avg       0.90      0.90      0.90     32016\n",
      "\n",
      "acc:  0.9025487256371814\n",
      "pre:  0.9210803226937917\n",
      "rec:  0.8255265639735933\n",
      "ma F1:  0.8962510024057739\n",
      "mi F1:  0.9025487256371814\n",
      "we F1:  0.9014948470031224\n",
      "[[829  43]\n",
      " [  3   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97       872\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.95       875\n",
      "   macro avg       0.50      0.48      0.49       875\n",
      "weighted avg       0.99      0.95      0.97       875\n",
      "\n",
      "acc:  0.9474285714285714\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4865023474178404\n",
      "mi F1:  0.9474285714285714\n",
      "we F1:  0.9696686787391012\n",
      "Loss:  0.0600670650601387\n",
      "Loss:  0.04496217146515846\n",
      "Loss:  0.05036115273833275\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.04974743351340294\n",
      "Loss:  0.0433746799826622\n",
      "Loss:  0.0531601682305336\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.06819411367177963\n",
      "Loss:  0.0428539514541626\n",
      "Loss:  0.0791572704911232\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.016600370407104492\n",
      "Eval Loss:  0.0021975040435791016\n",
      "Eval Loss:  0.021782875061035156\n",
      "[[18629   663]\n",
      " [ 2527 10197]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92     19292\n",
      "           1       0.94      0.80      0.86     12724\n",
      "\n",
      "    accuracy                           0.90     32016\n",
      "   macro avg       0.91      0.88      0.89     32016\n",
      "weighted avg       0.90      0.90      0.90     32016\n",
      "\n",
      "acc:  0.9003623188405797\n",
      "pre:  0.9389502762430939\n",
      "rec:  0.8013989311537253\n",
      "ma F1:  0.8929360564660873\n",
      "mi F1:  0.9003623188405797\n",
      "we F1:  0.8987206498336947\n",
      "[[847  25]\n",
      " [  3   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98       872\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.97       875\n",
      "   macro avg       0.50      0.49      0.49       875\n",
      "weighted avg       0.99      0.97      0.98       875\n",
      "\n",
      "acc:  0.968\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.491869918699187\n",
      "mi F1:  0.968\n",
      "we F1:  0.9803670150987225\n",
      "Loss:  0.030681125819683075\n",
      "Loss:  0.05074641481041908\n",
      "Loss:  0.10216747969388962\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.05954122543334961\n",
      "Loss:  0.05293572321534157\n",
      "Loss:  0.05009051784873009\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.07335232198238373\n",
      "Loss:  0.06648857146501541\n",
      "Loss:  0.0397714264690876\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.01129150390625\n",
      "Eval Loss:  0.002401590347290039\n",
      "Eval Loss:  0.014499902725219727\n",
      "[[18004  1288]\n",
      " [ 1393 11331]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19292\n",
      "           1       0.90      0.89      0.89     12724\n",
      "\n",
      "    accuracy                           0.92     32016\n",
      "   macro avg       0.91      0.91      0.91     32016\n",
      "weighted avg       0.92      0.92      0.92     32016\n",
      "\n",
      "acc:  0.9162606196901549\n",
      "pre:  0.8979316903082654\n",
      "rec:  0.8905218484753222\n",
      "ma F1:  0.912457618474708\n",
      "mi F1:  0.9162606196901549\n",
      "we F1:  0.9162007792694113\n",
      "[[731 141]\n",
      " [  2   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.84      0.91       872\n",
      "           1       0.01      0.33      0.01         3\n",
      "\n",
      "    accuracy                           0.84       875\n",
      "   macro avg       0.50      0.59      0.46       875\n",
      "weighted avg       0.99      0.84      0.91       875\n",
      "\n",
      "acc:  0.8365714285714285\n",
      "pre:  0.007042253521126761\n",
      "rec:  0.3333333333333333\n",
      "ma F1:  0.46234826511977656\n",
      "mi F1:  0.8365714285714286\n",
      "we F1:  0.9078276199683869\n",
      "Loss:  0.05966833606362343\n",
      "Loss:  0.04740850627422333\n",
      "Loss:  0.03930821269750595\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.04710660129785538\n",
      "Loss:  0.06962033361196518\n",
      "Loss:  0.03613667190074921\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.049773525446653366\n",
      "Loss:  0.04377671703696251\n",
      "Loss:  0.06293531507253647\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.02182459831237793\n",
      "Eval Loss:  0.0020563602447509766\n",
      "Eval Loss:  0.01557159423828125\n",
      "[[18657   635]\n",
      " [ 2533 10191]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92     19292\n",
      "           1       0.94      0.80      0.87     12724\n",
      "\n",
      "    accuracy                           0.90     32016\n",
      "   macro avg       0.91      0.88      0.89     32016\n",
      "weighted avg       0.90      0.90      0.90     32016\n",
      "\n",
      "acc:  0.9010494752623688\n",
      "pre:  0.9413449104008867\n",
      "rec:  0.8009273813266269\n",
      "ma F1:  0.8936103519469374\n",
      "mi F1:  0.9010494752623689\n",
      "we F1:  0.899381691651137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[822  50]\n",
      " [  3   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97       872\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.94       875\n",
      "   macro avg       0.50      0.47      0.48       875\n",
      "weighted avg       0.99      0.94      0.97       875\n",
      "\n",
      "acc:  0.9394285714285714\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4843842074248674\n",
      "mi F1:  0.9394285714285714\n",
      "we F1:  0.9654469231416786\n",
      "Loss:  0.0704549103975296\n",
      "Loss:  0.07324417680501938\n",
      "Loss:  0.03528929501771927\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.048249561339616776\n",
      "Loss:  0.04811643436551094\n",
      "Loss:  0.0517524853348732\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.0724748894572258\n",
      "Loss:  0.03898836299777031\n",
      "Loss:  0.048695601522922516\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.00974726676940918\n",
      "Eval Loss:  0.002059459686279297\n",
      "Eval Loss:  0.01333928108215332\n",
      "[[17820  1472]\n",
      " [ 1504 11220]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92     19292\n",
      "           1       0.88      0.88      0.88     12724\n",
      "\n",
      "    accuracy                           0.91     32016\n",
      "   macro avg       0.90      0.90      0.90     32016\n",
      "weighted avg       0.91      0.91      0.91     32016\n",
      "\n",
      "acc:  0.9070464767616192\n",
      "pre:  0.8840214308225653\n",
      "rec:  0.8817981766740018\n",
      "ma F1:  0.9029209516113035\n",
      "mi F1:  0.9070464767616192\n",
      "we F1:  0.9070264742154357\n",
      "[[791  81]\n",
      " [  3   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95       872\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.90       875\n",
      "   macro avg       0.50      0.45      0.47       875\n",
      "weighted avg       0.99      0.90      0.95       875\n",
      "\n",
      "acc:  0.904\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.47478991596638653\n",
      "mi F1:  0.904\n",
      "we F1:  0.9463241296518607\n",
      "Loss:  0.04414669796824455\n",
      "Loss:  0.0566139742732048\n",
      "Loss:  0.0427931472659111\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.04854336380958557\n",
      "Loss:  0.05644761770963669\n",
      "Loss:  0.043418701738119125\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.061494115740060806\n",
      "Loss:  0.03888196498155594\n",
      "Loss:  0.05146532505750656\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.010872125625610352\n",
      "Eval Loss:  0.002267122268676758\n",
      "Eval Loss:  0.015233516693115234\n",
      "[[18217  1075]\n",
      " [ 1479 11245]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19292\n",
      "           1       0.91      0.88      0.90     12724\n",
      "\n",
      "    accuracy                           0.92     32016\n",
      "   macro avg       0.92      0.91      0.92     32016\n",
      "weighted avg       0.92      0.92      0.92     32016\n",
      "\n",
      "acc:  0.9202273863068465\n",
      "pre:  0.9127435064935064\n",
      "rec:  0.8837629676202452\n",
      "ma F1:  0.9162560750573603\n",
      "mi F1:  0.9202273863068465\n",
      "we F1:  0.9199972644272149\n",
      "[[734 138]\n",
      " [  2   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.84      0.91       872\n",
      "           1       0.01      0.33      0.01         3\n",
      "\n",
      "    accuracy                           0.84       875\n",
      "   macro avg       0.50      0.59      0.46       875\n",
      "weighted avg       0.99      0.84      0.91       875\n",
      "\n",
      "acc:  0.84\n",
      "pre:  0.007194244604316547\n",
      "rec:  0.3333333333333333\n",
      "ma F1:  0.46350991521266904\n",
      "mi F1:  0.8399999999999999\n",
      "we F1:  0.9098535491556304\n",
      "Loss:  0.04866369068622589\n",
      "Loss:  0.05636483058333397\n",
      "Loss:  0.03242964670062065\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.05790603160858154\n",
      "Loss:  0.0465666837990284\n",
      "Loss:  0.0904221385717392\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.06831523030996323\n",
      "Loss:  0.06192944198846817\n",
      "Loss:  0.0510733500123024\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD4CAYAAAAKA1qZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAA190lEQVR4nO3deXwU9fkH8M+TQAIJ4Q4BucIRgchtQBA8uCyIFaj6K1jF2gNpQbReja221GpFK1pRCgVv61HPSg1yyCXIGZAbAgEDhCMEEAiEQI7n98fOhtnZmd2ZvWY387xfL17ZnWu/kyXzzHyP50vMDCGEEM4UZ3cBhBBC2EeCgBBCOJgEASGEcDAJAkII4WASBIQQwsFq2V0AK5o2bcrp6el2F0MIIWLKxo0bTzBzqt66mAoC6enpyM3NtbsYQggRU4jogNE6qQ4SQggHkyAghBAOJkFACCEcTIKAEEI4mAQBIYRwMAkCQgjhYBIEhBDCwRwRBAp/KMWyvON2F0MIIaKOqSBARMOJKI+I8okoW2d9ZyJaQ0QXiegR1fJORLRZ9e8sET2orJtKRIdV624O2VlpjHh5Je59c0O4Di+EEDHL74hhIooHMBPAMACFADYQ0Txm3qna7BSAKQBGq/dl5jwAPVXHOQzgc9UmLzHzC0GU35SSsopwf4QQQsQkM08CfQHkM/N+Zr4E4EMAo9QbMPNxZt4AoNzHcYYA2MfMhsOXw23+tqN2fbQQQkQlM0GgJYBDqveFyjKrxgL4QLNsMhFtJaI3iKiR3k5ENIGIcokot7i4OICPvez+D74Lan89mw+dxr9W7Av5cYUQIhLMBAHSWWZpYmIiSgBwK4CPVYtnAegAV3XRUQDT9fZl5jnMnMXMWampuknwTKusCv18yqNnfotnv9od8uMKIUQkmAkChQBaq963AnDE4ueMALCJmYvcC5i5iJkrmbkKwFy4qp3C7lJFVSQ+RgghYoKZILABQAYRtVPu6McCmGfxc8ZBUxVERC1Ub8cA2G7xmKY1S0msfn3lE1+F62OEECLm+A0CzFwBYDKAhQB2AfiImXcQ0UQimggARNSciAoBPATgCSIqJKL6yrokuHoWfaY59PNEtI2ItgIYBOB3ITsrjW8eG+TxfuOBH1Be6flE8PBHW5CenROuIgghRFQyNakMM88HMF+zbLbq9TG4qon09i0F0ERn+d2WShqEOrXjPd7fNms1fprVGs/d3r162aebCiNVHCGEiBqOGDGs5z+5h/xvJIQQNZxjg4AQQggHBYEFD15ndxGEECLqOCYIdG5e32vZnG/2ofSSpJQQQjiXY4KAnr/N341ncnbZXQwhhLCNo4MAAPxQesnuIgghhG0cHwSEEMLJHBUEfn5tuteyBduPRb4gQggRJRwVBEZ0be61LAw55YQQImY4KgjUinfU6QohhF+Ouir2at3Qa1nDpNpYuTe4eQqEECJWOSoIxMV5T41wurQczy2Q+QCEEM7kqCAQrDdWfY8NBafsLoYQQoSMqSyiwuWpL3cCAAqmjbS5JEIIERqOexJo2yTJ7iIIIUTUcFwQeOxHnb2WbT981oaSCCGE/RwXBK7t4DW/jYey8kqcKS2PUGmEEMJejgsCjZITfK6/9dVV6PHUogiVRggh7OW4IODPnqJzHu8LTpzHjCV7wRy6ocUvLd6Die9uDNnxhBAiUNI7yI/xb6zHwVOl+Gmf1iE75stL9obsWEKI0FudfwJ92zV2RJaBmn+GQSorr7S7CEKICFqdfwJ3vrYOry7Lt7soEWEqCBDRcCLKI6J8IsrWWd+ZiNYQ0UUiekSzroCIthHRZiLKVS1vTESLiWiv8rNR8KcTWRcuSYAQoqY5XnIRAPD9ifM2lyQy/AYBIooHMBPACACZAMYRUaZms1MApgB4weAwg5i5JzNnqZZlA1jCzBkAlijvI0Ine0RAHvjwu9AcSOX8xQr0feZrrNl3MuTHFkIILTNPAn0B5DPzfma+BOBDAKPUGzDzcWbeAMBK38pRAN5WXr8NYLSFfYNCFJookHvgh5AcR23X0bM4XnIRLyzK81j+2sr9krJCCBFyZoJASwCHVO8LlWVmMYBFRLSRiCaolqcx81EAUH4209uZiCYQUS4R5RYXhybbp9kQsHJvcfWj4f5iex8Nn87ZhTtmr7G1DEKImsdMENC7ZlrpLzmAmXvDVZ00iYiut7AvmHkOM2cxc1ZqaqqVXQ21T002td3bqw9Uv9Y2EDMzTp2X+YmFELHNTBAoBKDuH9kKwBGzH8DMR5SfxwF8Dlf1EgAUEVELAFB+Hjd7zGA9M6ZbUPsv2H4M/1530GPZS4v3YPaKfUEdN9pUVTGeydmJgydL7S6KECJMzASBDQAyiKgdESUAGAtgnpmDE1EyEaW4XwO4CcB2ZfU8APcor+8B8IWVggejbu14v9uUlVfi611Fuus+2ViI7zXVQy8v2YtpX9WseQnyikowd+X3+M17MrBNiJrK72AxZq4goskAFgKIB/AGM+8goonK+tlE1BxALoD6AKqI6EG4ehI1BfC50hBbC8D7zLxAOfQ0AB8R0S8BHARwR0jPzOc5+d+mpKzC1xFCVpbIHdk69++pUiZiFqLGMjVimJnnA5ivWTZb9foYXNVEWmcB9DA45kkAQ0yXNIQSaskYOSGEABw6YrhT8xS/27D2nlzVPM4MWO1lOuGdXPT72xKv5SVl5Vi445jexwghbBTCdGFRzZFBwA6Ldhbh2Nkyr+UPf7QF97270TGjE4WIVsyMqiq2fIMX6yQIGCiv9LwNuFRRVf16ye7jeH3V94b77i0qQXp2DjaaGEx28JSr5000pqC4VFnlfyMhaoinc3ah/R/mo8opjwAKCQIGLlzybBi+z0Lq5xV7XIPacrYeDWmZIm30zG/tLoIQEfPmt64buyrl3scpTwSODQIdm9Xzuf6bPScCPvbS3a4hD6dLL+F06SUcOX3B9L7B3INkf7oV6dk5QRxBCOE0jp1PoFvLBsg/fs5w/Utf7wn42KuV5G+7j5Wg/7NLcUE12nj3sfDNZ/zhhkP+NxIiTIrOluGbPcW4Iyt0c2+I8HPsk8CUIRk+15eHoD5859GzHgEAAIb/Y6XutvuKXQHJIU+goga65431ePSTrThdKulUYoljg0AtP/mky8oj0yh6Usk/dP8HoU9LLUQkuZMtyuDC2OLYIBAud8xebXrbP3y+zWPuYmaubk8INWbGnG/2YfD05ZiuSlPNzDhwUrqnCuFUEgRCbEOB+TkG3l93ECfOXX50nrflCP65PLAkdMdLvMcgqBWcLMXf5u/G/uLzeGXp5WnzPs4txA1/X461+40nsXHHqecX7MYXmw8HVD4hRHSSIBBFjp7xfSH3ZfL7vquTKqv0q7e+O3QawOU2CV/+uXwfHvhws9WiCSGimASBKBLMGBXfCe9qltyCUzhx7qLdxRA1nFPGjDk2CLRqVNfuIkSVSP2H3198zudoazNun70GY/4pA9miVaxfO50ySMzNseMEQjXPcKyL9K/htlmr8UNpOe7q1waJtfzP62Dk0CnzA/BEZMhfVGxy7JNANFJnLt1zrMTGkoTP+YvRlyNJCCeTIBClSi5WoKy80mvQWnp2jt+eQFbSVLj5eoT3SqsdQpcqqnDfu7nYU1Qzg56IPU5pC3CTIBBFtP/5Oj+5ALfPXuO13bbCMz6Pc+20paY/0+5H+G2Hz2DhjiL8/tOtNpdECE9OqTGWIBBF9Ibbb1G6cEbK8ZIybDl02ufQf/WTyPGSMnSbuhA7jvgOTEKI6OTYhuFoNHdlcL1mAjFv8xEAQJUy1L/vM67Zz1JTEg33uahKqbF8dzFKyirw1rcF+PsdujOJ6nLaI3dNNmPJXry4eA/q15HLSbBGzliJpvUS8fYv+kbsM+VbiwEf6WQHvVRRhUuVVSguuYh6iYF/jSUXK6qPp1ZcEqZ++A55xHaSf61wjXK/qPwfkgAfuB1Hwpdl2Iijg0Cd2nERSxQXjE82Fnq8P1tWjrteX4f135+qXtalRX3D/XceOYtfv5MbtvLZ5Z01BRjfP93uYgiFU+rQaxpTbQJENJyI8ogon4iyddZ3JqI1RHSRiB5RLW9NRMuIaBcR7SCiB1TrphLRYSLarPy7OTSnZF6jpIRIf2RA1hec8ng/7avdHgHACDNj0Y5juHnGShz202PIVw+gPUXncNdr6yzvF24vLQ58zgcRejXtCaCmnY8Rv0GAiOIBzAQwAkAmgHFElKnZ7BSAKQBe0CyvAPAwM3cB0A/AJM2+LzFzT+Xf/EBPIlCx+iUXnTVXVfPJxkJMsDAtZm6BcWBZle97pjX1XSAzY+GOY6isYpSVV+LZ+buicg7laPX26gL85t/mv7doE+tPBLFefqvMPAn0BZDPzPuZ+RKADwGMUm/AzMeZeQOAcs3yo8y8SXldAmAXgJYhKXkI1KQJpVlzLkVny/DoJ8bdLp/J2enRN58Zut1R9by0eA8qNOMXthy63Dvoy61Hcd+7G/H6qv14e3UB/vXNfsxeEVh21Fjz9Jc78erSvUEd48/zduCr7cc8lm0+dBozl+Ub7FGz7Cs+J9lqI8hMm0BLAOqWyUIA11j9ICJKB9ALgLpeYTIRjQeQC9cTg1ceZiKaAGACALRp08bqxzrWpgO+U1rPXfm9R28kK+Hws+8O47orm6JN4ySs2+96esgrKsHS3UVollIHT325EwBw5HRZdS8j96A3901WDYq/Hl5T8iJNHux75jqt70+cR7umyYbrR8905UqaNKhj4IUL0pkL5YgjIKVObd31ofpKh0xfAQAY1TNq7hdrNDNPAnoPR5a+byKqB+BTAA8ys7v5exaADgB6AjgKYLrevsw8h5mzmDkrNTXVysf6VUOvQxFRXsm4bdYafPbd5Tu2gydLccsrq3R7Fml/1w99tBnMjOV54ZlEJ5Ys3V2EQS8sx/+2HIno554uvYR2j+fgWz9VfW49/rII3aYu8lruhDxczIwn/7sdmwMYt1NRWYXCH0oD/uy9RSW467V1YRszZCYIFAJQzxzdCoDp/61EVBuuAPAeM3/mXs7MRcxcycxVAObCVe0UUTX1bhQAKixO8ReOP+OFOy5XaWjbBL7afgyfbCz0mODGqlBffF5clIe+z3wd0mOaseuoq1pu59HIdg/cdvgMmIFZAU5kZCTW/q7OX6zA0BdXwNefTOmlSry79gDunLvW8vGfztmFgc8tC7jb9dmyCqzKP4HTF8r9bxwAM0FgA4AMImpHRAkAxgKYZ+bg5PorfR3ALmZ+UbOuhertGADbzRU5lGLsf6sFn20q9L+RSih+E9pjuOdPBoC3Vhd4bX/kdOCT6ITDjKX51fPkCuti9Xlg08EfkH/c/6RKgVq5txgAcOaC8Sh8O/ltE2DmCiKaDGAhgHgAbzDzDiKaqKyfTUTN4arXrw+giogehKsnUXcAdwPYRkSblUP+QekJ9DwR9YTr2lEA4L4QnpcpsXbHYsWFcmu9ccL1uzB7s27nBWRPUYnXYLlo8vBHW+wugik1ZYL5mnxd0GNqsJhy0Z6vWTZb9foYXNVEWqtg8PfNzHebL2Z4jOnVsrohT1hk4g+FAJCPy7veGINdR8+iU1oK4uJc+xWdLUN5ZRVaNUry+3kbD5xCs5Q6aN3Y/7Zu5y5W4KaXvvG5zcWKSvzq7Vw8PqILMq/wHJRXVl6JD9cfxPj+6dVlBoA3Vn2PfcXn8MyYbqbLYuRTi091drFaBRmt3Gfh0e3ZlpJEhqMTyP3h5i52FyFq+JpoPlTOXayoTi2gZ+OBHzDi5ZWYu3J/9bJr/rYEA59bpru9NrzcNmsNrntef1sjXf+80O822w+fwcq9J/DEf7d5rXt5yV5M/d9OzNM06j715U68t+6gpbKYMen9TZg6b0dIj2nngL9IyD8eujTloXxiXbX3hKnU8OHm6CCgvnOradbu9z+iWG3FnuKgP9PfY/T2w74zjbp7UGwPUf6UHUfO4Ifz4a2HPV3qaqw7fyn4OZ7NVEPkbD1a3b6Sd6wE6dk5yDOYgIiZkZ6dE/KGXy1/f0VVVYzvT5wPaxmMLNh+DENf/Abztx2N+GczM15cvAcFJ/V7Brm/x80HT0euUDocHQRqkt0Rnons2Flzdy++2gTCXfc6csaqqJ+LuKqKAx4YlaNc2L7a7vsC99yC3T7XG1XZHT9bhsw/LQg6TfgrS/Mx6IXlIb0jN2v3sbPKT+ufrf7/6R6MaaVH2pEzZZixZG/Ut5VIEBABedFE3h7tRX7sHOvd6wKxQZX+wuguzJf07By8t+5AKItk6D+5h7CnKHw9U4KxLO84Si9V4m2dnl1q2kuctnrJ/X0cPRNdvcGM+BoYZaXuQDuKP1o5Pgj8YkA7u4tQY2j/y1+qrDL9R1NeydXVFqUXKzB2zhoUqKoQ3l17wCulttFN2f7i4C+qs1fsw8tf70V6do6pJxYz21RUVuG7g54juU+es79Laiy2CRw9Y20K1RlL9vp9IgrE8bNlOFsWnv77keL4IHDvgHS7i1BjaPMJAX6qg1Svtx0+U/3IvmT3cazdfwp/UjWAPvnf7XgsRFNQnjh3EenZOfjKRz3x2QsVeOlrz6cdvaoAK+PV/r4oD2P+uTrks7C5A1CPvyzC4595N14b8dVzK5ot2H4M/Z9danm0eTBtI0ZBvu/fluAGi50Roo3jg4CV7oTCt1M+pqQMxDc6jdXqO2dmYPqiPI8nBjP2KMHmnTXGVT5nVKMztVlbV+efsDwYD3DN6wAAX207pntuVmkv4WculOOD9cH3SDp/scJ0Kgm9coTblsLTAOyZgEXvZH8o9XwSKPyhFPuLz5luP7D7OczxQUCEl8+7zQDqTK9++nJah5PnL+GVpfm4960NHtu40zC4bTzwA86UBv7IXqJ53L/ztXV4KIgBXK8uy8f4N9Z7LZ+9Yh8+DMFF3CyjaqDf/Wczfvbauqgb0W3F/7YcQXp2ji2TRg18bhkGK0nwfImWlEsSBETIvGZxjuRQ3QF9f+K8R8+Tt1YXYJOq7v22Watx1+uu5LXp2Tl48gtXhpJSk6OqzZTTaJvFO4vw+XfGTw2vawYrZluozvH32WZpA7U7hYJ71Ll2fVl5pW7Vn9HxzilTmIYqNYOZewd3x4VjFtsO1OZtOXK5k4Hdt+thJEFAhIxeV7hI3e384XPP1FN/1LzfphqjsK/YVX1kNSuj3qn4O71fv5OL3/3H+KlBW5VgqTwGH15VxZZ6plhtGO785ALc/br3k4zR8dyZN6cv0u9Rlp6dE9AAODP/t74PoHeY2h3KHBvuc4qSm/eQkiAgwsquGcV26WTk1FbrBKv94zlB7e++Qw619n+Yj1tf/RYfb/TdbmFUVee+hPvq0rkmgBHmvgKTXoLBQExflIf31x2s/iy9QD9r+T6PCZW0fIVEo3r+L7ceQecnv0KZ6ulya5hSP4eaoyeaF+E33cd4gkh3ow71RdfqGKCVez0bW38fot5OerYdPoPHfMwsZ0ak5zfQqqxixFsc1e9OTZ7eRL/DR1UV47kFuzFjyV6Mv7Yt/rViP6YMtjZRz8GTpSirqMSVaSnVy56dvxtl5VU4oeq4sD+IUdJVVRyxjAbyJABgWGaa3UVwpG/2Bt9Dxm3999bSZFhhKlgFENEOnQquqiKYz9ZTXlmFtftPej0f+K12sXCtOltWbvrp8B9f699ABDOu4fBpVxtBWUUl/rXClaPq003+R2yrf8XX/32ZV9LBU0p6koMBVD/pfX3aXFThJEEAQPP6dewugiNtLQxtf/lw07sYupdpG3jNsPv8tRfT577ajbFz1gZ1B+tP96mLMGT6clPbbvQzRerzC/IMn+6MRopbTTCoZRQQ3Y3oc1TJD7W01Wu+Yqdn1WV4H5klCAjHKK8I7I9pn48RyP9e6+rSWXCyNKwTk/hkofW98IdSr7aRQ6dcd8d7Q1T+c2UVOK2MGTmtM3bkiEFbQ0VlFZbuLjI8bnllldfsXLsjPBtbMBbvND43LXW2XfcNht7vMhQkCCB6+uuK8PrTvMAmrzN7lz/0Rf99w0PFYxIck9VBlVWMgc8tQ7epi/D9ifPVDcMHT5X6bC/5cMMhS72NBk9fgZ5PLQbgOceAv8FT/1y+D794K9dw/R8/34Y+z3yNi5q+/9sPn6mu5jHLag2a2c2X54WmilPdwDx/m2ua1uNnw5NiRIIAgIR4+TU4Qaj+QKPBX/63w3LahyrVle/dNQc80i74q6fXDsAzy8rF9qCfNpKvlIuhdk6KW15ZhQHTlloum5FHPvbu0usOgueD6FzwzpoDHhPVqxMd2kl6BwFISpRfg/AvmnLtvLfuIBonJwS8/xvfej7dFJdc9JmUrdzH4DCzQpVV0yg1xid+usSGQnnl5XPQJjQ0Y/L7m7Dq94MBqMeIeP9efE2+FGpyCwzgxk6pdhdBCMtOWZgw58DJ83jJR3fdm2esDEtKays9efxdxEt07sLVNUx6d/ChsM5gTMRcH43AwbI6+j4YEgQA9G7TyO4iCBEwM5fZn7+5Af8M8wxjWunZOXhV6bcfCHcj8L9W7EN6dnAD83wp85M+5Kdz1oasf47Zh6ELOmUKV8pvCQJCOMBFk3mSQs0oU6s7S6uvHER7j5/DyXMX8e5a42yvv3rbuCHZrJNhnoJUzeyMfJFkKggQ0XAiyiOifCLK1lnfmYjWENFFInrEzL5E1JiIFhPRXuWn3I6LqLY+iIa8ud9Yrzr4dGMhrvrTgoA/U60syDpmo3vQMxfKUVJmrrFU3TtooNKQ668K5/QF36k+gsm9ZMU6i3N2A757HS7acSyI0oSW3yBARPEAZgIYASATwDgiytRsdgrAFAAvWNg3G8ASZs4AsER5L0SN9Mz8XZb3efKL7ThvYnStmSoGK+0HZq3cW4zZK8xXMakTDJZcrEBJWTn+u9ne1BRmTXp/k9eyYJ4gXjPR7ThSM5aZeRLoCyCfmfcz8yUAHwIYpd6AmY8z8wYA2lL72ncUgLeV128DGB3YKQgROTOXBV7HbVWpTcn3zPKVSVSPtp6729RFpvaL1nE8gQRW3fmLldioHWzYfeoij15Z4cq1ZaZvZEsA6r5QhQCuMXl8X/umMfNRAGDmo0TUTO8ARDQBwAQAaNOmjcmPFSI8/r4wz+4ieHl1WT4uVkR3wAiGe0RzTeAroOkNNtRLzx5qZp4EdIOXyeMHs69rY+Y5zJzFzFmpqdKVUwg9c8PcpXD0zG91l1eFORVszlbjeaCdIBKZds0EgUIArVXvWwEwW5Hna98iImoBAMpPa7NGCyFs587EGS55xwIbqRyt3AMOz2oavG3LOwVzQWADgAwiakdECQDGAphn8vi+9p0H4B7l9T0AvjBfbCGEiG6LdngnjLtUWYVT5y9htyq4nTx/KaJ5p7T8tgkwcwURTQawEEA8gDeYeQcRTVTWzyai5gByAdQHUEVEDwLIZOazevsqh54G4CMi+iWAgwDuCPG5CSFiXDDdcu222WBmsVzNOflK3qceIBaumiFTSXOYeT6A+Zpls1Wvj8FV1WNqX2X5SQBDrBQ2Enq0bmh57lkhRHhoU0fXRAdOhm/+BjNkxLDimTFdMbBjU8tTzQkhhBVrNQPPPlhvnIguWhqGHeFn17TFv39ltuerEEIERpvB1ZfCH8LfPVaCgEa0DkwRQjiPr1ntQkWCgBBCRKlwpqt2kyAghBBR6ruDp8P+GRIENOLj5FcihHAOueJpXNOusd1FEEKIiJEgoBEfJy3DQojoE67uohIENCQECCGcRIKABkkfUSGEg0gQEEIIB5MgoCHPAUKIaMRhSiEnQUBDaoOEEE4iQUBD2gSEEE4iQUDHM2O64slbMu0uhhBChJ0EAR0/u6Ytfjmwnd3FEEKIsJMgIIQQMUAGi9ng+itT7S6CEEKElQQBH975RV8s+t31dhdDCCHC1nNRgoAfV6al2F0EIYQIGwkCJlzRoI7dRRBCOJytbQJENJyI8ogon4iyddYTEc1Q1m8lot7K8k5EtFn17ywRPaism0pEh1Xrbg7pmYWQjB0QQtRUtfxtQETxAGYCGAagEMAGIprHzDtVm40AkKH8uwbALADXMHMegJ6q4xwG8Llqv5eY+YUQnIcQQtRodrYJ9AWQz8z7mfkSgA8BjNJsMwrAO+yyFkBDImqh2WYIgH3MfCDoUgshhMNsDtNUk2aCQEsAh1TvC5VlVrcZC+ADzbLJSvXRG0TUyERZbCG1QUIIu20/fCYsxzUTBPQugdomCp/bEFECgFsBfKxaPwtAB7iqi44CmK774UQTiCiXiHKLi4tNFDf0rsuQ8QJCiJrJTBAoBNBa9b4VgCMWtxkBYBMzF7kXMHMRM1cycxWAuXBVO3lh5jnMnMXMWamp9lyMx/Vt7X8jIYSIQWaCwAYAGUTUTrmjHwtgnmabeQDGK72E+gE4w8xHVevHQVMVpGkzGANgu+XSR0hCLelJK4Somfz2DmLmCiKaDGAhgHgAbzDzDiKaqKyfDWA+gJsB5AMoBXCve38iSoKrZ9F9mkM/T0Q94ao2KtBZHzU6paVg6o8z0bNNI4ye+a3dxRFCiJDxGwQAgJnnw3WhVy+brXrNACYZ7FsKoInO8rstldRGRISfD5CsokII+4RprJiMGLbqgSEZdhdBCOFAkkU0Svxu2JV2F0EI4UAyx7AQQoiQkyAghBAOJkEgAH3TG9tdBCGECAkJAgGYOz7L7iIIIRxGGoajSIOk2nhiZBe7iyGEEEGTIBCgX13X3u4iCCEcRMYJxJAr0+rZXQQhhDBFgkAIaNsIXhnX26aSCCFqqnC1CZhKGyH0LX/kRhw+fQEDOjb1WN6puUxOL4QItfBEAQkCQUhvmoz0psl2F0MI4Qjhmd1KqoNC7ItJA+wughBCmCZPAiHyv8kDkZqSiOYN6thdFCFEjSTVQVGtW6sGdhdBCCEsk+ogIYSIATJiWAghRMhJEAizRJmfWAgRxeQKFWY/6d3K1Hbzp1wX5pIIIWKZpI2IUd1ammswzryifphLIoQQ3iQICCGEg5kKAkQ0nIjyiCifiLJ11hMRzVDWbyWi3qp1BUS0jYg2E1GuanljIlpMRHuVn41Cc0rRIattIwzLTAOFZ5CfEEKEhN8gQETxAGYCGAEgE8A4IsrUbDYCQIbybwKAWZr1g5i5JzOrM61lA1jCzBkAlijva4xPfnOtV2K51JREm0ojhIh1HKY+omaeBPoCyGfm/cx8CcCHAEZpthkF4B12WQugIRG18HPcUQDeVl6/DWC0+WLHDvWDQLw8FgghAmRnw3BLAIdU7wuVZWa3YQCLiGgjEU1QbZPGzEcBQPnZzErBY8Woni3lCUAIEbXMBAG921dtUPK1zQBm7g1XldEkIrreQvlARBOIKJeIcouLi63sGhXqJsTjHz/t6XObOHlAEELYxEwQKATQWvW+FYAjZrdhZvfP4wA+h6t6CQCK3FVGys/jeh/OzHOYOYuZs1JTU00UN/as/+NQu4sghHAoM0FgA4AMImpHRAkAxgKYp9lmHoDxSi+hfgDOMPNRIkomohQAIKJkADcB2K7a5x7l9T0AvgjyXKIeG9TqNa1nvrqob3rjUBVHCBFDbMsdxMwVACYDWAhgF4CPmHkHEU0koonKZvMB7AeQD2AugN8qy9MArCKiLQDWA8hh5gXKumkAhhHRXgDDlPc1UrC1PT1UGUo/mtgfn/322iCPKIQQLqZSSTPzfLgu9Opls1WvGcAknf32A+hhcMyTAIZYKWysa9skGddnpOLGTs1wobwSj3y8xe8+Tesl4ovJA5GenXN5WbI0NAvhNP+XZS4FjVUyYjgC3E9x8UT4+x09MLJ7C9x+deBfaJsmSVj44PWYMiQjNAUUQkS9Nk3CM5WtBIEICmSYgNE+nZqn4MEhGch9wrtR+dPfSHWREDWOjYPFRJSKiyPdRmUiBPWkIYRwDgkCUeSln+o2nwTkmTFdMX/KdUir7x0k7ru+vcf7oV2aha2+UQgRGpJKOobVjnf9musl+m6HH9PL+0LcNYAU0wQgsVa8YXpq7X+m269ujVE9tYPAhRBOIEEgAvqkN8LjIzrjudu6+912fP+2+OfPqpOw4pU7e/vYOjD+ElHd0t1f2idj6U2SAt5XCGEsXOMETHURFcEhItx3QwdT2z41qisA4M8/zsRNVzWvfnq4f3BHXKqsMnWMjs3qXf5snVEK/v4zNUupg5TEWii5WGHq84QQ4WdnFlFhg3sHtEPLhnWr3z98Uyc8PqKL3/06pCYjpU7t6veP39zZ8mdf26EJBnRs6nObYZlpqB0fm0mP5tx9td1FEMKyKrtGDIvwqlM7+K9gxaM3YqZSbdQoKcFj3aieLTGgYxOPZXr/l9w3GT1aNcDQzDTc4aOh+Lc3dsCMsb2w6veDvdZRDKTLHtS5RiasFTVcuBqGpTrIRrv/Ojwkx2nbJBltmyTjVGlX/OiqNK/1dWt7fs2+nirr1XFtO6RLGj79zbW4bdZqr20eG+56uqibEG94nKUP34DB01d4LOuQmox9xeeNPzxCwlW3KkQ4SXVQDVSndjzq1Da+kFp1d7+2aJZSx2v52D6tPd4bJbLzZVzfNpj640zcP7ij4TZ3XtMGt/V29TJqkpyIlY8N8ugRFS1PCYGcvxA1lTwJOMDQzDQUTBuJ9d+fwr7ic8g7VmJp/4T4ODz7k24+t2mWkoinR3UFEfDr69sjsVY8GiTVxjePDULvvy7W3adpvUScOHfRUlmEcKoqeRIQwerbrjHG9W1jeb+rWvofq5BQKw5xcQQiQmKty083jZMTfOwlhLCbBAEBAKhf1/VQ2KZxYEmqzNykuNsrfjmwHQDgtXuyfG7fq01DAK5cSC+P7YncJ4Zi8iDj6iizpE1AxCIZJyBCRq+BqXurhnj9niyPrqH1lUbidiHKXvjwsE745cD2aJycgCdvyQTgqkY6XuK/Ssg9ojkjrZ6fLV1tIB9uOOR3OyFiiaSNECFz01XNAQCtG9f1WD6kS5pHQ3VGWgreurcPnhnjuz3ArLg48qoe8vUfu0UDVyN3XYuN5z/q2lx3+dzxWfjqgetM31EN1ulK+vzt3fGTXpJiQ0SebTOLiZpnQMemKJg2Eq+O643GyQm4pp3xlJU3dmrmsytoOD13W3e8PLanRw4kXz2M3EFjUKdmuPMa77aPYZlp6NLCfC6mKxp697Tq374JftI7epPtrXxskN1FEGEiDcMi5Hq0bohNTw5DIxsbb2vHuS7qjZIuj3L+y61XIfeJoUipU9tSYrsVjw5C3tOusRddmqcYblfL5EjnsX0uB5LsEa6xEYH+rvSytM4McV6oW3tcgdaNw5e76bHhncJ2bGEfCQIi7BY+eD1WZ3uPLla7um2j6tcZafV050kAgDgf1++EWnHVPZPu6tfWY6T05j8Nq37tzurqS4/WDT2CxcQbOqBg2kjUS6zlMc7gxz2uwKt39vJZLgAYq9Mra6SJRH03dkr1u42bUU+sTmnGAdGK394YfKO8CJwMFhNRadXvB6FVo7r4++3GGVI7NU/BFQ3rGq4H4NEWEeejysedEE970e3RqoHndkTo0vxy1U/DJPN38E3rJeI/E/r53W5gx6Z4ZVwv3NL9CvxaM0dDqATzd99d+Z1M8jHAT23Fozf63aZDanimOBT+SZuAiEqtGiVh1e8H41o/Cef8eUA1X3K8j9tqo/jQtWUD/RUWvf+ra5D7xFDUqR2PNJ3R10YmDeqIO69pY/gEY6SlJji+PLanpf19aZZirSwN6/oPlEMzvdOSAEBbVQpxmfs6PG7sFJ6cVxIEhK3cNzdJibWqxwX4qlpp19R1J9q/QxPjjTTHDpSV+v/6dWrjb2O64ZOJ/fHQsCuRbLIxfekjN3i875Dq2QXW1zk8Neoqj/feAdJamo44H1cDd6LDQQYXoms7XL4JeGjYlT4/544ITX36wh2hm6kvGiQlhqeDhqkgQETDiSiPiPKJKFtnPRHRDGX9ViLqrSxvTUTLiGgXEe0gogdU+0wlosNEtFn5d3PoTkvEGsLlVLm+qoO6tKiPdX8Ygrv7tQWA6jvvvjo9nPzV0+tp1cizYfXNe/vgr6O7eixLSnCNn9C7005vmowpQzKw7NEbkTNloN/PS6wVjweH6t85927T0KMe+NYeV1S/3vvMCIzvn47lj9yIh5WLrt7cEVaoU5Abl1f/kuEr62yg3v/VNUHtX9Pm2W7dKDyN/n6DABHFA5gJYASATADjiChTs9kIABnKvwkAZinLKwA8zMxdAPQDMEmz70vM3FP5Nz+4UxGxrrtSpeMv1URa/Tpw3+X2btMQuU8M1e1FdL+faomuLevjo/v6V7/PfWIo2mhmRhvUqVl1wHG7um0jTL+jh1dwUGuWUgdXXXG5iirFx9SiUwabqz5RV7O4G7fTmyZb6sKrDiRWuANMz9YNdc873kfgfvPePhjXt7XhereNTwz1eO8OyOP7t8WSh2/ADVeabyR/fIS5eTTu7tcW3xp0Whjd8wrcd0N42nqs6tw8BQkGAThYZo7aF0A+M+9n5ksAPgQwSrPNKADvsMtaAA2JqAUzH2XmTQDAzCUAdgGQkTaiWr/2rmqdpIR4PHlLJr68fyDaWhyhbFQPX79ObcN1W/50Ez6ZeK3HE4SV+vzbrm6FZD9zRqs1qFsbq7MHew3QA6DkXHK91raH9Gzd0PRnqI3s5t3zKLFWHLr5aTvJUvXS0kNEuLtfW3z222tNl6VvemM8+5PLHQf0qrh+NbAdmtRL9KhKatMkCcsfuRF//vFV6JBaD51bmO/l5J7J7/7BHfHOL/oabvfX0V3RsmFdjO/f1mvdP8b2wuMjuoS0ncYXbbZftZfH9grb55oJAi0BqMfgF8L7Qu53GyJKB9ALwDrV4slK9dEbRKT7v4+IJhBRLhHlFhcXmyiuiCXTbuuGrx+6AQ2TEpBQKy5kDbxuXz1wHf432btapkFS7ZCm8fYnPo5wRcO6ePvevshq2whv3dvHY/2X9w/EQ8OuROfmKR53vA8OvXxR9JeJ271+91+HY8Y4/YuGv2obd14nQLn7NOhO27NVQ9+Fscg9vkHbqJzeNLk6MAbS1fXhmzrhehNPEO5pXfX0au07MEZCOLOwmwkCeh+vDeY+tyGiegA+BfAgM59VFs8C0AFATwBHAUzX+3BmnsPMWcyclZpq/nFQxIbEWvEecyKb5W5EvlvnDk4tNSUR3VqFNrBYseGPQ/GPn/ZEE+Upo31qPXzym2u9enpcdUUDTBmSASKqnrOBiBCvSrXh/iPTtnVouw7WqR1v2MOqYVICVjx6Y/WgOl/mT7muul2jVSPPJ5g4neP7axD+x0974hYTYyOm/jgTH0/s77V8TK+WmG7Q2Pvmz/vg1TvDc7fcpkkShviYjS6Yahp19dwvBrbDoE6pmDyoY9iqfvSY+aRCAOrnlFYAjpjdhohqwxUA3mPmz9wbMHMRM1cycxWAuXBVOwlhSlr9OiiYNhLXZUT3jUFqSiJGB5lraPZdV2NYZlr11KH1DKqhtJflnympM9wD8e5TxjK0bZLske5bz/CrmiMujpCRloLZd/XGe7/230g7ZUgGCqaN9C6XUrDRvVriVYNR0uoG8J8PaIc+6d4N/URkWCU0qHMz3NLduL3j59em+yi5N23eqNd/3gcN6uo3nO95eoSlY+vpkJqMK9NS8Oa9ffHIjzqhT3rknj7MBIENADKIqB0RJQAYC2CeZpt5AMYrvYT6ATjDzEfJlejldQC7mPlF9Q5EpL4lGANge8BnIUQN1rddY8wdn1Xda0r7GG40U9qgzs1QMG0kWjasi4JpI/HQTYGlfRjetYXujHVqbTTpKl73kSY8VIOerssIbmyKVeEYsZveNBkju7fwW+cfzjn5/AYBZq4AMBnAQrgadj9i5h1ENJGIJiqbzQewH0A+XHf1v1WWDwBwN4DBOl1BnyeibUS0FcAgAL8L2VkJEcM6NU9BUoJO11GTbQJWfTKxf0DzNPykd0u0aZyEgmkjvcZUDOmSZin7q7ZrrhmP6AS1R3/UKeAeUMHI1fRs8kVd/VkrjjDzzt5ebWHqeDNpUIeAqkzNMtW9Qem+OV+zbLbqNQOYpLPfKhj812Xmuy2VVIgwyWhWD3uPn7O7GNVS6tTGzqe86+zdF3mjxlqr3FUkWemNkaVUvwzq3AzDMtPwx5Fd/O7/4v/19Ln+gwn98MnGQz6DwfQ7eiC9aRKubmucydaKSQbBbGT3FnhrdYHp4+hV/Tw1qiv+PG8Hzlwo91pnpmcZkeviPrRLGjqlpSBn21FTZXn0R+a6uwZKJpURjvfRff2x/8R5u4vhV/06tfG7oVdiZHfP+RJc4ybgNz+T1tRbr/JaVqd2POaO9z3jm1k9Wzf02cX1udu64bYgBnR1a9kAXVrUx66jZ/1u2ye9MQqmjUR6dg4A4OFhV+Lz7w7rbtskOQHP6+TCGt2rJUb3all9DLOu7dAEq/ed9FjWPopyMEkQEI7XKDkBV8fIXMgP6IwuvrXHFaiXWMswpUO0cbdh+BoZbrivUk3SpUV9xMURPp7YHz+cv2T5OPcPyTAcTLjxyWG6y9WaJCfgpMHname2cwcBgqs9x3X+vs/9hitTvQJHuEjuICFiHBFhSJc03W6b0czXBEF+91V+1kusFdI5FMwMzpt+Rw/8d9IAw/XP/qQb9v/NOwuO+nzdT23NG+g3uE8IU1ZaPfIkIIQQcI2LaKUzolvLqArrzXv7IKl2PIhcI8D/L6sVPsotrO7u2q99Y3yb77q7H9unNZqlJOpOYQoEFyCtkiAghBCAxzSmZnzz6CCUlldUv9dWxz1/ew88f7trcNuup4bjrdUF1UHA/fQWDaQ6SAgRUe4xBU3rxUY7jJE2TZLQubm5wFE3IR7DMl1B4sc+BrWpGQ1OCzV5EhBCRNTkQR3RvVWDgCZJ6dw8BSO7t6hOrRFLOjZL0R1RbWTZIzfqdkcNNQkCQoiIqhUfh8GdA6sKqRUfh5kGqSdqmsbJCX7TqoeCBAEhhGM8f3v36tnphIsEASGEY/xflv/JbZxGGoaFEMLBJAgIIYSDSRAQQggHkyAghBAOJkFACCEcTIKAEEI4mAQBIYRwMAkCQgjhYBSOyZPDhYiKARwIcPemAE6EsDh2qinnUlPOA5BziUY15TyA4M+lLTOn6q2IqSAQDCLKZebQzJtns5pyLjXlPAA5l2hUU84DCO+5SHWQEEI4mAQBIYRwMCcFgTl2FyCEasq51JTzAORcolFNOQ8gjOfimDYBIYQQ3pz0JCCEEEJDgoAQQjiYI4IAEQ0nojwiyieibLvLo4eICohoGxFtJqJcZVljIlpMRHuVn41U2z+unE8eEf1Itfxq5Tj5RDSDiCgCZX+DiI4T0XbVspCVnYgSieg/yvJ1RJQewfOYSkSHle9lMxHdHAPn0ZqIlhHRLiLaQUQPKMtj8TsxOpdY/F7qENF6ItqinMtflOX2fi/MXKP/AYgHsA9AewAJALYAyLS7XDrlLADQVLPseQDZyutsAM8przOV80gE0E45v3hl3XoA/QEQgK8AjIhA2a8H0BvA9nCUHcBvAcxWXo8F8J8InsdUAI/obBvN59ECQG/ldQqAPUp5Y/E7MTqXWPxeCEA95XVtAOsA9LP7ewnrxSEa/im/qIWq948DeNzucumUswDeQSAPQAvldQsAeXrnAGChcp4tAOxWLR8H4F8RKn86PC+eISu7exvldS24Rk5ShM7D6GIT1eehKesXAIbF6ndicC4x/b0ASAKwCcA1dn8vTqgOagngkOp9obIs2jCARUS0kYgmKMvSmPkoACg/mynLjc6ppfJau9wOoSx79T7MXAHgDIAmYSu5t8lEtFWpLnI/qsfEeSjVAb3guuuM6e9Ecy5ADH4vRBRPRJsBHAewmJlt/16cEAT06sSjsV/sAGbuDWAEgElEdL2PbY3OKRbONZCy23leswB0ANATwFEA0/2UKWrOg4jqAfgUwIPMfNbXpjrLov1cYvJ7YeZKZu4JoBWAvkTU1cfmETkXJwSBQgCtVe9bAThiU1kMMfMR5edxAJ8D6AugiIhaAIDy87iyudE5FSqvtcvtEMqyV+9DRLUANABwKmwlV2HmIuUPtwrAXLi+F48yacobFedBRLXhumi+x8yfKYtj8jvRO5dY/V7cmPk0gOUAhsPm78UJQWADgAwiakdECXA1lsyzuUweiCiZiFLcrwHcBGA7XOW8R9nsHrjqQ6EsH6v0BGgHIAPAeuVRsoSI+im9Bcar9om0UJZdfazbASxlpdIz3Nx/nIoxcH0v7jJF5Xkon/s6gF3M/KJqVcx9J0bnEqPfSyoRNVRe1wUwFMBu2P29hLshJxr+AbgZrl4F+wD80e7y6JSvPVy9ALYA2OEuI1x1eUsA7FV+Nlbt80flfPKg6gEEIAuuP4h9AF5FZBrrPoDrkbwcrjuRX4ay7ADqAPgYQD5cvSLaR/A83gWwDcBW5Q+sRQycx0C4qgC2Atis/Ls5Rr8To3OJxe+lO4DvlDJvB/AnZbmt34ukjRBCCAdzQnWQEEIIAxIEhBDCwSQICCGEg0kQEEIIB5MgIIQQDiZBQAghHEyCgBBCONj/A62mfFBIlq0YAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  25 Training Time 5817.549056053162 Best Test Acc:  0.9817142857142858\n",
      "test subjects:  ['./seg\\\\x02']\n",
      "*********\n",
      "33844 469\n",
      "32422 469\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.6437027454376221\n",
      "Eval Loss:  0.7129105925559998\n",
      "Eval Loss:  0.6410883665084839\n",
      "[[  379 19525]\n",
      " [  308 12210]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.02      0.04     19904\n",
      "           1       0.38      0.98      0.55     12518\n",
      "\n",
      "    accuracy                           0.39     32422\n",
      "   macro avg       0.47      0.50      0.29     32422\n",
      "weighted avg       0.49      0.39      0.24     32422\n",
      "\n",
      "acc:  0.3882857319104312\n",
      "pre:  0.3847487001733102\n",
      "rec:  0.9753954305799648\n",
      "ma F1:  0.2943195971423265\n",
      "mi F1:  0.38828573191043114\n",
      "we F1:  0.23565728022936713\n",
      "[[  4 256]\n",
      " [  3 206]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.02      0.03       260\n",
      "           1       0.45      0.99      0.61       209\n",
      "\n",
      "    accuracy                           0.45       469\n",
      "   macro avg       0.51      0.50      0.32       469\n",
      "weighted avg       0.52      0.45      0.29       469\n",
      "\n",
      "acc:  0.44776119402985076\n",
      "pre:  0.4458874458874459\n",
      "rec:  0.9856459330143541\n",
      "ma F1:  0.32198574434713684\n",
      "mi F1:  0.44776119402985076\n",
      "we F1:  0.29023055655595664\n",
      "Subject 26 Current Train Acc:  0.3882857319104312 Current Test Acc:  0.44776119402985076\n",
      "Loss:  0.1726057380437851\n",
      "Loss:  0.1656755656003952\n",
      "Loss:  0.1532415747642517\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.15268006920814514\n",
      "Loss:  0.12665072083473206\n",
      "Loss:  0.12763099372386932\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.1009373590350151\n",
      "Loss:  0.10275030881166458\n",
      "Loss:  0.12366792559623718\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.6495417356491089\n",
      "Eval Loss:  0.242467999458313\n",
      "Eval Loss:  0.19183677434921265\n",
      "[[16912  2992]\n",
      " [ 2695  9823]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.85      0.86     19904\n",
      "           1       0.77      0.78      0.78     12518\n",
      "\n",
      "    accuracy                           0.82     32422\n",
      "   macro avg       0.81      0.82      0.82     32422\n",
      "weighted avg       0.83      0.82      0.82     32422\n",
      "\n",
      "acc:  0.8245944112022701\n",
      "pre:  0.7665236051502146\n",
      "rec:  0.7847100175746925\n",
      "ma F1:  0.8157878017953151\n",
      "mi F1:  0.8245944112022701\n",
      "we F1:  0.8249633719857185\n",
      "[[207  53]\n",
      " [  9 200]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.80      0.87       260\n",
      "           1       0.79      0.96      0.87       209\n",
      "\n",
      "    accuracy                           0.87       469\n",
      "   macro avg       0.87      0.88      0.87       469\n",
      "weighted avg       0.88      0.87      0.87       469\n",
      "\n",
      "acc:  0.8678038379530917\n",
      "pre:  0.7905138339920948\n",
      "rec:  0.9569377990430622\n",
      "ma F1:  0.8677743824802648\n",
      "mi F1:  0.8678038379530917\n",
      "we F1:  0.8679889866394318\n",
      "Subject 26 Current Train Acc:  0.8245944112022701 Current Test Acc:  0.8678038379530917\n",
      "Loss:  0.08549243956804276\n",
      "Loss:  0.11683745682239532\n",
      "Loss:  0.09269025176763535\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.11567257344722748\n",
      "Loss:  0.10547994822263718\n",
      "Loss:  0.09832204133272171\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.076084665954113\n",
      "Loss:  0.10185350477695465\n",
      "Loss:  0.06114273890852928\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  1.404428482055664\n",
      "Eval Loss:  0.02217686176300049\n",
      "Eval Loss:  0.09610092639923096\n",
      "[[18987   917]\n",
      " [ 4218  8300]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.95      0.88     19904\n",
      "           1       0.90      0.66      0.76     12518\n",
      "\n",
      "    accuracy                           0.84     32422\n",
      "   macro avg       0.86      0.81      0.82     32422\n",
      "weighted avg       0.85      0.84      0.84     32422\n",
      "\n",
      "acc:  0.8416198877305533\n",
      "pre:  0.9005099273082348\n",
      "rec:  0.6630452148905576\n",
      "ma F1:  0.82231422689836\n",
      "mi F1:  0.8416198877305533\n",
      "we F1:  0.8356567562243242\n",
      "[[245  15]\n",
      " [ 38 171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.90       260\n",
      "           1       0.92      0.82      0.87       209\n",
      "\n",
      "    accuracy                           0.89       469\n",
      "   macro avg       0.89      0.88      0.88       469\n",
      "weighted avg       0.89      0.89      0.89       469\n",
      "\n",
      "acc:  0.8869936034115139\n",
      "pre:  0.9193548387096774\n",
      "rec:  0.8181818181818182\n",
      "ma F1:  0.8841084458120614\n",
      "mi F1:  0.886993603411514\n",
      "we F1:  0.8860968652387109\n",
      "Subject 26 Current Train Acc:  0.8416198877305533 Current Test Acc:  0.8869936034115139\n",
      "Loss:  0.08001624792814255\n",
      "Loss:  0.12361672520637512\n",
      "Loss:  0.11250371485948563\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.11425520479679108\n",
      "Loss:  0.06890501081943512\n",
      "Loss:  0.10191848874092102\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.09186059981584549\n",
      "Loss:  0.1160627156496048\n",
      "Loss:  0.07990037649869919\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  1.437256932258606\n",
      "Eval Loss:  0.017883658409118652\n",
      "Eval Loss:  0.11953157186508179\n",
      "[[19205   699]\n",
      " [ 4317  8201]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.96      0.88     19904\n",
      "           1       0.92      0.66      0.77     12518\n",
      "\n",
      "    accuracy                           0.85     32422\n",
      "   macro avg       0.87      0.81      0.83     32422\n",
      "weighted avg       0.86      0.85      0.84     32422\n",
      "\n",
      "acc:  0.8452902350255999\n",
      "pre:  0.9214606741573034\n",
      "rec:  0.6551366032912606\n",
      "ma F1:  0.8251488121573003\n",
      "mi F1:  0.8452902350256\n",
      "we F1:  0.8386679460454555\n",
      "[[247  13]\n",
      " [ 47 162]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89       260\n",
      "           1       0.93      0.78      0.84       209\n",
      "\n",
      "    accuracy                           0.87       469\n",
      "   macro avg       0.88      0.86      0.87       469\n",
      "weighted avg       0.88      0.87      0.87       469\n",
      "\n",
      "acc:  0.8720682302771855\n",
      "pre:  0.9257142857142857\n",
      "rec:  0.7751196172248804\n",
      "ma F1:  0.8677233754512634\n",
      "mi F1:  0.8720682302771855\n",
      "we F1:  0.8703302883468166\n",
      "Loss:  0.06631769239902496\n",
      "Loss:  0.061413805931806564\n",
      "Loss:  0.08508386462926865\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.09212032705545425\n",
      "Loss:  0.06646314263343811\n",
      "Loss:  0.07269991189241409\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.10010181367397308\n",
      "Loss:  0.0874394029378891\n",
      "Loss:  0.09916922450065613\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  1.3067741394042969\n",
      "Eval Loss:  0.01811814308166504\n",
      "Eval Loss:  0.13706660270690918\n",
      "[[19147   757]\n",
      " [ 3857  8661]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89     19904\n",
      "           1       0.92      0.69      0.79     12518\n",
      "\n",
      "    accuracy                           0.86     32422\n",
      "   macro avg       0.88      0.83      0.84     32422\n",
      "weighted avg       0.87      0.86      0.85     32422\n",
      "\n",
      "acc:  0.8576892233668497\n",
      "pre:  0.9196220004247186\n",
      "rec:  0.6918836874900144\n",
      "ma F1:  0.8410642183092256\n",
      "mi F1:  0.8576892233668497\n",
      "we F1:  0.8527743353562991\n",
      "[[250  10]\n",
      " [ 54 155]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.96      0.89       260\n",
      "           1       0.94      0.74      0.83       209\n",
      "\n",
      "    accuracy                           0.86       469\n",
      "   macro avg       0.88      0.85      0.86       469\n",
      "weighted avg       0.87      0.86      0.86       469\n",
      "\n",
      "acc:  0.8635394456289979\n",
      "pre:  0.9393939393939394\n",
      "rec:  0.7416267942583732\n",
      "ma F1:  0.8577009140213145\n",
      "mi F1:  0.8635394456289979\n",
      "we F1:  0.8608352836212287\n",
      "Loss:  0.09330131113529205\n",
      "Loss:  0.08978800475597382\n",
      "Loss:  0.07269196212291718\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.06007068604230881\n",
      "Loss:  0.07722324132919312\n",
      "Loss:  0.0788295567035675\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.08365831524133682\n",
      "Loss:  0.0643741637468338\n",
      "Loss:  0.07382126152515411\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  1.2366790771484375\n",
      "Eval Loss:  0.01708090305328369\n",
      "Eval Loss:  0.12017661333084106\n",
      "[[19185   719]\n",
      " [ 3432  9086]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.96      0.90     19904\n",
      "           1       0.93      0.73      0.81     12518\n",
      "\n",
      "    accuracy                           0.87     32422\n",
      "   macro avg       0.89      0.84      0.86     32422\n",
      "weighted avg       0.88      0.87      0.87     32422\n",
      "\n",
      "acc:  0.8719696502374931\n",
      "pre:  0.9266700662927078\n",
      "rec:  0.7258347978910369\n",
      "ma F1:  0.8582129698457743\n",
      "mi F1:  0.8719696502374931\n",
      "we F1:  0.8682740492965353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[253   7]\n",
      " [ 64 145]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.97      0.88       260\n",
      "           1       0.95      0.69      0.80       209\n",
      "\n",
      "    accuracy                           0.85       469\n",
      "   macro avg       0.88      0.83      0.84       469\n",
      "weighted avg       0.87      0.85      0.84       469\n",
      "\n",
      "acc:  0.8486140724946695\n",
      "pre:  0.9539473684210527\n",
      "rec:  0.69377990430622\n",
      "ma F1:  0.8401369198788269\n",
      "mi F1:  0.8486140724946695\n",
      "we F1:  0.8441400197251969\n",
      "Loss:  0.04947034642100334\n",
      "Loss:  0.06642884016036987\n",
      "Loss:  0.09442103654146194\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.05674048140645027\n",
      "Loss:  0.04692543298006058\n",
      "Loss:  0.09961455315351486\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.062081027776002884\n",
      "Loss:  0.0632084384560585\n",
      "Loss:  0.07684505730867386\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  1.31813383102417\n",
      "Eval Loss:  0.014595985412597656\n",
      "Eval Loss:  0.13438916206359863\n",
      "[[19283   621]\n",
      " [ 3487  9031]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.90     19904\n",
      "           1       0.94      0.72      0.81     12518\n",
      "\n",
      "    accuracy                           0.87     32422\n",
      "   macro avg       0.89      0.85      0.86     32422\n",
      "weighted avg       0.88      0.87      0.87     32422\n",
      "\n",
      "acc:  0.8732959101844426\n",
      "pre:  0.9356610029009532\n",
      "rec:  0.7214411247803163\n",
      "ma F1:  0.8592199256009978\n",
      "mi F1:  0.8732959101844426\n",
      "we F1:  0.869360895376\n",
      "[[257   3]\n",
      " [ 82 127]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.99      0.86       260\n",
      "           1       0.98      0.61      0.75       209\n",
      "\n",
      "    accuracy                           0.82       469\n",
      "   macro avg       0.87      0.80      0.80       469\n",
      "weighted avg       0.86      0.82      0.81       469\n",
      "\n",
      "acc:  0.8187633262260128\n",
      "pre:  0.9769230769230769\n",
      "rec:  0.6076555023923444\n",
      "ma F1:  0.8036796824599504\n",
      "mi F1:  0.8187633262260128\n",
      "we F1:  0.8095971119374056\n",
      "Loss:  0.05846090242266655\n",
      "Loss:  0.11023431271314621\n",
      "Loss:  0.11536389589309692\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.07942068576812744\n",
      "Loss:  0.07079309970140457\n",
      "Loss:  0.08681169897317886\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.08459170907735825\n",
      "Loss:  0.09806393086910248\n",
      "Loss:  0.05124937742948532\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  1.2022998332977295\n",
      "Eval Loss:  0.018871426582336426\n",
      "Eval Loss:  0.1448068618774414\n",
      "[[19189   715]\n",
      " [ 2916  9602]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91     19904\n",
      "           1       0.93      0.77      0.84     12518\n",
      "\n",
      "    accuracy                           0.89     32422\n",
      "   macro avg       0.90      0.87      0.88     32422\n",
      "weighted avg       0.89      0.89      0.89     32422\n",
      "\n",
      "acc:  0.8880081426192091\n",
      "pre:  0.9306969080158961\n",
      "rec:  0.7670554401661607\n",
      "ma F1:  0.8772779246846512\n",
      "mi F1:  0.8880081426192091\n",
      "we F1:  0.8855446806734533\n",
      "[[257   3]\n",
      " [ 94 115]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.99      0.84       260\n",
      "           1       0.97      0.55      0.70       209\n",
      "\n",
      "    accuracy                           0.79       469\n",
      "   macro avg       0.85      0.77      0.77       469\n",
      "weighted avg       0.84      0.79      0.78       469\n",
      "\n",
      "acc:  0.7931769722814499\n",
      "pre:  0.9745762711864406\n",
      "rec:  0.5502392344497608\n",
      "ma F1:  0.7723038884467734\n",
      "mi F1:  0.7931769722814499\n",
      "we F1:  0.7798005594014813\n",
      "Loss:  0.06177999824285507\n",
      "Loss:  0.07290451228618622\n",
      "Loss:  0.030096443369984627\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.06465373933315277\n",
      "Loss:  0.055779844522476196\n",
      "Loss:  0.07104883342981339\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.05898293852806091\n",
      "Loss:  0.07918812334537506\n",
      "Loss:  0.08666722476482391\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  1.5196382999420166\n",
      "Eval Loss:  0.018451929092407227\n",
      "Eval Loss:  0.2419070601463318\n",
      "[[19288   616]\n",
      " [ 2982  9536]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.91     19904\n",
      "           1       0.94      0.76      0.84     12518\n",
      "\n",
      "    accuracy                           0.89     32422\n",
      "   macro avg       0.90      0.87      0.88     32422\n",
      "weighted avg       0.89      0.89      0.89     32422\n",
      "\n",
      "acc:  0.8890259700203565\n",
      "pre:  0.9393223010244287\n",
      "rec:  0.761783032433296\n",
      "ma F1:  0.877987409858655\n",
      "mi F1:  0.8890259700203565\n",
      "we F1:  0.8863478287834219\n",
      "[[260   0]\n",
      " [135  74]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.79       260\n",
      "           1       1.00      0.35      0.52       209\n",
      "\n",
      "    accuracy                           0.71       469\n",
      "   macro avg       0.83      0.68      0.66       469\n",
      "weighted avg       0.81      0.71      0.67       469\n",
      "\n",
      "acc:  0.7121535181236673\n",
      "pre:  1.0\n",
      "rec:  0.35406698564593303\n",
      "ma F1:  0.6584306638254255\n",
      "mi F1:  0.7121535181236673\n",
      "we F1:  0.6731611238749434\n",
      "Loss:  0.046117063611745834\n",
      "Loss:  0.05290709808468819\n",
      "Loss:  0.0614025741815567\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.04394827410578728\n",
      "Loss:  0.07350803911685944\n",
      "Loss:  0.057248618453741074\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.09618217498064041\n",
      "Loss:  0.05322154983878136\n",
      "Loss:  0.0447261705994606\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.7360268235206604\n",
      "Eval Loss:  0.04902374744415283\n",
      "Eval Loss:  0.04506051540374756\n",
      "[[18858  1046]\n",
      " [ 1967 10551]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19904\n",
      "           1       0.91      0.84      0.88     12518\n",
      "\n",
      "    accuracy                           0.91     32422\n",
      "   macro avg       0.91      0.90      0.90     32422\n",
      "weighted avg       0.91      0.91      0.91     32422\n",
      "\n",
      "acc:  0.9070692739497872\n",
      "pre:  0.909804259722342\n",
      "rec:  0.8428662725675028\n",
      "ma F1:  0.9005401225733778\n",
      "mi F1:  0.9070692739497872\n",
      "we F1:  0.9063453846494774\n",
      "[[259   1]\n",
      " [137  72]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      1.00      0.79       260\n",
      "           1       0.99      0.34      0.51       209\n",
      "\n",
      "    accuracy                           0.71       469\n",
      "   macro avg       0.82      0.67      0.65       469\n",
      "weighted avg       0.80      0.71      0.67       469\n",
      "\n",
      "acc:  0.7057569296375267\n",
      "pre:  0.9863013698630136\n",
      "rec:  0.3444976076555024\n",
      "ma F1:  0.650136222106902\n",
      "mi F1:  0.7057569296375267\n",
      "we F1:  0.6653055059788905\n",
      "Loss:  0.06297267228364944\n",
      "Loss:  0.08754060417413712\n",
      "Loss:  0.03472404554486275\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.07215075939893723\n",
      "Loss:  0.03898671641945839\n",
      "Loss:  0.026318402960896492\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.04468522593379021\n",
      "Loss:  0.04325169324874878\n",
      "Loss:  0.07610224932432175\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.5582275390625\n",
      "Eval Loss:  0.026527881622314453\n",
      "Eval Loss:  0.06571853160858154\n",
      "[[18813  1091]\n",
      " [ 1891 10627]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19904\n",
      "           1       0.91      0.85      0.88     12518\n",
      "\n",
      "    accuracy                           0.91     32422\n",
      "   macro avg       0.91      0.90      0.90     32422\n",
      "weighted avg       0.91      0.91      0.91     32422\n",
      "\n",
      "acc:  0.9080254148417741\n",
      "pre:  0.9068953746373101\n",
      "rec:  0.8489375299568621\n",
      "ma F1:  0.9017630441127182\n",
      "mi F1:  0.9080254148417741\n",
      "we F1:  0.9074134069522988\n",
      "[[259   1]\n",
      " [113  96]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      1.00      0.82       260\n",
      "           1       0.99      0.46      0.63       209\n",
      "\n",
      "    accuracy                           0.76       469\n",
      "   macro avg       0.84      0.73      0.72       469\n",
      "weighted avg       0.83      0.76      0.73       469\n",
      "\n",
      "acc:  0.7569296375266524\n",
      "pre:  0.9896907216494846\n",
      "rec:  0.45933014354066987\n",
      "ma F1:  0.7235356167783569\n",
      "mi F1:  0.7569296375266524\n",
      "we F1:  0.7339840527180077\n",
      "Loss:  0.03782035782933235\n",
      "Loss:  0.061256855726242065\n",
      "Loss:  0.07931222766637802\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.04028098285198212\n",
      "Loss:  0.08258673548698425\n",
      "Loss:  0.0884067565202713\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.04716213420033455\n",
      "Loss:  0.06573060154914856\n",
      "Loss:  0.051966581493616104\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.49902570247650146\n",
      "Eval Loss:  0.04004108905792236\n",
      "Eval Loss:  0.048398733139038086\n",
      "[[18796  1108]\n",
      " [ 1883 10635]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.93     19904\n",
      "           1       0.91      0.85      0.88     12518\n",
      "\n",
      "    accuracy                           0.91     32422\n",
      "   macro avg       0.91      0.90      0.90     32422\n",
      "weighted avg       0.91      0.91      0.91     32422\n",
      "\n",
      "acc:  0.9077478255505521\n",
      "pre:  0.9056459167163416\n",
      "rec:  0.8495766096820578\n",
      "ma F1:  0.9015074529489462\n",
      "mi F1:  0.9077478255505521\n",
      "we F1:  0.9071552157274612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[259   1]\n",
      " [162  47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      1.00      0.76       260\n",
      "           1       0.98      0.22      0.37       209\n",
      "\n",
      "    accuracy                           0.65       469\n",
      "   macro avg       0.80      0.61      0.56       469\n",
      "weighted avg       0.78      0.65      0.58       469\n",
      "\n",
      "acc:  0.652452025586354\n",
      "pre:  0.9791666666666666\n",
      "rec:  0.22488038277511962\n",
      "ma F1:  0.5632024317637715\n",
      "mi F1:  0.652452025586354\n",
      "we F1:  0.5846728529192041\n",
      "Loss:  0.05014272779226303\n",
      "Loss:  0.04674608260393143\n",
      "Loss:  0.08123861253261566\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.06759735941886902\n",
      "Loss:  0.06400447338819504\n",
      "Loss:  0.0705040842294693\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.09685791283845901\n",
      "Loss:  0.07747316360473633\n",
      "Loss:  0.062424346804618835\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.5100367069244385\n",
      "Eval Loss:  0.0374600887298584\n",
      "Eval Loss:  0.06440365314483643\n",
      "[[18981   923]\n",
      " [ 1970 10548]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19904\n",
      "           1       0.92      0.84      0.88     12518\n",
      "\n",
      "    accuracy                           0.91     32422\n",
      "   macro avg       0.91      0.90      0.90     32422\n",
      "weighted avg       0.91      0.91      0.91     32422\n",
      "\n",
      "acc:  0.910770464499414\n",
      "pre:  0.9195362217766542\n",
      "rec:  0.8426266176705544\n",
      "ma F1:  0.90429582677154\n",
      "mi F1:  0.910770464499414\n",
      "we F1:  0.9099666051728299\n",
      "[[259   1]\n",
      " [164  45]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      1.00      0.76       260\n",
      "           1       0.98      0.22      0.35       209\n",
      "\n",
      "    accuracy                           0.65       469\n",
      "   macro avg       0.80      0.61      0.56       469\n",
      "weighted avg       0.78      0.65      0.58       469\n",
      "\n",
      "acc:  0.6481876332622601\n",
      "pre:  0.9782608695652174\n",
      "rec:  0.215311004784689\n",
      "ma F1:  0.5556799586598915\n",
      "mi F1:  0.6481876332622601\n",
      "we F1:  0.5777261801772785\n",
      "Loss:  0.0700230747461319\n",
      "Loss:  0.071415975689888\n",
      "Loss:  0.05649079754948616\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.07215414196252823\n",
      "Loss:  0.03783957660198212\n",
      "Loss:  0.03956787660717964\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.04026271402835846\n",
      "Loss:  0.0689128041267395\n",
      "Loss:  0.07830827683210373\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.4239646792411804\n",
      "Eval Loss:  0.040151000022888184\n",
      "Eval Loss:  0.08903789520263672\n",
      "[[18832  1072]\n",
      " [ 1609 10909]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     19904\n",
      "           1       0.91      0.87      0.89     12518\n",
      "\n",
      "    accuracy                           0.92     32422\n",
      "   macro avg       0.92      0.91      0.91     32422\n",
      "weighted avg       0.92      0.92      0.92     32422\n",
      "\n",
      "acc:  0.9173092344704213\n",
      "pre:  0.9105249979133628\n",
      "rec:  0.8714650902700112\n",
      "ma F1:  0.9120575545734765\n",
      "mi F1:  0.9173092344704213\n",
      "we F1:  0.9169532894868722\n",
      "[[260   0]\n",
      " [193  16]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      1.00      0.73       260\n",
      "           1       1.00      0.08      0.14       209\n",
      "\n",
      "    accuracy                           0.59       469\n",
      "   macro avg       0.79      0.54      0.44       469\n",
      "weighted avg       0.76      0.59      0.47       469\n",
      "\n",
      "acc:  0.5884861407249466\n",
      "pre:  1.0\n",
      "rec:  0.07655502392344497\n",
      "ma F1:  0.4357674925977872\n",
      "mi F1:  0.5884861407249466\n",
      "we F1:  0.4676881936407591\n",
      "Loss:  0.07569639384746552\n",
      "Loss:  0.06497339904308319\n",
      "Loss:  0.04342018812894821\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.08651464432477951\n",
      "Loss:  0.07888742536306381\n",
      "Loss:  0.07622416317462921\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.04179604724049568\n",
      "Loss:  0.05533842369914055\n",
      "Loss:  0.06284619867801666\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.6172164082527161\n",
      "Eval Loss:  0.03983962535858154\n",
      "Eval Loss:  0.07342660427093506\n",
      "[[18838  1066]\n",
      " [ 1671 10847]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     19904\n",
      "           1       0.91      0.87      0.89     12518\n",
      "\n",
      "    accuracy                           0.92     32422\n",
      "   macro avg       0.91      0.91      0.91     32422\n",
      "weighted avg       0.92      0.92      0.92     32422\n",
      "\n",
      "acc:  0.9155820122139288\n",
      "pre:  0.910517921598254\n",
      "rec:  0.8665122223997443\n",
      "ma F1:  0.9101222349869176\n",
      "mi F1:  0.9155820122139287\n",
      "we F1:  0.9151686515303671\n",
      "[[260   0]\n",
      " [167  42]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      1.00      0.76       260\n",
      "           1       1.00      0.20      0.33       209\n",
      "\n",
      "    accuracy                           0.64       469\n",
      "   macro avg       0.80      0.60      0.55       469\n",
      "weighted avg       0.78      0.64      0.57       469\n",
      "\n",
      "acc:  0.6439232409381663\n",
      "pre:  1.0\n",
      "rec:  0.20095693779904306\n",
      "ma F1:  0.5457877369706038\n",
      "mi F1:  0.6439232409381663\n",
      "we F1:  0.5687460429446666\n",
      "Loss:  0.03757328540086746\n",
      "Loss:  0.0800749659538269\n",
      "Loss:  0.046562448143959045\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.04063882678747177\n",
      "Loss:  0.05198168382048607\n",
      "Loss:  0.09459825605154037\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.0546719953417778\n",
      "Loss:  0.05323927104473114\n",
      "Loss:  0.03271108865737915\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.5172520875930786\n",
      "Eval Loss:  0.04817056655883789\n",
      "Eval Loss:  0.03097677230834961\n",
      "[[18591  1313]\n",
      " [ 1421 11097]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19904\n",
      "           1       0.89      0.89      0.89     12518\n",
      "\n",
      "    accuracy                           0.92     32422\n",
      "   macro avg       0.91      0.91      0.91     32422\n",
      "weighted avg       0.92      0.92      0.92     32422\n",
      "\n",
      "acc:  0.9156745419776695\n",
      "pre:  0.8941982272360999\n",
      "rec:  0.8864834638121105\n",
      "ma F1:  0.9109151482233357\n",
      "mi F1:  0.9156745419776695\n",
      "we F1:  0.9156059518354932\n",
      "[[259   1]\n",
      " [165  44]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      1.00      0.76       260\n",
      "           1       0.98      0.21      0.35       209\n",
      "\n",
      "    accuracy                           0.65       469\n",
      "   macro avg       0.79      0.60      0.55       469\n",
      "weighted avg       0.77      0.65      0.57       469\n",
      "\n",
      "acc:  0.6460554371002132\n",
      "pre:  0.9777777777777777\n",
      "rec:  0.21052631578947367\n",
      "ma F1:  0.5518833172169268\n",
      "mi F1:  0.6460554371002132\n",
      "we F1:  0.5742218200729622\n",
      "Loss:  0.0783139318227768\n",
      "Loss:  0.03754911944270134\n",
      "Loss:  0.08301861584186554\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.03261015564203262\n",
      "Loss:  0.034474167972803116\n",
      "Loss:  0.04626661539077759\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.03643328696489334\n",
      "Loss:  0.0600825771689415\n",
      "Loss:  0.033087655901908875\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.4921712875366211\n",
      "Eval Loss:  0.07746779918670654\n",
      "Eval Loss:  0.03860056400299072\n",
      "[[18414  1490]\n",
      " [ 1178 11340]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93     19904\n",
      "           1       0.88      0.91      0.89     12518\n",
      "\n",
      "    accuracy                           0.92     32422\n",
      "   macro avg       0.91      0.92      0.91     32422\n",
      "weighted avg       0.92      0.92      0.92     32422\n",
      "\n",
      "acc:  0.9177101967799642\n",
      "pre:  0.8838659392049883\n",
      "rec:  0.9058955104649306\n",
      "ma F1:  0.9135970015632346\n",
      "mi F1:  0.9177101967799642\n",
      "we F1:  0.9178916099701846\n",
      "[[259   1]\n",
      " [145  64]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      1.00      0.78       260\n",
      "           1       0.98      0.31      0.47       209\n",
      "\n",
      "    accuracy                           0.69       469\n",
      "   macro avg       0.81      0.65      0.62       469\n",
      "weighted avg       0.79      0.69      0.64       469\n",
      "\n",
      "acc:  0.6886993603411514\n",
      "pre:  0.9846153846153847\n",
      "rec:  0.3062200956937799\n",
      "ma F1:  0.623636883299622\n",
      "mi F1:  0.6886993603411514\n",
      "we F1:  0.640653223448945\n",
      "Loss:  0.04684802517294884\n",
      "Loss:  0.03409624844789505\n",
      "Loss:  0.05386127158999443\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.06550955772399902\n",
      "Loss:  0.044592469930648804\n",
      "Loss:  0.09874752908945084\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.07354402542114258\n",
      "Loss:  0.02184029296040535\n",
      "Loss:  0.05237845331430435\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.34269705414772034\n",
      "Eval Loss:  0.05083048343658447\n",
      "Eval Loss:  0.0459822416305542\n",
      "[[18493  1411]\n",
      " [ 1115 11403]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.94     19904\n",
      "           1       0.89      0.91      0.90     12518\n",
      "\n",
      "    accuracy                           0.92     32422\n",
      "   macro avg       0.92      0.92      0.92     32422\n",
      "weighted avg       0.92      0.92      0.92     32422\n",
      "\n",
      "acc:  0.922089938930356\n",
      "pre:  0.8898860621195568\n",
      "rec:  0.9109282633008468\n",
      "ma F1:  0.9181771400762444\n",
      "mi F1:  0.922089938930356\n",
      "we F1:  0.9222532941434473\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[260   0]\n",
      " [189  20]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      1.00      0.73       260\n",
      "           1       1.00      0.10      0.17       209\n",
      "\n",
      "    accuracy                           0.60       469\n",
      "   macro avg       0.79      0.55      0.45       469\n",
      "weighted avg       0.77      0.60      0.48       469\n",
      "\n",
      "acc:  0.5970149253731343\n",
      "pre:  1.0\n",
      "rec:  0.09569377990430622\n",
      "ma F1:  0.4540499257826695\n",
      "mi F1:  0.5970149253731343\n",
      "we F1:  0.48442998819564326\n",
      "Loss:  0.07847873866558075\n",
      "Loss:  0.036259882152080536\n",
      "Loss:  0.053672149777412415\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.06707878410816193\n",
      "Loss:  0.06041868031024933\n",
      "Loss:  0.06416888535022736\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.057236067950725555\n",
      "Loss:  0.059292908757925034\n",
      "Loss:  0.055073291063308716\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.5438790321350098\n",
      "Eval Loss:  0.057274460792541504\n",
      "Eval Loss:  0.03883635997772217\n",
      "[[18600  1304]\n",
      " [ 1328 11190]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19904\n",
      "           1       0.90      0.89      0.89     12518\n",
      "\n",
      "    accuracy                           0.92     32422\n",
      "   macro avg       0.91      0.91      0.91     32422\n",
      "weighted avg       0.92      0.92      0.92     32422\n",
      "\n",
      "acc:  0.9188205539448523\n",
      "pre:  0.8956299023531294\n",
      "rec:  0.8939127656175108\n",
      "ma F1:  0.914346492273787\n",
      "mi F1:  0.9188205539448523\n",
      "we F1:  0.9188060630568488\n",
      "[[260   0]\n",
      " [200   9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      1.00      0.72       260\n",
      "           1       1.00      0.04      0.08       209\n",
      "\n",
      "    accuracy                           0.57       469\n",
      "   macro avg       0.78      0.52      0.40       469\n",
      "weighted avg       0.76      0.57      0.44       469\n",
      "\n",
      "acc:  0.5735607675906184\n",
      "pre:  1.0\n",
      "rec:  0.0430622009569378\n",
      "ma F1:  0.4023955147808359\n",
      "mi F1:  0.5735607675906184\n",
      "we F1:  0.4371741119652937\n",
      "Loss:  0.05179133266210556\n",
      "Loss:  0.07211916893720627\n",
      "Loss:  0.05765142664313316\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.03176688030362129\n",
      "Loss:  0.04382206127047539\n",
      "Loss:  0.05974835157394409\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.05018359050154686\n",
      "Loss:  0.0403260737657547\n",
      "Loss:  0.0669340044260025\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.7812749147415161\n",
      "Eval Loss:  0.03446698188781738\n",
      "Eval Loss:  0.0666041374206543\n",
      "[[18872  1032]\n",
      " [ 1575 10943]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94     19904\n",
      "           1       0.91      0.87      0.89     12518\n",
      "\n",
      "    accuracy                           0.92     32422\n",
      "   macro avg       0.92      0.91      0.91     32422\n",
      "weighted avg       0.92      0.92      0.92     32422\n",
      "\n",
      "acc:  0.9195916353093578\n",
      "pre:  0.9138204592901878\n",
      "rec:  0.874181179102093\n",
      "ma F1:  0.9144766807385715\n",
      "mi F1:  0.9195916353093578\n",
      "we F1:  0.919241348976663\n",
      "[[260   0]\n",
      " [184  25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      1.00      0.74       260\n",
      "           1       1.00      0.12      0.21       209\n",
      "\n",
      "    accuracy                           0.61       469\n",
      "   macro avg       0.79      0.56      0.48       469\n",
      "weighted avg       0.77      0.61      0.50       469\n",
      "\n",
      "acc:  0.6076759061833689\n",
      "pre:  1.0\n",
      "rec:  0.11961722488038277\n",
      "ma F1:  0.47615578865578867\n",
      "mi F1:  0.6076759061833689\n",
      "we F1:  0.5046984524596465\n",
      "Loss:  0.0637582466006279\n",
      "Loss:  0.07615416496992111\n",
      "Loss:  0.07090514898300171\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.062097012996673584\n",
      "Loss:  0.031548015773296356\n",
      "Loss:  0.07285549491643906\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.08216546475887299\n",
      "Loss:  0.04224943369626999\n",
      "Loss:  0.07231438159942627\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  26 Training Time 5861.5424263477325 Best Test Acc:  0.8869936034115139\n",
      "test subjects:  ['./seg\\\\x06', './seg\\\\x24']\n",
      "*********\n",
      "33434 879\n",
      "32025 866\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.6328914761543274\n",
      "Eval Loss:  0.6482917666435242\n",
      "Eval Loss:  0.7475433349609375\n",
      "[[19253    46]\n",
      " [12628    98]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      1.00      0.75     19299\n",
      "           1       0.68      0.01      0.02     12726\n",
      "\n",
      "    accuracy                           0.60     32025\n",
      "   macro avg       0.64      0.50      0.38     32025\n",
      "weighted avg       0.63      0.60      0.46     32025\n",
      "\n",
      "acc:  0.6042466822794692\n",
      "pre:  0.6805555555555556\n",
      "rec:  0.007700770077007701\n",
      "ma F1:  0.38379670999835125\n",
      "mi F1:  0.6042466822794692\n",
      "we F1:  0.4594436777771968\n",
      "[[850  15]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       865\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.98       866\n",
      "   macro avg       0.50      0.49      0.50       866\n",
      "weighted avg       1.00      0.98      0.99       866\n",
      "\n",
      "acc:  0.9815242494226328\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49533799533799533\n",
      "mi F1:  0.9815242494226328\n",
      "we F1:  0.9895320230193209\n",
      "Subject 27 Current Train Acc:  0.6042466822794692 Current Test Acc:  0.9815242494226328\n",
      "Loss:  0.16381347179412842\n",
      "Loss:  0.16484779119491577\n",
      "Loss:  0.15913598239421844\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.1438557356595993\n",
      "Loss:  0.13447242975234985\n",
      "Loss:  0.13288450241088867\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.11126222461462021\n",
      "Loss:  0.11927467584609985\n",
      "Loss:  0.09176680445671082\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.03449070453643799\n",
      "Eval Loss:  0.035186171531677246\n",
      "Eval Loss:  0.21990364789962769\n",
      "[[16908  2391]\n",
      " [ 4165  8561]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.88      0.84     19299\n",
      "           1       0.78      0.67      0.72     12726\n",
      "\n",
      "    accuracy                           0.80     32025\n",
      "   macro avg       0.79      0.77      0.78     32025\n",
      "weighted avg       0.79      0.80      0.79     32025\n",
      "\n",
      "acc:  0.7952849336455894\n",
      "pre:  0.7816837107377648\n",
      "rec:  0.6727172717271728\n",
      "ma F1:  0.78036436576949\n",
      "mi F1:  0.7952849336455895\n",
      "we F1:  0.7921138437435647\n",
      "[[786  79]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95       865\n",
      "           1       0.01      1.00      0.02         1\n",
      "\n",
      "    accuracy                           0.91       866\n",
      "   macro avg       0.51      0.95      0.49       866\n",
      "weighted avg       1.00      0.91      0.95       866\n",
      "\n",
      "acc:  0.9087759815242494\n",
      "pre:  0.0125\n",
      "rec:  1.0\n",
      "ma F1:  0.4884207850087115\n",
      "mi F1:  0.9087759815242494\n",
      "we F1:  0.9510792433391889\n",
      "Loss:  0.12289725989103317\n",
      "Loss:  0.09954363107681274\n",
      "Loss:  0.12245845794677734\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.121817946434021\n",
      "Loss:  0.08906661719083786\n",
      "Loss:  0.09324036538600922\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.07311926037073135\n",
      "Loss:  0.08353157341480255\n",
      "Loss:  0.11162415891885757\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.026162147521972656\n",
      "Eval Loss:  0.014399290084838867\n",
      "Eval Loss:  0.07306170463562012\n",
      "[[18013  1286]\n",
      " [ 3534  9192]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88     19299\n",
      "           1       0.88      0.72      0.79     12726\n",
      "\n",
      "    accuracy                           0.85     32025\n",
      "   macro avg       0.86      0.83      0.84     32025\n",
      "weighted avg       0.85      0.85      0.85     32025\n",
      "\n",
      "acc:  0.8494925839188134\n",
      "pre:  0.8772666539415919\n",
      "rec:  0.7223008015087223\n",
      "ma F1:  0.8371364913243324\n",
      "mi F1:  0.8494925839188134\n",
      "we F1:  0.8463436783352749\n",
      "[[853  12]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       865\n",
      "           1       0.08      1.00      0.14         1\n",
      "\n",
      "    accuracy                           0.99       866\n",
      "   macro avg       0.54      0.99      0.57       866\n",
      "weighted avg       1.00      0.99      0.99       866\n",
      "\n",
      "acc:  0.9861431870669746\n",
      "pre:  0.07692307692307693\n",
      "rec:  1.0\n",
      "ma F1:  0.5679361383668717\n",
      "mi F1:  0.9861431870669746\n",
      "we F1:  0.9920334271895112\n",
      "Subject 27 Current Train Acc:  0.8494925839188134 Current Test Acc:  0.9861431870669746\n",
      "Loss:  0.08017101883888245\n",
      "Loss:  0.07688716799020767\n",
      "Loss:  0.09597507864236832\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.11545854806900024\n",
      "Loss:  0.07835336029529572\n",
      "Loss:  0.07235439866781235\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.08211598545312881\n",
      "Loss:  0.09231498092412949\n",
      "Loss:  0.1076480969786644\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.024549365043640137\n",
      "Eval Loss:  0.008737325668334961\n",
      "Eval Loss:  0.05894613265991211\n",
      "[[18283  1016]\n",
      " [ 3277  9449]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.89     19299\n",
      "           1       0.90      0.74      0.81     12726\n",
      "\n",
      "    accuracy                           0.87     32025\n",
      "   macro avg       0.88      0.84      0.85     32025\n",
      "weighted avg       0.87      0.87      0.86     32025\n",
      "\n",
      "acc:  0.8659484777517564\n",
      "pre:  0.9029144768275204\n",
      "rec:  0.7424956781392424\n",
      "ma F1:  0.8549082170027398\n",
      "mi F1:  0.8659484777517564\n",
      "we F1:  0.8631228008722538\n",
      "[[850  15]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       865\n",
      "           1       0.06      1.00      0.12         1\n",
      "\n",
      "    accuracy                           0.98       866\n",
      "   macro avg       0.53      0.99      0.55       866\n",
      "weighted avg       1.00      0.98      0.99       866\n",
      "\n",
      "acc:  0.9826789838337182\n",
      "pre:  0.0625\n",
      "rec:  1.0\n",
      "ma F1:  0.5544503515691991\n",
      "mi F1:  0.9826789838337182\n",
      "we F1:  0.990244860728851\n",
      "Loss:  0.11075347661972046\n",
      "Loss:  0.05778278410434723\n",
      "Loss:  0.08863291144371033\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.06352625042200089\n",
      "Loss:  0.05339670926332474\n",
      "Loss:  0.08022758364677429\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.07914992421865463\n",
      "Loss:  0.06611660122871399\n",
      "Loss:  0.05138935148715973\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.027228474617004395\n",
      "Eval Loss:  0.01616358757019043\n",
      "Eval Loss:  0.04421031475067139\n",
      "[[18619   680]\n",
      " [ 3673  9053]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.96      0.90     19299\n",
      "           1       0.93      0.71      0.81     12726\n",
      "\n",
      "    accuracy                           0.86     32025\n",
      "   macro avg       0.88      0.84      0.85     32025\n",
      "weighted avg       0.87      0.86      0.86     32025\n",
      "\n",
      "acc:  0.8640749414519906\n",
      "pre:  0.9301345936504675\n",
      "rec:  0.7113782806852114\n",
      "ma F1:  0.8507590420920185\n",
      "mi F1:  0.8640749414519906\n",
      "we F1:  0.8599086768916314\n",
      "[[855  10]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       865\n",
      "           1       0.09      1.00      0.17         1\n",
      "\n",
      "    accuracy                           0.99       866\n",
      "   macro avg       0.55      0.99      0.58       866\n",
      "weighted avg       1.00      0.99      0.99       866\n",
      "\n",
      "acc:  0.9884526558891455\n",
      "pre:  0.09090909090909091\n",
      "rec:  1.0\n",
      "ma F1:  0.5804263565891473\n",
      "mi F1:  0.9884526558891455\n",
      "we F1:  0.9932304814078808\n",
      "Subject 27 Current Train Acc:  0.8640749414519906 Current Test Acc:  0.9884526558891455\n",
      "Loss:  0.08170558512210846\n",
      "Loss:  0.061792243272066116\n",
      "Loss:  0.056086745113134384\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.06365851312875748\n",
      "Loss:  0.06607580929994583\n",
      "Loss:  0.07037841528654099\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.06808333098888397\n",
      "Loss:  0.050975702702999115\n",
      "Loss:  0.07905761897563934\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.1438494324684143\n",
      "Eval Loss:  0.025012493133544922\n",
      "Eval Loss:  0.044026970863342285\n",
      "[[18197  1102]\n",
      " [ 2572 10154]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91     19299\n",
      "           1       0.90      0.80      0.85     12726\n",
      "\n",
      "    accuracy                           0.89     32025\n",
      "   macro avg       0.89      0.87      0.88     32025\n",
      "weighted avg       0.89      0.89      0.88     32025\n",
      "\n",
      "acc:  0.8852771272443404\n",
      "pre:  0.9020966595593461\n",
      "rec:  0.7978940751217979\n",
      "ma F1:  0.8775538239983273\n",
      "mi F1:  0.8852771272443404\n",
      "we F1:  0.8838655574604738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[858   7]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       865\n",
      "           1       0.12      1.00      0.22         1\n",
      "\n",
      "    accuracy                           0.99       866\n",
      "   macro avg       0.56      1.00      0.61       866\n",
      "weighted avg       1.00      0.99      1.00       866\n",
      "\n",
      "acc:  0.9919168591224018\n",
      "pre:  0.125\n",
      "rec:  1.0\n",
      "ma F1:  0.609079770426259\n",
      "mi F1:  0.9919168591224018\n",
      "we F1:  0.9950438831840971\n",
      "Subject 27 Current Train Acc:  0.8852771272443404 Current Test Acc:  0.9919168591224018\n",
      "Loss:  0.08374734967947006\n",
      "Loss:  0.053376246243715286\n",
      "Loss:  0.07559037208557129\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.05562172457575798\n",
      "Loss:  0.06082889810204506\n",
      "Loss:  0.04884762316942215\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.08772612363100052\n",
      "Loss:  0.07394696027040482\n",
      "Loss:  0.0662824809551239\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.13739579916000366\n",
      "Eval Loss:  0.03827095031738281\n",
      "Eval Loss:  0.041121482849121094\n",
      "[[18382   917]\n",
      " [ 2449 10277]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.92     19299\n",
      "           1       0.92      0.81      0.86     12726\n",
      "\n",
      "    accuracy                           0.89     32025\n",
      "   macro avg       0.90      0.88      0.89     32025\n",
      "weighted avg       0.90      0.89      0.89     32025\n",
      "\n",
      "acc:  0.8948946135831382\n",
      "pre:  0.9180811148829731\n",
      "rec:  0.8075593273613075\n",
      "ma F1:  0.887701768999914\n",
      "mi F1:  0.8948946135831382\n",
      "we F1:  0.8935350284009667\n",
      "[[859   6]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       865\n",
      "           1       0.14      1.00      0.25         1\n",
      "\n",
      "    accuracy                           0.99       866\n",
      "   macro avg       0.57      1.00      0.62       866\n",
      "weighted avg       1.00      0.99      1.00       866\n",
      "\n",
      "acc:  0.9930715935334873\n",
      "pre:  0.14285714285714285\n",
      "rec:  1.0\n",
      "ma F1:  0.6232598607888631\n",
      "mi F1:  0.9930715935334873\n",
      "we F1:  0.9956576895666664\n",
      "Subject 27 Current Train Acc:  0.8948946135831382 Current Test Acc:  0.9930715935334873\n",
      "Loss:  0.08320902287960052\n",
      "Loss:  0.06539037823677063\n",
      "Loss:  0.056177765130996704\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.06408657133579254\n",
      "Loss:  0.039516039192676544\n",
      "Loss:  0.09113036841154099\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.06700026988983154\n",
      "Loss:  0.051991045475006104\n",
      "Loss:  0.044489216059446335\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.2621085047721863\n",
      "Eval Loss:  0.031456708908081055\n",
      "Eval Loss:  0.03626096248626709\n",
      "[[18358   941]\n",
      " [ 2328 10398]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     19299\n",
      "           1       0.92      0.82      0.86     12726\n",
      "\n",
      "    accuracy                           0.90     32025\n",
      "   macro avg       0.90      0.88      0.89     32025\n",
      "weighted avg       0.90      0.90      0.90     32025\n",
      "\n",
      "acc:  0.8979234972677596\n",
      "pre:  0.917012082194197\n",
      "rec:  0.817067421027817\n",
      "ma F1:  0.8912019547326091\n",
      "mi F1:  0.8979234972677596\n",
      "we F1:  0.8967522938134563\n",
      "[[860   5]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       865\n",
      "           1       0.17      1.00      0.29         1\n",
      "\n",
      "    accuracy                           0.99       866\n",
      "   macro avg       0.58      1.00      0.64       866\n",
      "weighted avg       1.00      0.99      1.00       866\n",
      "\n",
      "acc:  0.9942263279445728\n",
      "pre:  0.16666666666666666\n",
      "rec:  1.0\n",
      "ma F1:  0.641407867494824\n",
      "mi F1:  0.9942263279445728\n",
      "we F1:  0.996279986037994\n",
      "Subject 27 Current Train Acc:  0.8979234972677596 Current Test Acc:  0.9942263279445728\n",
      "Loss:  0.06730542331933975\n",
      "Loss:  0.07794933021068573\n",
      "Loss:  0.08005057275295258\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.09691832214593887\n",
      "Loss:  0.06331872940063477\n",
      "Loss:  0.07089164853096008\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.07510679960250854\n",
      "Loss:  0.05276939645409584\n",
      "Loss:  0.10074558109045029\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.18456768989562988\n",
      "Eval Loss:  0.04319262504577637\n",
      "Eval Loss:  0.02796649932861328\n",
      "[[18237  1062]\n",
      " [ 1980 10746]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     19299\n",
      "           1       0.91      0.84      0.88     12726\n",
      "\n",
      "    accuracy                           0.91     32025\n",
      "   macro avg       0.91      0.89      0.90     32025\n",
      "weighted avg       0.91      0.91      0.90     32025\n",
      "\n",
      "acc:  0.9050117096018735\n",
      "pre:  0.9100609756097561\n",
      "rec:  0.8444130127298444\n",
      "ma F1:  0.8995136641253518\n",
      "mi F1:  0.9050117096018735\n",
      "we F1:  0.9043379403124\n",
      "[[858   7]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       865\n",
      "           1       0.12      1.00      0.22         1\n",
      "\n",
      "    accuracy                           0.99       866\n",
      "   macro avg       0.56      1.00      0.61       866\n",
      "weighted avg       1.00      0.99      1.00       866\n",
      "\n",
      "acc:  0.9919168591224018\n",
      "pre:  0.125\n",
      "rec:  1.0\n",
      "ma F1:  0.609079770426259\n",
      "mi F1:  0.9919168591224018\n",
      "we F1:  0.9950438831840971\n",
      "Loss:  0.05589078739285469\n",
      "Loss:  0.06757607311010361\n",
      "Loss:  0.0510282926261425\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.04869023710489273\n",
      "Loss:  0.07045754045248032\n",
      "Loss:  0.0875411406159401\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.04589667543768883\n",
      "Loss:  0.06636004894971848\n",
      "Loss:  0.05524435266852379\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.20797806978225708\n",
      "Eval Loss:  0.0535200834274292\n",
      "Eval Loss:  0.023538589477539062\n",
      "[[18302   997]\n",
      " [ 1955 10771]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93     19299\n",
      "           1       0.92      0.85      0.88     12726\n",
      "\n",
      "    accuracy                           0.91     32025\n",
      "   macro avg       0.91      0.90      0.90     32025\n",
      "weighted avg       0.91      0.91      0.91     32025\n",
      "\n",
      "acc:  0.9078220140515223\n",
      "pre:  0.9152787219578518\n",
      "rec:  0.8463774948923464\n",
      "ma F1:  0.9024261570931419\n",
      "mi F1:  0.9078220140515222\n",
      "we F1:  0.9071356203500047\n",
      "[[857   8]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       865\n",
      "           1       0.11      1.00      0.20         1\n",
      "\n",
      "    accuracy                           0.99       866\n",
      "   macro avg       0.56      1.00      0.60       866\n",
      "weighted avg       1.00      0.99      0.99       866\n",
      "\n",
      "acc:  0.9907621247113164\n",
      "pre:  0.1111111111111111\n",
      "rec:  1.0\n",
      "ma F1:  0.5976771196283391\n",
      "mi F1:  0.9907621247113164\n",
      "we F1:  0.994435816347606\n",
      "Loss:  0.06786250323057175\n",
      "Loss:  0.06058058515191078\n",
      "Loss:  0.041346728801727295\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.05313277989625931\n",
      "Loss:  0.09136325865983963\n",
      "Loss:  0.07612639665603638\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.06710448861122131\n",
      "Loss:  0.07223214954137802\n",
      "Loss:  0.060190349817276\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.48532986640930176\n",
      "Eval Loss:  0.054360032081604004\n",
      "Eval Loss:  0.023632168769836426\n",
      "[[18012  1287]\n",
      " [ 1602 11124]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93     19299\n",
      "           1       0.90      0.87      0.89     12726\n",
      "\n",
      "    accuracy                           0.91     32025\n",
      "   macro avg       0.91      0.90      0.91     32025\n",
      "weighted avg       0.91      0.91      0.91     32025\n",
      "\n",
      "acc:  0.9097892271662763\n",
      "pre:  0.896301667875272\n",
      "rec:  0.8741159830268741\n",
      "ma F1:  0.905413638173763\n",
      "mi F1:  0.9097892271662763\n",
      "we F1:  0.9095891240111309\n",
      "[[860   5]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       865\n",
      "           1       0.17      1.00      0.29         1\n",
      "\n",
      "    accuracy                           0.99       866\n",
      "   macro avg       0.58      1.00      0.64       866\n",
      "weighted avg       1.00      0.99      1.00       866\n",
      "\n",
      "acc:  0.9942263279445728\n",
      "pre:  0.16666666666666666\n",
      "rec:  1.0\n",
      "ma F1:  0.641407867494824\n",
      "mi F1:  0.9942263279445728\n",
      "we F1:  0.996279986037994\n",
      "Loss:  0.055755358189344406\n",
      "Loss:  0.044171322137117386\n",
      "Loss:  0.0774349793791771\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.07721935212612152\n",
      "Loss:  0.05838022008538246\n",
      "Loss:  0.03799405321478844\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.06971877813339233\n",
      "Loss:  0.05164038762450218\n",
      "Loss:  0.0729275494813919\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.46499401330947876\n",
      "Eval Loss:  0.05147266387939453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss:  0.0267641544342041\n",
      "[[18308   991]\n",
      " [ 2014 10712]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     19299\n",
      "           1       0.92      0.84      0.88     12726\n",
      "\n",
      "    accuracy                           0.91     32025\n",
      "   macro avg       0.91      0.90      0.90     32025\n",
      "weighted avg       0.91      0.91      0.91     32025\n",
      "\n",
      "acc:  0.9061670569867292\n",
      "pre:  0.9153208578996839\n",
      "rec:  0.8417413169888417\n",
      "ma F1:  0.9005734219362005\n",
      "mi F1:  0.9061670569867292\n",
      "we F1:  0.9054137277796871\n",
      "[[860   5]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       865\n",
      "           1       0.17      1.00      0.29         1\n",
      "\n",
      "    accuracy                           0.99       866\n",
      "   macro avg       0.58      1.00      0.64       866\n",
      "weighted avg       1.00      0.99      1.00       866\n",
      "\n",
      "acc:  0.9942263279445728\n",
      "pre:  0.16666666666666666\n",
      "rec:  1.0\n",
      "ma F1:  0.641407867494824\n",
      "mi F1:  0.9942263279445728\n",
      "we F1:  0.996279986037994\n",
      "Loss:  0.08731292188167572\n",
      "Loss:  0.04945118725299835\n",
      "Loss:  0.07430187612771988\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.0602240227162838\n",
      "Loss:  0.057498421519994736\n",
      "Loss:  0.09978564083576202\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.04301914945244789\n",
      "Loss:  0.06206616386771202\n",
      "Loss:  0.046650394797325134\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.40318208932876587\n",
      "Eval Loss:  0.04151451587677002\n",
      "Eval Loss:  0.020674824714660645\n",
      "[[18536   763]\n",
      " [ 2144 10582]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     19299\n",
      "           1       0.93      0.83      0.88     12726\n",
      "\n",
      "    accuracy                           0.91     32025\n",
      "   macro avg       0.91      0.90      0.90     32025\n",
      "weighted avg       0.91      0.91      0.91     32025\n",
      "\n",
      "acc:  0.9092271662763466\n",
      "pre:  0.9327457029528426\n",
      "rec:  0.8315260097438315\n",
      "ma F1:  0.903259548390569\n",
      "mi F1:  0.9092271662763466\n",
      "we F1:  0.9081910485619565\n",
      "[[862   3]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       865\n",
      "           1       0.25      1.00      0.40         1\n",
      "\n",
      "    accuracy                           1.00       866\n",
      "   macro avg       0.62      1.00      0.70       866\n",
      "weighted avg       1.00      1.00      1.00       866\n",
      "\n",
      "acc:  0.9965357967667436\n",
      "pre:  0.25\n",
      "rec:  1.0\n",
      "ma F1:  0.699131441806601\n",
      "mi F1:  0.9965357967667436\n",
      "we F1:  0.9975720488746187\n",
      "Subject 27 Current Train Acc:  0.9092271662763466 Current Test Acc:  0.9965357967667436\n",
      "Loss:  0.06135951355099678\n",
      "Loss:  0.05604901164770126\n",
      "Loss:  0.05671454221010208\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.0765925794839859\n",
      "Loss:  0.048881251364946365\n",
      "Loss:  0.09882071614265442\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.04169420152902603\n",
      "Loss:  0.07927313446998596\n",
      "Loss:  0.07802540063858032\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.35051488876342773\n",
      "Eval Loss:  0.0761878490447998\n",
      "Eval Loss:  0.01578831672668457\n",
      "[[18277  1022]\n",
      " [ 1844 10882]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19299\n",
      "           1       0.91      0.86      0.88     12726\n",
      "\n",
      "    accuracy                           0.91     32025\n",
      "   macro avg       0.91      0.90      0.91     32025\n",
      "weighted avg       0.91      0.91      0.91     32025\n",
      "\n",
      "acc:  0.9105074160811866\n",
      "pre:  0.9141465053763441\n",
      "rec:  0.8550997956938551\n",
      "ma F1:  0.9054668144860525\n",
      "mi F1:  0.9105074160811866\n",
      "we F1:  0.9099471220296381\n",
      "[[862   3]\n",
      " [  1   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       865\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       866\n",
      "   macro avg       0.50      0.50      0.50       866\n",
      "weighted avg       1.00      1.00      1.00       866\n",
      "\n",
      "acc:  0.9953810623556582\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49884259259259256\n",
      "mi F1:  0.9953810623556582\n",
      "we F1:  0.9965331237704216\n",
      "Loss:  0.046416278928518295\n",
      "Loss:  0.0469847209751606\n",
      "Loss:  0.09117958694696426\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.06061125919222832\n",
      "Loss:  0.056306660175323486\n",
      "Loss:  0.06848478317260742\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.04096236824989319\n",
      "Loss:  0.03721316531300545\n",
      "Loss:  0.0425642654299736\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.563399076461792\n",
      "Eval Loss:  0.1539534330368042\n",
      "Eval Loss:  0.015430927276611328\n",
      "[[18050  1249]\n",
      " [ 1495 11231]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19299\n",
      "           1       0.90      0.88      0.89     12726\n",
      "\n",
      "    accuracy                           0.91     32025\n",
      "   macro avg       0.91      0.91      0.91     32025\n",
      "weighted avg       0.91      0.91      0.91     32025\n",
      "\n",
      "acc:  0.9143169398907104\n",
      "pre:  0.8999198717948718\n",
      "rec:  0.8825239666823825\n",
      "ma F1:  0.9102477451723042\n",
      "mi F1:  0.9143169398907104\n",
      "we F1:  0.9141701409611418\n",
      "[[860   5]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       865\n",
      "           1       0.17      1.00      0.29         1\n",
      "\n",
      "    accuracy                           0.99       866\n",
      "   macro avg       0.58      1.00      0.64       866\n",
      "weighted avg       1.00      0.99      1.00       866\n",
      "\n",
      "acc:  0.9942263279445728\n",
      "pre:  0.16666666666666666\n",
      "rec:  1.0\n",
      "ma F1:  0.641407867494824\n",
      "mi F1:  0.9942263279445728\n",
      "we F1:  0.996279986037994\n",
      "Loss:  0.0506257601082325\n",
      "Loss:  0.048178087919950485\n",
      "Loss:  0.0508316271007061\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.1132189929485321\n",
      "Loss:  0.06772948056459427\n",
      "Loss:  0.037963636219501495\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.0848766639828682\n",
      "Loss:  0.06303411722183228\n",
      "Loss:  0.04703202098608017\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.3460533916950226\n",
      "Eval Loss:  0.06855273246765137\n",
      "Eval Loss:  0.015456676483154297\n",
      "[[18024  1275]\n",
      " [ 1477 11249]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93     19299\n",
      "           1       0.90      0.88      0.89     12726\n",
      "\n",
      "    accuracy                           0.91     32025\n",
      "   macro avg       0.91      0.91      0.91     32025\n",
      "weighted avg       0.91      0.91      0.91     32025\n",
      "\n",
      "acc:  0.9140671350507416\n",
      "pre:  0.8981954647077611\n",
      "rec:  0.8839383938393839\n",
      "ma F1:  0.9100410329692763\n",
      "mi F1:  0.9140671350507417\n",
      "we F1:  0.91394709481156\n",
      "[[860   5]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       865\n",
      "           1       0.17      1.00      0.29         1\n",
      "\n",
      "    accuracy                           0.99       866\n",
      "   macro avg       0.58      1.00      0.64       866\n",
      "weighted avg       1.00      0.99      1.00       866\n",
      "\n",
      "acc:  0.9942263279445728\n",
      "pre:  0.16666666666666666\n",
      "rec:  1.0\n",
      "ma F1:  0.641407867494824\n",
      "mi F1:  0.9942263279445728\n",
      "we F1:  0.996279986037994\n",
      "Loss:  0.05770328268408775\n",
      "Loss:  0.07287092506885529\n",
      "Loss:  0.06455959379673004\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.021841196343302727\n",
      "Loss:  0.02596231922507286\n",
      "Loss:  0.02807152085006237\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.04682154953479767\n",
      "Loss:  0.07217036932706833\n",
      "Loss:  0.0521119087934494\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.3442581295967102\n",
      "Eval Loss:  0.033777594566345215\n",
      "Eval Loss:  0.01649308204650879\n",
      "[[18394   905]\n",
      " [ 1889 10837]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19299\n",
      "           1       0.92      0.85      0.89     12726\n",
      "\n",
      "    accuracy                           0.91     32025\n",
      "   macro avg       0.91      0.90      0.91     32025\n",
      "weighted avg       0.91      0.91      0.91     32025\n",
      "\n",
      "acc:  0.9127556596409055\n",
      "pre:  0.9229262476579799\n",
      "rec:  0.8515637278013516\n",
      "ma F1:  0.9076111983766406\n",
      "mi F1:  0.9127556596409055\n",
      "we F1:  0.9120857972769997\n",
      "[[862   3]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       865\n",
      "           1       0.25      1.00      0.40         1\n",
      "\n",
      "    accuracy                           1.00       866\n",
      "   macro avg       0.62      1.00      0.70       866\n",
      "weighted avg       1.00      1.00      1.00       866\n",
      "\n",
      "acc:  0.9965357967667436\n",
      "pre:  0.25\n",
      "rec:  1.0\n",
      "ma F1:  0.699131441806601\n",
      "mi F1:  0.9965357967667436\n",
      "we F1:  0.9975720488746187\n",
      "Loss:  0.06013133376836777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.07921531051397324\n",
      "Loss:  0.058049093931913376\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.0568162240087986\n",
      "Loss:  0.04010334983468056\n",
      "Loss:  0.046269491314888\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.05111328139901161\n",
      "Loss:  0.04147611930966377\n",
      "Loss:  0.056704334914684296\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.6975538730621338\n",
      "Eval Loss:  0.2123073935508728\n",
      "Eval Loss:  0.010264158248901367\n",
      "[[18001  1298]\n",
      " [ 1336 11390]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19299\n",
      "           1       0.90      0.90      0.90     12726\n",
      "\n",
      "    accuracy                           0.92     32025\n",
      "   macro avg       0.91      0.91      0.91     32025\n",
      "weighted avg       0.92      0.92      0.92     32025\n",
      "\n",
      "acc:  0.9177517564402811\n",
      "pre:  0.8976986128625473\n",
      "rec:  0.895018073235895\n",
      "ma F1:  0.9140907898669408\n",
      "mi F1:  0.9177517564402812\n",
      "we F1:  0.917730713219923\n",
      "[[859   6]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       865\n",
      "           1       0.14      1.00      0.25         1\n",
      "\n",
      "    accuracy                           0.99       866\n",
      "   macro avg       0.57      1.00      0.62       866\n",
      "weighted avg       1.00      0.99      1.00       866\n",
      "\n",
      "acc:  0.9930715935334873\n",
      "pre:  0.14285714285714285\n",
      "rec:  1.0\n",
      "ma F1:  0.6232598607888631\n",
      "mi F1:  0.9930715935334873\n",
      "we F1:  0.9956576895666664\n",
      "Loss:  0.050496771931648254\n",
      "Loss:  0.04589911177754402\n",
      "Loss:  0.06975529342889786\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.09613201767206192\n",
      "Loss:  0.034635428339242935\n",
      "Loss:  0.036366332322359085\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.03453972935676575\n",
      "Loss:  0.05879649892449379\n",
      "Loss:  0.04937351495027542\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.24583536386489868\n",
      "Eval Loss:  0.19304221868515015\n",
      "Eval Loss:  0.009792804718017578\n",
      "[[18067  1232]\n",
      " [ 1378 11348]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     19299\n",
      "           1       0.90      0.89      0.90     12726\n",
      "\n",
      "    accuracy                           0.92     32025\n",
      "   macro avg       0.92      0.91      0.91     32025\n",
      "weighted avg       0.92      0.92      0.92     32025\n",
      "\n",
      "acc:  0.9185011709601874\n",
      "pre:  0.9020667726550079\n",
      "rec:  0.8917177432028918\n",
      "ma F1:  0.9147485673559228\n",
      "mi F1:  0.9185011709601874\n",
      "we F1:  0.918419629045286\n",
      "[[862   3]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       865\n",
      "           1       0.25      1.00      0.40         1\n",
      "\n",
      "    accuracy                           1.00       866\n",
      "   macro avg       0.62      1.00      0.70       866\n",
      "weighted avg       1.00      1.00      1.00       866\n",
      "\n",
      "acc:  0.9965357967667436\n",
      "pre:  0.25\n",
      "rec:  1.0\n",
      "ma F1:  0.699131441806601\n",
      "mi F1:  0.9965357967667436\n",
      "we F1:  0.9975720488746187\n",
      "Loss:  0.07510628551244736\n",
      "Loss:  0.051130130887031555\n",
      "Loss:  0.05172373726963997\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.07841728627681732\n",
      "Loss:  0.042463649064302444\n",
      "Loss:  0.049974992871284485\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.03684873506426811\n",
      "Loss:  0.06080150604248047\n",
      "Loss:  0.07725891470909119\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.9993346333503723\n",
      "Eval Loss:  0.11557137966156006\n",
      "Eval Loss:  0.01203155517578125\n",
      "[[18108  1191]\n",
      " [ 1313 11413]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.94     19299\n",
      "           1       0.91      0.90      0.90     12726\n",
      "\n",
      "    accuracy                           0.92     32025\n",
      "   macro avg       0.92      0.92      0.92     32025\n",
      "weighted avg       0.92      0.92      0.92     32025\n",
      "\n",
      "acc:  0.9218110850897736\n",
      "pre:  0.9055061885115836\n",
      "rec:  0.8968253968253969\n",
      "ma F1:  0.9182377329987961\n",
      "mi F1:  0.9218110850897736\n",
      "we F1:  0.9217459694878171\n",
      "[[860   5]\n",
      " [  0   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       865\n",
      "           1       0.17      1.00      0.29         1\n",
      "\n",
      "    accuracy                           0.99       866\n",
      "   macro avg       0.58      1.00      0.64       866\n",
      "weighted avg       1.00      0.99      1.00       866\n",
      "\n",
      "acc:  0.9942263279445728\n",
      "pre:  0.16666666666666666\n",
      "rec:  1.0\n",
      "ma F1:  0.641407867494824\n",
      "mi F1:  0.9942263279445728\n",
      "we F1:  0.996279986037994\n",
      "Loss:  0.04434080421924591\n",
      "Loss:  0.06017779931426048\n",
      "Loss:  0.08363765478134155\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.0449468269944191\n",
      "Loss:  0.03829476609826088\n",
      "Loss:  0.04494517669081688\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.041844725608825684\n",
      "Loss:  0.05179699510335922\n",
      "Loss:  0.06332876533269882\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  27 Training Time 5797.11269903183 Best Test Acc:  0.9965357967667436\n",
      "test subjects:  ['./seg\\\\x09', './seg\\\\x23']\n",
      "*********\n",
      "33278 1035\n",
      "31856 1035\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.7447819113731384\n",
      "Eval Loss:  0.6239511370658875\n",
      "Eval Loss:  0.7543337345123291\n",
      "[[   24 19391]\n",
      " [   70 12371]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.00      0.00     19415\n",
      "           1       0.39      0.99      0.56     12441\n",
      "\n",
      "    accuracy                           0.39     31856\n",
      "   macro avg       0.32      0.50      0.28     31856\n",
      "weighted avg       0.31      0.39      0.22     31856\n",
      "\n",
      "acc:  0.38909467604218984\n",
      "pre:  0.3894905862351237\n",
      "rec:  0.9943734426493047\n",
      "ma F1:  0.2810980837159208\n",
      "mi F1:  0.3890946760421899\n",
      "we F1:  0.220097983701067\n",
      "[[  0 749]\n",
      " [  2 284]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       749\n",
      "           1       0.27      0.99      0.43       286\n",
      "\n",
      "    accuracy                           0.27      1035\n",
      "   macro avg       0.14      0.50      0.22      1035\n",
      "weighted avg       0.08      0.27      0.12      1035\n",
      "\n",
      "acc:  0.2743961352657005\n",
      "pre:  0.2749273959341723\n",
      "rec:  0.993006993006993\n",
      "ma F1:  0.21531463229719486\n",
      "mi F1:  0.2743961352657005\n",
      "we F1:  0.11899513978163813\n",
      "Subject 28 Current Train Acc:  0.38909467604218984 Current Test Acc:  0.2743961352657005\n",
      "Loss:  0.17237205803394318\n",
      "Loss:  0.16293111443519592\n",
      "Loss:  0.15794353187084198\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.1510663777589798\n",
      "Loss:  0.13943716883659363\n",
      "Loss:  0.1259809285402298\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.12227273732423782\n",
      "Loss:  0.12771520018577576\n",
      "Loss:  0.12609267234802246\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.02818155288696289\n",
      "Eval Loss:  0.31740814447402954\n",
      "Eval Loss:  0.029931068420410156\n",
      "[[16919  2496]\n",
      " [ 3119  9322]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.86     19415\n",
      "           1       0.79      0.75      0.77     12441\n",
      "\n",
      "    accuracy                           0.82     31856\n",
      "   macro avg       0.82      0.81      0.81     31856\n",
      "weighted avg       0.82      0.82      0.82     31856\n",
      "\n",
      "acc:  0.8237380713209442\n",
      "pre:  0.7887967507192418\n",
      "rec:  0.7492966803311631\n",
      "ma F1:  0.8131091340556407\n",
      "mi F1:  0.8237380713209442\n",
      "we F1:  0.8228664341067433\n",
      "[[684  65]\n",
      " [ 78 208]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.91       749\n",
      "           1       0.76      0.73      0.74       286\n",
      "\n",
      "    accuracy                           0.86      1035\n",
      "   macro avg       0.83      0.82      0.82      1035\n",
      "weighted avg       0.86      0.86      0.86      1035\n",
      "\n",
      "acc:  0.8618357487922705\n",
      "pre:  0.7619047619047619\n",
      "rec:  0.7272727272727273\n",
      "ma F1:  0.8247733673987656\n",
      "mi F1:  0.8618357487922705\n",
      "we F1:  0.8608235408970698\n",
      "Subject 28 Current Train Acc:  0.8237380713209442 Current Test Acc:  0.8618357487922705\n",
      "Loss:  0.09065178036689758\n",
      "Loss:  0.09073399007320404\n",
      "Loss:  0.09094847738742828\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.10232286900281906\n",
      "Loss:  0.07299253344535828\n",
      "Loss:  0.07902643084526062\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.10620047152042389\n",
      "Loss:  0.1363978385925293\n",
      "Loss:  0.10114583373069763\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.02106010913848877\n",
      "Eval Loss:  0.14613181352615356\n",
      "Eval Loss:  0.03531837463378906\n",
      "[[17801  1614]\n",
      " [ 2882  9559]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89     19415\n",
      "           1       0.86      0.77      0.81     12441\n",
      "\n",
      "    accuracy                           0.86     31856\n",
      "   macro avg       0.86      0.84      0.85     31856\n",
      "weighted avg       0.86      0.86      0.86     31856\n",
      "\n",
      "acc:  0.8588648920140632\n",
      "pre:  0.855544616486172\n",
      "rec:  0.7683465959328029\n",
      "ma F1:  0.848739589445682\n",
      "mi F1:  0.8588648920140632\n",
      "we F1:  0.8573071531573893\n",
      "[[703  46]\n",
      " [ 88 198]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91       749\n",
      "           1       0.81      0.69      0.75       286\n",
      "\n",
      "    accuracy                           0.87      1035\n",
      "   macro avg       0.85      0.82      0.83      1035\n",
      "weighted avg       0.87      0.87      0.87      1035\n",
      "\n",
      "acc:  0.8705314009661835\n",
      "pre:  0.8114754098360656\n",
      "rec:  0.6923076923076923\n",
      "ma F1:  0.8300784121538838\n",
      "mi F1:  0.8705314009661835\n",
      "we F1:  0.8671669939758536\n",
      "Subject 28 Current Train Acc:  0.8588648920140632 Current Test Acc:  0.8705314009661835\n",
      "Loss:  0.08613678067922592\n",
      "Loss:  0.0815945565700531\n",
      "Loss:  0.07770194113254547\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.07849796116352081\n",
      "Loss:  0.06818347424268723\n",
      "Loss:  0.09535513818264008\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.07271966338157654\n",
      "Loss:  0.11518552154302597\n",
      "Loss:  0.07720836997032166\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.018164515495300293\n",
      "Eval Loss:  0.10865211486816406\n",
      "Eval Loss:  0.08877205848693848\n",
      "[[18043  1372]\n",
      " [ 2710  9731]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.93      0.90     19415\n",
      "           1       0.88      0.78      0.83     12441\n",
      "\n",
      "    accuracy                           0.87     31856\n",
      "   macro avg       0.87      0.86      0.86     31856\n",
      "weighted avg       0.87      0.87      0.87     31856\n",
      "\n",
      "acc:  0.8718608739326972\n",
      "pre:  0.8764297937494371\n",
      "rec:  0.7821718511373684\n",
      "ma F1:  0.8624996557103726\n",
      "mi F1:  0.8718608739326972\n",
      "we F1:  0.8703539790841083\n",
      "[[700  49]\n",
      " [ 75 211]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.92       749\n",
      "           1       0.81      0.74      0.77       286\n",
      "\n",
      "    accuracy                           0.88      1035\n",
      "   macro avg       0.86      0.84      0.85      1035\n",
      "weighted avg       0.88      0.88      0.88      1035\n",
      "\n",
      "acc:  0.8801932367149758\n",
      "pre:  0.8115384615384615\n",
      "rec:  0.7377622377622378\n",
      "ma F1:  0.8457644717487237\n",
      "mi F1:  0.8801932367149758\n",
      "we F1:  0.8783626684345616\n",
      "Subject 28 Current Train Acc:  0.8718608739326972 Current Test Acc:  0.8801932367149758\n",
      "Loss:  0.11120793223381042\n",
      "Loss:  0.07662646472454071\n",
      "Loss:  0.08117112517356873\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.06112144887447357\n",
      "Loss:  0.09739667922258377\n",
      "Loss:  0.04451627656817436\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.11608349531888962\n",
      "Loss:  0.05287974700331688\n",
      "Loss:  0.07442653924226761\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.014467000961303711\n",
      "Eval Loss:  0.08595216274261475\n",
      "Eval Loss:  0.38171112537384033\n",
      "[[17752  1663]\n",
      " [ 2146 10295]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90     19415\n",
      "           1       0.86      0.83      0.84     12441\n",
      "\n",
      "    accuracy                           0.88     31856\n",
      "   macro avg       0.88      0.87      0.87     31856\n",
      "weighted avg       0.88      0.88      0.88     31856\n",
      "\n",
      "acc:  0.880430688096434\n",
      "pre:  0.860929921391537\n",
      "rec:  0.8275058275058275\n",
      "ma F1:  0.8734989873888244\n",
      "mi F1:  0.880430688096434\n",
      "we F1:  0.8799817124437888\n",
      "[[659  90]\n",
      " [ 60 226]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       749\n",
      "           1       0.72      0.79      0.75       286\n",
      "\n",
      "    accuracy                           0.86      1035\n",
      "   macro avg       0.82      0.84      0.82      1035\n",
      "weighted avg       0.86      0.86      0.86      1035\n",
      "\n",
      "acc:  0.855072463768116\n",
      "pre:  0.7151898734177216\n",
      "rec:  0.7902097902097902\n",
      "ma F1:  0.8243253641358957\n",
      "mi F1:  0.855072463768116\n",
      "we F1:  0.8572027478072997\n",
      "Loss:  0.08196363598108292\n",
      "Loss:  0.09131056815385818\n",
      "Loss:  0.10574118047952652\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.0635068342089653\n",
      "Loss:  0.11826834082603455\n",
      "Loss:  0.06517056375741959\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.07047310471534729\n",
      "Loss:  0.09736862778663635\n",
      "Loss:  0.05318635702133179\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.009897470474243164\n",
      "Eval Loss:  0.11191427707672119\n",
      "Eval Loss:  0.08173882961273193\n",
      "[[18189  1226]\n",
      " [ 2467  9974]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91     19415\n",
      "           1       0.89      0.80      0.84     12441\n",
      "\n",
      "    accuracy                           0.88     31856\n",
      "   macro avg       0.89      0.87      0.88     31856\n",
      "weighted avg       0.88      0.88      0.88     31856\n",
      "\n",
      "acc:  0.8840720743345053\n",
      "pre:  0.8905357142857143\n",
      "rec:  0.8017040430833534\n",
      "ma F1:  0.8758134601678864\n",
      "mi F1:  0.8840720743345053\n",
      "we F1:  0.8828244857549832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[703  46]\n",
      " [ 82 204]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92       749\n",
      "           1       0.82      0.71      0.76       286\n",
      "\n",
      "    accuracy                           0.88      1035\n",
      "   macro avg       0.86      0.83      0.84      1035\n",
      "weighted avg       0.87      0.88      0.87      1035\n",
      "\n",
      "acc:  0.8763285024154589\n",
      "pre:  0.816\n",
      "rec:  0.7132867132867133\n",
      "ma F1:  0.8388760240518398\n",
      "mi F1:  0.8763285024154589\n",
      "we F1:  0.8736265200084644\n",
      "Loss:  0.07255272567272186\n",
      "Loss:  0.04681196063756943\n",
      "Loss:  0.04414863884449005\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.08292397111654282\n",
      "Loss:  0.06617504358291626\n",
      "Loss:  0.06469065696001053\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.10727290064096451\n",
      "Loss:  0.0656125396490097\n",
      "Loss:  0.09287890791893005\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.00775456428527832\n",
      "Eval Loss:  0.10224711894989014\n",
      "Eval Loss:  0.1176939606666565\n",
      "[[18171  1244]\n",
      " [ 2188 10253]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.91     19415\n",
      "           1       0.89      0.82      0.86     12441\n",
      "\n",
      "    accuracy                           0.89     31856\n",
      "   macro avg       0.89      0.88      0.89     31856\n",
      "weighted avg       0.89      0.89      0.89     31856\n",
      "\n",
      "acc:  0.8922651933701657\n",
      "pre:  0.8917978603113855\n",
      "rec:  0.8241298930954103\n",
      "ma F1:  0.8851710510108572\n",
      "mi F1:  0.8922651933701657\n",
      "we F1:  0.8914194153470302\n",
      "[[678  71]\n",
      " [ 73 213]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.91      0.90       749\n",
      "           1       0.75      0.74      0.75       286\n",
      "\n",
      "    accuracy                           0.86      1035\n",
      "   macro avg       0.83      0.82      0.83      1035\n",
      "weighted avg       0.86      0.86      0.86      1035\n",
      "\n",
      "acc:  0.8608695652173913\n",
      "pre:  0.75\n",
      "rec:  0.7447552447552448\n",
      "ma F1:  0.8256842105263158\n",
      "mi F1:  0.8608695652173913\n",
      "we F1:  0.860718230358505\n",
      "Loss:  0.09584397822618484\n",
      "Loss:  0.053798820823431015\n",
      "Loss:  0.0511711910367012\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.08282965421676636\n",
      "Loss:  0.0742490366101265\n",
      "Loss:  0.05406223237514496\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.08275090903043747\n",
      "Loss:  0.08145541697740555\n",
      "Loss:  0.04010471701622009\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.011471748352050781\n",
      "Eval Loss:  0.10646653175354004\n",
      "Eval Loss:  0.24790453910827637\n",
      "[[17837  1578]\n",
      " [ 1686 10755]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92     19415\n",
      "           1       0.87      0.86      0.87     12441\n",
      "\n",
      "    accuracy                           0.90     31856\n",
      "   macro avg       0.89      0.89      0.89     31856\n",
      "weighted avg       0.90      0.90      0.90     31856\n",
      "\n",
      "acc:  0.8975389251632345\n",
      "pre:  0.872050595962053\n",
      "rec:  0.8644803472389679\n",
      "ma F1:  0.8922117009210175\n",
      "mi F1:  0.8975389251632345\n",
      "we F1:  0.8974576853696508\n",
      "[[687  62]\n",
      " [ 66 220]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       749\n",
      "           1       0.78      0.77      0.77       286\n",
      "\n",
      "    accuracy                           0.88      1035\n",
      "   macro avg       0.85      0.84      0.84      1035\n",
      "weighted avg       0.88      0.88      0.88      1035\n",
      "\n",
      "acc:  0.8763285024154589\n",
      "pre:  0.7801418439716312\n",
      "rec:  0.7692307692307693\n",
      "ma F1:  0.8447140901333434\n",
      "mi F1:  0.8763285024154589\n",
      "we F1:  0.8760577151582246\n",
      "Loss:  0.06136104837059975\n",
      "Loss:  0.08088930696249008\n",
      "Loss:  0.05647160857915878\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.10012926161289215\n",
      "Loss:  0.04947221651673317\n",
      "Loss:  0.07166972011327744\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.0532500222325325\n",
      "Loss:  0.051984816789627075\n",
      "Loss:  0.0583643913269043\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.006735563278198242\n",
      "Eval Loss:  0.11461979150772095\n",
      "Eval Loss:  0.06302940845489502\n",
      "[[18491   924]\n",
      " [ 2435 10006]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.92     19415\n",
      "           1       0.92      0.80      0.86     12441\n",
      "\n",
      "    accuracy                           0.89     31856\n",
      "   macro avg       0.90      0.88      0.89     31856\n",
      "weighted avg       0.90      0.89      0.89     31856\n",
      "\n",
      "acc:  0.8945567553992968\n",
      "pre:  0.9154620311070448\n",
      "rec:  0.8042761835865284\n",
      "ma F1:  0.8865048530466086\n",
      "mi F1:  0.8945567553992968\n",
      "we F1:  0.8931228809791539\n",
      "[[712  37]\n",
      " [ 99 187]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91       749\n",
      "           1       0.83      0.65      0.73       286\n",
      "\n",
      "    accuracy                           0.87      1035\n",
      "   macro avg       0.86      0.80      0.82      1035\n",
      "weighted avg       0.87      0.87      0.86      1035\n",
      "\n",
      "acc:  0.8685990338164251\n",
      "pre:  0.8348214285714286\n",
      "rec:  0.6538461538461539\n",
      "ma F1:  0.823076923076923\n",
      "mi F1:  0.8685990338164251\n",
      "we F1:  0.8632230893100458\n",
      "Loss:  0.057842910289764404\n",
      "Loss:  0.08551926165819168\n",
      "Loss:  0.08280867338180542\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.07365498691797256\n",
      "Loss:  0.07656604051589966\n",
      "Loss:  0.08159706741571426\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.030781636014580727\n",
      "Loss:  0.08584025502204895\n",
      "Loss:  0.06715631484985352\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.0036814212799072266\n",
      "Eval Loss:  0.12675422430038452\n",
      "Eval Loss:  0.018342018127441406\n",
      "[[18632   783]\n",
      " [ 2598  9843]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     19415\n",
      "           1       0.93      0.79      0.85     12441\n",
      "\n",
      "    accuracy                           0.89     31856\n",
      "   macro avg       0.90      0.88      0.89     31856\n",
      "weighted avg       0.90      0.89      0.89     31856\n",
      "\n",
      "acc:  0.8938661476644902\n",
      "pre:  0.9263128176171654\n",
      "rec:  0.7911743428984809\n",
      "ma F1:  0.8851216550857058\n",
      "mi F1:  0.8938661476644902\n",
      "we F1:  0.8920603388090467\n",
      "[[714  35]\n",
      " [110 176]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91       749\n",
      "           1       0.83      0.62      0.71       286\n",
      "\n",
      "    accuracy                           0.86      1035\n",
      "   macro avg       0.85      0.78      0.81      1035\n",
      "weighted avg       0.86      0.86      0.85      1035\n",
      "\n",
      "acc:  0.8599033816425121\n",
      "pre:  0.8341232227488151\n",
      "rec:  0.6153846153846154\n",
      "ma F1:  0.808034475127945\n",
      "mi F1:  0.8599033816425121\n",
      "we F1:  0.8526725861246821\n",
      "Loss:  0.09549151360988617\n",
      "Loss:  0.045619022101163864\n",
      "Loss:  0.06760487705469131\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.07166754454374313\n",
      "Loss:  0.06528393924236298\n",
      "Loss:  0.0419037900865078\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.06859339773654938\n",
      "Loss:  0.08668693900108337\n",
      "Loss:  0.07217936217784882\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.0037992000579833984\n",
      "Eval Loss:  0.14463448524475098\n",
      "Eval Loss:  0.054334282875061035\n",
      "[[18566   849]\n",
      " [ 2346 10095]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92     19415\n",
      "           1       0.92      0.81      0.86     12441\n",
      "\n",
      "    accuracy                           0.90     31856\n",
      "   macro avg       0.91      0.88      0.89     31856\n",
      "weighted avg       0.90      0.90      0.90     31856\n",
      "\n",
      "acc:  0.8997049221496736\n",
      "pre:  0.9224232456140351\n",
      "rec:  0.8114299493609839\n",
      "ma F1:  0.8920733204895394\n",
      "mi F1:  0.8997049221496736\n",
      "we F1:  0.8983562611078578\n",
      "[[707  42]\n",
      " [119 167]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90       749\n",
      "           1       0.80      0.58      0.67       286\n",
      "\n",
      "    accuracy                           0.84      1035\n",
      "   macro avg       0.83      0.76      0.79      1035\n",
      "weighted avg       0.84      0.84      0.84      1035\n",
      "\n",
      "acc:  0.8444444444444444\n",
      "pre:  0.7990430622009569\n",
      "rec:  0.583916083916084\n",
      "ma F1:  0.7862626262626262\n",
      "mi F1:  0.8444444444444444\n",
      "we F1:  0.8361481481481481\n",
      "Loss:  0.0645715594291687\n",
      "Loss:  0.07356911152601242\n",
      "Loss:  0.05757817625999451\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.03996751457452774\n",
      "Loss:  0.08144007623195648\n",
      "Loss:  0.0595843531191349\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.07902534306049347\n",
      "Loss:  0.05315154418349266\n",
      "Loss:  0.051794786006212234\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.0037620067596435547\n",
      "Eval Loss:  0.07811284065246582\n",
      "Eval Loss:  0.09019064903259277\n",
      "[[17994  1421]\n",
      " [ 1662 10779]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92     19415\n",
      "           1       0.88      0.87      0.87     12441\n",
      "\n",
      "    accuracy                           0.90     31856\n",
      "   macro avg       0.90      0.90      0.90     31856\n",
      "weighted avg       0.90      0.90      0.90     31856\n",
      "\n",
      "acc:  0.9032207433450528\n",
      "pre:  0.8835245901639345\n",
      "rec:  0.8664094526163492\n",
      "ma F1:  0.8979878474203115\n",
      "mi F1:  0.9032207433450528\n",
      "we F1:  0.9030459508408446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[651  98]\n",
      " [ 77 209]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88       749\n",
      "           1       0.68      0.73      0.70       286\n",
      "\n",
      "    accuracy                           0.83      1035\n",
      "   macro avg       0.79      0.80      0.79      1035\n",
      "weighted avg       0.84      0.83      0.83      1035\n",
      "\n",
      "acc:  0.8309178743961353\n",
      "pre:  0.6807817589576547\n",
      "rec:  0.7307692307692307\n",
      "ma F1:  0.7932034877680362\n",
      "mi F1:  0.8309178743961353\n",
      "we F1:  0.8327097343943028\n",
      "Loss:  0.056932322680950165\n",
      "Loss:  0.05817331373691559\n",
      "Loss:  0.07796257734298706\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.06642846018075943\n",
      "Loss:  0.06642621010541916\n",
      "Loss:  0.06342756003141403\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.05463045835494995\n",
      "Loss:  0.0633721798658371\n",
      "Loss:  0.05850888043642044\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.0040493011474609375\n",
      "Eval Loss:  0.140896737575531\n",
      "Eval Loss:  0.01851344108581543\n",
      "[[18474   941]\n",
      " [ 2096 10345]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     19415\n",
      "           1       0.92      0.83      0.87     12441\n",
      "\n",
      "    accuracy                           0.90     31856\n",
      "   macro avg       0.91      0.89      0.90     31856\n",
      "weighted avg       0.91      0.90      0.90     31856\n",
      "\n",
      "acc:  0.904664741336012\n",
      "pre:  0.916622363990785\n",
      "rec:  0.8315247970420384\n",
      "ma F1:  0.8980244388122134\n",
      "mi F1:  0.904664741336012\n",
      "we F1:  0.9037212612751204\n",
      "[[704  45]\n",
      " [125 161]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89       749\n",
      "           1       0.78      0.56      0.65       286\n",
      "\n",
      "    accuracy                           0.84      1035\n",
      "   macro avg       0.82      0.75      0.77      1035\n",
      "weighted avg       0.83      0.84      0.83      1035\n",
      "\n",
      "acc:  0.8357487922705314\n",
      "pre:  0.7815533980582524\n",
      "rec:  0.5629370629370629\n",
      "ma F1:  0.7733701196327554\n",
      "mi F1:  0.8357487922705314\n",
      "we F1:  0.8265585642575994\n",
      "Loss:  0.06917464733123779\n",
      "Loss:  0.08572065085172653\n",
      "Loss:  0.0638708770275116\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.08658398687839508\n",
      "Loss:  0.03527109697461128\n",
      "Loss:  0.06782543659210205\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.058476630598306656\n",
      "Loss:  0.06215427443385124\n",
      "Loss:  0.08667594939470291\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.002791881561279297\n",
      "Eval Loss:  0.11458677053451538\n",
      "Eval Loss:  0.02608180046081543\n",
      "[[18594   821]\n",
      " [ 2248 10193]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92     19415\n",
      "           1       0.93      0.82      0.87     12441\n",
      "\n",
      "    accuracy                           0.90     31856\n",
      "   macro avg       0.91      0.89      0.90     31856\n",
      "weighted avg       0.91      0.90      0.90     31856\n",
      "\n",
      "acc:  0.9036602209944752\n",
      "pre:  0.9254585073542764\n",
      "rec:  0.8193071296519573\n",
      "ma F1:  0.8964592548302439\n",
      "mi F1:  0.9036602209944752\n",
      "we F1:  0.9024370596188819\n",
      "[[714  35]\n",
      " [127 159]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90       749\n",
      "           1       0.82      0.56      0.66       286\n",
      "\n",
      "    accuracy                           0.84      1035\n",
      "   macro avg       0.83      0.75      0.78      1035\n",
      "weighted avg       0.84      0.84      0.83      1035\n",
      "\n",
      "acc:  0.8434782608695652\n",
      "pre:  0.8195876288659794\n",
      "rec:  0.5559440559440559\n",
      "ma F1:  0.7803066037735849\n",
      "mi F1:  0.8434782608695653\n",
      "we F1:  0.8330065627563578\n",
      "Loss:  0.06296482682228088\n",
      "Loss:  0.05555986240506172\n",
      "Loss:  0.08638196438550949\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.0765620544552803\n",
      "Loss:  0.06390994042158127\n",
      "Loss:  0.07053762674331665\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.06537281721830368\n",
      "Loss:  0.050815217196941376\n",
      "Loss:  0.06564218550920486\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.0032434463500976562\n",
      "Eval Loss:  0.07206451892852783\n",
      "Eval Loss:  0.05741536617279053\n",
      "[[18437   978]\n",
      " [ 1919 10522]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19415\n",
      "           1       0.91      0.85      0.88     12441\n",
      "\n",
      "    accuracy                           0.91     31856\n",
      "   macro avg       0.91      0.90      0.90     31856\n",
      "weighted avg       0.91      0.91      0.91     31856\n",
      "\n",
      "acc:  0.9090595178302361\n",
      "pre:  0.9149565217391304\n",
      "rec:  0.8457519492002251\n",
      "ma F1:  0.903076086746324\n",
      "mi F1:  0.9090595178302361\n",
      "we F1:  0.9083481585567097\n",
      "[[687  62]\n",
      " [110 176]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89       749\n",
      "           1       0.74      0.62      0.67       286\n",
      "\n",
      "    accuracy                           0.83      1035\n",
      "   macro avg       0.80      0.77      0.78      1035\n",
      "weighted avg       0.83      0.83      0.83      1035\n",
      "\n",
      "acc:  0.8338164251207729\n",
      "pre:  0.7394957983193278\n",
      "rec:  0.6153846153846154\n",
      "ma F1:  0.7802504369809309\n",
      "mi F1:  0.8338164251207729\n",
      "we F1:  0.8287847863131166\n",
      "Loss:  0.04983880743384361\n",
      "Loss:  0.07215070724487305\n",
      "Loss:  0.03258921205997467\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.09159821271896362\n",
      "Loss:  0.05816562846302986\n",
      "Loss:  0.04411732777953148\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.09516855329275131\n",
      "Loss:  0.07775593549013138\n",
      "Loss:  0.06782015413045883\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.0031414031982421875\n",
      "Eval Loss:  0.0937962532043457\n",
      "Eval Loss:  0.062455177307128906\n",
      "[[18434   981]\n",
      " [ 1950 10491]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.93     19415\n",
      "           1       0.91      0.84      0.88     12441\n",
      "\n",
      "    accuracy                           0.91     31856\n",
      "   macro avg       0.91      0.90      0.90     31856\n",
      "weighted avg       0.91      0.91      0.91     31856\n",
      "\n",
      "acc:  0.907992214967353\n",
      "pre:  0.9144874476987448\n",
      "rec:  0.8432601880877743\n",
      "ma F1:  0.9018928098893275\n",
      "mi F1:  0.907992214967353\n",
      "we F1:  0.9072481228710912\n",
      "[[690  59]\n",
      " [116 170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89       749\n",
      "           1       0.74      0.59      0.66       286\n",
      "\n",
      "    accuracy                           0.83      1035\n",
      "   macro avg       0.80      0.76      0.77      1035\n",
      "weighted avg       0.82      0.83      0.82      1035\n",
      "\n",
      "acc:  0.8309178743961353\n",
      "pre:  0.74235807860262\n",
      "rec:  0.5944055944055944\n",
      "ma F1:  0.7738269909156181\n",
      "mi F1:  0.8309178743961353\n",
      "we F1:  0.8246598352453861\n",
      "Loss:  0.042030125856399536\n",
      "Loss:  0.04384370893239975\n",
      "Loss:  0.04631287604570389\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.04758923873305321\n",
      "Loss:  0.056217268109321594\n",
      "Loss:  0.058201588690280914\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.04723944514989853\n",
      "Loss:  0.04276703670620918\n",
      "Loss:  0.06319451332092285\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.003082275390625\n",
      "Eval Loss:  0.10014915466308594\n",
      "Eval Loss:  0.05173134803771973\n",
      "[[18605   810]\n",
      " [ 1963 10478]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     19415\n",
      "           1       0.93      0.84      0.88     12441\n",
      "\n",
      "    accuracy                           0.91     31856\n",
      "   macro avg       0.92      0.90      0.91     31856\n",
      "weighted avg       0.91      0.91      0.91     31856\n",
      "\n",
      "acc:  0.9129520341536916\n",
      "pre:  0.9282423812898654\n",
      "rec:  0.8422152560083594\n",
      "ma F1:  0.9068921498430208\n",
      "mi F1:  0.9129520341536916\n",
      "we F1:  0.9120923015819918\n",
      "[[709  40]\n",
      " [134 152]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89       749\n",
      "           1       0.79      0.53      0.64       286\n",
      "\n",
      "    accuracy                           0.83      1035\n",
      "   macro avg       0.82      0.74      0.76      1035\n",
      "weighted avg       0.83      0.83      0.82      1035\n",
      "\n",
      "acc:  0.8318840579710145\n",
      "pre:  0.7916666666666666\n",
      "rec:  0.5314685314685315\n",
      "ma F1:  0.7633433905931329\n",
      "mi F1:  0.8318840579710145\n",
      "we F1:  0.8203170512681044\n",
      "Loss:  0.06281434744596481\n",
      "Loss:  0.03456607088446617\n",
      "Loss:  0.05256862938404083\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.07965132594108582\n",
      "Loss:  0.06809251010417938\n",
      "Loss:  0.052731968462467194\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.068720743060112\n",
      "Loss:  0.08527345955371857\n",
      "Loss:  0.06825011968612671\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.0027556419372558594\n",
      "Eval Loss:  0.05124664306640625\n",
      "Eval Loss:  0.07444632053375244\n",
      "[[18268  1147]\n",
      " [ 1521 10920]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19415\n",
      "           1       0.90      0.88      0.89     12441\n",
      "\n",
      "    accuracy                           0.92     31856\n",
      "   macro avg       0.91      0.91      0.91     31856\n",
      "weighted avg       0.92      0.92      0.92     31856\n",
      "\n",
      "acc:  0.9162481165243597\n",
      "pre:  0.9049473771442778\n",
      "rec:  0.877742946708464\n",
      "ma F1:  0.9115416537754822\n",
      "mi F1:  0.9162481165243597\n",
      "we F1:  0.9160085660251653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[650  99]\n",
      " [108 178]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.86       749\n",
      "           1       0.64      0.62      0.63       286\n",
      "\n",
      "    accuracy                           0.80      1035\n",
      "   macro avg       0.75      0.75      0.75      1035\n",
      "weighted avg       0.80      0.80      0.80      1035\n",
      "\n",
      "acc:  0.8\n",
      "pre:  0.6425992779783394\n",
      "rec:  0.6223776223776224\n",
      "ma F1:  0.747483914615159\n",
      "mi F1:  0.8000000000000002\n",
      "we F1:  0.7989986339651197\n",
      "Loss:  0.0743342787027359\n",
      "Loss:  0.03837261348962784\n",
      "Loss:  0.06124784052371979\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.046211402863264084\n",
      "Loss:  0.06521736085414886\n",
      "Loss:  0.04745825007557869\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.07183059304952621\n",
      "Loss:  0.05144553631544113\n",
      "Loss:  0.10103302448987961\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.003000020980834961\n",
      "Eval Loss:  0.060158610343933105\n",
      "Eval Loss:  0.05629837512969971\n",
      "[[18380  1035]\n",
      " [ 1515 10926]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94     19415\n",
      "           1       0.91      0.88      0.90     12441\n",
      "\n",
      "    accuracy                           0.92     31856\n",
      "   macro avg       0.92      0.91      0.92     31856\n",
      "weighted avg       0.92      0.92      0.92     31856\n",
      "\n",
      "acc:  0.919952285283777\n",
      "pre:  0.9134687735139202\n",
      "rec:  0.8782252230528093\n",
      "ma F1:  0.9153156893716836\n",
      "mi F1:  0.919952285283777\n",
      "we F1:  0.919653711895287\n",
      "[[700  49]\n",
      " [128 158]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.93      0.89       749\n",
      "           1       0.76      0.55      0.64       286\n",
      "\n",
      "    accuracy                           0.83      1035\n",
      "   macro avg       0.80      0.74      0.76      1035\n",
      "weighted avg       0.82      0.83      0.82      1035\n",
      "\n",
      "acc:  0.8289855072463768\n",
      "pre:  0.7632850241545893\n",
      "rec:  0.5524475524475524\n",
      "ma F1:  0.7643676017189287\n",
      "mi F1:  0.8289855072463768\n",
      "we F1:  0.8195670302414536\n",
      "Loss:  0.09920386970043182\n",
      "Loss:  0.053990744054317474\n",
      "Loss:  0.0467299222946167\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.046780575066804886\n",
      "Loss:  0.03354274109005928\n",
      "Loss:  0.08374219387769699\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.046919915825128555\n",
      "Loss:  0.06059015169739723\n",
      "Loss:  0.0450713187456131\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.0051958560943603516\n",
      "Eval Loss:  0.07700574398040771\n",
      "Eval Loss:  0.06237375736236572\n",
      "[[18184  1231]\n",
      " [ 1372 11069]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     19415\n",
      "           1       0.90      0.89      0.89     12441\n",
      "\n",
      "    accuracy                           0.92     31856\n",
      "   macro avg       0.91      0.91      0.91     31856\n",
      "weighted avg       0.92      0.92      0.92     31856\n",
      "\n",
      "acc:  0.9182885484681065\n",
      "pre:  0.8999186991869919\n",
      "rec:  0.8897194759263725\n",
      "ma F1:  0.913998384065674\n",
      "mi F1:  0.9182885484681065\n",
      "we F1:  0.9182035290470603\n",
      "[[691  58]\n",
      " [122 164]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.92      0.88       749\n",
      "           1       0.74      0.57      0.65       286\n",
      "\n",
      "    accuracy                           0.83      1035\n",
      "   macro avg       0.79      0.75      0.77      1035\n",
      "weighted avg       0.82      0.83      0.82      1035\n",
      "\n",
      "acc:  0.8260869565217391\n",
      "pre:  0.7387387387387387\n",
      "rec:  0.5734265734265734\n",
      "ma F1:  0.7652162077691633\n",
      "mi F1:  0.8260869565217391\n",
      "we F1:  0.8186946834284473\n",
      "Loss:  0.0428529754281044\n",
      "Loss:  0.0581895187497139\n",
      "Loss:  0.024005167186260223\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.026725521311163902\n",
      "Loss:  0.042598992586135864\n",
      "Loss:  0.055318430066108704\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.07238641381263733\n",
      "Loss:  0.06481831520795822\n",
      "Loss:  0.04283547401428223\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  28 Training Time 5792.496042728424 Best Test Acc:  0.8801932367149758\n",
      "test subjects:  ['./seg\\\\x10']\n",
      "*********\n",
      "33803 510\n",
      "32381 510\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.6447755694389343\n",
      "Eval Loss:  0.7216736674308777\n",
      "Eval Loss:  0.6712031364440918\n",
      "[[19629   121]\n",
      " [12274   357]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.99      0.76     19750\n",
      "           1       0.75      0.03      0.05     12631\n",
      "\n",
      "    accuracy                           0.62     32381\n",
      "   macro avg       0.68      0.51      0.41     32381\n",
      "weighted avg       0.67      0.62      0.48     32381\n",
      "\n",
      "acc:  0.6172137982150027\n",
      "pre:  0.7468619246861925\n",
      "rec:  0.02826379542395693\n",
      "ma F1:  0.40724984813123966\n",
      "mi F1:  0.6172137982150027\n",
      "we F1:  0.48480969457407425\n",
      "[[414   0]\n",
      " [ 96   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.90       414\n",
      "           1       0.00      0.00      0.00        96\n",
      "\n",
      "    accuracy                           0.81       510\n",
      "   macro avg       0.41      0.50      0.45       510\n",
      "weighted avg       0.66      0.81      0.73       510\n",
      "\n",
      "acc:  0.8117647058823529\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.44805194805194803\n",
      "mi F1:  0.8117647058823529\n",
      "we F1:  0.7274255156608097\n",
      "Subject 29 Current Train Acc:  0.6172137982150027 Current Test Acc:  0.8117647058823529\n",
      "Loss:  0.16709095239639282\n",
      "Loss:  0.15809758007526398\n",
      "Loss:  0.1479945033788681\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.1412559449672699\n",
      "Loss:  0.1328544318675995\n",
      "Loss:  0.11909451335668564\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.10399685800075531\n",
      "Loss:  0.10781565308570862\n",
      "Loss:  0.08704611659049988\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.02873861789703369\n",
      "Eval Loss:  0.2727324962615967\n",
      "Eval Loss:  1.5306401252746582\n",
      "[[17474  2276]\n",
      " [ 3486  9145]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.88      0.86     19750\n",
      "           1       0.80      0.72      0.76     12631\n",
      "\n",
      "    accuracy                           0.82     32381\n",
      "   macro avg       0.82      0.80      0.81     32381\n",
      "weighted avg       0.82      0.82      0.82     32381\n",
      "\n",
      "acc:  0.8220561440350823\n",
      "pre:  0.800717975658874\n",
      "rec:  0.7240123505660676\n",
      "ma F1:  0.8094490084388108\n",
      "mi F1:  0.8220561440350823\n",
      "we F1:  0.8202246355620977\n",
      "[[364  50]\n",
      " [ 53  43]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.88      0.88       414\n",
      "           1       0.46      0.45      0.46        96\n",
      "\n",
      "    accuracy                           0.80       510\n",
      "   macro avg       0.67      0.66      0.67       510\n",
      "weighted avg       0.80      0.80      0.80       510\n",
      "\n",
      "acc:  0.7980392156862746\n",
      "pre:  0.46236559139784944\n",
      "rec:  0.4479166666666667\n",
      "ma F1:  0.6655397016407847\n",
      "mi F1:  0.7980392156862746\n",
      "we F1:  0.7968009024708962\n",
      "Loss:  0.12383178621530533\n",
      "Loss:  0.09642266482114792\n",
      "Loss:  0.09122402966022491\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.08643297106027603\n",
      "Loss:  0.08966092765331268\n",
      "Loss:  0.08424845337867737\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.11075715720653534\n",
      "Loss:  0.10436155647039413\n",
      "Loss:  0.07570703327655792\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.0157315731048584\n",
      "Eval Loss:  0.15708309412002563\n",
      "Eval Loss:  1.6202232837677002\n",
      "[[18475  1275]\n",
      " [ 3576  9055]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.88     19750\n",
      "           1       0.88      0.72      0.79     12631\n",
      "\n",
      "    accuracy                           0.85     32381\n",
      "   macro avg       0.86      0.83      0.84     32381\n",
      "weighted avg       0.85      0.85      0.85     32381\n",
      "\n",
      "acc:  0.850189926191285\n",
      "pre:  0.8765730880929332\n",
      "rec:  0.7168870239885995\n",
      "ma F1:  0.8363394293197841\n",
      "mi F1:  0.850189926191285\n",
      "we F1:  0.8468066997261763\n",
      "[[379  35]\n",
      " [ 51  45]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       414\n",
      "           1       0.56      0.47      0.51        96\n",
      "\n",
      "    accuracy                           0.83       510\n",
      "   macro avg       0.72      0.69      0.70       510\n",
      "weighted avg       0.82      0.83      0.83       510\n",
      "\n",
      "acc:  0.8313725490196079\n",
      "pre:  0.5625\n",
      "rec:  0.46875\n",
      "ma F1:  0.70473395088324\n",
      "mi F1:  0.8313725490196079\n",
      "we F1:  0.8253060293484046\n",
      "Subject 29 Current Train Acc:  0.850189926191285 Current Test Acc:  0.8313725490196079\n",
      "Loss:  0.06776160001754761\n",
      "Loss:  0.0847363993525505\n",
      "Loss:  0.06718432158231735\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.05542275682091713\n",
      "Loss:  0.056250397115945816\n",
      "Loss:  0.07266886532306671\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.07280910760164261\n",
      "Loss:  0.09237756580114365\n",
      "Loss:  0.06428062170743942\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.010421037673950195\n",
      "Eval Loss:  0.26113611459732056\n",
      "Eval Loss:  1.5351264476776123\n",
      "[[18996   754]\n",
      " [ 4152  8479]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.96      0.89     19750\n",
      "           1       0.92      0.67      0.78     12631\n",
      "\n",
      "    accuracy                           0.85     32381\n",
      "   macro avg       0.87      0.82      0.83     32381\n",
      "weighted avg       0.86      0.85      0.84     32381\n",
      "\n",
      "acc:  0.848491399277354\n",
      "pre:  0.9183364020361746\n",
      "rec:  0.6712849338928034\n",
      "ma F1:  0.8306242867956064\n",
      "mi F1:  0.8484913992773538\n",
      "we F1:  0.8427186077766429\n",
      "[[394  20]\n",
      " [ 62  34]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.91       414\n",
      "           1       0.63      0.35      0.45        96\n",
      "\n",
      "    accuracy                           0.84       510\n",
      "   macro avg       0.75      0.65      0.68       510\n",
      "weighted avg       0.82      0.84      0.82       510\n",
      "\n",
      "acc:  0.8392156862745098\n",
      "pre:  0.6296296296296297\n",
      "rec:  0.3541666666666667\n",
      "ma F1:  0.6795402298850575\n",
      "mi F1:  0.8392156862745097\n",
      "we F1:  0.8205868830290738\n",
      "Subject 29 Current Train Acc:  0.848491399277354 Current Test Acc:  0.8392156862745098\n",
      "Loss:  0.08847761154174805\n",
      "Loss:  0.11400502175092697\n",
      "Loss:  0.08534552901983261\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.07873877137899399\n",
      "Loss:  0.09168552607297897\n",
      "Loss:  0.06299608945846558\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.07464488595724106\n",
      "Loss:  0.11357876658439636\n",
      "Loss:  0.077349454164505\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.00773310661315918\n",
      "Eval Loss:  0.12798136472702026\n",
      "Eval Loss:  0.6834156513214111\n",
      "[[19205   545]\n",
      " [ 4542  8089]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.97      0.88     19750\n",
      "           1       0.94      0.64      0.76     12631\n",
      "\n",
      "    accuracy                           0.84     32381\n",
      "   macro avg       0.87      0.81      0.82     32381\n",
      "weighted avg       0.86      0.84      0.84     32381\n",
      "\n",
      "acc:  0.8429017016151447\n",
      "pre:  0.9368774611999073\n",
      "rec:  0.6404085187237748\n",
      "ma F1:  0.8219150155735029\n",
      "mi F1:  0.8429017016151447\n",
      "we F1:  0.8353554813822873\n",
      "[[407   7]\n",
      " [ 72  24]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91       414\n",
      "           1       0.77      0.25      0.38        96\n",
      "\n",
      "    accuracy                           0.85       510\n",
      "   macro avg       0.81      0.62      0.64       510\n",
      "weighted avg       0.84      0.85      0.81       510\n",
      "\n",
      "acc:  0.8450980392156863\n",
      "pre:  0.7741935483870968\n",
      "rec:  0.25\n",
      "ma F1:  0.644743455220393\n",
      "mi F1:  0.8450980392156864\n",
      "we F1:  0.8110953030284955\n",
      "Subject 29 Current Train Acc:  0.8429017016151447 Current Test Acc:  0.8450980392156863\n",
      "Loss:  0.06696712225675583\n",
      "Loss:  0.09224100410938263\n",
      "Loss:  0.07190727442502975\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.06689362227916718\n",
      "Loss:  0.09092363715171814\n",
      "Loss:  0.04576325789093971\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.056678641587495804\n",
      "Loss:  0.05804877728223801\n",
      "Loss:  0.07056085765361786\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.006127357482910156\n",
      "Eval Loss:  0.06802022457122803\n",
      "Eval Loss:  0.28170865774154663\n",
      "[[18798   952]\n",
      " [ 2869  9762]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.95      0.91     19750\n",
      "           1       0.91      0.77      0.84     12631\n",
      "\n",
      "    accuracy                           0.88     32381\n",
      "   macro avg       0.89      0.86      0.87     32381\n",
      "weighted avg       0.88      0.88      0.88     32381\n",
      "\n",
      "acc:  0.8819987029430839\n",
      "pre:  0.9111442971812581\n",
      "rec:  0.7728604227693769\n",
      "ma F1:  0.8720339460166543\n",
      "mi F1:  0.8819987029430839\n",
      "we F1:  0.8798846658660625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[390  24]\n",
      " [ 44  52]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92       414\n",
      "           1       0.68      0.54      0.60        96\n",
      "\n",
      "    accuracy                           0.87       510\n",
      "   macro avg       0.79      0.74      0.76       510\n",
      "weighted avg       0.86      0.87      0.86       510\n",
      "\n",
      "acc:  0.8666666666666667\n",
      "pre:  0.6842105263157895\n",
      "rec:  0.5416666666666666\n",
      "ma F1:  0.7622312417727073\n",
      "mi F1:  0.8666666666666667\n",
      "we F1:  0.8604870557261957\n",
      "Subject 29 Current Train Acc:  0.8819987029430839 Current Test Acc:  0.8666666666666667\n",
      "Loss:  0.08740998804569244\n",
      "Loss:  0.11082141101360321\n",
      "Loss:  0.05605668947100639\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.051085215061903\n",
      "Loss:  0.048912957310676575\n",
      "Loss:  0.0958682969212532\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.06617515534162521\n",
      "Loss:  0.05412052199244499\n",
      "Loss:  0.054474614560604095\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.004357337951660156\n",
      "Eval Loss:  0.03309178352355957\n",
      "Eval Loss:  0.23197364807128906\n",
      "[[18818   932]\n",
      " [ 2607 10024]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91     19750\n",
      "           1       0.91      0.79      0.85     12631\n",
      "\n",
      "    accuracy                           0.89     32381\n",
      "   macro avg       0.90      0.87      0.88     32381\n",
      "weighted avg       0.89      0.89      0.89     32381\n",
      "\n",
      "acc:  0.8907075136654211\n",
      "pre:  0.9149324571011318\n",
      "rec:  0.7936030401393397\n",
      "ma F1:  0.8820047555344785\n",
      "mi F1:  0.8907075136654211\n",
      "we F1:  0.8890498925749811\n",
      "[[379  35]\n",
      " [ 29  67]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92       414\n",
      "           1       0.66      0.70      0.68        96\n",
      "\n",
      "    accuracy                           0.87       510\n",
      "   macro avg       0.79      0.81      0.80       510\n",
      "weighted avg       0.88      0.87      0.88       510\n",
      "\n",
      "acc:  0.8745098039215686\n",
      "pre:  0.6568627450980392\n",
      "rec:  0.6979166666666666\n",
      "ma F1:  0.7994543979945441\n",
      "mi F1:  0.8745098039215687\n",
      "we F1:  0.8759531771124731\n",
      "Subject 29 Current Train Acc:  0.8907075136654211 Current Test Acc:  0.8745098039215686\n",
      "Loss:  0.07219832390546799\n",
      "Loss:  0.044558219611644745\n",
      "Loss:  0.06887956708669662\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.07567495107650757\n",
      "Loss:  0.05161047354340553\n",
      "Loss:  0.06814635545015335\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.07468780875205994\n",
      "Loss:  0.10073034465312958\n",
      "Loss:  0.07649611681699753\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.003976345062255859\n",
      "Eval Loss:  0.03508412837982178\n",
      "Eval Loss:  0.10559570789337158\n",
      "[[18931   819]\n",
      " [ 2543 10088]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.96      0.92     19750\n",
      "           1       0.92      0.80      0.86     12631\n",
      "\n",
      "    accuracy                           0.90     32381\n",
      "   macro avg       0.90      0.88      0.89     32381\n",
      "weighted avg       0.90      0.90      0.89     32381\n",
      "\n",
      "acc:  0.8961736820975263\n",
      "pre:  0.9249106078665077\n",
      "rec:  0.7986699390388726\n",
      "ma F1:  0.8878063498429904\n",
      "mi F1:  0.8961736820975263\n",
      "we F1:  0.8945424165986209\n",
      "[[357  57]\n",
      " [ 25  71]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.86      0.90       414\n",
      "           1       0.55      0.74      0.63        96\n",
      "\n",
      "    accuracy                           0.84       510\n",
      "   macro avg       0.74      0.80      0.77       510\n",
      "weighted avg       0.86      0.84      0.85       510\n",
      "\n",
      "acc:  0.8392156862745098\n",
      "pre:  0.5546875\n",
      "rec:  0.7395833333333334\n",
      "ma F1:  0.7654567480258435\n",
      "mi F1:  0.8392156862745097\n",
      "we F1:  0.8474684346100249\n",
      "Loss:  0.04413876682519913\n",
      "Loss:  0.11216495931148529\n",
      "Loss:  0.05285480618476868\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.08284229040145874\n",
      "Loss:  0.11274024844169617\n",
      "Loss:  0.07460340857505798\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.09426150470972061\n",
      "Loss:  0.06570415198802948\n",
      "Loss:  0.05250488966703415\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.0035626888275146484\n",
      "Eval Loss:  0.027718305587768555\n",
      "Eval Loss:  0.10493350028991699\n",
      "[[18815   935]\n",
      " [ 2161 10470]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     19750\n",
      "           1       0.92      0.83      0.87     12631\n",
      "\n",
      "    accuracy                           0.90     32381\n",
      "   macro avg       0.91      0.89      0.90     32381\n",
      "weighted avg       0.91      0.90      0.90     32381\n",
      "\n",
      "acc:  0.9043883758994472\n",
      "pre:  0.9180184129767646\n",
      "rec:  0.8289129918454596\n",
      "ma F1:  0.8975864887047957\n",
      "mi F1:  0.9043883758994471\n",
      "we F1:  0.9033890812678543\n",
      "[[303 111]\n",
      " [ 21  75]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.73      0.82       414\n",
      "           1       0.40      0.78      0.53        96\n",
      "\n",
      "    accuracy                           0.74       510\n",
      "   macro avg       0.67      0.76      0.68       510\n",
      "weighted avg       0.84      0.74      0.77       510\n",
      "\n",
      "acc:  0.7411764705882353\n",
      "pre:  0.4032258064516129\n",
      "rec:  0.78125\n",
      "ma F1:  0.6765265524995675\n",
      "mi F1:  0.7411764705882353\n",
      "we F1:  0.7666961750969199\n",
      "Loss:  0.08740737289190292\n",
      "Loss:  0.10535044968128204\n",
      "Loss:  0.06806132197380066\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.05096697807312012\n",
      "Loss:  0.03430917114019394\n",
      "Loss:  0.08141868561506271\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.037010032683610916\n",
      "Loss:  0.05123932659626007\n",
      "Loss:  0.04739551991224289\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.003565549850463867\n",
      "Eval Loss:  0.01840829849243164\n",
      "Eval Loss:  0.12618672847747803\n",
      "[[18608  1142]\n",
      " [ 1884 10747]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92     19750\n",
      "           1       0.90      0.85      0.88     12631\n",
      "\n",
      "    accuracy                           0.91     32381\n",
      "   macro avg       0.91      0.90      0.90     32381\n",
      "weighted avg       0.91      0.91      0.91     32381\n",
      "\n",
      "acc:  0.9065501374262684\n",
      "pre:  0.90394482294558\n",
      "rec:  0.8508431636450003\n",
      "ma F1:  0.9006977342542544\n",
      "mi F1:  0.9065501374262684\n",
      "we F1:  0.90599772893452\n",
      "[[301 113]\n",
      " [ 18  78]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.73      0.82       414\n",
      "           1       0.41      0.81      0.54        96\n",
      "\n",
      "    accuracy                           0.74       510\n",
      "   macro avg       0.68      0.77      0.68       510\n",
      "weighted avg       0.84      0.74      0.77       510\n",
      "\n",
      "acc:  0.7431372549019608\n",
      "pre:  0.4083769633507853\n",
      "rec:  0.8125\n",
      "ma F1:  0.6824182040300231\n",
      "mi F1:  0.7431372549019608\n",
      "we F1:  0.7690041151388849\n",
      "Loss:  0.03763266280293465\n",
      "Loss:  0.074131079018116\n",
      "Loss:  0.06208379939198494\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.07591890543699265\n",
      "Loss:  0.06074870377779007\n",
      "Loss:  0.037618186324834824\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.05586130544543266\n",
      "Loss:  0.07628923654556274\n",
      "Loss:  0.09033001959323883\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.0029032230377197266\n",
      "Eval Loss:  0.016879677772521973\n",
      "Eval Loss:  0.07130444049835205\n",
      "[[18838   912]\n",
      " [ 2160 10471]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.95      0.92     19750\n",
      "           1       0.92      0.83      0.87     12631\n",
      "\n",
      "    accuracy                           0.91     32381\n",
      "   macro avg       0.91      0.89      0.90     32381\n",
      "weighted avg       0.91      0.91      0.90     32381\n",
      "\n",
      "acc:  0.9051295512800717\n",
      "pre:  0.9198805235878064\n",
      "rec:  0.8289921621407648\n",
      "ma F1:  0.8983422099681733\n",
      "mi F1:  0.9051295512800717\n",
      "we F1:  0.904117169069333\n",
      "[[298 116]\n",
      " [ 18  78]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.72      0.82       414\n",
      "           1       0.40      0.81      0.54        96\n",
      "\n",
      "    accuracy                           0.74       510\n",
      "   macro avg       0.67      0.77      0.68       510\n",
      "weighted avg       0.84      0.74      0.76       510\n",
      "\n",
      "acc:  0.7372549019607844\n",
      "pre:  0.4020618556701031\n",
      "rec:  0.8125\n",
      "ma F1:  0.6771846953235712\n",
      "mi F1:  0.7372549019607844\n",
      "we F1:  0.7640134485537247\n",
      "Loss:  0.07199344038963318\n",
      "Loss:  0.07536230981349945\n",
      "Loss:  0.056047823280096054\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.09393975138664246\n",
      "Loss:  0.06469637155532837\n",
      "Loss:  0.07084410637617111\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.08577702939510345\n",
      "Loss:  0.06527461111545563\n",
      "Loss:  0.06029722839593887\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.002744436264038086\n",
      "Eval Loss:  0.014266729354858398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss:  0.09403693675994873\n",
      "[[18306  1444]\n",
      " [ 1400 11231]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19750\n",
      "           1       0.89      0.89      0.89     12631\n",
      "\n",
      "    accuracy                           0.91     32381\n",
      "   macro avg       0.91      0.91      0.91     32381\n",
      "weighted avg       0.91      0.91      0.91     32381\n",
      "\n",
      "acc:  0.9121707173960039\n",
      "pre:  0.8860749506903353\n",
      "rec:  0.8891615865727179\n",
      "ma F1:  0.90776764663295\n",
      "mi F1:  0.9121707173960039\n",
      "we F1:  0.912198100450926\n",
      "[[236 178]\n",
      " [ 10  86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.57      0.72       414\n",
      "           1       0.33      0.90      0.48        96\n",
      "\n",
      "    accuracy                           0.63       510\n",
      "   macro avg       0.64      0.73      0.60       510\n",
      "weighted avg       0.84      0.63      0.67       510\n",
      "\n",
      "acc:  0.6313725490196078\n",
      "pre:  0.32575757575757575\n",
      "rec:  0.8958333333333334\n",
      "ma F1:  0.5964646464646465\n",
      "mi F1:  0.6313725490196078\n",
      "we F1:  0.6704693998811647\n",
      "Loss:  0.06792528182268143\n",
      "Loss:  0.05581660941243172\n",
      "Loss:  0.06315397471189499\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.0682990774512291\n",
      "Loss:  0.04376466944813728\n",
      "Loss:  0.060750287026166916\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.07365474849939346\n",
      "Loss:  0.0752900242805481\n",
      "Loss:  0.050718698650598526\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.0030608177185058594\n",
      "Eval Loss:  0.01898348331451416\n",
      "Eval Loss:  0.10733067989349365\n",
      "[[18611  1139]\n",
      " [ 1639 10992]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19750\n",
      "           1       0.91      0.87      0.89     12631\n",
      "\n",
      "    accuracy                           0.91     32381\n",
      "   macro avg       0.91      0.91      0.91     32381\n",
      "weighted avg       0.91      0.91      0.91     32381\n",
      "\n",
      "acc:  0.914208949692721\n",
      "pre:  0.9061083175335917\n",
      "rec:  0.8702398859947748\n",
      "ma F1:  0.9091809849769809\n",
      "mi F1:  0.914208949692721\n",
      "we F1:  0.9138789874459865\n",
      "[[261 153]\n",
      " [ 14  82]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.63      0.76       414\n",
      "           1       0.35      0.85      0.50        96\n",
      "\n",
      "    accuracy                           0.67       510\n",
      "   macro avg       0.65      0.74      0.63       510\n",
      "weighted avg       0.84      0.67      0.71       510\n",
      "\n",
      "acc:  0.6725490196078432\n",
      "pre:  0.34893617021276596\n",
      "rec:  0.8541666666666666\n",
      "ma F1:  0.6265440083487168\n",
      "mi F1:  0.6725490196078432\n",
      "we F1:  0.7082735814235893\n",
      "Loss:  0.05689132213592529\n",
      "Loss:  0.056221768260002136\n",
      "Loss:  0.05996687337756157\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.059124164283275604\n",
      "Loss:  0.04155127331614494\n",
      "Loss:  0.06816855072975159\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.05579753592610359\n",
      "Loss:  0.08238808065652847\n",
      "Loss:  0.039774339646101\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.002447843551635742\n",
      "Eval Loss:  0.01187753677368164\n",
      "Eval Loss:  0.07593607902526855\n",
      "[[18446  1304]\n",
      " [ 1428 11203]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19750\n",
      "           1       0.90      0.89      0.89     12631\n",
      "\n",
      "    accuracy                           0.92     32381\n",
      "   macro avg       0.91      0.91      0.91     32381\n",
      "weighted avg       0.92      0.92      0.92     32381\n",
      "\n",
      "acc:  0.9156295358389179\n",
      "pre:  0.895738386503558\n",
      "rec:  0.8869448183041723\n",
      "ma F1:  0.9111859009095554\n",
      "mi F1:  0.9156295358389178\n",
      "we F1:  0.9155534609070883\n",
      "[[258 156]\n",
      " [ 10  86]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.62      0.76       414\n",
      "           1       0.36      0.90      0.51        96\n",
      "\n",
      "    accuracy                           0.67       510\n",
      "   macro avg       0.66      0.76      0.63       510\n",
      "weighted avg       0.85      0.67      0.71       510\n",
      "\n",
      "acc:  0.6745098039215687\n",
      "pre:  0.35537190082644626\n",
      "rec:  0.8958333333333334\n",
      "ma F1:  0.6327369900570893\n",
      "mi F1:  0.6745098039215687\n",
      "we F1:  0.7099681226669987\n",
      "Loss:  0.07967045158147812\n",
      "Loss:  0.08735868334770203\n",
      "Loss:  0.038307420909404755\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.05687969923019409\n",
      "Loss:  0.09554656594991684\n",
      "Loss:  0.08761727809906006\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.05542972683906555\n",
      "Loss:  0.08279484510421753\n",
      "Loss:  0.028040388599038124\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.002374887466430664\n",
      "Eval Loss:  0.010038614273071289\n",
      "Eval Loss:  0.03526902198791504\n",
      "[[18635  1115]\n",
      " [ 1568 11063]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19750\n",
      "           1       0.91      0.88      0.89     12631\n",
      "\n",
      "    accuracy                           0.92     32381\n",
      "   macro avg       0.92      0.91      0.91     32381\n",
      "weighted avg       0.92      0.92      0.92     32381\n",
      "\n",
      "acc:  0.9171427689076928\n",
      "pre:  0.9084414517983248\n",
      "rec:  0.875860976961444\n",
      "ma F1:  0.9123499284540144\n",
      "mi F1:  0.9171427689076928\n",
      "we F1:  0.9168560339994101\n",
      "[[294 120]\n",
      " [ 17  79]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.71      0.81       414\n",
      "           1       0.40      0.82      0.54        96\n",
      "\n",
      "    accuracy                           0.73       510\n",
      "   macro avg       0.67      0.77      0.67       510\n",
      "weighted avg       0.84      0.73      0.76       510\n",
      "\n",
      "acc:  0.7313725490196078\n",
      "pre:  0.3969849246231156\n",
      "rec:  0.8229166666666666\n",
      "ma F1:  0.673313851548802\n",
      "mi F1:  0.7313725490196079\n",
      "we F1:  0.7591867157149242\n",
      "Loss:  0.029560238122940063\n",
      "Loss:  0.03373310714960098\n",
      "Loss:  0.047746676951646805\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.06708952784538269\n",
      "Loss:  0.06014410778880119\n",
      "Loss:  0.04138290882110596\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.060235824435949326\n",
      "Loss:  0.056197524070739746\n",
      "Loss:  0.06745196878910065\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.0027844905853271484\n",
      "Eval Loss:  0.008508920669555664\n",
      "Eval Loss:  0.043372154235839844\n",
      "[[18412  1338]\n",
      " [ 1402 11229]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19750\n",
      "           1       0.89      0.89      0.89     12631\n",
      "\n",
      "    accuracy                           0.92     32381\n",
      "   macro avg       0.91      0.91      0.91     32381\n",
      "weighted avg       0.92      0.92      0.92     32381\n",
      "\n",
      "acc:  0.9153824773787097\n",
      "pre:  0.8935306755788971\n",
      "rec:  0.8890032459821076\n",
      "ma F1:  0.9110031665175815\n",
      "mi F1:  0.9153824773787097\n",
      "we F1:  0.915343458042066\n",
      "[[253 161]\n",
      " [ 13  83]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.61      0.74       414\n",
      "           1       0.34      0.86      0.49        96\n",
      "\n",
      "    accuracy                           0.66       510\n",
      "   macro avg       0.65      0.74      0.62       510\n",
      "weighted avg       0.84      0.66      0.70       510\n",
      "\n",
      "acc:  0.6588235294117647\n",
      "pre:  0.3401639344262295\n",
      "rec:  0.8645833333333334\n",
      "ma F1:  0.6161764705882353\n",
      "mi F1:  0.6588235294117647\n",
      "we F1:  0.6959515570934256\n",
      "Loss:  0.03330342099070549\n",
      "Loss:  0.07789649069309235\n",
      "Loss:  0.0322810634970665\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.058244116604328156\n",
      "Loss:  0.04585912078619003\n",
      "Loss:  0.04200218617916107\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.03007625788450241\n",
      "Loss:  0.02495897188782692\n",
      "Loss:  0.05223026126623154\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.0026595592498779297\n",
      "Eval Loss:  0.010037422180175781\n",
      "Eval Loss:  0.01975405216217041\n",
      "[[18490  1260]\n",
      " [ 1456 11175]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     19750\n",
      "           1       0.90      0.88      0.89     12631\n",
      "\n",
      "    accuracy                           0.92     32381\n",
      "   macro avg       0.91      0.91      0.91     32381\n",
      "weighted avg       0.92      0.92      0.92     32381\n",
      "\n",
      "acc:  0.9161236527593342\n",
      "pre:  0.8986731001206273\n",
      "rec:  0.8847280500356266\n",
      "ma F1:  0.9116130312388033\n",
      "mi F1:  0.9161236527593342\n",
      "we F1:  0.9160027940008894\n",
      "[[279 135]\n",
      " [ 14  82]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.67      0.79       414\n",
      "           1       0.38      0.85      0.52        96\n",
      "\n",
      "    accuracy                           0.71       510\n",
      "   macro avg       0.67      0.76      0.66       510\n",
      "weighted avg       0.84      0.71      0.74       510\n",
      "\n",
      "acc:  0.707843137254902\n",
      "pre:  0.3778801843317972\n",
      "rec:  0.8541666666666666\n",
      "ma F1:  0.6566060074743211\n",
      "mi F1:  0.7078431372549021\n",
      "we F1:  0.7393136585922129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.1050202026963234\n",
      "Loss:  0.04668653383851051\n",
      "Loss:  0.057646870613098145\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.06781325489282608\n",
      "Loss:  0.044885989278554916\n",
      "Loss:  0.02968280389904976\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.059539325535297394\n",
      "Loss:  0.06200185418128967\n",
      "Loss:  0.040530286729335785\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.002240419387817383\n",
      "Eval Loss:  0.008258819580078125\n",
      "Eval Loss:  0.03101050853729248\n",
      "[[18860   890]\n",
      " [ 1801 10830]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19750\n",
      "           1       0.92      0.86      0.89     12631\n",
      "\n",
      "    accuracy                           0.92     32381\n",
      "   macro avg       0.92      0.91      0.91     32381\n",
      "weighted avg       0.92      0.92      0.92     32381\n",
      "\n",
      "acc:  0.9168957104474846\n",
      "pre:  0.924061433447099\n",
      "rec:  0.8574142981553321\n",
      "ma F1:  0.9114502057943275\n",
      "mi F1:  0.9168957104474846\n",
      "we F1:  0.9162779203180916\n",
      "[[305 109]\n",
      " [ 20  76]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.74      0.83       414\n",
      "           1       0.41      0.79      0.54        96\n",
      "\n",
      "    accuracy                           0.75       510\n",
      "   macro avg       0.67      0.76      0.68       510\n",
      "weighted avg       0.84      0.75      0.77       510\n",
      "\n",
      "acc:  0.7470588235294118\n",
      "pre:  0.41081081081081083\n",
      "rec:  0.7916666666666666\n",
      "ma F1:  0.6831825251975594\n",
      "mi F1:  0.7470588235294117\n",
      "we F1:  0.7718841097806556\n",
      "Loss:  0.03311634808778763\n",
      "Loss:  0.05783117190003395\n",
      "Loss:  0.05055347457528114\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.0606384240090847\n",
      "Loss:  0.05523740500211716\n",
      "Loss:  0.06482001394033432\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.056439485400915146\n",
      "Loss:  0.08229056745767593\n",
      "Loss:  0.07138609886169434\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.0022115707397460938\n",
      "Eval Loss:  0.009261846542358398\n",
      "Eval Loss:  0.02121877670288086\n",
      "[[18336  1414]\n",
      " [ 1284 11347]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19750\n",
      "           1       0.89      0.90      0.89     12631\n",
      "\n",
      "    accuracy                           0.92     32381\n",
      "   macro avg       0.91      0.91      0.91     32381\n",
      "weighted avg       0.92      0.92      0.92     32381\n",
      "\n",
      "acc:  0.9166795342948025\n",
      "pre:  0.8891936368623149\n",
      "rec:  0.8983453408281213\n",
      "ma F1:  0.9126083623465293\n",
      "mi F1:  0.9166795342948025\n",
      "we F1:  0.9167552607725927\n",
      "[[230 184]\n",
      " [ 11  85]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.56      0.70       414\n",
      "           1       0.32      0.89      0.47        96\n",
      "\n",
      "    accuracy                           0.62       510\n",
      "   macro avg       0.64      0.72      0.58       510\n",
      "weighted avg       0.83      0.62      0.66       510\n",
      "\n",
      "acc:  0.6176470588235294\n",
      "pre:  0.3159851301115242\n",
      "rec:  0.8854166666666666\n",
      "ma F1:  0.5840217504967061\n",
      "mi F1:  0.6176470588235294\n",
      "we F1:  0.6577655301376014\n",
      "Loss:  0.04963903874158859\n",
      "Loss:  0.07617735862731934\n",
      "Loss:  0.05589335784316063\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.06498584151268005\n",
      "Loss:  0.056798506528139114\n",
      "Loss:  0.026881519705057144\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.0393594354391098\n",
      "Loss:  0.08000168949365616\n",
      "Loss:  0.04906245321035385\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.0020530223846435547\n",
      "Eval Loss:  0.008030891418457031\n",
      "Eval Loss:  0.02077949047088623\n",
      "[[18767   983]\n",
      " [ 1566 11065]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94     19750\n",
      "           1       0.92      0.88      0.90     12631\n",
      "\n",
      "    accuracy                           0.92     32381\n",
      "   macro avg       0.92      0.91      0.92     32381\n",
      "weighted avg       0.92      0.92      0.92     32381\n",
      "\n",
      "acc:  0.9212809981161793\n",
      "pre:  0.9184096945551129\n",
      "rec:  0.8760193175520544\n",
      "ma F1:  0.9165603804133653\n",
      "mi F1:  0.9212809981161793\n",
      "we F1:  0.920923672730469\n",
      "[[283 131]\n",
      " [ 16  80]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.68      0.79       414\n",
      "           1       0.38      0.83      0.52        96\n",
      "\n",
      "    accuracy                           0.71       510\n",
      "   macro avg       0.66      0.76      0.66       510\n",
      "weighted avg       0.84      0.71      0.74       510\n",
      "\n",
      "acc:  0.711764705882353\n",
      "pre:  0.3791469194312796\n",
      "rec:  0.8333333333333334\n",
      "ma F1:  0.6575007652210461\n",
      "mi F1:  0.711764705882353\n",
      "we F1:  0.7425053619220097\n",
      "Loss:  0.08413606137037277\n",
      "Loss:  0.05959364026784897\n",
      "Loss:  0.07117860019207001\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.0822858214378357\n",
      "Loss:  0.05217861756682396\n",
      "Loss:  0.07187512516975403\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.0595313236117363\n",
      "Loss:  0.032171349972486496\n",
      "Loss:  0.06143293157219887\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  29 Training Time 5858.1225707530975 Best Test Acc:  0.8745098039215686\n",
      "test subjects:  ['./seg\\\\x13', './seg\\\\x26']\n",
      "*********\n",
      "33287 1026\n",
      "31865 1026\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.8185309171676636\n",
      "Eval Loss:  0.6071194410324097\n",
      "Eval Loss:  0.8312610387802124\n",
      "[[    0 19774]\n",
      " [    0 12091]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     19774\n",
      "           1       0.38      1.00      0.55     12091\n",
      "\n",
      "    accuracy                           0.38     31865\n",
      "   macro avg       0.19      0.50      0.28     31865\n",
      "weighted avg       0.14      0.38      0.21     31865\n",
      "\n",
      "acc:  0.3794445316177624\n",
      "pre:  0.3794445316177624\n",
      "rec:  1.0\n",
      "ma F1:  0.2750705250705251\n",
      "mi F1:  0.37944453161776237\n",
      "we F1:  0.20874801309447474\n",
      "[[  0 390]\n",
      " [  0 636]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       390\n",
      "           1       0.62      1.00      0.77       636\n",
      "\n",
      "    accuracy                           0.62      1026\n",
      "   macro avg       0.31      0.50      0.38      1026\n",
      "weighted avg       0.38      0.62      0.47      1026\n",
      "\n",
      "acc:  0.6198830409356725\n",
      "pre:  0.6198830409356725\n",
      "rec:  1.0\n",
      "ma F1:  0.38267148014440433\n",
      "mi F1:  0.6198830409356725\n",
      "we F1:  0.47442312158253636\n",
      "Subject 30 Current Train Acc:  0.3794445316177624 Current Test Acc:  0.6198830409356725\n",
      "Loss:  0.17396502196788788\n",
      "Loss:  0.16897550225257874\n",
      "Loss:  0.16578826308250427\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.123685322701931\n",
      "Loss:  0.15419083833694458\n",
      "Loss:  0.13299301266670227\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.1085534617304802\n",
      "Loss:  0.11372987180948257\n",
      "Loss:  0.11060023307800293\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.03944599628448486\n",
      "Eval Loss:  2.9462404251098633\n",
      "Eval Loss:  0.034629106521606445\n",
      "[[17239  2535]\n",
      " [ 3668  8423]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.87      0.85     19774\n",
      "           1       0.77      0.70      0.73     12091\n",
      "\n",
      "    accuracy                           0.81     31865\n",
      "   macro avg       0.80      0.78      0.79     31865\n",
      "weighted avg       0.80      0.81      0.80     31865\n",
      "\n",
      "acc:  0.8053350070610388\n",
      "pre:  0.7686621646285818\n",
      "rec:  0.6966338598957903\n",
      "ma F1:  0.7891993255367095\n",
      "mi F1:  0.8053350070610388\n",
      "we F1:  0.8032613084259361\n",
      "[[329  61]\n",
      " [294 342]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.84      0.65       390\n",
      "           1       0.85      0.54      0.66       636\n",
      "\n",
      "    accuracy                           0.65      1026\n",
      "   macro avg       0.69      0.69      0.65      1026\n",
      "weighted avg       0.73      0.65      0.65      1026\n",
      "\n",
      "acc:  0.6539961013645225\n",
      "pre:  0.8486352357320099\n",
      "rec:  0.5377358490566038\n",
      "ma F1:  0.6539405438633662\n",
      "mi F1:  0.6539961013645225\n",
      "we F1:  0.6549918627313988\n",
      "Subject 30 Current Train Acc:  0.8053350070610388 Current Test Acc:  0.6539961013645225\n",
      "Loss:  0.11295276880264282\n",
      "Loss:  0.10391645133495331\n",
      "Loss:  0.06925854831933975\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.09674329310655594\n",
      "Loss:  0.10727912187576294\n",
      "Loss:  0.10206790268421173\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.12236646562814713\n",
      "Loss:  0.08728325366973877\n",
      "Loss:  0.07673589140176773\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.03765881061553955\n",
      "Eval Loss:  3.3785324096679688\n",
      "Eval Loss:  0.021198272705078125\n",
      "[[18283  1491]\n",
      " [ 3009  9082]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89     19774\n",
      "           1       0.86      0.75      0.80     12091\n",
      "\n",
      "    accuracy                           0.86     31865\n",
      "   macro avg       0.86      0.84      0.85     31865\n",
      "weighted avg       0.86      0.86      0.86     31865\n",
      "\n",
      "acc:  0.8587792248548565\n",
      "pre:  0.8589804218291875\n",
      "rec:  0.7511372094946654\n",
      "ma F1:  0.8459337640583016\n",
      "mi F1:  0.8587792248548565\n",
      "we F1:  0.8566599541789331\n",
      "[[362  28]\n",
      " [486 150]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.93      0.58       390\n",
      "           1       0.84      0.24      0.37       636\n",
      "\n",
      "    accuracy                           0.50      1026\n",
      "   macro avg       0.63      0.58      0.48      1026\n",
      "weighted avg       0.68      0.50      0.45      1026\n",
      "\n",
      "acc:  0.49902534113060426\n",
      "pre:  0.8426966292134831\n",
      "rec:  0.2358490566037736\n",
      "ma F1:  0.4766822925142796\n",
      "mi F1:  0.49902534113060426\n",
      "we F1:  0.4507559247802424\n",
      "Loss:  0.08543983846902847\n",
      "Loss:  0.11389587074518204\n",
      "Loss:  0.0958312526345253\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.06923388689756393\n",
      "Loss:  0.08869292587041855\n",
      "Loss:  0.0852450430393219\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.0735316127538681\n",
      "Loss:  0.046577826142311096\n",
      "Loss:  0.06492279469966888\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.025739550590515137\n",
      "Eval Loss:  3.4937539100646973\n",
      "Eval Loss:  0.013632535934448242\n",
      "[[18878   896]\n",
      " [ 3305  8786]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.95      0.90     19774\n",
      "           1       0.91      0.73      0.81     12091\n",
      "\n",
      "    accuracy                           0.87     31865\n",
      "   macro avg       0.88      0.84      0.85     31865\n",
      "weighted avg       0.87      0.87      0.86     31865\n",
      "\n",
      "acc:  0.8681625608033893\n",
      "pre:  0.9074571369551746\n",
      "rec:  0.7266561905549582\n",
      "ma F1:  0.8534641445578476\n",
      "mi F1:  0.8681625608033893\n",
      "we F1:  0.8646539911704612\n",
      "[[373  17]\n",
      " [531 105]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.96      0.58       390\n",
      "           1       0.86      0.17      0.28       636\n",
      "\n",
      "    accuracy                           0.47      1026\n",
      "   macro avg       0.64      0.56      0.43      1026\n",
      "weighted avg       0.69      0.47      0.39      1026\n",
      "\n",
      "acc:  0.46588693957115007\n",
      "pre:  0.860655737704918\n",
      "rec:  0.1650943396226415\n",
      "ma F1:  0.42677590502950496\n",
      "mi F1:  0.46588693957115007\n",
      "we F1:  0.3908754778009799\n",
      "Loss:  0.08143704384565353\n",
      "Loss:  0.07379912585020065\n",
      "Loss:  0.10911062359809875\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.06545877456665039\n",
      "Loss:  0.044163599610328674\n",
      "Loss:  0.049284812062978745\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.0675453171133995\n",
      "Loss:  0.044076453894376755\n",
      "Loss:  0.10127362608909607\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.03020167350769043\n",
      "Eval Loss:  2.762263298034668\n",
      "Eval Loss:  0.012195825576782227\n",
      "[[18719  1055]\n",
      " [ 2611  9480]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91     19774\n",
      "           1       0.90      0.78      0.84     12091\n",
      "\n",
      "    accuracy                           0.88     31865\n",
      "   macro avg       0.89      0.87      0.87     31865\n",
      "weighted avg       0.89      0.88      0.88     31865\n",
      "\n",
      "acc:  0.8849521418484231\n",
      "pre:  0.8998576174655909\n",
      "rec:  0.7840542552311637\n",
      "ma F1:  0.8743928060213266\n",
      "mi F1:  0.8849521418484231\n",
      "we F1:  0.8831737755158154\n",
      "[[370  20]\n",
      " [486 150]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.95      0.59       390\n",
      "           1       0.88      0.24      0.37       636\n",
      "\n",
      "    accuracy                           0.51      1026\n",
      "   macro avg       0.66      0.59      0.48      1026\n",
      "weighted avg       0.71      0.51      0.46      1026\n",
      "\n",
      "acc:  0.50682261208577\n",
      "pre:  0.8823529411764706\n",
      "rec:  0.2358490566037736\n",
      "ma F1:  0.48305445913274836\n",
      "mi F1:  0.50682261208577\n",
      "we F1:  0.4564773426489151\n",
      "Loss:  0.08082140237092972\n",
      "Loss:  0.08200976252555847\n",
      "Loss:  0.0528704859316349\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.07432400435209274\n",
      "Loss:  0.0829695612192154\n",
      "Loss:  0.05646554008126259\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.07664524018764496\n",
      "Loss:  0.043089017271995544\n",
      "Loss:  0.07070620357990265\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.03657877445220947\n",
      "Eval Loss:  3.3116469383239746\n",
      "Eval Loss:  0.007643461227416992\n",
      "[[18767  1007]\n",
      " [ 2476  9615]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.92     19774\n",
      "           1       0.91      0.80      0.85     12091\n",
      "\n",
      "    accuracy                           0.89     31865\n",
      "   macro avg       0.89      0.87      0.88     31865\n",
      "weighted avg       0.89      0.89      0.89     31865\n",
      "\n",
      "acc:  0.8906951200376588\n",
      "pre:  0.9051967614385238\n",
      "rec:  0.7952195848151518\n",
      "ma F1:  0.8808678434155925\n",
      "mi F1:  0.8906951200376588\n",
      "we F1:  0.8891177304662193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[370  20]\n",
      " [482 154]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.95      0.60       390\n",
      "           1       0.89      0.24      0.38       636\n",
      "\n",
      "    accuracy                           0.51      1026\n",
      "   macro avg       0.66      0.60      0.49      1026\n",
      "weighted avg       0.71      0.51      0.46      1026\n",
      "\n",
      "acc:  0.5107212475633528\n",
      "pre:  0.8850574712643678\n",
      "rec:  0.24213836477987422\n",
      "ma F1:  0.48803005904455177\n",
      "mi F1:  0.5107212475633528\n",
      "we F1:  0.4621873165648062\n",
      "Loss:  0.07069321721792221\n",
      "Loss:  0.08394157141447067\n",
      "Loss:  0.05434253811836243\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.03954802826046944\n",
      "Loss:  0.0688156709074974\n",
      "Loss:  0.1084485650062561\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.06374980509281158\n",
      "Loss:  0.056130167096853256\n",
      "Loss:  0.05356356129050255\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.14136803150177002\n",
      "Eval Loss:  3.082228183746338\n",
      "Eval Loss:  0.007359981536865234\n",
      "[[18307  1467]\n",
      " [ 1700 10391]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92     19774\n",
      "           1       0.88      0.86      0.87     12091\n",
      "\n",
      "    accuracy                           0.90     31865\n",
      "   macro avg       0.90      0.89      0.89     31865\n",
      "weighted avg       0.90      0.90      0.90     31865\n",
      "\n",
      "acc:  0.9006119566922957\n",
      "pre:  0.8762860516107269\n",
      "rec:  0.8593995533868166\n",
      "ma F1:  0.8940748942772423\n",
      "mi F1:  0.9006119566922957\n",
      "we F1:  0.9004195444206045\n",
      "[[367  23]\n",
      " [458 178]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.94      0.60       390\n",
      "           1       0.89      0.28      0.43       636\n",
      "\n",
      "    accuracy                           0.53      1026\n",
      "   macro avg       0.67      0.61      0.51      1026\n",
      "weighted avg       0.72      0.53      0.49      1026\n",
      "\n",
      "acc:  0.5311890838206628\n",
      "pre:  0.8855721393034826\n",
      "rec:  0.279874213836478\n",
      "ma F1:  0.5147218903491304\n",
      "mi F1:  0.5311890838206628\n",
      "we F1:  0.49328840043380273\n",
      "Loss:  0.07358159869909286\n",
      "Loss:  0.07395074516534805\n",
      "Loss:  0.07746592909097672\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.06236676126718521\n",
      "Loss:  0.05678830295801163\n",
      "Loss:  0.0734306126832962\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.0897548720240593\n",
      "Loss:  0.08374417573213577\n",
      "Loss:  0.10719697177410126\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.2571995258331299\n",
      "Eval Loss:  4.286926746368408\n",
      "Eval Loss:  0.00849604606628418\n",
      "[[18569  1205]\n",
      " [ 1856 10235]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92     19774\n",
      "           1       0.89      0.85      0.87     12091\n",
      "\n",
      "    accuracy                           0.90     31865\n",
      "   macro avg       0.90      0.89      0.90     31865\n",
      "weighted avg       0.90      0.90      0.90     31865\n",
      "\n",
      "acc:  0.9039384905068256\n",
      "pre:  0.8946678321678322\n",
      "rec:  0.8464973947564304\n",
      "ma F1:  0.8968850539303441\n",
      "mi F1:  0.9039384905068256\n",
      "we F1:  0.9033875201191021\n",
      "[[366  24]\n",
      " [449 187]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.94      0.61       390\n",
      "           1       0.89      0.29      0.44       636\n",
      "\n",
      "    accuracy                           0.54      1026\n",
      "   macro avg       0.67      0.62      0.52      1026\n",
      "weighted avg       0.72      0.54      0.50      1026\n",
      "\n",
      "acc:  0.5389863547758285\n",
      "pre:  0.8862559241706162\n",
      "rec:  0.2940251572327044\n",
      "ma F1:  0.5245136606132457\n",
      "mi F1:  0.5389863547758285\n",
      "we F1:  0.5046238127696961\n",
      "Loss:  0.060929834842681885\n",
      "Loss:  0.04493355751037598\n",
      "Loss:  0.06696171313524246\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.07218415290117264\n",
      "Loss:  0.050951842218637466\n",
      "Loss:  0.07411112636327744\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.05954334884881973\n",
      "Loss:  0.07110833376646042\n",
      "Loss:  0.09663406759500504\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.2711068391799927\n",
      "Eval Loss:  4.774730205535889\n",
      "Eval Loss:  0.007888555526733398\n",
      "[[18795   979]\n",
      " [ 2352  9739]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     19774\n",
      "           1       0.91      0.81      0.85     12091\n",
      "\n",
      "    accuracy                           0.90     31865\n",
      "   macro avg       0.90      0.88      0.89     31865\n",
      "weighted avg       0.90      0.90      0.89     31865\n",
      "\n",
      "acc:  0.8954652439981171\n",
      "pre:  0.9086583317783169\n",
      "rec:  0.8054751468034075\n",
      "ma F1:  0.8862802039509239\n",
      "mi F1:  0.8954652439981171\n",
      "we F1:  0.8940726799538595\n",
      "[[379  11]\n",
      " [494 142]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.97      0.60       390\n",
      "           1       0.93      0.22      0.36       636\n",
      "\n",
      "    accuracy                           0.51      1026\n",
      "   macro avg       0.68      0.60      0.48      1026\n",
      "weighted avg       0.74      0.51      0.45      1026\n",
      "\n",
      "acc:  0.5077972709551657\n",
      "pre:  0.9281045751633987\n",
      "rec:  0.22327044025157233\n",
      "ma F1:  0.4800538280212784\n",
      "mi F1:  0.5077972709551657\n",
      "we F1:  0.45125683662154714\n",
      "Loss:  0.05208594724535942\n",
      "Loss:  0.04806267470121384\n",
      "Loss:  0.0571993887424469\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.10377729684114456\n",
      "Loss:  0.06381362676620483\n",
      "Loss:  0.07015273720026016\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.06234315037727356\n",
      "Loss:  0.08128368854522705\n",
      "Loss:  0.04906148463487625\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.25778019428253174\n",
      "Eval Loss:  4.497611999511719\n",
      "Eval Loss:  0.007957696914672852\n",
      "[[18473  1301]\n",
      " [ 1592 10499]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93     19774\n",
      "           1       0.89      0.87      0.88     12091\n",
      "\n",
      "    accuracy                           0.91     31865\n",
      "   macro avg       0.91      0.90      0.90     31865\n",
      "weighted avg       0.91      0.91      0.91     31865\n",
      "\n",
      "acc:  0.9092107327789111\n",
      "pre:  0.8897457627118645\n",
      "rec:  0.8683318170540071\n",
      "ma F1:  0.9031455454842551\n",
      "mi F1:  0.9092107327789111\n",
      "we F1:  0.9089893922342979\n",
      "[[357  33]\n",
      " [365 271]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.92      0.64       390\n",
      "           1       0.89      0.43      0.58       636\n",
      "\n",
      "    accuracy                           0.61      1026\n",
      "   macro avg       0.69      0.67      0.61      1026\n",
      "weighted avg       0.74      0.61      0.60      1026\n",
      "\n",
      "acc:  0.6120857699805068\n",
      "pre:  0.8914473684210527\n",
      "rec:  0.4261006289308176\n",
      "ma F1:  0.6093410378080515\n",
      "mi F1:  0.6120857699805068\n",
      "we F1:  0.6014898271752139\n",
      "Loss:  0.05524131655693054\n",
      "Loss:  0.0789743959903717\n",
      "Loss:  0.033610135316848755\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.056364379823207855\n",
      "Loss:  0.04512230306863785\n",
      "Loss:  0.039490289986133575\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.05057700350880623\n",
      "Loss:  0.05049481242895126\n",
      "Loss:  0.051922380924224854\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.14985787868499756\n",
      "Eval Loss:  4.92832088470459\n",
      "Eval Loss:  0.007166385650634766\n",
      "[[18992   782]\n",
      " [ 2356  9735]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.92     19774\n",
      "           1       0.93      0.81      0.86     12091\n",
      "\n",
      "    accuracy                           0.90     31865\n",
      "   macro avg       0.91      0.88      0.89     31865\n",
      "weighted avg       0.90      0.90      0.90     31865\n",
      "\n",
      "acc:  0.9015220461321198\n",
      "pre:  0.9256441951126747\n",
      "rec:  0.8051443222231411\n",
      "ma F1:  0.8924450286759991\n",
      "mi F1:  0.9015220461321198\n",
      "we F1:  0.8999786491918655\n",
      "[[384   6]\n",
      " [519 117]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.98      0.59       390\n",
      "           1       0.95      0.18      0.31       636\n",
      "\n",
      "    accuracy                           0.49      1026\n",
      "   macro avg       0.69      0.58      0.45      1026\n",
      "weighted avg       0.75      0.49      0.42      1026\n",
      "\n",
      "acc:  0.48830409356725146\n",
      "pre:  0.9512195121951219\n",
      "rec:  0.18396226415094338\n",
      "ma F1:  0.45113395632915454\n",
      "mi F1:  0.48830409356725146\n",
      "we F1:  0.41688731303113263\n",
      "Loss:  0.0929633304476738\n",
      "Loss:  0.07876653969287872\n",
      "Loss:  0.09443435072898865\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.05853812023997307\n",
      "Loss:  0.059417955577373505\n",
      "Loss:  0.05030546337366104\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.02817194163799286\n",
      "Loss:  0.06124638393521309\n",
      "Loss:  0.07858944684267044\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.4920206069946289\n",
      "Eval Loss:  4.160179138183594\n",
      "Eval Loss:  0.009847402572631836\n",
      "[[18738  1036]\n",
      " [ 1800 10291]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     19774\n",
      "           1       0.91      0.85      0.88     12091\n",
      "\n",
      "    accuracy                           0.91     31865\n",
      "   macro avg       0.91      0.90      0.90     31865\n",
      "weighted avg       0.91      0.91      0.91     31865\n",
      "\n",
      "acc:  0.9109995292640829\n",
      "pre:  0.9085371236867661\n",
      "rec:  0.8511289388801588\n",
      "ma F1:  0.9042726575566504\n",
      "mi F1:  0.9109995292640829\n",
      "we F1:  0.910391108524829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[375  15]\n",
      " [455 181]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.96      0.61       390\n",
      "           1       0.92      0.28      0.44       636\n",
      "\n",
      "    accuracy                           0.54      1026\n",
      "   macro avg       0.69      0.62      0.52      1026\n",
      "weighted avg       0.74      0.54      0.50      1026\n",
      "\n",
      "acc:  0.5419103313840156\n",
      "pre:  0.923469387755102\n",
      "rec:  0.28459119496855345\n",
      "ma F1:  0.5249251261034048\n",
      "mi F1:  0.5419103313840156\n",
      "we F1:  0.503387185386754\n",
      "Loss:  0.034779734909534454\n",
      "Loss:  0.08828603476285934\n",
      "Loss:  0.053022075444459915\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.05726849287748337\n",
      "Loss:  0.05884840339422226\n",
      "Loss:  0.045499932020902634\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.04250460863113403\n",
      "Loss:  0.07040916383266449\n",
      "Loss:  0.08922471106052399\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.041355013847351074\n",
      "Eval Loss:  5.043421745300293\n",
      "Eval Loss:  0.0045354366302490234\n",
      "[[19292   482]\n",
      " [ 2741  9350]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.98      0.92     19774\n",
      "           1       0.95      0.77      0.85     12091\n",
      "\n",
      "    accuracy                           0.90     31865\n",
      "   macro avg       0.91      0.87      0.89     31865\n",
      "weighted avg       0.90      0.90      0.90     31865\n",
      "\n",
      "acc:  0.8988545426016005\n",
      "pre:  0.9509764035801465\n",
      "rec:  0.7733024563725085\n",
      "ma F1:  0.8879465480582522\n",
      "mi F1:  0.8988545426016004\n",
      "we F1:  0.8963760513852029\n",
      "[[380  10]\n",
      " [499 137]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.97      0.60       390\n",
      "           1       0.93      0.22      0.35       636\n",
      "\n",
      "    accuracy                           0.50      1026\n",
      "   macro avg       0.68      0.59      0.47      1026\n",
      "weighted avg       0.74      0.50      0.44      1026\n",
      "\n",
      "acc:  0.5038986354775828\n",
      "pre:  0.9319727891156463\n",
      "rec:  0.21540880503144655\n",
      "ma F1:  0.4744164560745632\n",
      "mi F1:  0.5038986354775828\n",
      "we F1:  0.44457029914804946\n",
      "Loss:  0.06026139482855797\n",
      "Loss:  0.09441585838794708\n",
      "Loss:  0.08533142507076263\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.033260636031627655\n",
      "Loss:  0.057413458824157715\n",
      "Loss:  0.042117178440093994\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.061740145087242126\n",
      "Loss:  0.03695182874798775\n",
      "Loss:  0.045986197888851166\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.2261071801185608\n",
      "Eval Loss:  4.440895080566406\n",
      "Eval Loss:  0.005591630935668945\n",
      "[[18781   993]\n",
      " [ 1620 10471]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     19774\n",
      "           1       0.91      0.87      0.89     12091\n",
      "\n",
      "    accuracy                           0.92     31865\n",
      "   macro avg       0.92      0.91      0.91     31865\n",
      "weighted avg       0.92      0.92      0.92     31865\n",
      "\n",
      "acc:  0.9179978032323867\n",
      "pre:  0.9133810188415911\n",
      "rec:  0.8660160449921429\n",
      "ma F1:  0.9120138451798324\n",
      "mi F1:  0.9179978032323867\n",
      "we F1:  0.9175463060363636\n",
      "[[364  26]\n",
      " [428 208]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.93      0.62       390\n",
      "           1       0.89      0.33      0.48       636\n",
      "\n",
      "    accuracy                           0.56      1026\n",
      "   macro avg       0.67      0.63      0.55      1026\n",
      "weighted avg       0.73      0.56      0.53      1026\n",
      "\n",
      "acc:  0.557504873294347\n",
      "pre:  0.8888888888888888\n",
      "rec:  0.3270440251572327\n",
      "ma F1:  0.5470330824435499\n",
      "mi F1:  0.557504873294347\n",
      "we F1:  0.5305198737942157\n",
      "Loss:  0.03953985869884491\n",
      "Loss:  0.05914488434791565\n",
      "Loss:  0.056884270161390305\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.04617340862751007\n",
      "Loss:  0.0411117821931839\n",
      "Loss:  0.07868944853544235\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.0516008660197258\n",
      "Loss:  0.034781262278556824\n",
      "Loss:  0.02580077387392521\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.1504908800125122\n",
      "Eval Loss:  4.317646026611328\n",
      "Eval Loss:  0.0065081119537353516\n",
      "[[18830   944]\n",
      " [ 1632 10459]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94     19774\n",
      "           1       0.92      0.87      0.89     12091\n",
      "\n",
      "    accuracy                           0.92     31865\n",
      "   macro avg       0.92      0.91      0.91     31865\n",
      "weighted avg       0.92      0.92      0.92     31865\n",
      "\n",
      "acc:  0.9191589518280244\n",
      "pre:  0.9172147680434973\n",
      "rec:  0.865023571251344\n",
      "ma F1:  0.9131663578180639\n",
      "mi F1:  0.9191589518280244\n",
      "we F1:  0.9186664294676312\n",
      "[[364  26]\n",
      " [417 219]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.93      0.62       390\n",
      "           1       0.89      0.34      0.50       636\n",
      "\n",
      "    accuracy                           0.57      1026\n",
      "   macro avg       0.68      0.64      0.56      1026\n",
      "weighted avg       0.73      0.57      0.54      1026\n",
      "\n",
      "acc:  0.5682261208576999\n",
      "pre:  0.8938775510204081\n",
      "rec:  0.3443396226415094\n",
      "ma F1:  0.5594265890305927\n",
      "mi F1:  0.5682261208576999\n",
      "we F1:  0.5444977281377077\n",
      "Loss:  0.04569438472390175\n",
      "Loss:  0.06119253858923912\n",
      "Loss:  0.05592188239097595\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.06393678486347198\n",
      "Loss:  0.03674305975437164\n",
      "Loss:  0.06322144716978073\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.05532528832554817\n",
      "Loss:  0.06278279423713684\n",
      "Loss:  0.05211365595459938\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.11934804916381836\n",
      "Eval Loss:  4.0862321853637695\n",
      "Eval Loss:  0.0042743682861328125\n",
      "[[18989   785]\n",
      " [ 2000 10091]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93     19774\n",
      "           1       0.93      0.83      0.88     12091\n",
      "\n",
      "    accuracy                           0.91     31865\n",
      "   macro avg       0.92      0.90      0.91     31865\n",
      "weighted avg       0.91      0.91      0.91     31865\n",
      "\n",
      "acc:  0.9126000313823944\n",
      "pre:  0.9278227289444648\n",
      "rec:  0.834587709866843\n",
      "ma F1:  0.9052086490042215\n",
      "mi F1:  0.9126000313823944\n",
      "we F1:  0.9115907563105266\n",
      "[[380  10]\n",
      " [498 138]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.97      0.60       390\n",
      "           1       0.93      0.22      0.35       636\n",
      "\n",
      "    accuracy                           0.50      1026\n",
      "   macro avg       0.68      0.60      0.48      1026\n",
      "weighted avg       0.74      0.50      0.45      1026\n",
      "\n",
      "acc:  0.5048732943469786\n",
      "pre:  0.9324324324324325\n",
      "rec:  0.2169811320754717\n",
      "ma F1:  0.4757049507500161\n",
      "mi F1:  0.5048732943469786\n",
      "we F1:  0.44605448577128565\n",
      "Loss:  0.07160826027393341\n",
      "Loss:  0.052050016820430756\n",
      "Loss:  0.05963058024644852\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.04909549653530121\n",
      "Loss:  0.05054832622408867\n",
      "Loss:  0.03402918949723244\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.03881841152906418\n",
      "Loss:  0.07280489802360535\n",
      "Loss:  0.06312179565429688\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.3547810912132263\n",
      "Eval Loss:  4.494393348693848\n",
      "Eval Loss:  0.003785371780395508\n",
      "[[18775   999]\n",
      " [ 1600 10491]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94     19774\n",
      "           1       0.91      0.87      0.89     12091\n",
      "\n",
      "    accuracy                           0.92     31865\n",
      "   macro avg       0.92      0.91      0.91     31865\n",
      "weighted avg       0.92      0.92      0.92     31865\n",
      "\n",
      "acc:  0.9184371567550604\n",
      "pre:  0.9130548302872062\n",
      "rec:  0.8676701678934745\n",
      "ma F1:  0.9125251409535484\n",
      "mi F1:  0.9184371567550604\n",
      "we F1:  0.9180082430060613\n",
      "[[383   7]\n",
      " [506 130]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.98      0.60       390\n",
      "           1       0.95      0.20      0.34       636\n",
      "\n",
      "    accuracy                           0.50      1026\n",
      "   macro avg       0.69      0.59      0.47      1026\n",
      "weighted avg       0.75      0.50      0.44      1026\n",
      "\n",
      "acc:  0.5\n",
      "pre:  0.948905109489051\n",
      "rec:  0.20440251572327045\n",
      "ma F1:  0.46762863532412835\n",
      "mi F1:  0.5\n",
      "we F1:  0.4361529210543085\n",
      "Loss:  0.062475405633449554\n",
      "Loss:  0.07033400982618332\n",
      "Loss:  0.08131475001573563\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.0340849868953228\n",
      "Loss:  0.036687519401311874\n",
      "Loss:  0.06563066691160202\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.03036462515592575\n",
      "Loss:  0.05039573088288307\n",
      "Loss:  0.05600437894463539\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.6751269102096558\n",
      "Eval Loss:  4.177532196044922\n",
      "Eval Loss:  0.003528118133544922\n",
      "[[18605  1169]\n",
      " [ 1483 10608]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     19774\n",
      "           1       0.90      0.88      0.89     12091\n",
      "\n",
      "    accuracy                           0.92     31865\n",
      "   macro avg       0.91      0.91      0.91     31865\n",
      "weighted avg       0.92      0.92      0.92     31865\n",
      "\n",
      "acc:  0.9167738898477954\n",
      "pre:  0.9007387280292095\n",
      "rec:  0.8773467868662642\n",
      "ma F1:  0.9111796810105977\n",
      "mi F1:  0.9167738898477954\n",
      "we F1:  0.916554234780285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[384   6]\n",
      " [514 122]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.98      0.60       390\n",
      "           1       0.95      0.19      0.32       636\n",
      "\n",
      "    accuracy                           0.49      1026\n",
      "   macro avg       0.69      0.59      0.46      1026\n",
      "weighted avg       0.75      0.49      0.42      1026\n",
      "\n",
      "acc:  0.49317738791423\n",
      "pre:  0.953125\n",
      "rec:  0.1918238993710692\n",
      "ma F1:  0.4578225098370785\n",
      "mi F1:  0.49317738791423\n",
      "we F1:  0.42462670828372245\n",
      "Loss:  0.04435105621814728\n",
      "Loss:  0.06624878197908401\n",
      "Loss:  0.04236561805009842\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.04799279570579529\n",
      "Loss:  0.040036365389823914\n",
      "Loss:  0.0911364033818245\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.05842247232794762\n",
      "Loss:  0.05646056309342384\n",
      "Loss:  0.03248249366879463\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.09719884395599365\n",
      "Eval Loss:  2.468506336212158\n",
      "Eval Loss:  0.0029234886169433594\n",
      "[[18836   938]\n",
      " [ 1550 10541]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.94     19774\n",
      "           1       0.92      0.87      0.89     12091\n",
      "\n",
      "    accuracy                           0.92     31865\n",
      "   macro avg       0.92      0.91      0.92     31865\n",
      "weighted avg       0.92      0.92      0.92     31865\n",
      "\n",
      "acc:  0.9219206025419739\n",
      "pre:  0.9182855649446816\n",
      "rec:  0.8718054751468034\n",
      "ma F1:  0.9162449480820882\n",
      "mi F1:  0.9219206025419739\n",
      "we F1:  0.9215018562454761\n",
      "[[382   8]\n",
      " [496 140]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.98      0.60       390\n",
      "           1       0.95      0.22      0.36       636\n",
      "\n",
      "    accuracy                           0.51      1026\n",
      "   macro avg       0.69      0.60      0.48      1026\n",
      "weighted avg       0.75      0.51      0.45      1026\n",
      "\n",
      "acc:  0.5087719298245614\n",
      "pre:  0.9459459459459459\n",
      "rec:  0.22012578616352202\n",
      "ma F1:  0.47983325822442546\n",
      "mi F1:  0.5087719298245614\n",
      "we F1:  0.450416261473874\n",
      "Loss:  0.09685877710580826\n",
      "Loss:  0.07786250114440918\n",
      "Loss:  0.05215499550104141\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.057587720453739166\n",
      "Loss:  0.035079970955848694\n",
      "Loss:  0.04191433638334274\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.075868159532547\n",
      "Loss:  0.08126431703567505\n",
      "Loss:  0.026891356334090233\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.25161540508270264\n",
      "Eval Loss:  1.0431275367736816\n",
      "Eval Loss:  0.0036764144897460938\n",
      "[[18686  1088]\n",
      " [ 1564 10527]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19774\n",
      "           1       0.91      0.87      0.89     12091\n",
      "\n",
      "    accuracy                           0.92     31865\n",
      "   macro avg       0.91      0.91      0.91     31865\n",
      "weighted avg       0.92      0.92      0.92     31865\n",
      "\n",
      "acc:  0.9167738898477954\n",
      "pre:  0.9063280241067585\n",
      "rec:  0.8706475891158713\n",
      "ma F1:  0.9109346717962642\n",
      "mi F1:  0.9167738898477954\n",
      "we F1:  0.9164332270468971\n",
      "[[385   5]\n",
      " [526 110]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.99      0.59       390\n",
      "           1       0.96      0.17      0.29       636\n",
      "\n",
      "    accuracy                           0.48      1026\n",
      "   macro avg       0.69      0.58      0.44      1026\n",
      "weighted avg       0.75      0.48      0.41      1026\n",
      "\n",
      "acc:  0.4824561403508772\n",
      "pre:  0.9565217391304348\n",
      "rec:  0.17295597484276728\n",
      "ma F1:  0.4423975821118857\n",
      "mi F1:  0.4824561403508772\n",
      "we F1:  0.4065633809235514\n",
      "Loss:  0.0519929863512516\n",
      "Loss:  0.04434395954012871\n",
      "Loss:  0.045572202652692795\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.055965911597013474\n",
      "Loss:  0.032140348106622696\n",
      "Loss:  0.02493060752749443\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.05349643528461456\n",
      "Loss:  0.0631897822022438\n",
      "Loss:  0.06327555328607559\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  30 Training Time 5996.24725317955 Best Test Acc:  0.6539961013645225\n",
      "test subjects:  ['./seg\\\\x17', './seg\\\\x22']\n",
      "*********\n",
      "33431 882\n",
      "32353 538\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.7531260848045349\n",
      "Eval Loss:  0.7213926315307617\n",
      "Eval Loss:  0.7357646822929382\n",
      "[[   95 19533]\n",
      " [  129 12596]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.00      0.01     19628\n",
      "           1       0.39      0.99      0.56     12725\n",
      "\n",
      "    accuracy                           0.39     32353\n",
      "   macro avg       0.41      0.50      0.29     32353\n",
      "weighted avg       0.41      0.39      0.23     32353\n",
      "\n",
      "acc:  0.39226655951534634\n",
      "pre:  0.3920445703258738\n",
      "rec:  0.9898624754420432\n",
      "ma F1:  0.2856076352622552\n",
      "mi F1:  0.39226655951534634\n",
      "we F1:  0.22671102266173615\n",
      "[[  4 532]\n",
      " [  0   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.01      0.01       536\n",
      "           1       0.00      1.00      0.01         2\n",
      "\n",
      "    accuracy                           0.01       538\n",
      "   macro avg       0.50      0.50      0.01       538\n",
      "weighted avg       1.00      0.01      0.01       538\n",
      "\n",
      "acc:  0.011152416356877323\n",
      "pre:  0.003745318352059925\n",
      "rec:  1.0\n",
      "ma F1:  0.011138750690989498\n",
      "mi F1:  0.011152416356877323\n",
      "we F1:  0.014787483483039163\n",
      "Subject 31 Current Train Acc:  0.39226655951534634 Current Test Acc:  0.011152416356877323\n",
      "Loss:  0.16937118768692017\n",
      "Loss:  0.16422079503536224\n",
      "Loss:  0.15188728272914886\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.13760195672512054\n",
      "Loss:  0.1348484754562378\n",
      "Loss:  0.13263966143131256\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.1610325425863266\n",
      "Loss:  0.10830747336149216\n",
      "Loss:  0.11961104720830917\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.2276431918144226\n",
      "Eval Loss:  0.05908095836639404\n",
      "Eval Loss:  0.02353048324584961\n",
      "[[16880  2748]\n",
      " [ 3157  9568]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.86      0.85     19628\n",
      "           1       0.78      0.75      0.76     12725\n",
      "\n",
      "    accuracy                           0.82     32353\n",
      "   macro avg       0.81      0.81      0.81     32353\n",
      "weighted avg       0.82      0.82      0.82     32353\n",
      "\n",
      "acc:  0.8174821500324545\n",
      "pre:  0.7768756089639494\n",
      "rec:  0.7519056974459725\n",
      "ma F1:  0.8076574662102243\n",
      "mi F1:  0.8174821500324545\n",
      "we F1:  0.8169326019357241\n",
      "[[504  32]\n",
      " [  1   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.94      0.97       536\n",
      "           1       0.03      0.50      0.06         2\n",
      "\n",
      "    accuracy                           0.94       538\n",
      "   macro avg       0.51      0.72      0.51       538\n",
      "weighted avg       0.99      0.94      0.96       538\n",
      "\n",
      "acc:  0.9386617100371747\n",
      "pre:  0.030303030303030304\n",
      "rec:  0.5\n",
      "ma F1:  0.5127212844792095\n",
      "mi F1:  0.9386617100371747\n",
      "we F1:  0.9649125116123178\n",
      "Subject 31 Current Train Acc:  0.8174821500324545 Current Test Acc:  0.9386617100371747\n",
      "Loss:  0.09684256464242935\n",
      "Loss:  0.10646598786115646\n",
      "Loss:  0.08642244338989258\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.0808713436126709\n",
      "Loss:  0.0859546810388565\n",
      "Loss:  0.09136637300252914\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.10720225423574448\n",
      "Loss:  0.12088669091463089\n",
      "Loss:  0.09619217365980148\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.16176486015319824\n",
      "Eval Loss:  0.030417323112487793\n",
      "Eval Loss:  0.023983478546142578\n",
      "[[17813  1815]\n",
      " [ 2805  9920]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.89     19628\n",
      "           1       0.85      0.78      0.81     12725\n",
      "\n",
      "    accuracy                           0.86     32353\n",
      "   macro avg       0.85      0.84      0.85     32353\n",
      "weighted avg       0.86      0.86      0.86     32353\n",
      "\n",
      "acc:  0.8572002596358915\n",
      "pre:  0.8453344695355773\n",
      "rec:  0.7795677799607073\n",
      "ma F1:  0.8481630897210284\n",
      "mi F1:  0.8572002596358915\n",
      "we F1:  0.8560667491562622\n",
      "[[516  20]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98       536\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.96       538\n",
      "   macro avg       0.50      0.48      0.49       538\n",
      "weighted avg       0.99      0.96      0.98       538\n",
      "\n",
      "acc:  0.9591078066914498\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.48956356736242884\n",
      "mi F1:  0.9591078066914497\n",
      "we F1:  0.9754872569006016\n",
      "Subject 31 Current Train Acc:  0.8572002596358915 Current Test Acc:  0.9591078066914498\n",
      "Loss:  0.08373585343360901\n",
      "Loss:  0.0947553962469101\n",
      "Loss:  0.07262936979532242\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.1080271527171135\n",
      "Loss:  0.08862575888633728\n",
      "Loss:  0.10681211948394775\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.06980432569980621\n",
      "Loss:  0.07779309153556824\n",
      "Loss:  0.06201348081231117\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.11164891719818115\n",
      "Eval Loss:  0.01756882667541504\n",
      "Eval Loss:  0.019527435302734375\n",
      "[[18627  1001]\n",
      " [ 3639  9086]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.95      0.89     19628\n",
      "           1       0.90      0.71      0.80     12725\n",
      "\n",
      "    accuracy                           0.86     32353\n",
      "   macro avg       0.87      0.83      0.84     32353\n",
      "weighted avg       0.86      0.86      0.85     32353\n",
      "\n",
      "acc:  0.8565820789416746\n",
      "pre:  0.9007633587786259\n",
      "rec:  0.7140275049115914\n",
      "ma F1:  0.8429212823985412\n",
      "mi F1:  0.8565820789416745\n",
      "we F1:  0.8528049925481325\n",
      "[[526  10]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       536\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.98       538\n",
      "   macro avg       0.50      0.49      0.49       538\n",
      "weighted avg       0.99      0.98      0.99       538\n",
      "\n",
      "acc:  0.9776951672862454\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4943609022556391\n",
      "mi F1:  0.9776951672862454\n",
      "we F1:  0.9850462587696005\n",
      "Subject 31 Current Train Acc:  0.8565820789416746 Current Test Acc:  0.9776951672862454\n",
      "Loss:  0.08603186905384064\n",
      "Loss:  0.09009052813053131\n",
      "Loss:  0.08346402645111084\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.09979361295700073\n",
      "Loss:  0.0743352547287941\n",
      "Loss:  0.06327491253614426\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.11079396307468414\n",
      "Loss:  0.09298244118690491\n",
      "Loss:  0.08248171210289001\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.10760438442230225\n",
      "Eval Loss:  0.011665105819702148\n",
      "Eval Loss:  0.014587640762329102\n",
      "[[18990   638]\n",
      " [ 4288  8437]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.97      0.89     19628\n",
      "           1       0.93      0.66      0.77     12725\n",
      "\n",
      "    accuracy                           0.85     32353\n",
      "   macro avg       0.87      0.82      0.83     32353\n",
      "weighted avg       0.86      0.85      0.84     32353\n",
      "\n",
      "acc:  0.8477420950143727\n",
      "pre:  0.9296969696969697\n",
      "rec:  0.6630255402750491\n",
      "ma F1:  0.8296137898208886\n",
      "mi F1:  0.8477420950143727\n",
      "we F1:  0.8414719998796986\n",
      "[[533   3]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       536\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.99       538\n",
      "   macro avg       0.50      0.50      0.50       538\n",
      "weighted avg       0.99      0.99      0.99       538\n",
      "\n",
      "acc:  0.9907063197026023\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49766573295985067\n",
      "mi F1:  0.9907063197026023\n",
      "we F1:  0.9916313489460222\n",
      "Subject 31 Current Train Acc:  0.8477420950143727 Current Test Acc:  0.9907063197026023\n",
      "Loss:  0.07807781547307968\n",
      "Loss:  0.07743523269891739\n",
      "Loss:  0.0854034274816513\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.05701630562543869\n",
      "Loss:  0.07838184386491776\n",
      "Loss:  0.07094768434762955\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.08803378790616989\n",
      "Loss:  0.0733901858329773\n",
      "Loss:  0.09217271953821182\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.42888179421424866\n",
      "Eval Loss:  0.020684480667114258\n",
      "Eval Loss:  0.01197957992553711\n",
      "[[18118  1510]\n",
      " [ 2393 10332]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90     19628\n",
      "           1       0.87      0.81      0.84     12725\n",
      "\n",
      "    accuracy                           0.88     32353\n",
      "   macro avg       0.88      0.87      0.87     32353\n",
      "weighted avg       0.88      0.88      0.88     32353\n",
      "\n",
      "acc:  0.8793620375235681\n",
      "pre:  0.8724877554467151\n",
      "rec:  0.8119449901768173\n",
      "ma F1:  0.8719456209126211\n",
      "mi F1:  0.8793620375235681\n",
      "we F1:  0.8785209514887021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[529   7]\n",
      " [  1   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99       536\n",
      "           1       0.12      0.50      0.20         2\n",
      "\n",
      "    accuracy                           0.99       538\n",
      "   macro avg       0.56      0.74      0.60       538\n",
      "weighted avg       0.99      0.99      0.99       538\n",
      "\n",
      "acc:  0.9851301115241635\n",
      "pre:  0.125\n",
      "rec:  0.5\n",
      "ma F1:  0.5962476547842401\n",
      "mi F1:  0.9851301115241635\n",
      "we F1:  0.9895492303507535\n",
      "Loss:  0.06711418926715851\n",
      "Loss:  0.09017488360404968\n",
      "Loss:  0.06449753046035767\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.061705365777015686\n",
      "Loss:  0.08834289759397507\n",
      "Loss:  0.07782581448554993\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.07913971692323685\n",
      "Loss:  0.06898127496242523\n",
      "Loss:  0.06296364963054657\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.2879623770713806\n",
      "Eval Loss:  0.018564701080322266\n",
      "Eval Loss:  0.008145570755004883\n",
      "[[18086  1542]\n",
      " [ 2162 10563]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.91     19628\n",
      "           1       0.87      0.83      0.85     12725\n",
      "\n",
      "    accuracy                           0.89     32353\n",
      "   macro avg       0.88      0.88      0.88     32353\n",
      "weighted avg       0.89      0.89      0.88     32353\n",
      "\n",
      "acc:  0.8855129354310265\n",
      "pre:  0.8726146220570012\n",
      "rec:  0.830098231827112\n",
      "ma F1:  0.8789688307615873\n",
      "mi F1:  0.8855129354310265\n",
      "we F1:  0.8849736100428766\n",
      "[[532   4]\n",
      " [  1   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       536\n",
      "           1       0.20      0.50      0.29         2\n",
      "\n",
      "    accuracy                           0.99       538\n",
      "   macro avg       0.60      0.75      0.64       538\n",
      "weighted avg       1.00      0.99      0.99       538\n",
      "\n",
      "acc:  0.9907063197026023\n",
      "pre:  0.2\n",
      "rec:  0.5\n",
      "ma F1:  0.6405185086195375\n",
      "mi F1:  0.9907063197026023\n",
      "we F1:  0.9926847819121107\n",
      "Loss:  0.08091086149215698\n",
      "Loss:  0.09446205198764801\n",
      "Loss:  0.08085154742002487\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.0709744393825531\n",
      "Loss:  0.08033439517021179\n",
      "Loss:  0.06560776382684708\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.04812243580818176\n",
      "Loss:  0.048895787447690964\n",
      "Loss:  0.0639379695057869\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.37464046478271484\n",
      "Eval Loss:  0.01482534408569336\n",
      "Eval Loss:  0.0065190792083740234\n",
      "[[18583  1045]\n",
      " [ 2592 10133]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91     19628\n",
      "           1       0.91      0.80      0.85     12725\n",
      "\n",
      "    accuracy                           0.89     32353\n",
      "   macro avg       0.89      0.87      0.88     32353\n",
      "weighted avg       0.89      0.89      0.89     32353\n",
      "\n",
      "acc:  0.8875838407566532\n",
      "pre:  0.9065127929862229\n",
      "rec:  0.7963064833005894\n",
      "ma F1:  0.879353882083541\n",
      "mi F1:  0.8875838407566532\n",
      "we F1:  0.8860771252457295\n",
      "[[534   2]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       536\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.99       538\n",
      "   macro avg       0.50      0.50      0.50       538\n",
      "weighted avg       0.99      0.99      0.99       538\n",
      "\n",
      "acc:  0.9925650557620818\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.498134328358209\n",
      "mi F1:  0.9925650557620818\n",
      "we F1:  0.9925650557620818\n",
      "Subject 31 Current Train Acc:  0.8875838407566532 Current Test Acc:  0.9925650557620818\n",
      "Loss:  0.04922338202595711\n",
      "Loss:  0.05460512638092041\n",
      "Loss:  0.052380699664354324\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.057168345898389816\n",
      "Loss:  0.0909029096364975\n",
      "Loss:  0.06992147117853165\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.10676726698875427\n",
      "Loss:  0.05412214994430542\n",
      "Loss:  0.08487792313098907\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.183826744556427\n",
      "Eval Loss:  0.010985612869262695\n",
      "Eval Loss:  0.0038940906524658203\n",
      "[[18650   978]\n",
      " [ 2496 10229]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.95      0.91     19628\n",
      "           1       0.91      0.80      0.85     12725\n",
      "\n",
      "    accuracy                           0.89     32353\n",
      "   macro avg       0.90      0.88      0.88     32353\n",
      "weighted avg       0.89      0.89      0.89     32353\n",
      "\n",
      "acc:  0.8926220134145211\n",
      "pre:  0.9127331132328009\n",
      "rec:  0.8038506876227898\n",
      "ma F1:  0.8848186779367623\n",
      "mi F1:  0.8926220134145211\n",
      "we F1:  0.8912153558613519\n",
      "[[535   1]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       536\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.99       538\n",
      "   macro avg       0.50      0.50      0.50       538\n",
      "weighted avg       0.99      0.99      0.99       538\n",
      "\n",
      "acc:  0.9944237918215614\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4986020503261882\n",
      "mi F1:  0.9944237918215614\n",
      "we F1:  0.9934970222112895\n",
      "Subject 31 Current Train Acc:  0.8926220134145211 Current Test Acc:  0.9944237918215614\n",
      "Loss:  0.07257628440856934\n",
      "Loss:  0.07289356738328934\n",
      "Loss:  0.08605093508958817\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.06393961608409882\n",
      "Loss:  0.06897369027137756\n",
      "Loss:  0.07373597472906113\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.046049583703279495\n",
      "Loss:  0.062186986207962036\n",
      "Loss:  0.07042790204286575\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.13922369480133057\n",
      "Eval Loss:  0.013247489929199219\n",
      "Eval Loss:  0.0027785301208496094\n",
      "[[18577  1051]\n",
      " [ 2262 10463]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     19628\n",
      "           1       0.91      0.82      0.86     12725\n",
      "\n",
      "    accuracy                           0.90     32353\n",
      "   macro avg       0.90      0.88      0.89     32353\n",
      "weighted avg       0.90      0.90      0.90     32353\n",
      "\n",
      "acc:  0.8975983680029672\n",
      "pre:  0.9087198193503561\n",
      "rec:  0.8222396856581532\n",
      "ma F1:  0.8907251332585298\n",
      "mi F1:  0.8975983680029672\n",
      "we F1:  0.8965725500000693\n",
      "[[534   2]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       536\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.99       538\n",
      "   macro avg       0.50      0.50      0.50       538\n",
      "weighted avg       0.99      0.99      0.99       538\n",
      "\n",
      "acc:  0.9925650557620818\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.498134328358209\n",
      "mi F1:  0.9925650557620818\n",
      "we F1:  0.9925650557620818\n",
      "Loss:  0.05957729369401932\n",
      "Loss:  0.04792691394686699\n",
      "Loss:  0.08443541824817657\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.044164083898067474\n",
      "Loss:  0.039271775633096695\n",
      "Loss:  0.0693092942237854\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.08396247774362564\n",
      "Loss:  0.041003525257110596\n",
      "Loss:  0.05887782573699951\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.21071934700012207\n",
      "Eval Loss:  0.010441064834594727\n",
      "Eval Loss:  0.00231170654296875\n",
      "[[18661   967]\n",
      " [ 2288 10437]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.95      0.92     19628\n",
      "           1       0.92      0.82      0.87     12725\n",
      "\n",
      "    accuracy                           0.90     32353\n",
      "   macro avg       0.90      0.89      0.89     32353\n",
      "weighted avg       0.90      0.90      0.90     32353\n",
      "\n",
      "acc:  0.8993910920161964\n",
      "pre:  0.9152051911609962\n",
      "rec:  0.8201964636542239\n",
      "ma F1:  0.8924411148126482\n",
      "mi F1:  0.8993910920161964\n",
      "we F1:  0.8982747350261807\n",
      "[[535   1]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       536\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.99       538\n",
      "   macro avg       0.50      0.50      0.50       538\n",
      "weighted avg       0.99      0.99      0.99       538\n",
      "\n",
      "acc:  0.9944237918215614\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4986020503261882\n",
      "mi F1:  0.9944237918215614\n",
      "we F1:  0.9934970222112895\n",
      "Loss:  0.07047519087791443\n",
      "Loss:  0.04500183090567589\n",
      "Loss:  0.06287037581205368\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.06423008441925049\n",
      "Loss:  0.07842274755239487\n",
      "Loss:  0.04882745072245598\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.09186197817325592\n",
      "Loss:  0.06185213476419449\n",
      "Loss:  0.05624545365571976\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.8030145764350891\n",
      "Eval Loss:  0.019437789916992188\n",
      "Eval Loss:  0.0018908977508544922\n",
      "[[17265  2363]\n",
      " [ 1005 11720]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91     19628\n",
      "           1       0.83      0.92      0.87     12725\n",
      "\n",
      "    accuracy                           0.90     32353\n",
      "   macro avg       0.89      0.90      0.89     32353\n",
      "weighted avg       0.90      0.90      0.90     32353\n",
      "\n",
      "acc:  0.8958983710938707\n",
      "pre:  0.8322090463679613\n",
      "rec:  0.9210216110019647\n",
      "ma F1:  0.8927478679322354\n",
      "mi F1:  0.8958983710938707\n",
      "we F1:  0.8966699460791729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[532   4]\n",
      " [  1   1]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00       536\n",
      "           1       0.20      0.50      0.29         2\n",
      "\n",
      "    accuracy                           0.99       538\n",
      "   macro avg       0.60      0.75      0.64       538\n",
      "weighted avg       1.00      0.99      0.99       538\n",
      "\n",
      "acc:  0.9907063197026023\n",
      "pre:  0.2\n",
      "rec:  0.5\n",
      "ma F1:  0.6405185086195375\n",
      "mi F1:  0.9907063197026023\n",
      "we F1:  0.9926847819121107\n",
      "Loss:  0.08167903125286102\n",
      "Loss:  0.049281973391771317\n",
      "Loss:  0.08370178937911987\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.06962461769580841\n",
      "Loss:  0.058212459087371826\n",
      "Loss:  0.07433663308620453\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.061359260231256485\n",
      "Loss:  0.06814046204090118\n",
      "Loss:  0.06842048466205597\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.6253386735916138\n",
      "Eval Loss:  0.014335393905639648\n",
      "Eval Loss:  0.002077341079711914\n",
      "[[18304  1324]\n",
      " [ 1604 11121]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.93     19628\n",
      "           1       0.89      0.87      0.88     12725\n",
      "\n",
      "    accuracy                           0.91     32353\n",
      "   macro avg       0.91      0.90      0.90     32353\n",
      "weighted avg       0.91      0.91      0.91     32353\n",
      "\n",
      "acc:  0.909498346366643\n",
      "pre:  0.8936118923262354\n",
      "rec:  0.8739489194499017\n",
      "ma F1:  0.9048059757791092\n",
      "mi F1:  0.909498346366643\n",
      "we F1:  0.9093154334104256\n",
      "[[535   1]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       536\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.99       538\n",
      "   macro avg       0.50      0.50      0.50       538\n",
      "weighted avg       0.99      0.99      0.99       538\n",
      "\n",
      "acc:  0.9944237918215614\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4986020503261882\n",
      "mi F1:  0.9944237918215614\n",
      "we F1:  0.9934970222112895\n",
      "Loss:  0.05632972717285156\n",
      "Loss:  0.05748159438371658\n",
      "Loss:  0.05937068164348602\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.05141661688685417\n",
      "Loss:  0.04603193700313568\n",
      "Loss:  0.059242069721221924\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.07404536753892899\n",
      "Loss:  0.0325607992708683\n",
      "Loss:  0.02457520365715027\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.14263296127319336\n",
      "Eval Loss:  0.0074350833892822266\n",
      "Eval Loss:  0.0018172264099121094\n",
      "[[18845   783]\n",
      " [ 2763  9962]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91     19628\n",
      "           1       0.93      0.78      0.85     12725\n",
      "\n",
      "    accuracy                           0.89     32353\n",
      "   macro avg       0.90      0.87      0.88     32353\n",
      "weighted avg       0.89      0.89      0.89     32353\n",
      "\n",
      "acc:  0.8903965629153402\n",
      "pre:  0.9271288971614704\n",
      "rec:  0.7828683693516699\n",
      "ma F1:  0.8814603423989933\n",
      "mi F1:  0.8903965629153402\n",
      "we F1:  0.8884047001862659\n",
      "[[536   0]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       536\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           1.00       538\n",
      "   macro avg       0.50      0.50      0.50       538\n",
      "weighted avg       0.99      1.00      0.99       538\n",
      "\n",
      "acc:  0.9962825278810409\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49906890130353815\n",
      "mi F1:  0.9962825278810409\n",
      "we F1:  0.9944272531550055\n",
      "Subject 31 Current Train Acc:  0.8903965629153402 Current Test Acc:  0.9962825278810409\n",
      "Loss:  0.05145116522908211\n",
      "Loss:  0.04710296913981438\n",
      "Loss:  0.03915456682443619\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.05956054851412773\n",
      "Loss:  0.050195347517728806\n",
      "Loss:  0.07951420545578003\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.033438537269830704\n",
      "Loss:  0.057201869785785675\n",
      "Loss:  0.07276560366153717\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.373311847448349\n",
      "Eval Loss:  0.01129007339477539\n",
      "Eval Loss:  0.0014333724975585938\n",
      "[[17986  1642]\n",
      " [ 1310 11415]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.92      0.92     19628\n",
      "           1       0.87      0.90      0.89     12725\n",
      "\n",
      "    accuracy                           0.91     32353\n",
      "   macro avg       0.90      0.91      0.90     32353\n",
      "weighted avg       0.91      0.91      0.91     32353\n",
      "\n",
      "acc:  0.9087565295335827\n",
      "pre:  0.8742437006969441\n",
      "rec:  0.8970530451866404\n",
      "ma F1:  0.9048307070147401\n",
      "mi F1:  0.9087565295335827\n",
      "we F1:  0.9089548818507727\n",
      "[[535   1]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       536\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.99       538\n",
      "   macro avg       0.50      0.50      0.50       538\n",
      "weighted avg       0.99      0.99      0.99       538\n",
      "\n",
      "acc:  0.9944237918215614\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.4986020503261882\n",
      "mi F1:  0.9944237918215614\n",
      "we F1:  0.9934970222112895\n",
      "Loss:  0.09509878605604172\n",
      "Loss:  0.039484377950429916\n",
      "Loss:  0.08613168448209763\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.046355780214071274\n",
      "Loss:  0.0391850620508194\n",
      "Loss:  0.0881337970495224\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.06850100308656693\n",
      "Loss:  0.06386879086494446\n",
      "Loss:  0.06890164315700531\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.22780179977416992\n",
      "Eval Loss:  0.007023334503173828\n",
      "Eval Loss:  0.0012850761413574219\n",
      "[[18392  1236]\n",
      " [ 1889 10836]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.94      0.92     19628\n",
      "           1       0.90      0.85      0.87     12725\n",
      "\n",
      "    accuracy                           0.90     32353\n",
      "   macro avg       0.90      0.89      0.90     32353\n",
      "weighted avg       0.90      0.90      0.90     32353\n",
      "\n",
      "acc:  0.9034092665286063\n",
      "pre:  0.8976143141153081\n",
      "rec:  0.8515520628683694\n",
      "ma F1:  0.8978367755430151\n",
      "mi F1:  0.9034092665286063\n",
      "we F1:  0.9029276841287135\n",
      "[[536   0]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       536\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           1.00       538\n",
      "   macro avg       0.50      0.50      0.50       538\n",
      "weighted avg       0.99      1.00      0.99       538\n",
      "\n",
      "acc:  0.9962825278810409\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49906890130353815\n",
      "mi F1:  0.9962825278810409\n",
      "we F1:  0.9944272531550055\n",
      "Loss:  0.07792332768440247\n",
      "Loss:  0.07410039007663727\n",
      "Loss:  0.049731526523828506\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.0767785906791687\n",
      "Loss:  0.05241049453616142\n",
      "Loss:  0.05235712602734566\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.08562012016773224\n",
      "Loss:  0.06863249838352203\n",
      "Loss:  0.07762666046619415\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.16401147842407227\n",
      "Eval Loss:  0.009185791015625\n",
      "Eval Loss:  0.0010600090026855469\n",
      "[[18413  1215]\n",
      " [ 1553 11172]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     19628\n",
      "           1       0.90      0.88      0.89     12725\n",
      "\n",
      "    accuracy                           0.91     32353\n",
      "   macro avg       0.91      0.91      0.91     32353\n",
      "weighted avg       0.91      0.91      0.91     32353\n",
      "\n",
      "acc:  0.9144437919203783\n",
      "pre:  0.9019132961976265\n",
      "rec:  0.8779567779960707\n",
      "ma F1:  0.9099321155282023\n",
      "mi F1:  0.9144437919203783\n",
      "we F1:  0.9142331930223594\n",
      "[[536   0]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       536\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           1.00       538\n",
      "   macro avg       0.50      0.50      0.50       538\n",
      "weighted avg       0.99      1.00      0.99       538\n",
      "\n",
      "acc:  0.9962825278810409\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49906890130353815\n",
      "mi F1:  0.9962825278810409\n",
      "we F1:  0.9944272531550055\n",
      "Loss:  0.060209646821022034\n",
      "Loss:  0.028809983283281326\n",
      "Loss:  0.07934574782848358\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.060376666486263275\n",
      "Loss:  0.04746934399008751\n",
      "Loss:  0.043449100106954575\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.07900872826576233\n",
      "Loss:  0.03574720397591591\n",
      "Loss:  0.06368721276521683\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.18124639987945557\n",
      "Eval Loss:  0.009092330932617188\n",
      "Eval Loss:  0.001241922378540039\n",
      "[[18565  1063]\n",
      " [ 1698 11027]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     19628\n",
      "           1       0.91      0.87      0.89     12725\n",
      "\n",
      "    accuracy                           0.91     32353\n",
      "   macro avg       0.91      0.91      0.91     32353\n",
      "weighted avg       0.91      0.91      0.91     32353\n",
      "\n",
      "acc:  0.9146601551633542\n",
      "pre:  0.9120760959470637\n",
      "rec:  0.8665618860510805\n",
      "ma F1:  0.9097615220698647\n",
      "mi F1:  0.9146601551633542\n",
      "we F1:  0.9142474950394003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[536   0]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       536\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           1.00       538\n",
      "   macro avg       0.50      0.50      0.50       538\n",
      "weighted avg       0.99      1.00      0.99       538\n",
      "\n",
      "acc:  0.9962825278810409\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49906890130353815\n",
      "mi F1:  0.9962825278810409\n",
      "we F1:  0.9944272531550055\n",
      "Loss:  0.05572459474205971\n",
      "Loss:  0.07872763276100159\n",
      "Loss:  0.07988506555557251\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.043924905359745026\n",
      "Loss:  0.037292852997779846\n",
      "Loss:  0.07243729382753372\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.059591829776763916\n",
      "Loss:  0.09617775678634644\n",
      "Loss:  0.05640522390604019\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.5182658433914185\n",
      "Eval Loss:  0.009009361267089844\n",
      "Eval Loss:  0.0012240409851074219\n",
      "[[18211  1417]\n",
      " [ 1286 11439]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19628\n",
      "           1       0.89      0.90      0.89     12725\n",
      "\n",
      "    accuracy                           0.92     32353\n",
      "   macro avg       0.91      0.91      0.91     32353\n",
      "weighted avg       0.92      0.92      0.92     32353\n",
      "\n",
      "acc:  0.9164528791765834\n",
      "pre:  0.8897790914747977\n",
      "rec:  0.8989390962671906\n",
      "ma F1:  0.9126246888758918\n",
      "mi F1:  0.9164528791765834\n",
      "we F1:  0.9165269330645619\n",
      "[[536   0]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       536\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           1.00       538\n",
      "   macro avg       0.50      0.50      0.50       538\n",
      "weighted avg       0.99      1.00      0.99       538\n",
      "\n",
      "acc:  0.9962825278810409\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49906890130353815\n",
      "mi F1:  0.9962825278810409\n",
      "we F1:  0.9944272531550055\n",
      "Loss:  0.07528997212648392\n",
      "Loss:  0.057445112615823746\n",
      "Loss:  0.05232008919119835\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.04190685227513313\n",
      "Loss:  0.05485649034380913\n",
      "Loss:  0.04487922787666321\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.04326760396361351\n",
      "Loss:  0.0526558980345726\n",
      "Loss:  0.04542359337210655\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.24164581298828125\n",
      "Eval Loss:  0.008944511413574219\n",
      "Eval Loss:  0.000881195068359375\n",
      "[[18197  1431]\n",
      " [ 1422 11303]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     19628\n",
      "           1       0.89      0.89      0.89     12725\n",
      "\n",
      "    accuracy                           0.91     32353\n",
      "   macro avg       0.91      0.91      0.91     32353\n",
      "weighted avg       0.91      0.91      0.91     32353\n",
      "\n",
      "acc:  0.9118165239699564\n",
      "pre:  0.8876236846238417\n",
      "rec:  0.8882514734774066\n",
      "ma F1:  0.907622006904591\n",
      "mi F1:  0.9118165239699564\n",
      "we F1:  0.911821999840799\n",
      "[[536   0]\n",
      " [  2   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       536\n",
      "           1       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           1.00       538\n",
      "   macro avg       0.50      0.50      0.50       538\n",
      "weighted avg       0.99      1.00      0.99       538\n",
      "\n",
      "acc:  0.9962825278810409\n",
      "pre:  0.0\n",
      "rec:  0.0\n",
      "ma F1:  0.49906890130353815\n",
      "mi F1:  0.9962825278810409\n",
      "we F1:  0.9944272531550055\n",
      "Loss:  0.05719226598739624\n",
      "Loss:  0.04050126299262047\n",
      "Loss:  0.07105398178100586\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.07791973650455475\n",
      "Loss:  0.0631338357925415\n",
      "Loss:  0.04761680215597153\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.06854698807001114\n",
      "Loss:  0.031734298914670944\n",
      "Loss:  0.043552763760089874\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  31 Training Time 5882.562224149704 Best Test Acc:  0.9962825278810409\n",
      "test subjects:  ['./seg\\\\x31', './seg\\\\x32']\n",
      "*********\n",
      "33218 1095\n",
      "31796 1095\n",
      "Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) init\n",
      "epoch:  0\n",
      "Eval Loss:  0.5676442384719849\n",
      "Eval Loss:  0.8330312967300415\n",
      "Eval Loss:  0.8010936379432678\n",
      "[[    0 20010]\n",
      " [    0 11786]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00     20010\n",
      "           1       0.37      1.00      0.54     11786\n",
      "\n",
      "    accuracy                           0.37     31796\n",
      "   macro avg       0.19      0.50      0.27     31796\n",
      "weighted avg       0.14      0.37      0.20     31796\n",
      "\n",
      "acc:  0.3706755566737954\n",
      "pre:  0.3706755566737954\n",
      "rec:  1.0\n",
      "ma F1:  0.2704327474645496\n",
      "mi F1:  0.3706755566737954\n",
      "we F1:  0.20048561841849172\n",
      "[[  0 154]\n",
      " [  0 941]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       154\n",
      "           1       0.86      1.00      0.92       941\n",
      "\n",
      "    accuracy                           0.86      1095\n",
      "   macro avg       0.43      0.50      0.46      1095\n",
      "weighted avg       0.74      0.86      0.79      1095\n",
      "\n",
      "acc:  0.8593607305936073\n",
      "pre:  0.8593607305936073\n",
      "rec:  1.0\n",
      "ma F1:  0.46218074656188607\n",
      "mi F1:  0.8593607305936073\n",
      "we F1:  0.7943599680634426\n",
      "Subject 32 Current Train Acc:  0.3706755566737954 Current Test Acc:  0.8593607305936073\n",
      "Loss:  0.17370593547821045\n",
      "Loss:  0.16697709262371063\n",
      "Loss:  0.16191978752613068\n",
      "0 **********\n",
      "epoch:  1\n",
      "Loss:  0.13865266740322113\n",
      "Loss:  0.12159869819879532\n",
      "Loss:  0.12882502377033234\n",
      "1 **********\n",
      "epoch:  2\n",
      "Loss:  0.11225572228431702\n",
      "Loss:  0.09929752349853516\n",
      "Loss:  0.0973004400730133\n",
      "2 **********\n",
      "epoch:  3\n",
      "Eval Loss:  0.24956858158111572\n",
      "Eval Loss:  0.03110063076019287\n",
      "Eval Loss:  0.3075770139694214\n",
      "[[18080  1930]\n",
      " [ 3603  8183]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.90      0.87     20010\n",
      "           1       0.81      0.69      0.75     11786\n",
      "\n",
      "    accuracy                           0.83     31796\n",
      "   macro avg       0.82      0.80      0.81     31796\n",
      "weighted avg       0.82      0.83      0.82     31796\n",
      "\n",
      "acc:  0.8259844005535287\n",
      "pre:  0.8091565311974686\n",
      "rec:  0.6942983200407263\n",
      "ma F1:  0.8073159663635987\n",
      "mi F1:  0.8259844005535287\n",
      "we F1:  0.8228286674627181\n",
      "[[125  29]\n",
      " [109 832]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.81      0.64       154\n",
      "           1       0.97      0.88      0.92       941\n",
      "\n",
      "    accuracy                           0.87      1095\n",
      "   macro avg       0.75      0.85      0.78      1095\n",
      "weighted avg       0.91      0.87      0.88      1095\n",
      "\n",
      "acc:  0.873972602739726\n",
      "pre:  0.9663182346109176\n",
      "rec:  0.8841657810839533\n",
      "ma F1:  0.7838741604402897\n",
      "mi F1:  0.873972602739726\n",
      "we F1:  0.8841676174270737\n",
      "Subject 32 Current Train Acc:  0.8259844005535287 Current Test Acc:  0.873972602739726\n",
      "Loss:  0.10293935984373093\n",
      "Loss:  0.1187119334936142\n",
      "Loss:  0.09784620255231857\n",
      "3 **********\n",
      "epoch:  4\n",
      "Loss:  0.10570231080055237\n",
      "Loss:  0.08426273614168167\n",
      "Loss:  0.06888177245855331\n",
      "4 **********\n",
      "epoch:  5\n",
      "Loss:  0.07359182834625244\n",
      "Loss:  0.11858169734477997\n",
      "Loss:  0.08271895349025726\n",
      "5 **********\n",
      "epoch:  6\n",
      "Eval Loss:  0.10589480400085449\n",
      "Eval Loss:  0.0173642635345459\n",
      "Eval Loss:  0.18397128582000732\n",
      "[[19053   957]\n",
      " [ 3779  8007]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.95      0.89     20010\n",
      "           1       0.89      0.68      0.77     11786\n",
      "\n",
      "    accuracy                           0.85     31796\n",
      "   macro avg       0.86      0.82      0.83     31796\n",
      "weighted avg       0.86      0.85      0.85     31796\n",
      "\n",
      "acc:  0.8510504465970562\n",
      "pre:  0.893239625167336\n",
      "rec:  0.6793653487188189\n",
      "ma F1:  0.8306066549940014\n",
      "mi F1:  0.8510504465970562\n",
      "we F1:  0.84582752609155\n",
      "[[141  13]\n",
      " [133 808]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.92      0.66       154\n",
      "           1       0.98      0.86      0.92       941\n",
      "\n",
      "    accuracy                           0.87      1095\n",
      "   macro avg       0.75      0.89      0.79      1095\n",
      "weighted avg       0.92      0.87      0.88      1095\n",
      "\n",
      "acc:  0.8666666666666667\n",
      "pre:  0.9841656516443362\n",
      "rec:  0.8586609989373007\n",
      "ma F1:  0.7880090593739061\n",
      "mi F1:  0.8666666666666667\n",
      "we F1:  0.8808179603325308\n",
      "Loss:  0.08908240497112274\n",
      "Loss:  0.07907998561859131\n",
      "Loss:  0.08969879150390625\n",
      "6 **********\n",
      "epoch:  7\n",
      "Loss:  0.07155122607946396\n",
      "Loss:  0.05910056084394455\n",
      "Loss:  0.06419216841459274\n",
      "7 **********\n",
      "epoch:  8\n",
      "Loss:  0.08041450381278992\n",
      "Loss:  0.06138096749782562\n",
      "Loss:  0.06979718804359436\n",
      "8 **********\n",
      "epoch:  9\n",
      "Eval Loss:  0.09702551364898682\n",
      "Eval Loss:  0.014523506164550781\n",
      "Eval Loss:  0.18514329195022583\n",
      "[[19247   763]\n",
      " [ 3938  7848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89     20010\n",
      "           1       0.91      0.67      0.77     11786\n",
      "\n",
      "    accuracy                           0.85     31796\n",
      "   macro avg       0.87      0.81      0.83     31796\n",
      "weighted avg       0.86      0.85      0.85     31796\n",
      "\n",
      "acc:  0.852151213989181\n",
      "pre:  0.9113924050632911\n",
      "rec:  0.6658747666723231\n",
      "ma F1:  0.8303464446956641\n",
      "mi F1:  0.8521512139891809\n",
      "we F1:  0.8460778617208315\n",
      "[[142  12]\n",
      " [120 821]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.92      0.68       154\n",
      "           1       0.99      0.87      0.93       941\n",
      "\n",
      "    accuracy                           0.88      1095\n",
      "   macro avg       0.76      0.90      0.80      1095\n",
      "weighted avg       0.92      0.88      0.89      1095\n",
      "\n",
      "acc:  0.8794520547945206\n",
      "pre:  0.985594237695078\n",
      "rec:  0.8724760892667375\n",
      "ma F1:  0.8041420952215765\n",
      "mi F1:  0.8794520547945206\n",
      "we F1:  0.8914306639754895\n",
      "Subject 32 Current Train Acc:  0.852151213989181 Current Test Acc:  0.8794520547945206\n",
      "Loss:  0.091026172041893\n",
      "Loss:  0.09014108031988144\n",
      "Loss:  0.07131357491016388\n",
      "9 **********\n",
      "epoch:  10\n",
      "Loss:  0.06822020560503006\n",
      "Loss:  0.09419478476047516\n",
      "Loss:  0.09494737535715103\n",
      "10 **********\n",
      "epoch:  11\n",
      "Loss:  0.07381188124418259\n",
      "Loss:  0.11722808331251144\n",
      "Loss:  0.07313761115074158\n",
      "11 **********\n",
      "epoch:  12\n",
      "Eval Loss:  0.07000851631164551\n",
      "Eval Loss:  0.011290311813354492\n",
      "Eval Loss:  0.1898106336593628\n",
      "[[19061   949]\n",
      " [ 3172  8614]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.90     20010\n",
      "           1       0.90      0.73      0.81     11786\n",
      "\n",
      "    accuracy                           0.87     31796\n",
      "   macro avg       0.88      0.84      0.85     31796\n",
      "weighted avg       0.87      0.87      0.87     31796\n",
      "\n",
      "acc:  0.8703925022015347\n",
      "pre:  0.9007633587786259\n",
      "rec:  0.7308671304938062\n",
      "ma F1:  0.8547076285292303\n",
      "mi F1:  0.8703925022015347\n",
      "we F1:  0.8670549436513737\n",
      "[[144  10]\n",
      " [108 833]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.94      0.71       154\n",
      "           1       0.99      0.89      0.93       941\n",
      "\n",
      "    accuracy                           0.89      1095\n",
      "   macro avg       0.78      0.91      0.82      1095\n",
      "weighted avg       0.93      0.89      0.90      1095\n",
      "\n",
      "acc:  0.8922374429223744\n",
      "pre:  0.9881376037959668\n",
      "rec:  0.8852284803400637\n",
      "ma F1:  0.8216080540767412\n",
      "mi F1:  0.8922374429223744\n",
      "we F1:  0.902283422758183\n",
      "Subject 32 Current Train Acc:  0.8703925022015347 Current Test Acc:  0.8922374429223744\n",
      "Loss:  0.0693243145942688\n",
      "Loss:  0.12459874153137207\n",
      "Loss:  0.07368768006563187\n",
      "12 **********\n",
      "epoch:  13\n",
      "Loss:  0.06669613718986511\n",
      "Loss:  0.08441953361034393\n",
      "Loss:  0.07402893155813217\n",
      "13 **********\n",
      "epoch:  14\n",
      "Loss:  0.04546475037932396\n",
      "Loss:  0.07224822044372559\n",
      "Loss:  0.13746428489685059\n",
      "14 **********\n",
      "epoch:  15\n",
      "Eval Loss:  0.05988574028015137\n",
      "Eval Loss:  0.0093994140625\n",
      "Eval Loss:  0.19895285367965698\n",
      "[[19085   925]\n",
      " [ 2984  8802]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.95      0.91     20010\n",
      "           1       0.90      0.75      0.82     11786\n",
      "\n",
      "    accuracy                           0.88     31796\n",
      "   macro avg       0.88      0.85      0.86     31796\n",
      "weighted avg       0.88      0.88      0.87     31796\n",
      "\n",
      "acc:  0.8770600075481193\n",
      "pre:  0.9049038758096022\n",
      "rec:  0.7468182589512982\n",
      "ma F1:  0.8626996098926871\n",
      "mi F1:  0.8770600075481193\n",
      "we F1:  0.8741845763731183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[139  15]\n",
      " [ 88 853]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.90      0.73       154\n",
      "           1       0.98      0.91      0.94       941\n",
      "\n",
      "    accuracy                           0.91      1095\n",
      "   macro avg       0.80      0.90      0.84      1095\n",
      "weighted avg       0.93      0.91      0.91      1095\n",
      "\n",
      "acc:  0.9059360730593607\n",
      "pre:  0.9827188940092166\n",
      "rec:  0.9064824654622742\n",
      "ma F1:  0.836360629050722\n",
      "mi F1:  0.9059360730593607\n",
      "we F1:  0.9130495288193476\n",
      "Subject 32 Current Train Acc:  0.8770600075481193 Current Test Acc:  0.9059360730593607\n",
      "Loss:  0.08767709881067276\n",
      "Loss:  0.07686404883861542\n",
      "Loss:  0.06986475735902786\n",
      "15 **********\n",
      "epoch:  16\n",
      "Loss:  0.07931375503540039\n",
      "Loss:  0.06863869726657867\n",
      "Loss:  0.07484053075313568\n",
      "16 **********\n",
      "epoch:  17\n",
      "Loss:  0.0912182480096817\n",
      "Loss:  0.06616752594709396\n",
      "Loss:  0.09664727002382278\n",
      "17 **********\n",
      "epoch:  18\n",
      "Eval Loss:  0.07938694953918457\n",
      "Eval Loss:  0.007029533386230469\n",
      "Eval Loss:  0.2971647381782532\n",
      "[[19110   900]\n",
      " [ 2772  9014]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91     20010\n",
      "           1       0.91      0.76      0.83     11786\n",
      "\n",
      "    accuracy                           0.88     31796\n",
      "   macro avg       0.89      0.86      0.87     31796\n",
      "weighted avg       0.89      0.88      0.88     31796\n",
      "\n",
      "acc:  0.88451377531765\n",
      "pre:  0.9092192858583821\n",
      "rec:  0.7648057016799593\n",
      "ma F1:  0.8715647213968242\n",
      "mi F1:  0.88451377531765\n",
      "we F1:  0.88211276215008\n",
      "[[141  13]\n",
      " [ 96 845]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.92      0.72       154\n",
      "           1       0.98      0.90      0.94       941\n",
      "\n",
      "    accuracy                           0.90      1095\n",
      "   macro avg       0.79      0.91      0.83      1095\n",
      "weighted avg       0.93      0.90      0.91      1095\n",
      "\n",
      "acc:  0.9004566210045662\n",
      "pre:  0.9848484848484849\n",
      "rec:  0.89798087141339\n",
      "ma F1:  0.8303192026260682\n",
      "mi F1:  0.9004566210045662\n",
      "we F1:  0.9087256632281677\n",
      "Loss:  0.05475354939699173\n",
      "Loss:  0.07015932351350784\n",
      "Loss:  0.06940474361181259\n",
      "18 **********\n",
      "epoch:  19\n",
      "Loss:  0.08510100096464157\n",
      "Loss:  0.0962880328297615\n",
      "Loss:  0.049361735582351685\n",
      "19 **********\n",
      "epoch:  20\n",
      "Loss:  0.07962165772914886\n",
      "Loss:  0.09597119688987732\n",
      "Loss:  0.04655879735946655\n",
      "20 **********\n",
      "epoch:  21\n",
      "Eval Loss:  0.07290732860565186\n",
      "Eval Loss:  0.0051212310791015625\n",
      "Eval Loss:  0.31464606523513794\n",
      "[[19202   808]\n",
      " [ 2808  8978]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91     20010\n",
      "           1       0.92      0.76      0.83     11786\n",
      "\n",
      "    accuracy                           0.89     31796\n",
      "   macro avg       0.89      0.86      0.87     31796\n",
      "weighted avg       0.89      0.89      0.88     31796\n",
      "\n",
      "acc:  0.8862750031450497\n",
      "pre:  0.91743306764766\n",
      "rec:  0.7617512302732055\n",
      "ma F1:  0.873160520720136\n",
      "mi F1:  0.8862750031450496\n",
      "we F1:  0.883709572310755\n",
      "[[144  10]\n",
      " [105 836]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.94      0.71       154\n",
      "           1       0.99      0.89      0.94       941\n",
      "\n",
      "    accuracy                           0.89      1095\n",
      "   macro avg       0.78      0.91      0.83      1095\n",
      "weighted avg       0.93      0.89      0.90      1095\n",
      "\n",
      "acc:  0.8949771689497716\n",
      "pre:  0.9881796690307328\n",
      "rec:  0.8884165781083954\n",
      "ma F1:  0.825143266575113\n",
      "mi F1:  0.8949771689497716\n",
      "we F1:  0.9045641931197032\n",
      "Loss:  0.06207515671849251\n",
      "Loss:  0.07037342339754105\n",
      "Loss:  0.0640730932354927\n",
      "21 **********\n",
      "epoch:  22\n",
      "Loss:  0.056072406470775604\n",
      "Loss:  0.08231862634420395\n",
      "Loss:  0.0313006266951561\n",
      "22 **********\n",
      "epoch:  23\n",
      "Loss:  0.07863777875900269\n",
      "Loss:  0.08393598347902298\n",
      "Loss:  0.05661791190505028\n",
      "23 **********\n",
      "epoch:  24\n",
      "Eval Loss:  0.06719851493835449\n",
      "Eval Loss:  0.00452876091003418\n",
      "Eval Loss:  0.3619520664215088\n",
      "[[18892  1118]\n",
      " [ 2180  9606]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.94      0.92     20010\n",
      "           1       0.90      0.82      0.85     11786\n",
      "\n",
      "    accuracy                           0.90     31796\n",
      "   macro avg       0.90      0.88      0.89     31796\n",
      "weighted avg       0.90      0.90      0.90     31796\n",
      "\n",
      "acc:  0.8962762611649264\n",
      "pre:  0.8957478552778814\n",
      "rec:  0.8150347870354658\n",
      "ma F1:  0.8866044357525644\n",
      "mi F1:  0.8962762611649264\n",
      "we F1:  0.8951701359670018\n",
      "[[139  15]\n",
      " [104 837]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.90      0.70       154\n",
      "           1       0.98      0.89      0.93       941\n",
      "\n",
      "    accuracy                           0.89      1095\n",
      "   macro avg       0.78      0.90      0.82      1095\n",
      "weighted avg       0.92      0.89      0.90      1095\n",
      "\n",
      "acc:  0.891324200913242\n",
      "pre:  0.9823943661971831\n",
      "rec:  0.8894792773645058\n",
      "ma F1:  0.8169413377801442\n",
      "mi F1:  0.891324200913242\n",
      "we F1:  0.9008085487912445\n",
      "Loss:  0.05880235508084297\n",
      "Loss:  0.06274749338626862\n",
      "Loss:  0.06649507582187653\n",
      "24 **********\n",
      "epoch:  25\n",
      "Loss:  0.08611765503883362\n",
      "Loss:  0.049819011241197586\n",
      "Loss:  0.05942046642303467\n",
      "25 **********\n",
      "epoch:  26\n",
      "Loss:  0.06798264384269714\n",
      "Loss:  0.0746200755238533\n",
      "Loss:  0.05823804810643196\n",
      "26 **********\n",
      "epoch:  27\n",
      "Eval Loss:  0.06352150440216064\n",
      "Eval Loss:  0.0038836002349853516\n",
      "Eval Loss:  0.06208443641662598\n",
      "[[18557  1453]\n",
      " [ 1752 10034]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92     20010\n",
      "           1       0.87      0.85      0.86     11786\n",
      "\n",
      "    accuracy                           0.90     31796\n",
      "   macro avg       0.89      0.89      0.89     31796\n",
      "weighted avg       0.90      0.90      0.90     31796\n",
      "\n",
      "acc:  0.8992011573782865\n",
      "pre:  0.8735091842952903\n",
      "rec:  0.8513490582046496\n",
      "ma F1:  0.8913978556354456\n",
      "mi F1:  0.8992011573782865\n",
      "we F1:  0.898927405504403\n",
      "[[142  12]\n",
      " [105 836]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.92      0.71       154\n",
      "           1       0.99      0.89      0.93       941\n",
      "\n",
      "    accuracy                           0.89      1095\n",
      "   macro avg       0.78      0.91      0.82      1095\n",
      "weighted avg       0.93      0.89      0.90      1095\n",
      "\n",
      "acc:  0.8931506849315068\n",
      "pre:  0.9858490566037735\n",
      "rec:  0.8884165781083954\n",
      "ma F1:  0.8214148809084053\n",
      "mi F1:  0.8931506849315068\n",
      "we F1:  0.9027636961334499\n",
      "Loss:  0.0671481043100357\n",
      "Loss:  0.09752196073532104\n",
      "Loss:  0.04253216087818146\n",
      "27 **********\n",
      "epoch:  28\n",
      "Loss:  0.06713493168354034\n",
      "Loss:  0.05260350927710533\n",
      "Loss:  0.06255461275577545\n",
      "28 **********\n",
      "epoch:  29\n",
      "Loss:  0.08207020908594131\n",
      "Loss:  0.0656324103474617\n",
      "Loss:  0.05502256006002426\n",
      "29 **********\n",
      "epoch:  30\n",
      "Eval Loss:  0.03993880748748779\n",
      "Eval Loss:  0.0028913021087646484\n",
      "Eval Loss:  0.040235042572021484\n",
      "[[18368  1642]\n",
      " [ 1584 10202]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.92      0.92     20010\n",
      "           1       0.86      0.87      0.86     11786\n",
      "\n",
      "    accuracy                           0.90     31796\n",
      "   macro avg       0.89      0.89      0.89     31796\n",
      "weighted avg       0.90      0.90      0.90     31796\n",
      "\n",
      "acc:  0.8985406969430118\n",
      "pre:  0.8613644039175954\n",
      "rec:  0.8656032581028339\n",
      "ma F1:  0.8913759692528893\n",
      "mi F1:  0.8985406969430118\n",
      "we F1:  0.8985915852856553\n",
      "[[135  19]\n",
      " [ 81 860]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.88      0.73       154\n",
      "           1       0.98      0.91      0.95       941\n",
      "\n",
      "    accuracy                           0.91      1095\n",
      "   macro avg       0.80      0.90      0.84      1095\n",
      "weighted avg       0.93      0.91      0.91      1095\n",
      "\n",
      "acc:  0.908675799086758\n",
      "pre:  0.9783845278725825\n",
      "rec:  0.9139213602550478\n",
      "ma F1:  0.8373923373923373\n",
      "mi F1:  0.908675799086758\n",
      "we F1:  0.9147717640868326\n",
      "Subject 32 Current Train Acc:  0.8985406969430118 Current Test Acc:  0.908675799086758\n",
      "Loss:  0.06649740785360336\n",
      "Loss:  0.05486420542001724\n",
      "Loss:  0.12481953203678131\n",
      "30 **********\n",
      "epoch:  31\n",
      "Loss:  0.05435410514473915\n",
      "Loss:  0.04768827557563782\n",
      "Loss:  0.08024752885103226\n",
      "31 **********\n",
      "epoch:  32\n",
      "Loss:  0.08739414811134338\n",
      "Loss:  0.041284047067165375\n",
      "Loss:  0.0667402446269989\n",
      "32 **********\n",
      "epoch:  33\n",
      "Eval Loss:  0.049205780029296875\n",
      "Eval Loss:  0.00226593017578125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Loss:  0.035860419273376465\n",
      "[[18587  1423]\n",
      " [ 1812  9974]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.93      0.92     20010\n",
      "           1       0.88      0.85      0.86     11786\n",
      "\n",
      "    accuracy                           0.90     31796\n",
      "   macro avg       0.89      0.89      0.89     31796\n",
      "weighted avg       0.90      0.90      0.90     31796\n",
      "\n",
      "acc:  0.898257642470751\n",
      "pre:  0.8751425813810652\n",
      "rec:  0.8462582725267266\n",
      "ma F1:  0.8902008356095823\n",
      "mi F1:  0.898257642470751\n",
      "we F1:  0.8978937625370468\n",
      "[[149   5]\n",
      " [136 805]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.97      0.68       154\n",
      "           1       0.99      0.86      0.92       941\n",
      "\n",
      "    accuracy                           0.87      1095\n",
      "   macro avg       0.76      0.91      0.80      1095\n",
      "weighted avg       0.93      0.87      0.89      1095\n",
      "\n",
      "acc:  0.8712328767123287\n",
      "pre:  0.9938271604938271\n",
      "rec:  0.8554729011689692\n",
      "ma F1:  0.7991450378501579\n",
      "mi F1:  0.8712328767123287\n",
      "we F1:  0.88562846648511\n",
      "Loss:  0.056373752653598785\n",
      "Loss:  0.07374909520149231\n",
      "Loss:  0.042304445058107376\n",
      "33 **********\n",
      "epoch:  34\n",
      "Loss:  0.07365092635154724\n",
      "Loss:  0.05637642741203308\n",
      "Loss:  0.08656744658946991\n",
      "34 **********\n",
      "epoch:  35\n",
      "Loss:  0.06351137906312943\n",
      "Loss:  0.07817740738391876\n",
      "Loss:  0.06517394632101059\n",
      "35 **********\n",
      "epoch:  36\n",
      "Eval Loss:  0.041980862617492676\n",
      "Eval Loss:  0.0018696784973144531\n",
      "Eval Loss:  0.03207242488861084\n",
      "[[18708  1302]\n",
      " [ 1495 10291]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     20010\n",
      "           1       0.89      0.87      0.88     11786\n",
      "\n",
      "    accuracy                           0.91     31796\n",
      "   macro avg       0.91      0.90      0.91     31796\n",
      "weighted avg       0.91      0.91      0.91     31796\n",
      "\n",
      "acc:  0.91203296012077\n",
      "pre:  0.8876908479254723\n",
      "rec:  0.8731545901917529\n",
      "ma F1:  0.9054040485196941\n",
      "mi F1:  0.91203296012077\n",
      "we F1:  0.9118809606032448\n",
      "[[143  11]\n",
      " [ 93 848]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.93      0.73       154\n",
      "           1       0.99      0.90      0.94       941\n",
      "\n",
      "    accuracy                           0.91      1095\n",
      "   macro avg       0.80      0.91      0.84      1095\n",
      "weighted avg       0.93      0.91      0.91      1095\n",
      "\n",
      "acc:  0.9050228310502283\n",
      "pre:  0.9871944121071012\n",
      "rec:  0.9011689691817216\n",
      "ma F1:  0.8377777777777777\n",
      "mi F1:  0.9050228310502283\n",
      "we F1:  0.9128442415017757\n",
      "Loss:  0.07743847370147705\n",
      "Loss:  0.06630200147628784\n",
      "Loss:  0.08159822970628738\n",
      "36 **********\n",
      "epoch:  37\n",
      "Loss:  0.06293341517448425\n",
      "Loss:  0.05837307870388031\n",
      "Loss:  0.06456533074378967\n",
      "37 **********\n",
      "epoch:  38\n",
      "Loss:  0.059846825897693634\n",
      "Loss:  0.04570606350898743\n",
      "Loss:  0.056395191699266434\n",
      "38 **********\n",
      "epoch:  39\n",
      "Eval Loss:  0.060486555099487305\n",
      "Eval Loss:  0.0015807151794433594\n",
      "Eval Loss:  0.0488356351852417\n",
      "[[18788  1222]\n",
      " [ 1542 10244]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93     20010\n",
      "           1       0.89      0.87      0.88     11786\n",
      "\n",
      "    accuracy                           0.91     31796\n",
      "   macro avg       0.91      0.90      0.91     31796\n",
      "weighted avg       0.91      0.91      0.91     31796\n",
      "\n",
      "acc:  0.913070826519059\n",
      "pre:  0.8934240362811792\n",
      "rec:  0.8691668080773799\n",
      "ma F1:  0.9063054523391019\n",
      "mi F1:  0.913070826519059\n",
      "we F1:  0.9128174417183117\n",
      "[[150   4]\n",
      " [129 812]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.97      0.69       154\n",
      "           1       1.00      0.86      0.92       941\n",
      "\n",
      "    accuracy                           0.88      1095\n",
      "   macro avg       0.77      0.92      0.81      1095\n",
      "weighted avg       0.93      0.88      0.89      1095\n",
      "\n",
      "acc:  0.8785388127853881\n",
      "pre:  0.9950980392156863\n",
      "rec:  0.8629117959617428\n",
      "ma F1:  0.8085717177479459\n",
      "mi F1:  0.8785388127853881\n",
      "we F1:  0.8917501222713101\n",
      "Loss:  0.04852306470274925\n",
      "Loss:  0.05172892287373543\n",
      "Loss:  0.04363599419593811\n",
      "39 **********\n",
      "epoch:  40\n",
      "Loss:  0.08434353768825531\n",
      "Loss:  0.06553241610527039\n",
      "Loss:  0.10842215269804001\n",
      "40 **********\n",
      "epoch:  41\n",
      "Loss:  0.0811123251914978\n",
      "Loss:  0.059510502964258194\n",
      "Loss:  0.08276595920324326\n",
      "41 **********\n",
      "epoch:  42\n",
      "Eval Loss:  0.11126422882080078\n",
      "Eval Loss:  0.0017042160034179688\n",
      "Eval Loss:  0.06588268280029297\n",
      "[[18984  1026]\n",
      " [ 1914  9872]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93     20010\n",
      "           1       0.91      0.84      0.87     11786\n",
      "\n",
      "    accuracy                           0.91     31796\n",
      "   macro avg       0.91      0.89      0.90     31796\n",
      "weighted avg       0.91      0.91      0.91     31796\n",
      "\n",
      "acc:  0.9075355390615172\n",
      "pre:  0.9058542851899432\n",
      "rec:  0.8376039368742576\n",
      "ma F1:  0.8992623227726206\n",
      "mi F1:  0.9075355390615172\n",
      "we F1:  0.9067292818112386\n",
      "[[150   4]\n",
      " [136 805]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.97      0.68       154\n",
      "           1       1.00      0.86      0.92       941\n",
      "\n",
      "    accuracy                           0.87      1095\n",
      "   macro avg       0.76      0.91      0.80      1095\n",
      "weighted avg       0.93      0.87      0.89      1095\n",
      "\n",
      "acc:  0.8721461187214612\n",
      "pre:  0.9950556242274413\n",
      "rec:  0.8554729011689692\n",
      "ma F1:  0.800909090909091\n",
      "mi F1:  0.8721461187214612\n",
      "we F1:  0.886502283105023\n",
      "Loss:  0.05813651159405708\n",
      "Loss:  0.06410878896713257\n",
      "Loss:  0.0591513030230999\n",
      "42 **********\n",
      "epoch:  43\n",
      "Loss:  0.045877669006586075\n",
      "Loss:  0.05704038217663765\n",
      "Loss:  0.0490996353328228\n",
      "43 **********\n",
      "epoch:  44\n",
      "Loss:  0.05423362925648689\n",
      "Loss:  0.06541691720485687\n",
      "Loss:  0.056647416204214096\n",
      "44 **********\n",
      "epoch:  45\n",
      "Eval Loss:  0.09378135204315186\n",
      "Eval Loss:  0.0013823509216308594\n",
      "Eval Loss:  0.0999901294708252\n",
      "[[18918  1092]\n",
      " [ 1703 10083]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.95      0.93     20010\n",
      "           1       0.90      0.86      0.88     11786\n",
      "\n",
      "    accuracy                           0.91     31796\n",
      "   macro avg       0.91      0.90      0.90     31796\n",
      "weighted avg       0.91      0.91      0.91     31796\n",
      "\n",
      "acc:  0.9120958611146056\n",
      "pre:  0.9022818791946309\n",
      "rec:  0.8555065331749533\n",
      "ma F1:  0.9047410060007488\n",
      "mi F1:  0.9120958611146056\n",
      "we F1:  0.9115872231435173\n",
      "[[141  13]\n",
      " [ 90 851]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.92      0.73       154\n",
      "           1       0.98      0.90      0.94       941\n",
      "\n",
      "    accuracy                           0.91      1095\n",
      "   macro avg       0.80      0.91      0.84      1095\n",
      "weighted avg       0.93      0.91      0.91      1095\n",
      "\n",
      "acc:  0.9059360730593607\n",
      "pre:  0.9849537037037037\n",
      "rec:  0.9043570669500531\n",
      "ma F1:  0.8377019102780876\n",
      "mi F1:  0.9059360730593607\n",
      "we F1:  0.9133361160652171\n",
      "Loss:  0.05263937637209892\n",
      "Loss:  0.05554932355880737\n",
      "Loss:  0.03894263133406639\n",
      "45 **********\n",
      "epoch:  46\n",
      "Loss:  0.03952513635158539\n",
      "Loss:  0.04071226343512535\n",
      "Loss:  0.04571663215756416\n",
      "46 **********\n",
      "epoch:  47\n",
      "Loss:  0.061537936329841614\n",
      "Loss:  0.054836466908454895\n",
      "Loss:  0.07553961873054504\n",
      "47 **********\n",
      "epoch:  48\n",
      "Eval Loss:  0.02357637882232666\n",
      "Eval Loss:  0.0012760162353515625\n",
      "Eval Loss:  0.17392456531524658\n",
      "[[18607  1403]\n",
      " [ 1336 10450]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93     20010\n",
      "           1       0.88      0.89      0.88     11786\n",
      "\n",
      "    accuracy                           0.91     31796\n",
      "   macro avg       0.91      0.91      0.91     31796\n",
      "weighted avg       0.91      0.91      0.91     31796\n",
      "\n",
      "acc:  0.9138570889420052\n",
      "pre:  0.8816333417700161\n",
      "rec:  0.8866451722382488\n",
      "ma F1:  0.9077883008579752\n",
      "mi F1:  0.9138570889420051\n",
      "we F1:  0.9139069367784193\n",
      "[[145   9]\n",
      " [ 86 855]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.94      0.75       154\n",
      "           1       0.99      0.91      0.95       941\n",
      "\n",
      "    accuracy                           0.91      1095\n",
      "   macro avg       0.81      0.93      0.85      1095\n",
      "weighted avg       0.94      0.91      0.92      1095\n",
      "\n",
      "acc:  0.91324200913242\n",
      "pre:  0.9895833333333334\n",
      "rec:  0.9086078639744952\n",
      "ma F1:  0.8503075871496925\n",
      "mi F1:  0.91324200913242\n",
      "we F1:  0.9200672915164624\n",
      "Subject 32 Current Train Acc:  0.9138570889420052 Current Test Acc:  0.91324200913242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.04614965245127678\n",
      "Loss:  0.0459587462246418\n",
      "Loss:  0.04435030370950699\n",
      "48 **********\n",
      "epoch:  49\n",
      "Loss:  0.0886215791106224\n",
      "Loss:  0.06392153352499008\n",
      "Loss:  0.04211891442537308\n",
      "49 **********\n",
      "epoch:  50\n",
      "Loss:  0.027211979031562805\n",
      "Loss:  0.07490206509828568\n",
      "Loss:  0.028272751718759537\n",
      "50 **********\n",
      "epoch:  51\n",
      "Eval Loss:  0.02182137966156006\n",
      "Eval Loss:  0.0010123252868652344\n",
      "Eval Loss:  0.12873291969299316\n",
      "[[18846  1164]\n",
      " [ 1460 10326]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     20010\n",
      "           1       0.90      0.88      0.89     11786\n",
      "\n",
      "    accuracy                           0.92     31796\n",
      "   macro avg       0.91      0.91      0.91     31796\n",
      "weighted avg       0.92      0.92      0.92     31796\n",
      "\n",
      "acc:  0.9174738960875581\n",
      "pre:  0.8986945169712793\n",
      "rec:  0.8761242151705413\n",
      "ma F1:  0.9110900156166184\n",
      "mi F1:  0.9174738960875581\n",
      "we F1:  0.9172521086909151\n",
      "[[147   7]\n",
      " [109 832]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.95      0.72       154\n",
      "           1       0.99      0.88      0.93       941\n",
      "\n",
      "    accuracy                           0.89      1095\n",
      "   macro avg       0.78      0.92      0.83      1095\n",
      "weighted avg       0.93      0.89      0.90      1095\n",
      "\n",
      "acc:  0.8940639269406393\n",
      "pre:  0.9916567342073898\n",
      "rec:  0.8841657810839533\n",
      "ma F1:  0.8259523157029323\n",
      "mi F1:  0.8940639269406392\n",
      "we F1:  0.9042060938694657\n",
      "Loss:  0.052292775362730026\n",
      "Loss:  0.0662481039762497\n",
      "Loss:  0.046530622988939285\n",
      "51 **********\n",
      "epoch:  52\n",
      "Loss:  0.028518274426460266\n",
      "Loss:  0.05233093723654747\n",
      "Loss:  0.06194566562771797\n",
      "52 **********\n",
      "epoch:  53\n",
      "Loss:  0.08950117975473404\n",
      "Loss:  0.05258992314338684\n",
      "Loss:  0.07980836182832718\n",
      "53 **********\n",
      "epoch:  54\n",
      "Eval Loss:  0.017827510833740234\n",
      "Eval Loss:  0.0008504390716552734\n",
      "Eval Loss:  0.3141825199127197\n",
      "[[18424  1586]\n",
      " [ 1089 10697]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93     20010\n",
      "           1       0.87      0.91      0.89     11786\n",
      "\n",
      "    accuracy                           0.92     31796\n",
      "   macro avg       0.91      0.91      0.91     31796\n",
      "weighted avg       0.92      0.92      0.92     31796\n",
      "\n",
      "acc:  0.9158699207447477\n",
      "pre:  0.870878449890092\n",
      "rec:  0.9076022399456983\n",
      "ma F1:  0.9105895407958232\n",
      "mi F1:  0.9158699207447477\n",
      "we F1:  0.9162095543457074\n",
      "[[147   7]\n",
      " [101 840]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.95      0.73       154\n",
      "           1       0.99      0.89      0.94       941\n",
      "\n",
      "    accuracy                           0.90      1095\n",
      "   macro avg       0.79      0.92      0.84      1095\n",
      "weighted avg       0.94      0.90      0.91      1095\n",
      "\n",
      "acc:  0.9013698630136986\n",
      "pre:  0.9917355371900827\n",
      "rec:  0.8926673751328374\n",
      "ma F1:  0.8354702995091656\n",
      "mi F1:  0.9013698630136986\n",
      "we F1:  0.9103086205453381\n",
      "Loss:  0.03575027361512184\n",
      "Loss:  0.039526112377643585\n",
      "Loss:  0.042391255497932434\n",
      "54 **********\n",
      "epoch:  55\n",
      "Loss:  0.040685467422008514\n",
      "Loss:  0.03709189593791962\n",
      "Loss:  0.04050271213054657\n",
      "55 **********\n",
      "epoch:  56\n",
      "Loss:  0.0350634939968586\n",
      "Loss:  0.051810186356306076\n",
      "Loss:  0.04184896498918533\n",
      "56 **********\n",
      "epoch:  57\n",
      "Eval Loss:  0.04751002788543701\n",
      "Eval Loss:  0.0014519691467285156\n",
      "Eval Loss:  0.364501953125\n",
      "[[18791  1219]\n",
      " [ 1437 10349]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.94      0.93     20010\n",
      "           1       0.89      0.88      0.89     11786\n",
      "\n",
      "    accuracy                           0.92     31796\n",
      "   macro avg       0.91      0.91      0.91     31796\n",
      "weighted avg       0.92      0.92      0.92     31796\n",
      "\n",
      "acc:  0.916467480186187\n",
      "pre:  0.8946230982019364\n",
      "rec:  0.8780756830137452\n",
      "ma F1:  0.9101324510615125\n",
      "mi F1:  0.916467480186187\n",
      "we F1:  0.9163038890526667\n",
      "[[143  11]\n",
      " [ 78 863]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.93      0.76       154\n",
      "           1       0.99      0.92      0.95       941\n",
      "\n",
      "    accuracy                           0.92      1095\n",
      "   macro avg       0.82      0.92      0.86      1095\n",
      "weighted avg       0.94      0.92      0.92      1095\n",
      "\n",
      "acc:  0.9187214611872146\n",
      "pre:  0.9874141876430206\n",
      "rec:  0.9171094580233794\n",
      "ma F1:  0.8568154269972452\n",
      "mi F1:  0.9187214611872145\n",
      "we F1:  0.9244821615910033\n",
      "Subject 32 Current Train Acc:  0.916467480186187 Current Test Acc:  0.9187214611872146\n",
      "Loss:  0.026987973600625992\n",
      "Loss:  0.04917248338460922\n",
      "Loss:  0.04605105519294739\n",
      "57 **********\n",
      "epoch:  58\n",
      "Loss:  0.05444490537047386\n",
      "Loss:  0.03242306411266327\n",
      "Loss:  0.06428102403879166\n",
      "58 **********\n",
      "epoch:  59\n",
      "Loss:  0.048429716378450394\n",
      "Loss:  0.050557177513837814\n",
      "Loss:  0.039351340383291245\n",
      "59 **********\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Subject:  32 Training Time 5815.797738790512 Best Test Acc:  0.9187214611872146\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "final_acc=[]\n",
    "for fold in range(1,33):\n",
    "    begin=time()\n",
    "    train_dtlist,test_dtlist=getTrainTestList(reclist,opt='leaveonepatient',fold=fold)#fold 1-10\n",
    "    print('*********')\n",
    "    print(len(train_dtlist),len(test_dtlist))\n",
    "    train_dtlist=[file for file in train_dtlist if dtclean(file)==0]\n",
    "    np.random.shuffle(train_dtlist)\n",
    "    test_dtlist=[file for file in test_dtlist if dtclean(file)==0]\n",
    "    print(len(train_dtlist),len(test_dtlist))\n",
    "    \n",
    "    train_dataset = ApneaECGDataset(train_dtlist,istrain=False)\n",
    "    #test_dataset = ApneaECGDataset(test_dtlist,istrain=False)\n",
    "    train_loader=DataLoader(train_dataset, batch_size=64, shuffle=True, sampler=None, num_workers=0)\n",
    "    #test_loader=DataLoader(test_dataset, batch_size=256, shuffle=False, sampler=None, num_workers=0)\n",
    "    len(train_dataset),len(train_loader)#,len(test_dataset)\n",
    "    \n",
    "    model=MyLSTM(2,100).to(device)\n",
    "    #model=alexnet(num_classes=2)\n",
    "    #model.features[0]=nn.Conv2d(1, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            if(m.in_channels!=m.out_channels or m.out_channels!=m.groups or m.bias is not None):\n",
    "                # don't want to reinitialize downsample layers, code assuming normal conv layers will not have these characteristics\n",
    "                #nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                print(m,'init')\n",
    "            else:\n",
    "                print('Not initializing')\n",
    "    criterion=nn.CrossEntropyLoss().to(device)#weight=torch.FloatTensor([1,1.5])\n",
    "    #criterion = CircleLoss(m=0.25, gamma=256)\n",
    "    opt=torch.optim.Adam(model.parameters(), lr=0.0003, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001)\n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt,T_max=20,eta_min=0.000005)\n",
    "    loss_list=[]\n",
    "    max_test=0\n",
    "    opt.zero_grad()\n",
    "    epoch_flag=0\n",
    "    test_acc=0\n",
    "    for epoch in range(60):\n",
    "        print('epoch: ', epoch)\n",
    "        if epoch_flag%3==0:# and epoch_flag!=0:\n",
    "            model.eval()\n",
    "            tmp_train_acc=eval(model,train_dtlist,criterion)\n",
    "            #eval(model,train_dtlist[0:int(len(train_dtlist)*0.9)],criterion)\n",
    "            tmp_test_acc=eval(model,test_dtlist,criterion)\n",
    "\n",
    "            if tmp_test_acc >test_acc:\n",
    "                test_acc=tmp_test_acc.copy()\n",
    "                torch.save(model.state_dict(),'LeaveOneParamSub_%.0f.pkl'%(fold))\n",
    "                print('Subject',fold,'Current Train Acc: ',tmp_train_acc,'Current Test Acc: ',test_acc)\n",
    "            model.train()\n",
    "\n",
    "        flag=0\n",
    "        accum_flag=0\n",
    "        for fe, label in train_loader:\n",
    "            fe=fe.float().to(device)\n",
    "                #fe=(fe-fe_mean)/fe_std\n",
    "                #fe=torch.stft(fe,100,hop_length=50,onesided=True)\n",
    "\n",
    "            label=label.long().to(device)\n",
    "            pred_prob_no_softmax=model(fe)\n",
    "            loss=criterion(pred_prob_no_softmax,label)/4#/256\n",
    "            del label\n",
    "            del fe\n",
    "            #inp_sp, inp_sn = convert_label_to_similarity(pred_prob_no_softmax,label)\n",
    "            #loss = criterion(inp_sp, inp_sn)/256\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "            loss.backward()\n",
    "            accum_flag+=1\n",
    "            if accum_flag%4==0:\n",
    "                opt.step()\n",
    "                accum_flag=0\n",
    "                opt.zero_grad()\n",
    "            flag+=1\n",
    "            if flag%128==0:\n",
    "                print(\"Loss: \", loss.item())\n",
    "\n",
    "            #print('step')\n",
    "        print(epoch_flag,'*'*10)\n",
    "        epoch_flag+=1\n",
    "        #scheduler.step()\n",
    "\n",
    "    plt.plot(loss_list)\n",
    "    plt.show()\n",
    "    plt.close('all')\n",
    "    print('------')\n",
    "    print('Subject: ', fold, 'Training Time', time()-begin,'Best Test Acc: ',test_acc)\n",
    "    final_acc.append(test_acc)\n",
    "    print(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <!-- \n",
    "# [[ 63  82]\n",
    "#  [ 68 785]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.48      0.43      0.46       145\n",
    "#            1       0.91      0.92      0.91       853\n",
    "\n",
    "#     accuracy                           0.85       998\n",
    "#    macro avg       0.69      0.68      0.68       998\n",
    "# weighted avg       0.84      0.85      0.85       998\n",
    "\n",
    "# acc:  0.8496993987975952\n",
    "# pre:  0.9054209919261822\n",
    "# rec:  0.9202813599062134\n",
    "# ma F1:  0.6846562184024267\n",
    "# mi F1:  0.8496993987975952\n",
    "# we F1:  0.8464991155212345\n",
    "# Subject 1 Current Train Acc:  0.900573793622425 Current Test Acc:  0.8496993987975\n",
    "        \n",
    "# [[120  39]\n",
    "#  [469 390]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.20      0.75      0.32       159\n",
    "#            1       0.91      0.45      0.61       859\n",
    "\n",
    "#     accuracy                           0.50      1018\n",
    "#    macro avg       0.56      0.60      0.46      1018\n",
    "# weighted avg       0.80      0.50      0.56      1018\n",
    "\n",
    "# acc:  0.5009823182711198\n",
    "# pre:  0.9090909090909091\n",
    "# rec:  0.4540162980209546\n",
    "# ma F1:  0.46322283854253166\n",
    "# mi F1:  0.5009823182711198\n",
    "# we F1:  0.5611177859870196\n",
    "# Subject 2 Current Train Acc:  0.919900856524331 Current Test Acc:  0.5009823182711\n",
    "        \n",
    "# [[288  65]\n",
    "#  [ 77 576]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.79      0.82      0.80       353\n",
    "#            1       0.90      0.88      0.89       653\n",
    "\n",
    "#     accuracy                           0.86      1006\n",
    "#    macro avg       0.84      0.85      0.85      1006\n",
    "# weighted avg       0.86      0.86      0.86      1006\n",
    "\n",
    "# acc:  0.8588469184890656\n",
    "# pre:  0.8985959438377535\n",
    "# rec:  0.8820826952526799\n",
    "# ma F1:  0.8462455817077319\n",
    "# mi F1:  0.8588469184890656\n",
    "# we F1:  0.8593719741882879\n",
    "# Subject 3 Current Train Acc:  0.9203387172651717 Current Test Acc:  0.858846918489\n",
    "        \n",
    "# [[ 54   6]\n",
    "#  [148 523]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.27      0.90      0.41        60\n",
    "#            1       0.99      0.78      0.87       671\n",
    "\n",
    "#     accuracy                           0.79       731\n",
    "#    macro avg       0.63      0.84      0.64       731\n",
    "# weighted avg       0.93      0.79      0.83       731\n",
    "\n",
    "# acc:  0.7893296853625171\n",
    "# pre:  0.9886578449905482\n",
    "# rec:  0.7794336810730254\n",
    "# ma F1:  0.6419402035623409\n",
    "# mi F1:  0.7893296853625171\n",
    "# we F1:  0.8339550721762164\n",
    "# Subject 4 Current Train Acc:  0.9038246268656717 Current Test Acc:  0.789329685362\n",
    "        \n",
    "# [[849 210]\n",
    "#  [356 575]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.70      0.80      0.75      1059\n",
    "#            1       0.73      0.62      0.67       931\n",
    "\n",
    "#     accuracy                           0.72      1990\n",
    "#    macro avg       0.72      0.71      0.71      1990\n",
    "# weighted avg       0.72      0.72      0.71      1990\n",
    "\n",
    "# acc:  0.7155778894472362\n",
    "# pre:  0.732484076433121\n",
    "# rec:  0.6176154672395274\n",
    "# ma F1:  0.7100815850815851\n",
    "# mi F1:  0.7155778894472362\n",
    "# we F1:  0.7126492017195536\n",
    "# Subject 5 Current Train Acc:  0.8792595708876736 Current Test Acc:  0.715577889447\n",
    "        \n",
    "# [[531  71]\n",
    "#  [107 299]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.83      0.88      0.86       602\n",
    "#            1       0.81      0.74      0.77       406\n",
    "\n",
    "#     accuracy                           0.82      1008\n",
    "#    macro avg       0.82      0.81      0.81      1008\n",
    "# weighted avg       0.82      0.82      0.82      1008\n",
    "\n",
    "# acc:  0.8234126984126984\n",
    "# pre:  0.8081081081081081\n",
    "# rec:  0.7364532019704434\n",
    "# ma F1:  0.8135350848021283\n",
    "# mi F1:  0.8234126984126985\n",
    "# we F1:  0.8218799652662304\n",
    "# Subject 6 Current Train Acc:  0.9191732271116269 Current Test Acc:  0.823412698412\n",
    "\n",
    "# [[18018  1472]\n",
    "#  [ 1119 10265]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.94      0.92      0.93     19490\n",
    "#            1       0.87      0.90      0.89     11384\n",
    "\n",
    "#     accuracy                           0.92     30874\n",
    "#    macro avg       0.91      0.91      0.91     30874\n",
    "# weighted avg       0.92      0.92      0.92     30874\n",
    "\n",
    "# acc:  0.9160782535466736\n",
    "# pre:  0.874584646843316\n",
    "# rec:  0.9017041461700632\n",
    "# ma F1:  0.9104299700336314\n",
    "# mi F1:  0.9160782535466736\n",
    "# we F1:  0.9163354242006274\n",
    "# [[ 456  218]\n",
    "#  [ 155 1188]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.75      0.68      0.71       674\n",
    "#            1       0.84      0.88      0.86      1343\n",
    "\n",
    "#     accuracy                           0.82      2017\n",
    "#    macro avg       0.80      0.78      0.79      2017\n",
    "# weighted avg       0.81      0.82      0.81      2017\n",
    "\n",
    "# acc:  0.8150718889439762\n",
    "# pre:  0.844950213371266\n",
    "# rec:  0.8845867460908414\n",
    "# ma F1:  0.7870209612834098\n",
    "# mi F1:  0.8150718889439762\n",
    "# we F1:  0.8126576697600749\n",
    "# Subject 7 Current Train Acc:  0.9160782535466736 Current Test Acc:  0.815071888943\n",
    "\n",
    "# [[727  83]\n",
    "#  [256 439]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.74      0.90      0.81       810\n",
    "#            1       0.84      0.63      0.72       695\n",
    "\n",
    "#     accuracy                           0.77      1505\n",
    "#    macro avg       0.79      0.76      0.77      1505\n",
    "# weighted avg       0.79      0.77      0.77      1505\n",
    "\n",
    "# acc:  0.774750830564784\n",
    "# pre:  0.8409961685823755\n",
    "# rec:  0.6316546762589929\n",
    "# ma F1:  0.7661887895087305\n",
    "# mi F1:  0.774750830564784\n",
    "# we F1:  0.7696076600693075\n",
    "# Subject 8 Current Train Acc:  0.9239788440706047 Current Test Acc:  0.774750830564\n",
    "\n",
    "# [[156   9]\n",
    "#  [424 395]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.27      0.95      0.42       165\n",
    "#            1       0.98      0.48      0.65       819\n",
    "\n",
    "#     accuracy                           0.56       984\n",
    "#    macro avg       0.62      0.71      0.53       984\n",
    "# weighted avg       0.86      0.56      0.61       984\n",
    "\n",
    "# acc:  0.5599593495934959\n",
    "# pre:  0.9777227722772277\n",
    "# rec:  0.4822954822954823\n",
    "# ma F1:  0.5323722609712062\n",
    "# mi F1:  0.5599593495934959\n",
    "# we F1:  0.6078616164480992\n",
    "# Subject 9 Current Train Acc:  0.9102704735637948 Current Test Acc:  0.559959349593\n",
    "\n",
    "# [[221  23]\n",
    "#  [130  92]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.63      0.91      0.74       244\n",
    "#            1       0.80      0.41      0.55       222\n",
    "\n",
    "#     accuracy                           0.67       466\n",
    "#    macro avg       0.71      0.66      0.64       466\n",
    "# weighted avg       0.71      0.67      0.65       466\n",
    "\n",
    "# acc:  0.6716738197424893\n",
    "# pre:  0.8\n",
    "# rec:  0.4144144144144144\n",
    "# ma F1:  0.6444256040695209\n",
    "# mi F1:  0.6716738197424893\n",
    "# we F1:  0.6490725865873914\n",
    "# Subject 10 Current Train Acc:  0.915065535851966 Current Test Acc:  0.671673819742\n",
    "\n",
    "# [[ 125   90]\n",
    "#  [ 187 1092]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.40      0.58      0.47       215\n",
    "#            1       0.92      0.85      0.89      1279\n",
    "\n",
    "#     accuracy                           0.81      1494\n",
    "#    macro avg       0.66      0.72      0.68      1494\n",
    "# weighted avg       0.85      0.81      0.83      1494\n",
    "\n",
    "# acc:  0.8145917001338688\n",
    "# pre:  0.9238578680203046\n",
    "# rec:  0.8537920250195465\n",
    "# ma F1:  0.680913715055434\n",
    "# mi F1:  0.8145917001338688\n",
    "# we F1:  0.828000970612264\n",
    "# Subject 11 Current Train Acc:  0.8126891104245628 Current Test Acc:  0.81459170013\n",
    "# [[751  46]\n",
    "#  [ 72 142]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.91      0.94      0.93       797\n",
    "#            1       0.76      0.66      0.71       214\n",
    "\n",
    "#     accuracy                           0.88      1011\n",
    "#    macro avg       0.83      0.80      0.82      1011\n",
    "# weighted avg       0.88      0.88      0.88      1011\n",
    "\n",
    "# acc:  0.8832838773491593\n",
    "# pre:  0.7553191489361702\n",
    "# rec:  0.6635514018691588\n",
    "# ma F1:  0.8168140777593514\n",
    "# mi F1:  0.8832838773491593\n",
    "# we F1:  0.8804460862336667\n",
    "# Subject 12 Current Train Acc:  0.9123902132998746 Current Test Acc:  0.88328387734\n",
    "\n",
    "# [[ 729  169]\n",
    "#  [ 126 1010]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.85      0.81      0.83       898\n",
    "#            1       0.86      0.89      0.87      1136\n",
    "\n",
    "#     accuracy                           0.85      2034\n",
    "#    macro avg       0.85      0.85      0.85      2034\n",
    "# weighted avg       0.85      0.85      0.85      2034\n",
    "\n",
    "# acc:  0.8549655850540806\n",
    "# pre:  0.8566581849024597\n",
    "# rec:  0.8890845070422535\n",
    "# ma F1:  0.852143625429532\n",
    "# mi F1:  0.8549655850540806\n",
    "# we F1:  0.8545337549335981\n",
    "# Subject 13 Current Train Acc:  0.9207635220533429 Current Test Acc:  0.85496558505\n",
    "\n",
    "# [[917   4]\n",
    "#  [ 31   0]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.97      1.00      0.98       921\n",
    "#            1       0.00      0.00      0.00        31\n",
    "\n",
    "#     accuracy                           0.96       952\n",
    "#    macro avg       0.48      0.50      0.49       952\n",
    "# weighted avg       0.94      0.96      0.95       952\n",
    "\n",
    "# acc:  0.9632352941176471\n",
    "# pre:  0.0\n",
    "# rec:  0.0\n",
    "# ma F1:  0.49063670411985016\n",
    "# mi F1:  0.9632352941176471\n",
    "# we F1:  0.9493201775092059\n",
    "# Subject 14 Current Train Acc:  0.9017189016562823 Current Test Acc:  0.96323529411\n",
    "\n",
    "# [[1608   24]\n",
    "#  [ 299   52]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.84      0.99      0.91      1632\n",
    "#            1       0.68      0.15      0.24       351\n",
    "\n",
    "#     accuracy                           0.84      1983\n",
    "#    macro avg       0.76      0.57      0.58      1983\n",
    "# weighted avg       0.82      0.84      0.79      1983\n",
    "\n",
    "# acc:  0.8371154815935451\n",
    "# pre:  0.6842105263157895\n",
    "# rec:  0.14814814814814814\n",
    "# ma F1:  0.5761454994960802\n",
    "# mi F1:  0.8371154815935451\n",
    "# we F1:  0.7909928947832605\n",
    "# Subject 15 Current Train Acc:  0.8769574220266597 Current Test Acc:  0.83711548159\n",
    "\n",
    "# [[808  12]\n",
    "#  [  9   1]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.99      0.99       820\n",
    "#            1       0.08      0.10      0.09        10\n",
    "\n",
    "#     accuracy                           0.97       830\n",
    "#    macro avg       0.53      0.54      0.54       830\n",
    "# weighted avg       0.98      0.97      0.98       830\n",
    "\n",
    "# acc:  0.9746987951807229\n",
    "# pre:  0.07692307692307693\n",
    "# rec:  0.1\n",
    "# ma F1:  0.5370640886032243\n",
    "# mi F1:  0.9746987951807229\n",
    "# we F1:  0.9763256900007135\n",
    "# Subject 16 Current Train Acc:  0.8277346308599233 Current Test Acc:  0.97469879518\n",
    "\n",
    "# [[816   4]\n",
    "#  [ 70   0]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.92      1.00      0.96       820\n",
    "#            1       0.00      0.00      0.00        70\n",
    "\n",
    "#     accuracy                           0.92       890\n",
    "#    macro avg       0.46      0.50      0.48       890\n",
    "# weighted avg       0.85      0.92      0.88       890\n",
    "\n",
    "# acc:  0.9168539325842696\n",
    "# pre:  0.0\n",
    "# rec:  0.0\n",
    "# ma F1:  0.4783118405627198\n",
    "# mi F1:  0.9168539325842696\n",
    "# we F1:  0.8813836163178207\n",
    "# Subject 17 Current Train Acc:  0.6041998687541015 Current Test Acc:  0.91685393258\n",
    "\n",
    "# [[952]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       1.00      1.00      1.00       952\n",
    "\n",
    "#     accuracy                           1.00       952\n",
    "#    macro avg       1.00      1.00      1.00       952\n",
    "# weighted avg       1.00      1.00      1.00       952\n",
    "\n",
    "# acc:  1.0\n",
    "# pre:  0.0\n",
    "# rec:  0.0\n",
    "# ma F1:  1.0\n",
    "# mi F1:  1.0\n",
    "# we F1:  1.0\n",
    "# Subject 18 Current Train Acc:  0.6013337925420332 Current Test Acc:  1.0\n",
    "\n",
    "# [[938  10]\n",
    "#  [  2   1]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       1.00      0.99      0.99       948\n",
    "#            1       0.09      0.33      0.14         3\n",
    "\n",
    "#     accuracy                           0.99       951\n",
    "#    macro avg       0.54      0.66      0.57       951\n",
    "# weighted avg       1.00      0.99      0.99       951\n",
    "\n",
    "# acc:  0.9873817034700315\n",
    "# pre:  0.09090909090909091\n",
    "# rec:  0.3333333333333333\n",
    "# ma F1:  0.5682506053268765\n",
    "# mi F1:  0.9873817034700315\n",
    "# we F1:  0.9909601973709337\n",
    "# Subject 19 Current Train Acc:  0.843143393863494 Current Test Acc:  0.987381703470\n",
    "\n",
    "# [[930]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       1.00      1.00      1.00       930\n",
    "\n",
    "#     accuracy                           1.00       930\n",
    "#    macro avg       1.00      1.00      1.00       930\n",
    "# weighted avg       1.00      1.00      1.00       930\n",
    "\n",
    "# acc:  1.0\n",
    "# pre:  0.0\n",
    "# rec:  0.0\n",
    "# ma F1:  1.0\n",
    "# mi F1:  1.0\n",
    "# we F1:  1.0\n",
    "# Subject 20 Current Train Acc:  0.601795938800413 Current Test Acc:  1.0\n",
    "\n",
    "# [[930   2]\n",
    "#  [  0   0]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       1.00      1.00      1.00       932\n",
    "#          1.0       0.00      0.00      0.00         0\n",
    "\n",
    "#     accuracy                           1.00       932\n",
    "#    macro avg       0.50      0.50      0.50       932\n",
    "# weighted avg       1.00      1.00      1.00       932\n",
    "\n",
    "# acc:  0.9978540772532188\n",
    "# pre:  0.0\n",
    "# rec:  0.0\n",
    "# ma F1:  0.49946294307196565\n",
    "# mi F1:  0.9978540772532188\n",
    "# we F1:  0.9989258861439313\n",
    "# Subject 21 Current Train Acc:  0.913670640508151 Current Test Acc:  0.997854077253\n",
    "\n",
    "# [[735   0]\n",
    "#  [  4   0]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      1.00      1.00       735\n",
    "#            1       0.00      0.00      0.00         4\n",
    "\n",
    "#     accuracy                           0.99       739\n",
    "#    macro avg       0.50      0.50      0.50       739\n",
    "# weighted avg       0.99      0.99      0.99       739\n",
    "\n",
    "# acc:  0.9945872801082544\n",
    "# pre:  0.0\n",
    "# rec:  0.0\n",
    "# ma F1:  0.4986431478968792\n",
    "# mi F1:  0.9945872801082544\n",
    "# we F1:  0.9918882644227502\n",
    "# Subject 22 Current Train Acc:  0.604192585220204 Current Test Acc:  0.994587280108\n",
    "\n",
    "# [[467   0]\n",
    "#  [  1   0]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       1.00      1.00      1.00       467\n",
    "#            1       0.00      0.00      0.00         1\n",
    "\n",
    "#     accuracy                           1.00       468\n",
    "#    macro avg       0.50      0.50      0.50       468\n",
    "# weighted avg       1.00      1.00      1.00       468\n",
    "\n",
    "# acc:  0.9978632478632479\n",
    "# pre:  0.0\n",
    "# rec:  0.0\n",
    "# ma F1:  0.4994652406417112\n",
    "# mi F1:  0.9978632478632479\n",
    "# we F1:  0.9967960144430733\n",
    "# Subject 23 Current Train Acc:  0.6074391635567344 Current Test Acc:  0.99786324786\n",
    "\n",
    "# [[586   0]\n",
    "#  [  2   0]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       1.00      1.00      1.00       586\n",
    "#            1       0.00      0.00      0.00         2\n",
    "\n",
    "#     accuracy                           1.00       588\n",
    "#    macro avg       0.50      0.50      0.50       588\n",
    "# weighted avg       0.99      1.00      0.99       588\n",
    "\n",
    "# acc:  0.9965986394557823\n",
    "# pre:  0.0\n",
    "# rec:  0.0\n",
    "# ma F1:  0.4991482112436116\n",
    "# mi F1:  0.9965986394557823\n",
    "# we F1:  0.9949008564243415\n",
    "# Subject 24 Current Train Acc:  0.6060737392811814 Current Test Acc:  0.99659863945\n",
    "\n",
    "# [[859  13]\n",
    "#  [  3   0]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       1.00      0.99      0.99       872\n",
    "#            1       0.00      0.00      0.00         3\n",
    "\n",
    "#     accuracy                           0.98       875\n",
    "#    macro avg       0.50      0.49      0.50       875\n",
    "# weighted avg       0.99      0.98      0.99       875\n",
    "\n",
    "# acc:  0.9817142857142858\n",
    "# pre:  0.0\n",
    "# rec:  0.0\n",
    "# ma F1:  0.4953863898500577\n",
    "# mi F1:  0.9817142857142858\n",
    "# we F1:  0.9873758444554293\n",
    "# Subject 25 Current Train Acc:  0.8617253873063468 Current Test Acc:  0.98171428571\n",
    "\n",
    "# [[245  15]\n",
    "#  [ 38 171]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.87      0.94      0.90       260\n",
    "#            1       0.92      0.82      0.87       209\n",
    "\n",
    "#     accuracy                           0.89       469\n",
    "#    macro avg       0.89      0.88      0.88       469\n",
    "# weighted avg       0.89      0.89      0.89       469\n",
    "\n",
    "# acc:  0.8869936034115139\n",
    "# pre:  0.9193548387096774\n",
    "# rec:  0.8181818181818182\n",
    "# ma F1:  0.8841084458120614\n",
    "# mi F1:  0.886993603411514\n",
    "# we F1:  0.8860968652387109\n",
    "# Subject 26 Current Train Acc:  0.8416198877305533 Current Test Acc:  0.88699360341\n",
    "\n",
    "# [[862   3]\n",
    "#  [  0   1]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       1.00      1.00      1.00       865\n",
    "#            1       0.25      1.00      0.40         1\n",
    "\n",
    "#     accuracy                           1.00       866\n",
    "#    macro avg       0.62      1.00      0.70       866\n",
    "# weighted avg       1.00      1.00      1.00       866\n",
    "\n",
    "# acc:  0.9965357967667436\n",
    "# pre:  0.25\n",
    "# rec:  1.0\n",
    "# ma F1:  0.699131441806601\n",
    "# mi F1:  0.9965357967667436\n",
    "# we F1:  0.9975720488746187\n",
    "# Subject 27 Current Train Acc:  0.9092271662763466 Current Test Acc:  0.99653579676\n",
    "\n",
    "# [[700  49]\n",
    "#  [ 75 211]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.90      0.93      0.92       749\n",
    "#            1       0.81      0.74      0.77       286\n",
    "\n",
    "#     accuracy                           0.88      1035\n",
    "#    macro avg       0.86      0.84      0.85      1035\n",
    "# weighted avg       0.88      0.88      0.88      1035\n",
    "\n",
    "# acc:  0.8801932367149758\n",
    "# pre:  0.8115384615384615\n",
    "# rec:  0.7377622377622378\n",
    "# ma F1:  0.8457644717487237\n",
    "# mi F1:  0.8801932367149758\n",
    "# we F1:  0.8783626684345616\n",
    "# Subject 28 Current Train Acc:  0.8718608739326972 Current Test Acc:  0.88019323671\n",
    "\n",
    "# [[379  35]\n",
    "#  [ 29  67]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.93      0.92      0.92       414\n",
    "#            1       0.66      0.70      0.68        96\n",
    "\n",
    "#     accuracy                           0.87       510\n",
    "#    macro avg       0.79      0.81      0.80       510\n",
    "# weighted avg       0.88      0.87      0.88       510\n",
    "\n",
    "# acc:  0.8745098039215686\n",
    "# pre:  0.6568627450980392\n",
    "# rec:  0.6979166666666666\n",
    "# ma F1:  0.7994543979945441\n",
    "# mi F1:  0.8745098039215687\n",
    "# we F1:  0.8759531771124731\n",
    "# Subject 29 Current Train Acc:  0.8907075136654211 Current Test Acc:  0.87450980392\n",
    "\n",
    "# [[329  61]\n",
    "#  [294 342]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.53      0.84      0.65       390\n",
    "#            1       0.85      0.54      0.66       636\n",
    "\n",
    "#     accuracy                           0.65      1026\n",
    "#    macro avg       0.69      0.69      0.65      1026\n",
    "# weighted avg       0.73      0.65      0.65      1026\n",
    "\n",
    "# acc:  0.6539961013645225\n",
    "# pre:  0.8486352357320099\n",
    "# rec:  0.5377358490566038\n",
    "# ma F1:  0.6539405438633662\n",
    "# mi F1:  0.6539961013645225\n",
    "# we F1:  0.6549918627313988\n",
    "# Subject 30 Current Train Acc:  0.8053350070610388 Current Test Acc:  0.65399610136\n",
    "\n",
    "# [[536   0]\n",
    "#  [  2   0]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       1.00      1.00      1.00       536\n",
    "#            1       0.00      0.00      0.00         2\n",
    "\n",
    "#     accuracy                           1.00       538\n",
    "#    macro avg       0.50      0.50      0.50       538\n",
    "# weighted avg       0.99      1.00      0.99       538\n",
    "\n",
    "# acc:  0.9962825278810409\n",
    "# pre:  0.0\n",
    "# rec:  0.0\n",
    "# ma F1:  0.49906890130353815\n",
    "# mi F1:  0.9962825278810409\n",
    "# we F1:  0.9944272531550055\n",
    "# Subject 31 Current Train Acc:  0.8903965629153402 Current Test Acc:  0.99628252788\n",
    "\n",
    "# [[143  11]\n",
    "#  [ 78 863]]\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.65      0.93      0.76       154\n",
    "#            1       0.99      0.92      0.95       941\n",
    "\n",
    "#     accuracy                           0.92      1095\n",
    "#    macro avg       0.82      0.92      0.86      1095\n",
    "# weighted avg       0.94      0.92      0.92      1095\n",
    "\n",
    "# acc:  0.9187214611872146\n",
    "# pre:  0.9874141876430206\n",
    "# rec:  0.9171094580233794\n",
    "# ma F1:  0.8568154269972452\n",
    "# mi F1:  0.9187214611872145\n",
    "# we F1:  0.9244821615910033\n",
    "# Subject 32 Current Train Acc:  0.916467480186187 Current Test Acc:  0.918721461187 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51.28256513026052 52.12424849699399 0.8496993987975952\n",
      "50.6286836935167 25.284872298624755 0.5009823182711198\n",
      "38.94632206759444 38.230616302186874 0.8588469184890656\n",
      "55.07523939808481 43.41997264021888 0.7893296853625171\n",
      "28.07035175879397 23.668341708542712 0.7155778894472362\n",
      "24.166666666666668 22.023809523809522 0.8234126984126984\n",
      "39.950421417947446 41.82449181953396 0.8150718889439762\n",
      "27.70764119601329 20.81063122923588 0.774750830564784\n",
      "49.9390243902439 24.634146341463417 0.5599593495934959\n",
      "28.583690987124466 14.806866952789699 0.6716738197424893\n",
      "51.365461847389554 47.46987951807229 0.8145917001338688\n",
      "12.700296735905045 11.15727002967359 0.8832838773491593\n",
      "33.51032448377581 34.7787610619469 0.8549655850540806\n",
      "1.9537815126050422 0.25210084033613445 0.9632352941176471\n",
      "10.620272314674736 2.2995461422087744 0.8371154815935451\n",
      "0.7228915662650602 0.9397590361445783 0.9746987951807229\n",
      "4.719101123595506 0.26966292134831465 0.9168539325842696\n",
      "0.0 0.0 1.0\n",
      "0.1892744479495268 0.6940063091482649 0.9873817034700315\n",
      "0.0 0.0 1.0\n",
      "0.0 0.12875536480686695 0.9978540772532188\n",
      "0.32476319350473615 0.0 0.9945872801082544\n",
      "0.12820512820512822 0.0 0.9978632478632479\n",
      "0.20408163265306123 0.0 0.9965986394557823\n",
      "0.2057142857142857 0.8914285714285715 0.9817142857142858\n",
      "26.737739872068232 23.795309168443495 0.8869936034115139\n",
      "0.06928406466512702 0.27713625866050806 0.9965357967667436\n",
      "16.579710144927535 15.072463768115941 0.8801932367149758\n",
      "11.294117647058822 12.0 0.8745098039215686\n",
      "37.19298245614035 23.567251461988306 0.6539961013645225\n",
      "0.22304832713754646 0.0 0.9962825278810409\n",
      "51.56164383561644 47.890410958904106 0.9187214611872146\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "def ahi(TN,FP,FN,TP):\n",
    "    ref_ahi=(TP+FN)/(TN+FP+FN+TP)*60\n",
    "    pred_ahi=(TP+FP)/(TN+FP+FN+TP)*60\n",
    "    #ref_ahi =1 if ref_ahi>=15 else 0\n",
    "    #pred_ahi =1 if pred_ahi>=15 else 0\n",
    "    acc=(TP+TN)/(TN+FP+FN+TP)\n",
    "    return ref_ahi,pred_ahi,acc\n",
    "\n",
    "ref_ahi_list,pred_ahi_list=[],[]\n",
    "cfms=[[63,82,68,785],[120,39,469,390],[288,65,77,576],[54,6,148,523],\n",
    "      [849,210,356,575],[531,71,107,299],[456,218,155,1188],[727,83,256,439],\n",
    "      [156,9,424,395],[221,23,130,92],[125,90,187,1092],[751,46,72,142],\n",
    "      [729,169,126,1010],[917,4,31,0],[1608,24,299,52],[808,12,9,1],\n",
    "      [816,4,70,0],[952,0,0,0],[938,10,2,1],[930,0,0,0],\n",
    "      [930,2,0,0],[735,0,4 ,0],[467,0,1,0],[586,0,2,0],\n",
    "      [859,13,3,0],[245,15,38,171],[862,3,0,1],[700,49,75,211],\n",
    "      [379,35,29,67],[329,61,294,342],[536,0,2,0],[143,11,78,863]]\n",
    "for cfm in cfms:\n",
    "    ref,pred,acc=ahi(cfm[0],cfm[1],cfm[2],cfm[3])\n",
    "    ref_ahi_list.append(ref)\n",
    "    pred_ahi_list.append(pred)\n",
    "    print(ref,pred,acc)\n",
    "plt.plot(np.array(ref_ahi_list),np.array(pred_ahi_list),'o')\n",
    "plt.plot(np.arange(200)/10,np.arange(200)/10,'--')\n",
    "plt.show()\n",
    "plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9696969696969697"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "490\t20\t470\t9\t12.5\t57.1\t69.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <!-- \n",
    "# [[ 63,82,68,785]]\n",
    "# Subject 1 Current Train Acc:  0.900573793622425 Current Test Acc:  0.8496993987975\n",
    "        \n",
    "# [[120,39,469,390]]\n",
    "# Subject 2 Current Train Acc:  0.919900856524331 Current Test Acc:  0.5009823182711\n",
    "        \n",
    "# [[288,65,77,576]]\n",
    "# Subject 3 Current Train Acc:  0.9203387172651717 Current Test Acc:  0.858846918489\n",
    "        \n",
    "# [[54,6,148,523]]\n",
    "# Subject 4 Current Train Acc:  0.9038246268656717 Current Test Acc:  0.789329685362\n",
    "        \n",
    "# [[849,210,356,575]]\n",
    "# Subject 5 Current Train Acc:  0.8792595708876736 Current Test Acc:  0.715577889447\n",
    "        \n",
    "# [[531,71,107,299]]\n",
    "# Subject 6 Current Train Acc:  0.9191732271116269 Current Test Acc:  0.823412698412\n",
    "\n",
    "# [[456,218,155,1188]]\n",
    "# Subject 7 Current Train Acc:  0.9160782535466736 Current Test Acc:  0.815071888943\n",
    "\n",
    "# [[727,83,256 439]]\n",
    "# Subject 8 Current Train Acc:  0.9239788440706047 Current Test Acc:  0.774750830564\n",
    "\n",
    "# [[156,9,424 395]]\n",
    "# Subject 9 Current Train Acc:  0.9102704735637948 Current Test Acc:  0.559959349593\n",
    "\n",
    "# [[221,23,130,92]]\n",
    "# Subject 10 Current Train Acc:  0.915065535851966 Current Test Acc:  0.671673819742\n",
    "\n",
    "# [[125,90,187,1092]]\n",
    "# Subject 11 Current Train Acc:  0.8126891104245628 Current Test Acc:  0.81459170013\n",
    "\n",
    "# [[751,46,72,142]]\n",
    "# Subject 12 Current Train Acc:  0.9123902132998746 Current Test Acc:  0.88328387734\n",
    "\n",
    "# [[729,169,126,1010]]\n",
    "# Subject 13 Current Train Acc:  0.9207635220533429 Current Test Acc:  0.85496558505\n",
    "\n",
    "# [[917,4,31,0]]\n",
    "# Subject 14 Current Train Acc:  0.9017189016562823 Current Test Acc:  0.96323529411\n",
    "\n",
    "# [[1608,24,299,52]]\n",
    "# Subject 15 Current Train Acc:  0.8769574220266597 Current Test Acc:  0.83711548159\n",
    "\n",
    "# [[808,12,9,1]]\n",
    "# Subject 16 Current Train Acc:  0.8277346308599233 Current Test Acc:  0.97469879518\n",
    "\n",
    "# [[816,4,70,0]]\n",
    "# Subject 17 Current Train Acc:  0.6041998687541015 Current Test Acc:  0.91685393258\n",
    "\n",
    "# [[952,0,0,0]]\n",
    "# Subject 18 Current Train Acc:  0.6013337925420332 Current Test Acc:  1.0\n",
    "\n",
    "# [[938,10,2,1]]\n",
    "# Subject 19 Current Train Acc:  0.843143393863494 Current Test Acc:  0.987381703470\n",
    "\n",
    "# [[930,0,0,0]]\n",
    "# Subject 20 Current Train Acc:  0.601795938800413 Current Test Acc:  1.0\n",
    "\n",
    "# [[930,2,0,0]]\n",
    "# Subject 21 Current Train Acc:  0.913670640508151 Current Test Acc:  0.997854077253\n",
    "\n",
    "# [[735,0,4 ,0]]\n",
    "# Subject 22 Current Train Acc:  0.604192585220204 Current Test Acc:  0.994587280108\n",
    "\n",
    "# [[467,0,1,0]]\n",
    "# Subject 23 Current Train Acc:  0.6074391635567344 Current Test Acc:  0.99786324786\n",
    "\n",
    "# [[586,0,2,0]]\n",
    "# Subject 24 Current Train Acc:  0.6060737392811814 Current Test Acc:  0.99659863945\n",
    "\n",
    "# [[859,13,3,0]]\n",
    "# Subject 25 Current Train Acc:  0.8617253873063468 Current Test Acc:  0.98171428571\n",
    "\n",
    "# [[245,15,38,171]]\n",
    "# Subject 26 Current Train Acc:  0.8416198877305533 Current Test Acc:  0.88699360341\n",
    "\n",
    "# [[862,3,0,1]]\n",
    "# Subject 27 Current Train Acc:  0.9092271662763466 Current Test Acc:  0.99653579676\n",
    "\n",
    "# [[700,49,75,211]]\n",
    "# Subject 28 Current Train Acc:  0.8718608739326972 Current Test Acc:  0.88019323671\n",
    "\n",
    "# [[379,35,29,67]]\n",
    "# Subject 29 Current Train Acc:  0.8907075136654211 Current Test Acc:  0.87450980392\n",
    "\n",
    "# [[329,61,294,342]]\n",
    "# Subject 30 Current Train Acc:  0.8053350070610388 Current Test Acc:  0.65399610136\n",
    "\n",
    "# [[536,0,2,0]]\n",
    "# Subject 31 Current Train Acc:  0.8903965629153402 Current Test Acc:  0.99628252788\n",
    "\n",
    "# [[143,11,78,863]]\n",
    "# Subject 32 Current Train Acc:  0.916467480186187 Current Test Acc:  0.918721461187 -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
